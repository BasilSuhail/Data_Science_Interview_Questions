question_text,company,difficulty,question_type,topics,source,answer_text,category,tags,constraints,examples,hints
"More Data: Generally reduces variance, but can also help a high-bias model better capture underlying patterns.",,medium,stats,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.639618,,,,
Feature Selection/Engineering: Aims to reduce overfitting by focusing on the most relevant features.,,medium,coding,feature_engineering,Devinterview-io/data-scientist-interview-questions,,,,,,
Simpler Models: Helps alleviate overfitting; reduces variance but might increase bias.,,medium,stats,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Regularization: A technique that adds a penalty term for model complexity, which can help decrease overfitting.",,medium,ml,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.639712,,,,
"Ensemble Methods: Combine multiple models to reduce variance and, in some cases, improve bias.",,medium,stats,ensemble,,2025-11-17T10:49:35.639737,,,
"Cross-Validation: Helps estimate the performance of a model on an independent dataset, providing insights into both bias and variance. <br> ## 6. Explain the concept of _Cross-Validation_ and its importance in ML. Cross-Validation (CV) is a robust technique for assessing the performance of a machine learning model, especially when it involves hyperparameter tuning or comparing multiple models. It addresses issues such as overfitting and ensures a more reliable performance estimate on unseen data. ### Kinds of Cross-Validation",,medium,stats,,,2025-11-17T10:49:35.639810,,,
Holdout Method: Data is simply split into training and test sets.,,medium,ml,,Devinterview-io/data-scientist-interview-questions,,,,,,
"K-Fold CV: Data is divided into K folds; each fold is used as a test set, and the rest are used for training.",,medium,ml,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.639857,,,,
"Stratified K-Fold CV: Like K-Fold, but preserves the class distribution in each fold, useful for balanced datasets.",,medium,stats,probability,,2025-11-17T10:49:35.639882,,,
Leave-One-Out (LOO) CV: A special case of K-Fold where K equals the number of instances; each observation is used as a test set once.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Time Series CV: Specifically designed for temporal data, where the training set always precedes the test set. ### Benefits of K-Fold Cross-Validation - Data Utilization: Every data point is used for both training and testing, providing a more comprehensive model evaluation. - Performance Stability: Averaging results from multiple folds can help reduce variability. - Hyperparameter Tuning: Helps in tuning model parameters more effectively, especially when combined with techniques like grid search. ### Code Example: K-Fold Cross-Validation Here is the Python code: `python import numpy as np from sklearn.model_selection import KFold # Create sample data X = np.array([[1, 2], [3, 4], 6], [7, 8], [9, 10]]) y = np.array([1
"Sparse Data: As the number of dimensions increases, the data points become more spread out, and the density of data points decreases.",,medium,mixed,,,2025-11-17T10:49:35.640980,,,
"Increased Volume of Data: With each additional dimension, the volume of the sample space grows exponentially, necessitating a larger dataset to maintain coverage.",,medium,mixed,,,2025-11-17T10:49:35.641018,,,
Overfitting: High-dimensional spaces make it easier for models to fit to noise rather than the underlying pattern in the data.,,medium,ml,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Computational Complexity: Many machine learning algorithms exhibit slower performance and require more resources as the number of dimensions increases. ### Visual Example Consider a hypercube (n-dimensional cube) inscribed in a hypersphere (n-dimensional sphere) with a large number of dimensions, say 100. If you were to place a ""grid"" or uniformly spaced points within the hypercube, you'd find that the majority of these points actually fall outside the hypersphere. This disparity grows more pronounced as the number of dimensions increases, leading to a ""density gulf"" between the data contained within the hypercube and that within the hypersphere. !curse-of-dimensionality.png?alt=media&token=24d3cde6-89ae-4eb3-8d05-1d6358bb5ac9) ### Recommendations to Mitigate the Curse of Dimensionality",Google,medium,coding,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641133,,
"Feature Selection and Dimensionality Reduction: Prioritize quality over quantity of features. Techniques like PCA, t-SNE, and LDA can help reduce dimensions.",,medium,coding,feature_engineering,,2025-11-17T10:49:35.641161,,,
"Simpler Models: Consider using algorithms with less sensitivity to high dimensions, even if it means sacrificing a bit of performance.",,medium,coding,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.641187,,,,
"Sparse Models: For high-dimensional, sparse datasets, models that can handle sparsity, like LASSO or ElasticNet, might be beneficial.",,medium,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641215,
Feature Engineering: Craft domain-specific features that can capture relevant information more efficiently.,,medium,mixed,feature_engineering,Devinterview-io/data-scientist-interview-questions,,,,,,
"Data Quality: Strive for a high-quality dataset, as more data doesn't necessarily counteract the curse of dimensionality.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.641273,,,,
"Data Stratification and Sampling: When possible, stratify and sample data to ensure coverage across the high-dimensional space.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.641303,,,,
"Computational Resources: Leverage cloud computing or powerful hardware to handle the increased computational demands. <br> ## 10. Explain the concept of _Feature Engineering_ and its significance in ML. Feature engineering is a vital component of the machine-learning pipeline. It entails creating meaningful and robust representations of the data upon which the model will be built. ### Significance of Feature Engineering - Improved Model Performance: High-quality features can make even simple models more effective, while poor features can hamper the performance of the most advanced models. - Dimensionality Reduction: Carefully engineered features can distill relevant information from high-dimensional data, leading to more efficient and accurate models. - Model Interpretability: Certain feature engineering techniques, such as binning or one-hot encoding, make it easier to understand and interpret the model's decisions. - Computational Efficiency: Engineered features can often streamline computational processes, making predictions faster and cheaper. ### Common Feature Engineering Techniques",,stats,hypothesis_testing|metrics|feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641434
"Handling Missing Data - Removing or imputing missing values. - Creating a separate ""missing"" category.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Handling Categorical Data - Converting categories into ordinal values. - Using one-hot encoding to create binary ""dummy"" variables. - Grouping rare categories into an ""other"" category.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Handling Temporal Data - Extracting specific time-related features from timestamps, such as hour or month. - Converting timestamps into different representations, like age or duration since a specific event.",,medium,mixed,feature_engineering,,2025-11-17T10:49:35.641544,,,
Variable Transformation - Using mathematical transformations such as logarithms. - Normalizing or scaling data to a specific range.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,,,,,
"Discretization - Converting continuous variables into discrete bins, e.g., converting age to age groups.",,medium,mixed,,,2025-11-17T10:49:35.641609,,,
Feature Extraction - Reducing dimensionality through techniques like PCA or LDA.,,medium,mixed,feature_engineering,Devinterview-io/data-scientist-interview-questions,,,,,,
"Feature Creation - Engineering domain-specific metrics. - Generating polynomial or interaction features. <br> ## 11. What is _Data Preprocessing_ and why is it important in ML? Data Preprocessing is a vital early-stage task in any machine learning project. It involves cleaning, transforming, and standardizing data to make it more suitable for predictive modeling. ### Key Steps in Data Preprocessing",,medium,ml,metrics|feature_engineering,,2025-11-17T10:49:35.641691,,,
Data Cleaning: - Address missing values: Implement strategies like imputation or removal. - Outlier detection and handling: Identify and deal with data points that deviate significantly from the rest.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,,,,,
Feature Selection and Engineering: - Choose the most relevant features that contribute to the model's predictive accuracy. - Create new features that might improve the model's performance.,,medium,coding,feature_engineering,Devinterview-io/data-scientist-interview-questions,,,,,,
"Data Transformation: - Normalize or standardize numerical data to ensure all features contribute equally. - Convert categorical data into a format understandable by the model, often using techniques like one-hot encoding. - Discretize continuous data when required.",,medium,ml,feature_engineering,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.641805,,,,
"Data Integration: - Combine data from multiple sources, ensuring compatibility and consistency.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,2025-11-17T10:49:35.641847,,,,
"Data Reduction: - Reduce the dimensionality of the feature space, often to eliminate noise or improve computational efficiency. ### Code Example: Handling Missing Data Here is the Python code: `python # Drop rows with missing values cleaned_data = raw_data.dropna() # Fill missing values using the mean mean_value = raw_data['column_name'].mean() raw_data['column_name'].fillna(mean_value, inplace=True) ` ### Code Example: Feature Scaling Here is the Python code: `python from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) ` ### Code Example: Dimensionality Reduction Using PCA Here is the Python code: `python from sklearn.decomposition import PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) ` <br> ## 12. Explain the difference between _Feature Scaling_ and _Normalization_. Both Feature Scaling and Normalization are data preprocessing techniques that aim to make machine learning models more robust and accurate. While they share similarities in standardizing data, they serve slightly different purposes. ### Key Distinctions - Feature Scaling adjusts the range of independent variables or features so that they are on a similar scale. Common methods include Min-Max Scaling and Standardization. - Normalization, in the machine learning context, typically refers to scaling the magnitude of a vector to make its Euclidean length 1. It's also known as Unit Vector transformation. In some contexts, it may be used more generally to refer to scaling quantities to be in a range (like Min-Max), such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM). - Normalization: More useful for algorithms that work with vector dot products, like the K-Means clustering algorithm and Neural Networks. <br> ## 13. What is the purpose of _One-Hot Encoding_ and when is it used? One-Hot Encoding is a technique frequently used to prepare categorical data for machine learning algorithms. ### Purpose of One-Hot Encoding It is employed when: - Categorical Data: The data on hand is categorical, and the algorithm or model being used does not support categorical input. - Nominal Data Order: The categorical data is nominal, i.e., not ordinal
TN / True Negative: case was negative and predicted negative,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
TP / True Positive: case was positive and predicted positive,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
FN / False Negative: case was positive but predicted negative,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
"FP / False Positive: case was negative but predicted positive !alt text Now, your boss asks you three questions: * What percent of your predictions were correct? You answer: the ""accuracy"" was (9,760+60) out of 10,000 = 98.2% * What percent of the positive cases did you catch? You answer: the ""recall"" was 60 out of 100 = 60% * What percent of positive predictions were correct? You answer: the ""precision"" was 60 out of 200 = 30% See also a very good explanation of Precision and recall in Wikipedia. !alt text ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION) and is commonly used to measure the performance of binary classifiers. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more representative picture of performance. Remember, a ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION). Sensitivity is the other name for recall but specificity is not PRECISION. Recall/Sensitivity is the measure of the probability that your estimate is 1 given all the samples whose true class label is 1. It is a measure of how many of the positive samples have been identified as being positive. Specificity is the measure of the probability that your estimate is 0 given all the samples whose true class label is 0. It is a measure of how many of the negative samples have been identified as being negative. PRECISION on the other hand is different. It is a measure of the probability that a sample is a true positive class given that your classifier said it is positive. It is a measure of how many of the samples predicted by the classifier as positive is indeed positive. Note here that this changes when the base probability or prior probability of the positive class changes. Which means PRECISION depends on how rare is the positive class. In other words, Sensitivity = TP / (TP + FN). Since the formula doesn’t contain FP and TN, Sensitivity may give you a biased result, especially for imbalanced classes. In the example of Fraud detection, it gives you the percentage of Correctly Predicted Frauds from the pool of Actual Frauds pool of Actual Non-Frauds. * Specificity, also known as True Negative Rate is calculated as
Can You Collect More Data?</br> A larger dataset might expose a different and perhaps more balanced perspective on the classes. More examples of minor classes may be useful later when we look at resampling your dataset.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
"Try Changing Your Performance Metric</br> Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading. From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy: - Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned). - Precision: A measure of a classifiers exactness. Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV). Precision can be thought of as a measure of a classifiers exactness. A low precision can also indicate a large number of False Positives. - Recall: A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate. Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives. - F1 Score (or F-score): A weighted average of precision and recall. I would also advise you to take a look at the following: - Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data. ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.",,medium,ml,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.281008,,
"Try Resampling Your Dataset * You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement) * You can delete instances from the over-represented class, called under-sampling.",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,2025-11-17T10:49:36.281081,,,,
Try Different Algorithms,,medium,coding,,iamtodor/data-science-interview-questions-and-answers,,,,,,
Try Penalized Models</br> You can use the same algorithms but give them a different perspective on the problem. Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA. Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you’re getting poor results. It provides yet another way to “balance” the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.,,medium,coding,classification,iamtodor/data-science-interview-questions-and-answers,,,,,,
"Try a Different Perspective</br> Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas. Two you might like to consider are anomaly detection and change detection. ## 8. What is statistical power? Statistical power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true—that is, the ability of a test to detect an effect, if the effect actually exists. To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is). A type I error (or error of the first kind) is the incorrect rejection of a true null hypothesis. Usually a type I error leads one to conclude that a supposed effect or relationship exists when in fact it doesn't. Examples of type I errors include a test that shows a patient to have a disease when in fact the patient does not have the disease, or an experiment indicating that a medical treatment should cure a disease when in fact it does not. A type II error (or error of the second kind) is the failure to reject a false null hypothesis. Examples of type II errors would be a blood test failing to detect the disease it was designed to detect, in a patient who really has the disease; a fire breaking out and the fire alarm does not ring; or a clinical trial of a medical treatment failing to show that the treatment works when really it does. !alt text ## 9. What are bias and variance, and what are their relation to modeling data? Bias is how far removed a model's predictions are from correctness, while variance is the degree to which these predictions vary between model iterations. Bias is generally the distance between the model that you build on the training data (the best model that your model space can provide) and the “real model” (which generates data). Error due to Bias: Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value. The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again
"Point anomalies: A single instance of data is anomalous if it's too far off from the rest. Business use case: Detecting credit card fraud based on ""amount spent.""",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
"Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: Spending $100 on food every day during the holiday season is normal, but may be odd otherwise.",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,2025-11-17T10:49:36.284852,,,,
"Collective anomalies: A set of data instances collectively helps in detecting anomalies. Business use case: Someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack. Best steps to prevent anomalies is to implement policies or checks that can catch them during the data collection stage. Unfortunately, you do not often get to collect your own data, and often the data you're mining was collected for another purpose. About 68% of all the data points are within one standard deviation from the mean. About 95% of the data points are within two standard deviations from the mean. Finally, over 99% of the data is within three standard deviations from the mean. When the value deviate too much from the mean, let’s say by ± 4σ, then we can considerate this almost impossible value as anomaly. (This limit can also be calculated using the percentile). #### Statistical methods Statistically based anomaly detection uses this knowledge to discover outliers. A dataset can be standardized by taking the z-score of each point. A z-score is a measure of how many standard deviations a data point is away from the mean of the data. Any data-point that has a z-score higher than 3 is an outlier, points become more obviously anomalous. A z-score is calculated using the following equation. A box-plot is perfect for this application. #### Metric method Judging by the number of publications, metric methods are the most popular methods among researchers. They postulate the existence of a certain metric in the space of objects, which helps to find anomalies. Intuitively, the anomaly has few neighbors in the instannce space, and a typical point has many. Therefore
High R2 and negligible odds.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
Strong pair correlation of predictors.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
"High VIF - variance inflation factor. Confidence interval (CI) is a type of interval estimate (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level. Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the true population, the confidence interval obtained from the data is also random. If a corresponding hypothesis test is performed, the confidence level is the complement of the level of significance, then the estimate is significantly different from zero at the 5% significance level. The desired level of confidence is set by the researcher (not determined by data). Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%. Factors affecting the width of the confidence interval include the size of the sample
A loss function to be optimized.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
A weak learner to make predictions.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,,,,,
"An additive model to add weak learners to minimize the loss function. #### Loss Function The loss function used depends on the type of problem being solved. It must be differentiable, but many standard loss functions are supported and you can define your own. For example, regression may use a squared error and classification may use logarithmic loss. A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. #### Weak Learner Decision trees are used as the weak learner in gradient boosting. Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss. Initially, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes. This is to ensure that the learners remain weak
Explain Difference between joins,,hard,mixed,,jayinai/data-science-question-answer,"* **(INNER) JOIN**: Returns records that have matching values in both tables,,,,,
* **LEFT (OUTER) JOIN**: Return all records from the left table, and the matched records from the right table,,,,,,,,,,
* **RIGHT (OUTER) JOIN**: Return all records from the right table, and the matched records from the left table,,,,,,,,,,
* **FULL (OUTER) JOIN**: Return all records when there is a match in either left or right table,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer),,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
## Tools and Framework,,,,,,,,,,,
,,,,,,,,,,,
The resources here are only meant to help you",2025-11-17T11:42:20.246097,,,,,,,,,,
Explain Spark,,hard,mixed,,jayinai/data-science-question-answer,"Using PySpark API.,,,,,
,,,,,,,,,,,
* The best resource is of course [Spark's documentation](https://spark.apache.org/docs/latest/). Take a thorough review of the topics,,,,,,,,,,,
* If you are really time constrained, scan the Spark's documentation and check [PySpark cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf) for the basics,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer),,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
## Statistics and ML In General,,,,,,,,,,,
,,,,,,,,,,,
* [Project Workflow](#project-workflow),,,,,,,,,,,
* [Cross Validation](#cross-vali",2025-11-17T11:42:20.246124,,,,,,,,,,
Explain Project Workflow,,hard,mixed,,jayinai/data-science-question-answer,"Given a data science / machine learning project,,,,,
how I would tackle it:,,,,,,,,,,,
,,,,,,,,,,,
* **Specify business objective.** Are we trying to win more customers, achieve higher satisfaction, or gain more revenues?,,,,,,,,,
* **Define problem.** What is the specific gap in your ideal world and the real one that requires machine learning to fill? Ask questions that can be addressed using your data and predictive modeling (ML algorithms).,,,,,,,,,,,
* **Create a common sense baseline.** But before yo",2025-11-17T11:42:20.246149,,,,,,,,,,
Explain Cross Validation,,medium,mixed,,jayinai/data-science-question-answer,"Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, a k-fold cross validation divides the data into k folds (or partitions), trains on each k-1 fold, and evaluate on the remaining 1 fold. This results to k models/evaluations, which can be averaged to get a overall model performance.,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246161,,,,,,,,,,
Explain Feature Importance,,medium,mixed,,jayinai/data-science-question-answer,"* In linear models,,,,,
* In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree.  We can get a feature's importance for random forest by computing the averaging depth at which it appears across all trees in the forest.,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246169,,,,,,,,,,
Explain Mean Squared Error vs. Mean Absolute Error,,medium,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both measure the average model prediction error; range from 0 to infinity; the lower the better,,,,,
* Mean Squared Error (MSE) gives higher weights to large error (e.g., being off by 10 just MORE THAN TWICE as bad as being off by 5), whereas Mean Absolute Error (MAE) assign equal weights (being off by 10 is just twice as bad as being off by 5),,,,,,,,,
* MSE is continuously differentiable, MAE is not (where y_pred == y_true),,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246178,,,,,,,,,,
Explain L1 vs L2 regularization,,hard,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both L1 and L2 regularization **prevent overfitting** by shrinking (imposing a penalty) on the coefficients,,,,,
* **Difference**: L2 (Ridge) shrinks all the coefficient by the same proportions but eliminates none, while L1 (Lasso) can shrink some coefficients to zero, performing variable selection.,,,,,,,,,
* **Which to choose**: If all the features are correlated with the label, ridge outperforms lasso, as the coefficients are never zero in ridge. If only a subset of features are correlate",2025-11-17T11:42:20.246187,,,,,,,,
Explain Correlation vs Covariance,,hard,mixed,,jayinai/data-science-question-answer,"* Both determine the relationship and measure the dependency between two random variables,,,,,
* Correlation is when the change in one item may result in the change in the another item, while covariance is when two items vary together (joint variability),,,,,,,,,,
* Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance,,,,,,,,,,
* Range: correlation is between -1 and +1, while covariance lies between negative infinity and infinity.,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-quest",2025-11-17T11:42:20.246195,,,,,,,,,,
Explain Would adding more data address underfitting,,medium,mixed,,jayinai/data-science-question-answer,"Underfitting happens when a model is not complex enough to learn well from the data. It is the problem of model rather than data size. So a potential way to address underfitting is to increase the model complexity (e.g., increase depth for tree-based methods, add more layers / number of neurons for neural networks etc.),,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246203,,,,,,,,,,
Explain Activation Function,,medium,mixed,,jayinai/data-science-question-answer,"For neural networks,,,,,
,,,,,,,,,,,
* Non-linearity: ReLU is often used. Use Leaky ReLU (a small positive gradient for negative input, say, `y = 0.01x` when x < 0) to address dead ReLU issue,,,,,,,,,
* Multi-class: softmax,,,,,,,,,,,
* Binary: sigmoid,,,,,,,,,,,
* Regression: linear,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246210,,,,,,,,,,
Explain Bagging,,hard,mixed,,jayinai/data-science-question-answer,"To address overfitting,,,,,
which reduces the variance of the meta learning algorithm. Bagging can be applied,,,,,,,,,,,
to decision tree or other algorithms.,,,,,,,,,,,
,,,,,,,,,,,
Here is a [great illustration](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py) of a single estimator vs. bagging.,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
* Bagging is when samlping is performed *with* replacement. When sampling is perfor",2025-11-17T11:42:20.246219,,,,,,,,,,
Explain Stacking,,hard,mixed,,jayinai/data-science-question-answer,"* Instead of using trivial functions (such as hard voting) to aggregate the predictions from individual learners,,,,,
* First split the training set into two subsets: the first subset is used to train the learners in the first layer,,,,,,,,,,,
* Next the first layer learners are used to make predictions (meta features) on the second subset, and those predictions are used to train another models (to obtain the weigts of different learners) in the second layer,,,,,,,,,,
* We can t",2025-11-17T11:42:20.246227,,,,,,,,,,
Explain Generative vs discriminative,,hard,mixed,,jayinai/data-science-question-answer,"* Discriminative algorithms model *p(y|x; w)*, given the dataset and learned,,,,
parameter, what is the probability of y belonging to a specific class. A discriminative algorithm,,,,,,,,,,
doesn't care about how the data was generated, it simply categorizes a given example,,,,,,,,,,
* Generative algorithms try to model *p(x|y)*, that is, the distribution of features given,,,,,,,,,
that it belongs to a certain class. A generative algorithm models how the data was,,,,,,,,,,,
generated.,,,,,,,,,,,
,,,,,,,,,,,
> Given a training set, an algorithm like log",2025-11-17T11:42:20.246240,,,,,,,,,
Explain Parametric vs Nonparametric,,medium,mixed,,jayinai/data-science-question-answer,"* A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model.,,,,,
* A model where the number of parameters is not determined prior to training. Nonparametric does not mean that they have no parameters. On the contrary, nonparametric models (can) become more and more complex with an increasing amount of data.,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246247,,,,,,,,,,
Explain Recommender System,,hard,mixed,,jayinai/data-science-question-answer,"* I put recommend system here since technically it falls neither under supervised nor unsupervised learning,,,,,
* A recommender system seeks to predict the 'rating' or 'preference' a user would give to items and then recommend items accordingly,,,,,,,,,,,
* Content based recommender systems recommends items similar to those a given user has liked in the past, based on either explicit (ratings, like/dislike button) or implicit (viewed/finished an article) feedbacks. Content based recommenders work solely with t",2025-11-17T11:42:20.246259,,,,,,,,
Explain Linear regression,,hard,mixed,,jayinai/data-science-question-answer,"* How to learn the parameter: minimize the cost function,,,,,
* How to minimize cost function: gradient descent,,,,,,,,,,,
* Regularization:,,,,,,,,,,,
    - L1 (Lasso): can shrink certain coef to zero, thus performing feature selection,,,,,,,,,,
    - L2 (Ridge): shrink all coef with the same proportion; almost always outperforms L1,,,,,,,,,,,
    - Elastic Net: combined L1 and L2 priors as regularizer,,,,,,,,,,,
* Assumes linear relationship between features and the label,,,,,,,,,,,
* Can add polynomial and interaction features to add non-linearity,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
(#data-sci",2025-11-17T11:42:20.246268,,,,,,,,,,
Explain Logistic regression,,medium,mixed,,jayinai/data-science-question-answer,"* Generalized linear model (GLM) for binary classification problems,,,,,
* Apply the sigmoid function to the output of linear models, squeezing the target,,,,,,,,,,
to range [0, 1],,,,,,,,,,
* Threshold to make prediction: usually if the output > .5, prediction 1; otherwise prediction 0,,,,,,,,,,
* A special case of softmax function, which deals with multi-class problems,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer)",2025-11-17T11:42:20.246275,,,,,,,,,,
Explain Naive Bayes,,hard,mixed,,jayinai/data-science-question-answer,"* Naive Bayes (NB) is a supervised learning algorithm based on applying [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem),,,,,
* It is called naive because it builds the naive assumption that each feature,,,,,,,,,,,
are independent of each other,,,,,,,,,,,
* NB can make different assumptions (i.e., data distributions, such as Gaussian,,,,,,,,,
Multinomial, Bernoulli),,,,,,,,,,
* Despite the over-simplified assumptions, NB classifier works quite well in real-world,,,,,,,,,,
applications, especially for text classification (e.g., spam f",2025-11-17T11:42:20.246284,,,,,,,,
Explain Decision tree,,hard,mixed,,jayinai/data-science-question-answer,"* Non-parametric,,,,,
* Given the training data, a decision tree algorithm divides the feature space into,,,,,,,,,,
regions. For inference, we first see which,,,,,,,,,,
region does the test data point fall in, and take the mean label values (regression),,,,,,,,,,
or the majority label value (classification).,,,,,,,,,,,
* **Construction**: top-down, chooses a variable to split the data such that the,,,,,,,,,,
target variables within each region are as homogeneous as possible. Two common,,,,,,,,,,,
metrics: gini impurity or informa",2025-11-17T11:42:20.246301,,,,,,,,,,
Explain Random forest,,hard,mixed,,jayinai/data-science-question-answer,"Random forest improves bagging further by adding some randomness. In random forest,,,,,
only a subset of features are selected at random to construct a tree (while often not subsample instances).,,,,,,,,,,,
The benefit is that random forest **decorrelates** the trees.,,,,,,,,,,,
,,,,,,,,,,,
For example, suppose we have a dataset. There is one very predicative feature, and a couple,,,,,,,,,
of moderately predicative features. In bagging trees, most of the trees,,,,,,,,,,
will use this very predicative feature in the top split, and therefore making mos",2025-11-17T11:42:20.246309,,,,,,,,,
Explain Boosting Tree,,hard,mixed,,jayinai/data-science-question-answer,"**How it works**,,,,,
,,,,,,,,,,,
Boosting builds on weak learners, and in an iterative fashion. In each iteration,,,,,,,,,,
a new learner is added, while all existing learners are kept unchanged. All learners,,,,,,,,,,
are weighted based on their performance (e.g., accuracy), and after a weak learner,,,,,,,,,
is added, the data are re-weighted: examples that are misclassified gain more weights,,,,,,,,,,
while examples that are correctly classified lose weights. Thus, future weak learners,,,,,,,,,,
focus more on examples that previous weak learners misclass",2025-11-17T11:42:20.246318,,,,,,,,,,
Explain RNN and LSTM,,hard,mixed,,jayinai/data-science-question-answer,"RNN is another paradigm of neural network where we have difference layers of cells,,,,,
and each cell not only takes as input the cell from the previous layer, but also the previous,,,,,,,,,,
cell within the same layer. This gives RNN the power to model sequence.,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
This seems great, but in practice RNN barely works due to exploding/vanishing gradient, which,,,,,,,,,
is cause by a series of multiplication of the same matrix. To solve this, we can use,,,,,,,,,,
a variation of RNN, called long short-term memory (LSTM), which is c",2025-11-17T11:42:20.246333,,,,,,,,
Explain Clustering,,hard,mixed,,jayinai/data-science-question-answer,"* Clustering is a unsupervised learning algorithm that groups data in such,,,,,
a way that data points in the same group are more similar to each other than to,,,,,,,,,,,
those from other groups,,,,,,,,,,,
* Similarity is usually defined using a distance measure (e.g, Euclidean, Cosine, Jaccard, etc.),,,,,,,
* The goal is usually to discover the underlying structure within the data (usually high dimensional),,,,,,,,,,,
* The most common clustering algorithm is K-means, where we define K (the number of clusters),,,,,,,,,,
and the algorithm iterativel",2025-11-17T11:42:20.246342,,,,,,,,,,
Explain Principal Component Analysis,,hard,mixed,,jayinai/data-science-question-answer,"* Principal Component Analysis (PCA) is a dimension reduction technique that projects,,,,,
the data into a lower dimensional space,,,,,,,,,,,
* PCA uses Singular Value Decomposition (SVD), which is a matrix factorization method,,,,,,,,,,
that decomposes a matrix into three smaller matrices (more details of SVD [here](https://en.wikipedia.org/wiki/Singular-value_decomposition)),,,,,,,,,,,
* PCA finds top N principal components, which are dimensions along which the data vary,,,,,,,,,,
(spread out) the most. Intuitively, the more spread out the",2025-11-17T11:42:20.246351,,,,,,,,,
Explain Autoencoder,,medium,mixed,,jayinai/data-science-question-answer,"* The aim of an autoencoder is to learn a representation (encoding) for a set of data,,,,,
* An autoencoder always consists of two parts, the encoder and the decoder. The encoder would find a lower dimension representation (latent variable) of the original input, while the decoder is used to reconstruct from the lower-dimension vector such that the distance between the original and reconstruction is minimized,,,,,,,,,
* Can be used for data denoising and dimensionality reduction",2025-11-17T11:42:20.246359,,,,,,,,,,
Explain Generative Adversarial Network,,hard,mixed,,jayinai/data-science-question-answer,"* Generative Adversarial Network (GAN) is an unsupervised learning algorithm that also has supervised flavor: using supervised loss as part of training,,,,,
* GAN typically has two major components: the **generator** and the **discriminator**. The generator tries to generate ""fake"" data (e.g, images or sentences) that fool the discriminator into thinking that they're real, while the discriminator tries to distinguish between real and generated data. It's a fight between the two players thus the name ",2025-11-17T11:42:20.246371,,,,,,,,
Explain Tokenization,,hard,mixed,,jayinai/data-science-question-answer,"* Tokenization is the process of converting a sequence of characters into a sequence of tokens,,,,,
* Consider this example: `The quick brown fox jumped over the lazy dog`. In this case each word (separated by space) would be a token,,,,,,,,,,,
* Sometimes tokenization doesn't have a definitive answer. For instance, `O'Neill` can be tokenized to `o` and `neill`, `oneill`, or `o'neill`.,,,,,,,,
* In some cases tokenization requires language-specific knowledge. For example, it doesn't make sense to tokenize `aren't` into",2025-11-17T11:42:20.246379,,,,,,,,,
Explain Stemming and lemmatization,,hard,mixed,,jayinai/data-science-question-answer,"* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form,,,,,
* Stemming usually refers to a crude heuristic process that chops off the ends of words,,,,,,,,,,,
* Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words,,,,,,,,,,,
* If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` ",2025-11-17T11:42:20.246387,,,,,,,,
Explain N gram,,hard,mixed,,jayinai/data-science-question-answer,"* n-gram is a contiguous sequence of n items from a given sample of text or speech,,,,,
* An n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" size 3 is a ""trigram"". Larger sizes are sometimes referred to by the value of n in modern language, e.g., ""four-gram"", ""five-gram"", and so on.,,,,,,,
* Consider this example: `The quick brown fox jumped over the lazy dog.`,,,,,,,,,,,
  - bigram would be `the quick`, `quick brown`, `brown fox`, ..., i.e, every two consecutive words (or tokens),,,,,,
  - trigram woul",2025-11-17T11:42:20.246394,,,,,,,,,,
Explain Bag of Words,,hard,mixed,,jayinai/data-science-question-answer,"* Why? Machine learning models cannot work with raw text directly; rather,,,,,
* Bag of words (BoW) builds a **vocabulary** of all the unique words in our dataset, and associate a unique index to each word in the vocabulary,,,,,,,,,,
* It is called a ""bag"" of words, because it is a representation that completely ignores the order of words,,,,,,,,,,
* Consider this example of two sentences: (1) `John likes to watch movies, especially horor movies.`, (2) `Mary likes movies too.` We wo",2025-11-17T11:42:20.246402,,,,,,,,
Explain word2vec,,hard,mixed,,jayinai/data-science-question-answer,"* Shallow,,,,,
* Takes as input a large corpus, and produce a vector space, typically of several hundred,,,,,,,,,
dimension, and each word in the corpus is assigned a vector in the space,,,,,,,,,,
* The key idea is **context**: words that occur often in the same context should have same/opposite,,,,,,,,,,,
meanings.,,,,,,,,,,,
* Two flavors,,,,,,,,,,,
    - continuous bag of words (CBOW): the model predicts the current word given a window of surrounding context words,,,,,,,,,,,
",2025-11-17T11:42:20.246413,,,,,,,,,,
Explain Cron job,,hard,mixed,,jayinai/data-science-question-answer,"The software utility **cron** is a **time-based job scheduler** in Unix-like computer operating systems. People who set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run periodically at fixed times, or intervals. It typically automates system maintenance or administration -- though its general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular intervals.,,,,
,,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
Tools:,,,,,,,,,,,
* [Apache Ai",2025-11-17T11:42:20.246423,,,,,,,,,,
Explain Linux,,medium,mixed,,jayinai/data-science-question-answer,"Using **Ubuntu** as an example.,,,,,
,,,,,,,,,,,
* Become root: `sudo su`,,,,,,,,,,,
* Install package: `sudo apt-get install <package>`,,,,,,,,,,,
,,,,,,,,,,,
(#data-science-question-answer),,,,,,,,,,,
,,,,,,,,,,,
,,,,,,,,,,,
Confession: some images are adopted from the internet without proper credit. If you are the author and this would be an issue for you, please let me know.",2025-11-17T11:42:20.246431,,,,,,,,,
Explain to me a technical concept related to the role that you’re interviewing for.,,medium,behavioral,communication,kojino/120-DS-Questions,,,,,,
Introduce me to something you’re passionate about.,,medium,behavioral,communication,kojino/120-DS-Questions,,,,,,
How would you explain an A/B test to an engineer with no statistics background? A linear regression?,,medium,behavioral,communication,kojino/120-DS-Questions,"- A/B testing, multivariate testing, is the testing of different elements of a user's experience to determine which variation helps the business achieve its goal more effectively (i.e. increasing conversions, etc..)  This can be copy on a web site, button colors, different user interfaces
How would you explain a confidence interval to an engineer with no statistics background? What does 95% confidence mean?,,medium,behavioral,communication,kojino/120-DS-Questions,- [link](https://www.quora.com/What-is-a-confidence-interval-in-laymans-terms),,,,,
How would you explain to a group of senior executives why data is important?,,medium,behavioral,communication,kojino/120-DS-Questions,,,,,,
(Given a Dataset) Analyze this dataset and tell me what you can learn from it.,,medium,case,data_analysis,kojino/120-DS-Questions,,,,,,
What is R2? What are some other metrics that could be better than R2 and why?,,medium,case,data_analysis,kojino/120-DS-Questions,"- goodness of fit measure. variance explained by the regression / total variance,,,,,
  - the more predictors you add the higher R^2 becomes.,,,,,,,,,,,
    - hence use adjusted R^2 which adjusts for the degrees of freedom ,,,,,,,,,,,
    - or train error metrics",2025-11-18T16:04:43.447469,,,,,,,,,,
What is the curse of dimensionality?,,hard,case,data_analysis,kojino/120-DS-Questions,"- High dimensionality makes clustering hard,,,,,
  - For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases,,,,,,,,,,
  - All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.,,,,,,,,,,,
  - The sampling density decreases exponentially as p in",2025-11-18T16:04:43.447483,,,,,,,,,,
Is more data always better?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Statistically,,,,,
    - It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.,,,,,,,,
    - It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.,,,,,,,,,
  - Practically,,,,,,,,,,,
    - Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more ",2025-11-18T16:04:43.447495,,,,,,,
What are advantages of plotting your data before per- forming analysis?,,hard,case,data_analysis,kojino/120-DS-Questions,"- 1) Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.  ,,,,,
2) Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.  ,,,,,,,,,,
3) Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect.",2025-11-18T16:04:43.447506,,,,,,,,,,
How can you make sure that you don’t analyze something that ends up meaningless?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Proper exploratory data analysis.  ,,,,,
In every data analysis task, there's the exploratory phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.  ,,,,,,,
Then there's the exploitatory phase, where you look deeply into a set of hypotheses.   ,,,,,,,,,,
The exploratory phase will generate lots of possible hypotheses, and the exploitatory phase will let you really understand a few",2025-11-18T16:04:43.447523,,,,,,,,,
What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?,,hard,case,data_analysis,kojino/120-DS-Questions,"- data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.,,,,,
  - The scientific method is eminently inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is.",2025-11-18T16:04:43.447536,,,,,,,
How can you determine which features are the most im- portant in your model?,,medium,case,data_analysis,kojino/120-DS-Questions,"- run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.,,,,,
  - Look at the variables added in forward variable selection",2025-11-18T16:04:43.447545,,,,,,,,,,
How do you deal with some of your predictors being missing?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Remove rows with missing values - This works well if 1) the values are missing randomly (see [Vinay Prabhu's answer](https://www.quora.com/How-can-I-deal-with-missing-values-in-a-predictive-model/answer/Vinay-Prabhu-7) for more details on this) 2) if you don't lose too much of the dataset after doing so.,,,,,
  - Build another predictive model to predict the missing values - This could be a whole project in itself, so simple techniques are usually used here.,,,,,,,,,,
  - Use a model that can incorporate mis",2025-11-18T16:04:43.447558,,,,,,,,,,
"You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?",,hard,case,kojino/120-DS-Questions,"- Multicollinearity refers to a situation in which two or more explanatory variables in a [multiple regression](https://en.wikipedia.org/wiki/Multiple_regression ""Multiple regression"") model are highly linearly related. ,,,
  - Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data o",2025-11-18T16:04:43.447579,,,,,,,,,
Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?,,medium,case,data_analysis,kojino/120-DS-Questions,- PCA,,,,,
"Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?",,hard,case,data_analysis,kojino/120-DS-Questions,,,,,
  - Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores,,,,,,,,,,,
  - ""Recursive Feature Elimination"":  ,,,,,,,,,,,
    - First, train a model with all the feature and evaluate its performance on held out data.,,,,,,,,,,
    - Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining feature",2025-11-18T16:04:43.447604,,,,,,,,,,
Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?,,medium,case,data_analysis,kojino/120-DS-Questions,"- p > n.,,,,,
  - If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique.",2025-11-18T16:04:43.447614,,,,,,,,,,
"You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?",,medium,case,data_analysis,"- The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.",2025-11-18T16:04:43.447627
"What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?",,hard,case,data_analysis,kojino/120-DS-Questions,,,,,
  - Hence the combined model is expected to perform better than an individual model.,,,,,,,,,,,
  - Assumptions:,,,,,,,,,,,
    - average out biases,,,,,,,,,,,
    - reduce variance,,,,,,,,,,,
  - Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability le",2025-11-18T16:04:43.447660,,,,,,,,,
"Given that you have wi data in your o ce, how would you determine which rooms and areas are underutilized and overutilized?",,medium,case,data_analysis,kojino/120-DS-Questions, then that one is over utilized! Maybe account for the room capacity and normalize the data.",2025-11-18T16:04:43.447669,,,
How could you use GPS data from a car to determine the quality of a driver?,,medium,case,data_analysis,kojino/120-DS-Questions,,,,,,
"Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?",,medium,case,kojino/120-DS-Questions,,2025-11-18T16:04:43.447680,,
"Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?",,medium,case,data_analysis,kojino/120-DS-Questions,2025-11-18T16:04:43.447685,,,,
How would you quantify the influence of a Twitter user?,,medium,case,data_analysis,kojino/120-DS-Questions,- like page rank with each user corresponding to the webpages and linking to the page equivalent to following.,,,,,
"Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?",,medium,case,data_analysis,kojino/120-DS-Questions,2025-11-18T16:04:43.447697,,,,
"You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of di culty?",,hard,case,data_analysis,kojino/120-DS-Questions,,,,,
  - The Rasch model for dichotomous data takes the form:  ,,,,,,,,,,,
{\displaystyle \Pr\\{X_{ni}=1\\}={\frac {\exp({\beta _{n}}-{\delta _{i}})}{1+\exp({\beta _{n}}-{\d",2025-11-18T16:04:43.447714,,,,,,,,,,
You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?,,medium,case,data_analysis,kojino/120-DS-Questions,"- Some people would take the mean rank of each sushi.  If I wanted something simple, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong).",2025-11-18T16:04:43.447726,,
"Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?",,medium,case,data_analysis,kojino/120-DS-Questions,,,,,
  - for liberal and republican parties, find the mean vector and find the representative closest to the center point",2025-11-18T16:04:43.447740,,,,,,,,,
How would you come up with an algorithm to detect plagiarism in online content?,,medium,case,data_analysis,kojino/120-DS-Questions,"- reduce the text to a more compact form (e.g. fingerprinting,2025-11-18T16:04:43.447747,,,,
You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?,,medium,case,data_analysis,kojino/120-DS-Questions,"- KMeans,,,,,
  - choose a small value of k that still has a low SSE (elbow method),,,,,,,,,,,
  - <https://bl.ocks.org/rpgove/0060ff3b656618e9136b>",2025-11-18T16:04:43.447759,,,,,,,,,,
Let's say you're building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?,,medium,case,data_analysis,kojino/120-DS-Questions,- [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering),,,,,
(Given a Dataset) Analyze this dataset and give me a model that can predict this response variable.,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Start by fitting a simple model (multivariate regression, do some feature engineering accordingly, and then try some complicated models. Always split the dataset into train, validation, test dataset and use cross validation to check their performance.,
- Determine if the problem is classification or regression,,,,,,,,,,,
- Favor simple models that run quickly and you can easily explain.,,,,,,,,,,,
- Mention cross validation as a means to evaluate the model.,,,,,,,,,,,
- Plot and visualize the data.",2025-11-18T16:04:43.447814,,,,,,,,,,
What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- The model that has high training accuracy might have low test accuracy. Without further knowledge,,,,,
- When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely",2025-11-18T16:04:43.447836,,,,,,,,
What are some ways I can make my model more robust to outliers?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can have regularization such as L1 or L2 to reduce variance (increase bias).,,,,,
- Changes to the algorithm:,,,,,,,,,,,
  - Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.,,,,,,,,,,
  - Use robust error metrics such as MAE or Huber Loss instead of MSE.,,,,,,,,,,,
- Changes to the data:,,,,,,,,,,,
  - Winsorizing the data,,,,,,,,,,,
  - Transforming the data (e.g. log),,,,,,,,,,,
  - Remove them only if you’re certain they’re anomalies not ",2025-11-18T16:04:43.447849,,,,,,,,,,
"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?",,hard,ml,predictive_modeling,kojino/120-DS-Questions, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.,
- MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient,,,,,,,,,,
- MAE more robust to outliers. If the consequences of large errors are great, use MSE,,,,,,,,,,
- MSE ",2025-11-18T16:04:43.447866,,,,,,,,,,
What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Accuracy: proportion of instances you predict correctly. Pros: intuitive, Cons: works poorly when the class labels are imbalanced and the signal from the data is weak,,,,
- AUROC: plot fpr on the x axis and tpr on the y axis for different threshold. Given a random positive instance and a random negative instance, the AUC is the probability that you can identify who's who. Pros: Works well when testing the ability of distinguishing the two classes, Cons: can’t interpret predictions",2025-11-18T16:04:43.447887,,,,,,,,
"What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)",,hard,predictive_modeling,kojino/120-DS-Questions,"- Things to look at: N, P, linearly seperable?
- Logistic Regression,,,,,,,,,,,
  - features roughly linear, problem roughly linearly separable,,,,,,,,,,
  - robust to noise, use l1,l2 regularization for model selection, avoid overfitting,,,,,,,,
  - the output come as probabilities,,,,,,,,,,,
  - efficient and the computation can be distributed,,,,,,,,,,,
  - can be used as a baseline for other algorithms,,,,,,,,,,,
  - (-) can hardly handle categorical features,,,,,,,,,,,
- SVM,,,,,,,,,,,
  - with a ",2025-11-18T16:04:43.447919,,,,,,,,,,
What is regularization and where might it be helpful? What is an example of using regularization in a model?,,medium,ml,predictive_modeling,kojino/120-DS-Questions,"- Regularization is useful for reducing variance in the model, we can use L1 regularization in Lasso regression to penalize large coefficients.",2025-11-18T16:04:43.447928,,,
Why might it be preferable to include fewer predictors over many?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- When we add irrelevant features, they might be harder to interpret in case of regression, etc.,,,
- curse of dimensionality,,,,,,,,,,,
- adding random noise makes the model more complicated but useless,,,,,,,,,,,
- computational cost,,,,,,,,,,,
- Ask someone for more details.",2025-11-18T16:04:43.447938,,,,,,,,,,
"Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?",,hard,ml,predictive_modeling,kojino/120-DS-Questions,,,,,
- Ask someone for more details.,,,,,,,,,,,
- Build a regression function to estimate the number of retweets as a function of time t,,,,,,,,,,,
- to determine if one regression function can be built, see if there are clusters in terms of the trends in the number of retweets,,,,,,,,,,
- if not, we have to add features to the regression function,,,,,,,,,,
- features + # of retweets on the first and the second da",2025-11-18T16:04:43.447956,,,,,,,,,,
How could you collect and analyze data to use social media to predict the weather?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can collect social media data using twitter, instagram API’s. Then, for example, for twitter, we can construct features from each tweet, e.g. the tweeted date
- Ask someone for more details.",2025-11-18T16:04:43.447967,,,,,,,,,,
How would you construct a feed to show relevant content for a site that involves user interactions with items?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can do so using building a recommendation engine. The easiest we can do is to show contents that are popular other users, we can build a content based filtering or collaborative filtering. If there’s enough user usage data, we can try collaborative filtering and recommend contents other similar users have consumed. If there isn’t, we can recommend similar items based on vectorization of items",2025-11-18T16:04:43.447980,
How would you design the people you may know feature on LinkedIn or Facebook?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Find strong unconnected people in weighted connection graph,,,,,
  - Define similarity as how strong the two people are connected,,,,,,,,,,,
  - Given a certain feature, we can calculate the similarity based on,,,,,,,,,,
    - friend connections (neighbors),,,,,,,,,,,
    - Check-in’s people being at the same location all the time.,,,,,,,,,,,
    - same college, workplace,,,,,,,,,,
    - Have randomly dropped graphs test the performance of the algorithm,,,,,,,,,,,
- ref. News Feed Optimization,,,,,,,,,,,
  - Affinity score: how close the content creator and the users are,,,,,,,,,,,
",2025-11-18T16:04:43.447995,,,,,,,,,,
How would you predict who someone may want to send a Snapchat or Gmail to?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- for each user,,,,,
- the rest is feature engineering:,,,,,,,,,,,
  - number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.,,,,,,
- Ask someone for more details.,,,,,,,,,,,
- People who someone sent emails the most in the past, conditioning on time decay.",2025-11-18T16:04:43.448006,,,,,,,,,
How would you suggest to a franchise where to open a new store?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- build a master dataset with local demographic information available for each location.,,,,,
  - local income levels, proximity to traffic, weather, population density, proximity to other businesses,,,,,,,
  - a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.),,,,,,
  - any data on the local franchise owner-operators, to the degree the manager,,,,,,,,,,
- identify a set of KPIs acceptable to the management that had requested the analysis ",2025-11-18T16:04:43.448022,,,,,,,,,,
"In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?",,hard,ml,predictive_modeling,"- Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.,,,
- To further improve this algorithm,,,,,,,,,,,
  - we can put more weight on past sequences which showed up more recently and near your location to account for trends,,,,,,,,,,,
  - show your recent searches given partial data",2025-11-18T16:04:43.448036,,,,,,,,,,
"Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?",,medium,ml,predictive_modeling,kojino/120-DS-Questions, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.",2025-11-18T16:04:43.448045
You’re Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Based on the past pickup location of passengers around the same time of the day, year), construct,,,
- Ask someone for more details.,,,,,,,,,,,
- Based on the number of past pickups,,,,,,,,,,,
  - account for periodicity (seasonal, monthly, weekly, daily, hourly),,,,,,,
  - special events (concerts, festivals, etc.) from tweets",2025-11-18T16:04:43.448056,,,,,,,,
How would you build a model to predict a March Madness bracket?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament,,,,,
- Some extensions:,,,,,,,,,,,
  - Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc),,,,,,,,,
  - Consider using a RNN type model that ",2025-11-18T16:04:43.448069,,,,,,,,,,
"You want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?",,medium,ml,predictive_modeling,kojino/120-DS-Questions,,,,,
- See Q3.",2025-11-18T16:04:43.448079,,,,,,,,,,
"Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 offspring, respectively. Each of Bobo’s descendants also have the same probabilities. What is the probability that Bobo’s lineage dies out?",,stats,probability,kojino/120-DS-Questions,- p=1/4+1/4*p+1/2*p^2 => p=1/2,2025-11-18T16:04:43.448114
"In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba- bility that you see at least one shooting star in the period of an hour?",,hard,stats,probability,kojino/120-DS-Questions, we can use Poisson processes",2025-11-18T16:04:43.448124,,,
How can you generate a random number between 1 - 7 with only a die?,,hard,stats,probability,kojino/120-DS-Questions,"* Launch it 3 times: each throw sets the nth bit of the result. ,,,,,
* For each launch, if the value is 1-3, record a 0, else 1.,,,,,,,,
The result is between 0 (000) and 7 (111), evenly spread (3 independent throw). Repeat the throws if 0 was obtained: the process stops on evenly spread values.",2025-11-18T16:04:43.448133,,,,,,,,,
How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?,,hard,stats,probability,kojino/120-DS-Questions,"- Flip twice and if HT then H,2025-11-18T16:04:43.448139,,,,
You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?,,hard,stats,probability,kojino/120-DS-Questions,- more than two standard deviations,,,,,
"Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?",,hard,stats,probability,kojino/120-DS-Questions,2025-11-18T16:04:43.448154,,,,
"A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?",,hard,stats,probability,kojino/120-DS-Questions,2025-11-18T16:04:43.448161,,,,
"You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?",,hard,stats,probability,kojino/120-DS-Questions,2025-11-18T16:04:43.448174,,,,
How many ways can you split 12 people into 3 teams of 4?,,hard,stats,probability,kojino/120-DS-Questions,- the outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable,,,,,
"Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.",,hard,stats,probability,"- the probability of a hash collision: 1-(10!/10^10),,,,
  - the expected number of hash collisions: 1-10*(9/10)^10,,,,,,,,,,,
  - the expected number of hashes that are unused: 10*(9/10)^10",2025-11-18T16:04:43.448192,,,,,,,,,,
"You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?",,hard,stats,probability,kojino/120-DS-Questions,,,,,
    * probability that the first car is Lyft = 3/5,,,,,,,,,,,
    * probability that the second car is Lyft = 2/4,,,,,,,,,,,
    * probability that the third car is Lyft = 1/3,,,,,,,,,,,
    Therefore, probability that all the Lyfts arrive first = (3/5) * (2/4) * (1/3) = 1/10,,,,,,,,,,
  - All Uber's first,,,,,,,,,,,
    * probability that the first car is Uber = 2/5,,,,,,,,,,,
    * probability that the second car is Uber = 1/4,,,,,,,,,,,
    Therefore, probability that all the Ubers arrive first = (2/5) * (1/4) = 1/10",2025-11-18T16:04:43.448209,,,,,,,,,
"I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?",,stats,probability,kojino/120-DS-Questions,- 100+60-20=140,2025-11-18T16:04:43.448222
"On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?",,hard,stats,probability,- 24C5*(1+5(24-5))/24C5*24C5 = 4/1771,2025-11-18T16:04:43.448232,,,
"A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?",,hard,stats,probability,kojino/120-DS-Questions,2025-11-18T16:04:43.448241,,,,
"Let’s say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?",,hard,stats,kojino/120-DS-Questions,- Shorter. Regression to the mean,2025-11-18T16:04:43.448250,,
What’s the expected number of coin flips until you get two heads in a row? What’s the expected number of coin flips until you get two tails in a row?,,hard,stats,probability,kojino/120-DS-Questions,"- After the first two flips, with states HH, HT, TH, TT. ,
  - HH is the final state. You can than define the expected number of steps N before reaching HH: E(N) = 2 + 0.25nHH, 0.25nHT, 0.25nTH, 0.25nTT. nXX represents the expected number of steps before reaching HH starting from state XX.,,,,,,,,
  - Solve linear equation:,,,,,,,,,,,
  * nHH = 0,,,,,,,,,,,
  * nHT = 1 + 0.5nTT + 0.5nTH,,,,,,,,,,,
  * nTH = 1 + 0.5nHH + 0.5nHT,,,,,,,,,,,
  * nTT = 1 + 0.5nTH + 0.5nTT,,,,,,,,,,,
  - Result gives E(N) = 6.",2025-11-18T16:04:43.448265,,,,,,,,,,
"Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?",,hard,stats,probability,kojino/120-DS-Questions,2025-11-18T16:04:43.448274,,,,
"You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?",,hard,stats,kojino/120-DS-Questions,- 4/13,2025-11-18T16:04:43.448285,,
"You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What’s the chance that you picked up the fair coin, given the information that you observed?",,hard,stats,probability,"* Events: F = ""picked a fair coin"", T = ""10 heads in a row"",,,
  * (1) P(F|T) = P(T|F)P(F)/P(T) (Bayes formula),,,,,,,,,,,
  * (2) P(T) = P(T|F)P(F) + P(T|¬F)P(¬F) (total probabilities formula),,,,,,,,,,,
  * Injecting (2) in (1): P(F|T) = P(T|F)P(F)/(P(T|F)P(F) + P(T|¬F)P(¬F)) = 1 / (1 + P(T|¬F)P(¬F)/(P(T|F)P(F))),,,,,,,,,,,
  * Numerically: 1/(1 + 0.001 * 2^10 /0.999).,,,,,,,,,,,
  * With 2^10 ≈ 1000 and 0.999 ≈ 1 this simplifies to 1/2",2025-11-18T16:04:43.448303,,,,,,,,,,
What is a P-Value ?,,hard,stats,probability,kojino/120-DS-Questions,"* The probability to obtain a similar or more extreme result than observed when the null hypothesis is assumed.,,,,,
  * ⇒ If the p-value is small, the null hypothesis is unlikely",2025-11-18T16:04:43.448308,,,,,,,,,
"What would be good metrics of success for an advertising-driven consumer product? (Buzzfeed, YouTube, Google Search, etc.) A service-driven consumer product? (Uber, Flickr, Venmo, etc.)",medium,case,product_metrics,kojino/120-DS-Questions,"* advertising-driven: Pageviews and daily actives
    * click-ads  ,,,,,,,,,,,
    * display-ads  ,,,,,,,,,,,
  * service-driven: number of purchases, conversion rate",2025-11-18T16:04:43.448343,,,,,,,,,
"What would be good metrics of success for a productiv- ity tool? (Evernote, Asana, Google Docs, etc.) A MOOC? (edX, Coursera, Udacity, etc.)",medium,case,product_metrics,kojino/120-DS-Questions,"* productivity tool: same as premium subscriptions
  * MOOC: same as premium subscriptions, completion rate",2025-11-18T16:04:43.448351,,,,,,,,,
"What would be good metrics of success for an e-commerce product? (Etsy, Groupon, Birchbox, etc.) A subscrip- tion product? (Net ix, Birchbox, Hulu, etc.) Premium subscriptions? (OKCupid, Spotify, etc.)",,hard,case
  * subscription,,,,,,,,,,,
    * churn, CoCA, ARPU, MRR, LTV,,,,,,,
  * premium subscriptions:",2025-11-18T16:04:43.448365,,,,,,,,,,
"What would be good metrics of success for a consumer product that relies heavily on engagement and interac- tion? (Snapchat, Pinterest, Facebook, etc.) A messaging product? (GroupMe, Hangouts, Snapchat, etc.)",medium,case,product_metrics,kojino/120-DS-Questions,"* heavily on engagement and interaction: uses AU ratios
  * messaging product:",2025-11-18T16:04:43.448377,,,,,,,,,,
"What would be good metrics of success for a product that o ered in-app purchases? (Zynga, Angry Birds, other gaming apps)",,medium,case,product_metrics,"* Average Revenue Per Paid User,,,,
  * Average Revenue Per User",2025-11-18T16:04:43.448383,,,,,,,,,,
A certain metric is violating your expectations by going down or up more than you expect. How would you try to identify the cause of the change?,,medium,case,product_metrics,kojino/120-DS-Questions,"* breakdown the KPI’s into what consists them and find where the change is,,,,,
  * then further breakdown that basic KPI by channel, user cluster, etc. and relate them with any campaigns, changes in user behaviors in that segment",2025-11-18T16:04:43.448394,,,,,,,
Growth for total number of tweets sent has been slow this month. What data would you look at to determine the cause of the problem?,,medium,case,product_metrics,kojino/120-DS-Questions,"* look at competitors' tweet growth,,,,,
  * look at your social media engagement on other platforms,,,,,,,,,,,
  * look at your sales data",2025-11-18T16:04:43.448402,,,,,,,,,,
You’re a restaurant and are approached by Groupon to run a deal. What data would you ask from them in order to determine whether or not to do the deal?,,medium,case,product_metrics,kojino/120-DS-Questions,"* for similar restaurants (they should define similarity), average increase in customers per coupon, number of meals sold",2025-11-18T16:04:43.448412,,
You are tasked with improving the e ciency of a subway system. Where would you start?,,medium,case,product_metrics,kojino/120-DS-Questions,* define efficiency,,,,,
Say you are working on Facebook News Feed. What would be some metrics that you think are important? How would you make the news each person gets more relevant?,,hard,case,product_metrics,kojino/120-DS-Questions,"* rate for each action, CTR for sponsor feed posts,,,,
  * ref. News Feed Optimization,,,,,,,,,,,
    * Affinity score: how close the content creator and the users are,,,,,,,,,,,
    * Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote,,,,,,,,
    * Time decay: the older the less important",2025-11-18T16:04:43.448429,,,,,,,,,,
How would you measure the impact that sponsored stories on Facebook News Feed have on user engagement? How would you determine the optimum balance between sponsored stories and organic content on a user’s News Feed?,,medium,case,product_metrics,kojino/120-DS-Questions,* AB test on different balance ratio and see,,,,,
You are on the data science team at Uber and you are asked to start thinking about surge pricing. What would be the objectives of such a product and how would you start looking into this?,,hard,case,product_metrics,kojino/120-DS-Questions,"*  there is a gradual step-function type scaling mechanism until that imbalance of requests-to-drivers is alleviated and then vice versa as too many drivers come online enticed by the surge pricing structure. ,,,,,
  * I would bet the algorithm is custom tailored and calibrated to each location as price elasticities almost certainly vary across different cities depending on a huge multitude of variables: income, distance/sprawl, traffic patterns, car ownership, etc. With the massive troves of user da",2025-11-18T16:04:43.448460,,,,,,
Say that you are Netflix. How would you determine what original series you should invest in and create?,,medium,case,product_metrics,kojino/120-DS-Questions,* Netflix uses data to estimate the potential market size for an original series before giving it the go-ahead.,,,,,
What kind of services would nd churn (metric that tracks how many customers leave the service) helpful? How would you calculate churn?,,medium,case,product_metrics,kojino/120-DS-Questions,* subscription based services,,,,,
Let’s say that you’re are scheduling content for a content provider on television. How would you determine the best times to schedule content?,,medium,case,product_metrics,kojino/120-DS-Questions,,,,,,
"Write a function to calculate all possible assignment vectors of 2n users, where n users are assigned to group 0 (control), and n users are assigned to group 1 (treatment).",,medium,coding,programming,- Recursive programming (sol in code),2025-11-18T16:04:43.448517,,,
"Given a list of tweets, determine the top 10 most used hashtags.",,medium,coding,programming,kojino/120-DS-Questions,2025-11-18T16:04:43.448523,,,,
Program an algorithm to find the best approximate solution to the knapsack problem1 in a given time.,,medium,coding,programming,kojino/120-DS-Questions,- Greedy solution (add the best v/w as much as possible and move on to the next),,,,,
"You have a stream of data coming in of size n, but you don’t know what n is ahead of time. Write an algorithm that will take a random sample of k elements. Can you write one that takes O(k) space?",,medium,coding,programming,kojino/120-DS-Questions,2025-11-18T16:04:43.448545,,,,
Write an algorithm that can calculate the square root of a number.,,medium,coding,programming,kojino/120-DS-Questions,"- <https://www.quora.com/What-is-the-method-to-calculate-a-square-root-by-hand?redirected_qid=664405>,,,,,
  - https://en.wikipedia.org/wiki/Newton's_method#Square_root_of_a_number",2025-11-18T16:04:43.448552,,,,,,,,,,
"Given a list of numbers, can you return the outliers?",,medium,coding,programming,kojino/120-DS-Questions,2025-11-18T16:04:43.448556,,,,
When can parallelism make your algorithms run faster?,,medium,coding,programming,kojino/120-DS-Questions,"When could it make your algorithms run slower?,,,,,
  - Ask someone for more details.,,,,,,,,,,,
  - compute in parallel when communication cost < computation cost,,,,,,,,,,,
    - ensemble trees,,,,,,,,,,,
    - minibatch,,,,,,,,,,,
    - cross validation,,,,,,,,,,,
    - forward propagation,,,,,,,,,,,
    - minibatch,,,,,,,,,,,
    - not suitable for online learning",2025-11-18T16:04:43.448565,,,,,,,,,,
What are the different types of joins? What are the differences between them?,,medium,coding,programming,kojino/120-DS-Questions,"- (INNER) JOIN: Returns records that have matching values in both tables,,,,,
    LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table,,,,,,,,,,
    RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table,,,,,,,,,,
    FULL (OUTER) JOIN: Return all records when there is a match in either left or right table",2025-11-18T16:04:43.448575,,,,,,,,,,
Why might a join on a subquery be slow? How might you speed it up?,,medium,coding,programming,kojino/120-DS-Questions,- Change the subquery to a join.,,,,,
Describe the difference between primary keys and foreign keys in a SQL database.,,medium,coding,programming,kojino/120-DS-Questions,- Primary keys are columns whose value combinations must be unique in a specific table so that each row can be referenced uniquely. Foreign keys are columns that references columns (often primary keys) in other tables.,,,,,
"Given a COURSES table with columns course_id and course_name, a FACULTY table with columns faculty_id and faculty_name, and a COURSE_FACULTY table with columns faculty_id and course_id, how would you return a list of faculty who teach a course given the name of a course?",,medium,coding,kojino/120-DS-Questions,- select faculty_name from faculty_id c join (select faculty_id from (select course_id from COURSES where course_name=xxx) as a join COURSE_FACULTY b on a.course_id = b.course_id) d on c.faculty_id = d.faculty_id,2025-11-18T16:04:43.448601,,
"Given a IMPRESSIONS table with ad_id, click (an indicator that the ad was clicked), and date, write a SQL query that will tell me the click-through-rate of each ad by month.",,medium,coding,kojino/120-DS-Questions,"- select id, average(click) from (select count(click) as click from IMPRESSIONS group by id,month(date)) group by id",2025-11-18T16:04:43.448610
Write a query that returns the name of each department and a count of the number of employees in each:,,medium,coding,programming,kojino/120-DS-Questions,"EMPLOYEES containing: Emp_ID (Primary key) and Emp_Name  ,,,,,
EMPLOYEE_DEPT containing: Emp_ID (Foreign key) and Dept_ID (Foreign key)  ,,,,,,,,,,,
DEPTS containing: Dept_ID (Primary key) and Dept_Name,,,,,,,,,,,
  - select Dept_Name, count(1) from DEPTS a right join EMPLOYEE_DEPT b on a.Dept_id = b.Dept_id group by Dept_Name",2025-11-18T16:04:43.448620,,,,,,,,,
"In an A/B test, how can you check if assignment to the various buckets was truly random?",,hard,stats,statistical_inference,kojino/120-DS-Questions, we can conduct a permutation test to see if the distributions are the same.,,,,
  - MANOVA to compare different means",2025-11-18T16:04:43.448663,,,,,,,,,,
"What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?",,hard,stats,statistical_inference,kojino/120-DS-Questions,2025-11-18T16:04:43.448670,,,,
What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket,2025-11-18T16:04:43.448678,,,,
What would be some issues if blogs decide to cover one of your experimental groups?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- Same as the previous question. The above problem can happen in larger scale.,,,,,
How would you conduct an A/B test on an opt-in feature?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- Ask someone for more details.,,,,,
"How would you run an A/B test for many variants, say 20 or more?",,hard,stats,statistical_inference,kojino/120-DS-Questions, 20 treatment, if the sample size for each group is big enough.,,,
  - Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD).",2025-11-18T16:04:43.448698,,,,,,,,,,
How would you run an A/B test if the observations are extremely right-skewed?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- lower the variability by modifying the KPI,,,,,
  - cap values,,,,,,,,,,,
  - percentile metrics,,,,,,,,,,,
  - log transform,,,,,,,,,,,
  - <https://www.quora.com/How-would-you-run-an-A-B-test-if-the-observations-are-extremely-right-skewed>",2025-11-18T16:04:43.448706,,,,,,,,,,
I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- exclusive -> ok,,,,,
What is a p-value? What is the difference between type-1 and type-2 error?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- A p-value is defined such that under the null hypothesis less than the fraction p of events have parameter values more extreme than the observed parameter. It is not the probability that the null hypothesis is wrong. ,,,,,
  - type-1 error: rejecting Ho when Ho is true,,,,,,,,,,,
  - type-2 error: not rejecting Ho when Ha is true",2025-11-18T16:04:43.448722,,,,,,,,,,
You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- For randomly selected listings with more than 1 pictures, and show all for group B. Compare the booking rate for the two groups.,,,,
  - Ask someone for more details.",2025-11-18T16:04:43.448733,,,,,,,,,,
How would you design an experiment to determine the impact of latency on user engagement?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, add a delay in an A/B test.",2025-11-18T16:04:43.448740,,,
What is maximum likelihood estimation? Could there be any case where it doesn’t exist?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model).,,,,,
  - maximum likelihood estimation (MLE) is a method of [estimating](https://en.wikipedia.org/wiki/Estimator ""Estimator"") the [parameters](https://en.wikipedia.org/wiki/Statistical_parameter ""Statistical parameter"") of a [statistical model](https://en.wikipedia.org/wiki/Statistical_model ""Statistical mod",2025-11-18T16:04:43.448765,,,,,,,,,,
"What’s the difference between a MAP, MOM, MLE estima\- tor? In which cases would you want to use each?",,hard,stats,statistical_inference,"- MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution.,,,,
  - MOM sets moment values and solves for the parameters. MOM is not used much anymore because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.",2025-11-18T16:04:43.448777,,,,,,,,,,
What is a confidence interval and how do you interpret it?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- For example, the constructed intervals include the true mean 95% of the time.,,,,
  - if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.,,,,,,,,,,
  - [confidence intervals refresher from khanacademy](https://www.khana",2025-11-18T16:04:43.448794,,,,,,,,,,
What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However,2025-11-18T16:04:43.448808,,,,
What is Selection Bias?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn’t random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, then some conclusions of the study may not be accurate.,,,,
  - The types of selection bias include:,,,,,,,,,,,
  - Sampling ",2025-11-18T16:04:43.448829,,,,,,,,,,
What is Machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"Machine learning is a branch of computer science which deals with system programming to automatically learn and improve with experience. For example,2025-11-18T17:20:54.000373,,,,
Mention the difference between Data Mining and Machine learning?,,hard,ml,machine_learning,165_ML_Interview_QA,"Machine learning relates to the study, and development of the algorithms that give computers the capability to learn without being explicitly programmed. While data mining can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. During this processing machine, learning algorithms are used.",2025-11-18T17:20:54.000398,,
What is ‘Overfitting’ in Machine learning?,,hard,ml,machine_learning,165_ML_Interview_QA,"In machine learning, overfitting is normally observed, because of having too many parameters concerning the number of training data types. The model exhibits poor performance which has been overfitted.",2025-11-18T17:20:54.000416,,
Why does overfitting happen?,,medium,ml,machine_learning,165_ML_Interview_QA,The possibility of overfitting exists as the criteria used for training the model is not the same as the criteria used to judge the efficacy of a model.,,,,,
How can you avoid overfitting ?,,hard,ml,machine_learning,165_ML_Interview_QA,"By using a lot of data overfitting can be avoided, and you try to learn from it. But if you have a small database and you are forced to come with a model based on that. In such a situation, you can use a technique known as cross-validation. We can use data augmentation techniques or regularization. We can also try to reduce the futures or we can try to simplify our model structure.",2025-11-18T17:20:54.000443,,
What is inductive machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The inductive machine learning involves the process of learning by examples, from a set of observed instances, tries to induce a general rule.",2025-11-18T17:20:54.000451,,
What are the five popular algorithms of Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Decision Trees Neural Networks (backpropagation) Probabilistic networks Nearest Neighbor Support vector machines,,,,,
What are the different Algorithm techniques in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different types of techniques in Machine Learning are Supervised Learning Unsupervised Learning Semi-supervised Learning Reinforcement Learning Transduction Learning to Learn,,,,,
What are the three stages to build the hypotheses or model in machine learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Model building Model testing Applying the model,,,,,
What is the standard approach to supervised learning?,,easy,ml,machine_learning,165_ML_Interview_QA,The standard approach to supervised learning is to split the set of examples into the training set and the test.,,,,,
What is ‘Training set’ and ‘Test set’?,,hard,ml,machine_learning,165_ML_Interview_QA,"In various areas of information science like machine learning, while the Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of examples held back from the learner. The training set is distinct from the Test set.",2025-11-18T17:20:54.000496,,
List down various approaches for machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different approaches in Machine Learning are Concept Vs Classification Learning Symbolic Vs Statistical Learning Inductive Vs Analytical Learning ** 13) What is not Machine Learning?** => Artificial Intelligence Rule-based inference,,,,,
Explain what is the function of ‘Unsupervised Learning’?,,medium,ml,machine_learning,165_ML_Interview_QA,Find clusters of the data Find low-dimensional representations of the data Find interesting directions in data Interesting coordinates and correlations Find novel observations/ database cleaning,,,,,
What is algorithm independent machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,Machine learning in where mathematical foundations are independent of any particular classifier or learning algorithm is referred to as algorithm independent machine learning?,,,,,
What is the difference between artificial learning and machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"Designing and developing algorithms according to the behaviors based on empirical data are known as Machine Learning. While artificial intelligence in addition to machine learning, natural language processing, planning, robotics, etc.",2025-11-18T17:20:54.000543
What is a classifier in machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"A classifier in Machine Learning is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value,2025-11-18T17:20:54.000551,,,,
What are the advantages of Naive Bayes?,,medium,ml,machine_learning,165_ML_Interview_QA,"In Naïve Bayes classifier will converge quicker than discriminative models like logistic regression,2025-11-18T17:20:54.000561,,,,
In what areas Pattern Recognition is used?,,easy,ml,machine_learning,165_ML_Interview_QA,Pattern Recognition can be used in Computer Vision Speech Recognition Data Mining Statistics Informal Retrieval Bio-Informatics,,,,,
What is Genetic Programming?,,medium,ml,machine_learning,165_ML_Interview_QA,Genetic programming is one of the two techniques used in machine learning. The model is based on testing and selecting the best choice among a set of results.,,,,,
What is Inductive Logic Programming in Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Inductive Logic Programming (ILP) is a subfield of machine learning which uses logic programming representing background knowledge and examples.,,,,,
What is Model Selection in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The process of selecting models among different mathematical models, machine learning, and data mining.",2025-11-18T17:20:54.000593,,
What are the two methods used for the calibration in Supervised Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The two methods used for predicting good probabilities in Supervised Learning are Platt Calibration Isotonic Regression These methods are designed for binary classification,2025-11-18T17:20:54.000602,,,,
Which method is frequently used to prevent overfitting?,,easy,ml,machine_learning,165_ML_Interview_QA,When there is sufficient data ‘Isotonic Regression’ is used to prevent an overfitting issue.,,,,,
What is the difference between heuristic for rule learning and heuristics for decision trees?,,medium,ml,machine_learning,165_ML_Interview_QA,The difference is that the heuristics for decision trees evaluate the average quality of many disjointed sets while rule learners only evaluate the quality of the set of instances that are covered with the candidate rule.,,,,,
What is Perceptron in Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,"In Machine Learning,2025-11-18T17:20:54.000627,,,,
Explain the two components of the Bayesian logic program?,,medium,ml,machine_learning,165_ML_Interview_QA,"Bayesian logic program consists of two components. The first component is a logical one; it consists of a set of Bayesian Clauses, it encodes the quantitative information about the domain.",2025-11-18T17:20:54.000639,,,
What are Bayesian Networks (BN) ?,,easy,ml,machine_learning,165_ML_Interview_QA,Bayesian Network is used to represent the graphical model for the probability relationship among a set of variables.,,,,,
Why instance-based learning algorithm sometimes referred to as a Lazy learning algorithm?,,medium,ml,machine_learning,165_ML_Interview_QA,Instance-based learning algorithm is also referred to as the Lazy learning algorithm as they delay the induction or generalization process until classification is performed.,,,,,
What are the two classification methods that SVM ( Support Vector Machine) can handle?,,easy,ml,machine_learning,165_ML_Interview_QA,Combining binary classifiers Modifying binary to incorporate multiclass learning,,,,,
What is ensemble learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"To solve a particular computational program,2025-11-18T17:20:54.000668,,,,
Why ensemble learning is used?,,easy,ml,machine_learning,165_ML_Interview_QA,"Ensemble learning is used to improve the classification, function approximation, etc of a model.",2025-11-18T17:20:54.000674,,
When to use ensemble learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Ensemble learning is used when you build component classifiers that are more accurate and independent from each other.,,,,,
What are the two paradigms of ensemble methods?,,easy,ml,machine_learning,165_ML_Interview_QA,The two paradigms of ensemble methods are Sequential ensemble methods Parallel ensemble methods,,,,,
What is the general principle of an ensemble method and what is bagging and boosting in the ensemble method?,,hard,ml,machine_learning,165_ML_Interview_QA,The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm to improve robustness over a single model. Bagging is a method in an ensemble for improving unstable estimation or classification schemes. While boosting methods are used sequentially to reduce the bias of the combined model. Boosting and Bagging both can reduce errors by reducing the variance term.,,,,,
What is a bias-variance decomposition of classification error in the ensemble method?,,hard,ml,machine_learning,165_ML_Interview_QA,The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets.,,,,,
What is an Incremental Learning algorithm in the ensemble?,,medium,ml,machine_learning,165_ML_Interview_QA,Incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from the already available dataset.,,,,,
"What are PCA, KPCA, and ICA used for?",,medium,ml,machine_learning,"PCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis), and ICA ( Independent Component Analysis) are important feature extraction techniques used for dimensionality reduction.",2025-11-18T17:20:54.000737,
What is dimension reduction in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"In Machine Learning and statistics,2025-11-18T17:20:54.000746,,,,
What are support vector machines?,,easy,ml,machine_learning,165_ML_Interview_QA,Support vector machines are supervised learning algorithms used for classification and regression analysis.,,,,,
What are the components of relational evaluation techniques?,,medium,ml,machine_learning,165_ML_Interview_QA,The important components of relational evaluation techniques are Data Acquisition Ground Truth Acquisition Cross-Validation Technique Query Type Scoring Metric Significance Test,,,,,
What are the different methods for Sequential Supervised Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different methods to solve Sequential Supervised Learning problems are Sliding-window methods Recurrent sliding windows Hidden Markow models Maximum entropy Markow models Conditional random fields Graph transformer networks,,,,,
What are the areas in robotics and information processing where sequential prediction problem arises?,,medium,ml,machine_learning,165_ML_Interview_QA,The areas in robotics and information processing where sequential prediction problem arises are: Imitation Learning Structured prediction Model-based reinforcement learning,,,,,
What is batch statistical learning?,,hard,ml,machine_learning,165_ML_Interview_QA,Statistical learning techniques allow learning a function or predictor from a set of observed data that can make predictions about unseen or future data. These techniques provide guarantees on the performance of the learned predictor on the future unseen data based on a statistical assumption on the data generating process.,,,,,
What are the different categories you can categorize the sequence learning process?,,easy,ml,machine_learning,165_ML_Interview_QA,Sequence prediction Sequence generation Sequence recognition Sequential decision,,,,,
What is sequence learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Sequence learning is a method of teaching and learning in a logical manner.,,,,,
What are two techniques of Machine Learning ?,,hard,ml,machine_learning,165_ML_Interview_QA,The two techniques of Machine Learning are Genetic Programming Inductive Learning Here is one more explanation of Overfitting and Underfitting for you. https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171229/overfitting_1.png Overfitting – High variance and low bias Techniques to reduce overfitting: Increase the training data. Reduce model complexity. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop,,,,,
What is SQL?,,medium,sql,sql_database,SQL_Interview_QA,SQL is Structured Query Language designed specifically for communicating with databases (for inserting and modifying in a relational database management system). SQL is an ANSI (American National Standards Institute) standard.,,,,,
What are the Advantages of SQL?,,medium,sql,sql_database,SQL_Interview_QA,"SQL is not a proprietary language used by specific database vendors. Almost every major DBMS supports SQL, SQL, MYSQL etc. SQL is easy to learn. The statements are all made up of descriptive English words, and there aren't that many of them. SQL is actually an immensely powerful language and by using its language elements you can perform very complex and sophisticated database operations",2025-11-18T17:20:54.032715,
What is a field in a database?,,easy,sql,sql_database,SQL_Interview_QA,"A field is an area within a record reserved for a specific piece of data. Examples: Employee Name,2025-11-18T17:20:54.032738,,,,
What is a database transaction?,,hard,sql,sql_database,SQL_Interview_QA,"A database transaction takes the database from one consistent state to another. At the end of the transaction the system must be in the prior state if transaction fails or the status of the system should reflect the successful completion if the transaction goes through. A Database Transaction is a set of database operations that must be treated as whole,2025-11-18T17:20:54.032793,,,,
What are the properties of a transaction?,,hard,sql,sql_database,SQL_Interview_QA,"Properties of the transaction can be summarized as ACID Properties. Atomicity A transaction consists of many steps. When all the steps in a transaction are completed, all the transactions are rolled back. Consistency The database will move from one consistent state to another, if the transaction succeeds and remain in the original state if the transaction fails. Isolation Every transaction should operate as if it is the only transaction in the sy",2025-11-18T17:20:54.032819,,
What is SQL Order of Execution?,,medium,sql,sql_database,SQL_Interview_QA,About 90% of data analysts will get the SQL order of execution WRONG The correct order is: FROM – Define which tables you’ll source data from WHERE – Apply filters to your base data GROUP BY – Aggregate your data HAVING – Filter the aggregated data SELECT – Display the final data ORDER BY – Sort data for easy viewing LIMIT – Restrict the number of results Why is SELECT not first in the order of execution you ask? Because it is different from the order of writing! Our machines FIRST look for the ,,,,,
What is the difference between having and where clause?,,medium,sql,sql_database,SQL_Interview_QA,"Ans: HAVING is used to specify a condition for a group or an aggregate function used in select statement. The WHEREclause selects before grouping. The HAVINGclause selects rows after grouping. Unlike HAVINGclause,2025-11-18T17:20:54.032861,,,,
What is Join?,,medium,sql,sql_database,SQL_Interview_QA,"Ans: An SQL Join is used to combine data from two or more tables, consider the following two tables. Student Table EnrollNo StudentName Address 1000 geek1 geeksquiz1 1001 geek2 geeksquiz2 1002 geek3 geeksquiz3 StudentCourse Table CourseID EnrollNo 1 1000 2 1000 3 1000 1 1002 2 1003 The following is a join query that shows names of students enrolled in different courseIDs. SELECT StudentCourse.CourseID, Student.StudentName FROM StudentCourse INNE",2025-11-18T17:20:54.032903,,
What is a view in SQL? How to create one,,easy,sql,sql_database,SQL_Interview_QA,Ans: A view is a virtual table based on the result-set of an SQL statement. We can create using create view syntax. CREATE VIEW view_name AS SELECT column_name(s) FROM table_name WHERE condition,,,,,
What are the uses of a view? (Level: Intermediate),,medium,sql,sql_database,SQL_Interview_QA,"Views can represent a subset of the data contained in a table; consequently, while denied access to the rest of the base table. Views can join and simplify multiple tables into a single virtual table. Views can act as aggregated tables, where the database engine aggregates data (sum, average etc.) and presents the calculated results as part of the data Views ca",2025-11-18T17:20:54.032941,
What are Primary Keys and Foreign Keys? (Level: Beginner),,medium,sql,sql_database,SQL_Interview_QA,"Ans: Primary keys are the unique identifiers for each row. They must contain unique values and cannot be null. Due to their importance in relational databases,2025-11-18T17:20:54.032958,,,,
What are the different types of SQL or different commands in SQL?,,medium,sql,sql_database,SQL_Interview_QA,"DDL– Data Definition Language. DDL is used to define the structure that holds the data. DML– Data Manipulation Language. DML is used for manipulation of the data itself. Typical operations are Insert, Update and retrieving the data from the table. DCL–Data Control Language. DCL is used to control the visibility of data like granting database access and set privileges to create tables etc. TCL -Transaction Control Language. TCL commands are basically used for managing and controlling the ",2025-11-18T17:20:54.032986,,,
In SQL interviews you will often be asked what constraint commands are,,medium,sql,sql_database,SQL_Interview_QA,Here's the short answer for you: Constraints are rules that limit the types of data that can go into a table. The most important commands are: NOT NULL - Ensures that a column cannot have a NULL value UNIQUE - Ensures that all values in a column are different PRIMARY KEY - A combination of a NOT NULL and UNIQUE. FOREIGN KEY - Prevents actions that would destroy links between tables CHECK - Ensures that the values in a column satisfies a specific condition DEFAULT - Sets a default value for a col,,,,,
"What is a foreign key, and what is it used for?",,medium,sql,sql_database,SQL_Interview_QA, a foreign key is a column (or columns) appearing in one relation that is (are) the primary key of another table. Although there may be exceptions, the values in the foreign key columns usually must correspond to values existing in the set of primary key values. This correspondence requirement is created in a database using a referential integrity constraint on the foreign key.",2025-11-18T17:20:54.033033,,
What is Aggregate Functions?,,medium,sql,sql_database,SQL_Interview_QA,"Aggregate functions perform a calculation on a set of values and return a single value. Aggregate functions ignore NULL values except COUNT function. HAVING clause is used, for filtering query using aggregate values. Following functions are aggregate functions. AVG, MIN, CHECKSUM_AGG, SUM, COUNT
What is CTE?,,medium,sql,sql_database,SQL_Interview_QA,CTE is an abbreviation Common Table Expression. A Common Table Expression (CTE) is an expression that can be thought of as a temporary result set which is defined within the execution of a single SQL statement. A CTE is similar to a derived table in that it is not stored as an object and lasts only for the duration of the query.,,,,,
What is PRIMARY KEY?,,medium,sql,sql_database,SQL_Interview_QA,A PRIMARY KEY constraint is a unique identifier for a row within a database table. Every table should have a primary key constraint to uniquely identify each row and only one primary key constraint can be created for each table. The primary key constraints are used to enforce entity integrity.,,,,,
What is UNIQUE KEY constraint?,,medium,sql,sql_database,SQL_Interview_QA,"A UNIQUE constraint enforces the uniqueness of the values in a set of columns,2025-11-18T17:20:54.033103,,,,
What is FOREIGN KEY?,,medium,sql,sql_database,SQL_Interview_QA,A FOREIGN KEY constraint prevents any actions that would destroy links between tables with the corresponding data values. A foreign key in one table points to a primary key in another table. Foreign keys prevent actions that would leave rows with foreign key values when there are no primary keys with that value. The foreign key constraints are used to enforce referential integrity.,,,,,
What is CHECK Constraint?,,easy,sql,sql_database,SQL_Interview_QA,A CHECK constraint is used to limit the values that can be placed in a column. The check constraints are used to enforce domain integrity.,,,,,
What is NOT NULL Constraint?,,easy,sql,sql_database,SQL_Interview_QA,"A NOT NULL constraint enforces that the column will not accept null values. The not null constraints are used to enforce domain integrity,2025-11-18T17:20:54.033134,,,,
"Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success?",,hard,case,data_science,DS_Interview_Notebook, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction. * One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data! * First step will be to do EDA and understand our data and intesity of class inbalance. * In order to handle inbalance data prob"
Explain inner working on linear regression,,easy,ml,data_science,DS_Interview_Notebook,,,,,,
What are the assumptions for linear regression,,hard,ml,data_science,DS_Interview_Notebook,"Linear regression assumptions are as below * Data should have linear relationship between X and Y (actually mean of Y) * Data should be normally distributed * No or little multicollinearity (observations should be independent of each other) * Assumption of additivity: This means that each feature (X) should affect the target (Y) independently. In other words,2025-11-18T17:20:54.033787,,,,
How can AI be used in spam email detection?,,hard,mixed,data_science,DS_Interview_Notebook,"AI, analyzes the content of emails to determine if they are spam. It does this by identifying patterns, keywords, and stylistic choices that are common in spam messages. Here's a simplified breakdown of the process: **Steps in AI-Based Spam Detection** 1. **Data Collection:** * A large dataset of emails, both spam and legitimate (""ham""), is gathered. 2. **Preprocessing:** * Emails are cleaned and standardized. * This often involves: * Removing punctuation and"
How to build sentiment analysis model from scratch?,,hard,ml,data_science,DS_Interview_Notebook,"**1. Data Gathering:** * Collect a set of text data (e.g., tweets, product feedback) where each piece of text is labeled with its corresponding sentiment (positive, negative, or neutral). **2. Text Preprocessing:** * Clean and standardize the text: * Remove punctuation, special characters
When to use tokenization and stemming/Lemmatization?,,hard,mixed,data_science,DS_Interview_Notebook,"* **Tokenization:** * **Always** use tokenization as the first step in any NLP task that involves analyzing the text at the word level. * It's fundamental for breaking down sentences into individual words or subwords,2025-11-18T17:20:54.034056,,,,
What are the advantages and disadvantages of neural networks?,,hard,ml,data_science,DS_Interview_Notebook,"**Here are some advantages of Neural Networks** * Storing information on the entire network: Information such as in traditional programming is stored on the entire network, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missi",2025-11-18T17:20:54.034124,,,
What is the difference between bias and variance?,,hard,mixed,data_science,DS_Interview_Notebook,"* Bias comes from model underfitting some set of data,2025-11-18T17:20:54.034160,,,,
What is bias-variance tradeoff,,hard,mixed,data_science,DS_Interview_Notebook,"* As we increase the complexity of the model, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance. * The goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is referred as bias-variance tradeoff. We can acheive bias-variance tradeoff by selecting optimum model complexity. <img src="" width=""500"" ",2025-11-18T17:20:54.034226,,,
What is more important model accuracy or model performance?,,hard,ml,data_science,DS_Interview_Notebook,"* Short answer is: Model accuracy matters the most! inaccurate information is not usefull. * Model performance can be improved by increasing the compute resources. * Model accuracy and performance can be subjective to the problem in hand. For example, the accuracy extremely critical, even if the models would take minutes or hours to make a prediction. * Some applications require real time performance, even if this",2025-11-18T17:20:54.034258,
What is the difference between machine learning and deep learning?,,hard,ml,data_science,DS_Interview_Notebook,"Deep Learning out performs traditional ML techniques if the data size is large. But with small data size, natural language processing, and speech recognition. Few important differences are as below, |Machine Learning|Deep Learning| |:-|:-| | Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on wha"
Explain standard deviation and variance,,hard,mixed,data_science,DS_Interview_Notebook,"![normal-distrubution]( **Variance:** * **In a Nutshell:** Variance measures how spread out a set of data is. A high variance indicates that the data points are widely scattered, 70, 75, 80, 90 * Group B: Scores are 40, 70
Explain confusion matrix,,hard,mixed,data_science,DS_Interview_Notebook,"![]( **What is a Confusion Matrix?** * Imagine a table that helps you see how well your machine learning model is performing, true labels of your data. **Why is it useful?** * Beyond Accuracy: A confusion matrix gives you a more detailed look at your model's performance than just overall accuracy. * Spotting Weaknesses: You can see which classes your m",2025-11-18T17:20:54.034499,,,
Why do we need confusion matrix?,,hard,mixed,data_science,DS_Interview_Notebook,"* We can not rely on a single value of accuracy in classification when the classes are imbalanced. * For example, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%. * Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. * Confusion matrices are useful becaus",2025-11-18T17:20:54.034521
Explain collinearity and technique to reduce it?,,hard,mixed,data_science,DS_Interview_Notebook,"In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other. ## Technique to reduce multicollearity * **Remove highly correlated predictors from the model**. If you have two or more factors with a high collinearity, removing one of the correlated factors usually doesn't drastically reduce the R-squared",2025-11-18T17:20:54.034545,,,
Difference between statistics and machine learning,,hard,ml,data_science,DS_Interview_Notebook,"* The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables. * Statistics is mathematical study of data. Lots of statistical models that can make predictions,2025-11-18T17:20:54.034558,,,,
"In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?",,hard,stats,data_science,"To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. ```Z score= (X- mean)/Standard Deviation``` Melissa's Z score = (90-75)/10 = 1.5 Ryan's Z score = (90-80)/12 = 0.83 Melissa has performed better.",2025-11-18T17:20:54.034581,,
What is null hypothesis and alternate hypothesis?,,hard,stats,data_science,DS_Interview_Notebook,"* The null hypothesis states that a population parameter (such as the mean, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge. * The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true. * So when r",2025-11-18T17:20:54.034604,
What is a hypothesis test and p-value?,,hard,stats,data_science,DS_Interview_Notebook,"* A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of ""no effect"" or ""no difference"". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data. * Based on the sample data, to make t",2025-11-18T17:20:54.034660,,,
What is power of hypothesis test? Why is it important?,,hard,stats,data_science,DS_Interview_Notebook,"* Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, the better our hypothesis test is.",2025-11-18T17:20:54.034675,,,
What is the difference betweeen K nearest neighbors and K means,,hard,mixed,data_science,DS_Interview_Notebook,"* KNN or K nearest neighbor is a classification algorithm, K means is unsupervised algorithm. * In KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K referes to the number of neighbors with whom similarity is being compared. * K-means is the process of defining clusters or groups around predefined cent",2025-11-18T17:20:54.034696,,,
Explain Random forest algorithm,,hard,ml,data_science,DS_Interview_Notebook,"* Random forest is supervised learning algorithm and can be used to solve classification and regression problems. * Since decision-tree create only one tree to fit the dataset,2025-11-18T17:20:54.034723,,,,
Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?,,medium,ml,data_science,DS_Interview_Notebook,"* Yes, and the regression model refers to the numeric or continuous dependent variable.",2025-11-18T17:20:54.034734,,,
What do you mean by Bagging?,,hard,mixed,data_science,DS_Interview_Notebook,"![EnsembleI_Learning_Bagging]( * In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions. * Bagging is a short form of **Bootstrap Aggregating**. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms. * Since multiple model predictions are averaged together to form the final predictions,2025-11-18T17:20:54.034798,,,,
What is Out-of-Bag Error in Random Forests?,,hard,mixed,data_science,DS_Interview_Notebook,"* Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oob_score = True' then Out-of-Bag score will be calculated for every decision tree. * Finally,2025-11-18T17:20:54.034816,,,,
What is the use of proximity matrix in the random forest algorithm?,,easy,ml,data_science,DS_Interview_Notebook,A proximity matrix is used for the following cases : * Missing value imputation * Detection of outliers,,,,,
List down the parameters used to fine-tune the Random Forest.,,medium,mixed,data_science,DS_Interview_Notebook,Two parameters that have to fine-tune to improve the predictions that are important in the random forest algorithm are as follows: * Number of trees used in the forest (n_tree) * Number of random variables used in each of the trees in the forest (mtry),,,,,
What is K Fold cross validation? Why do you use it?,,hard,mixed,data_science,DS_Interview_Notebook,"* In case of K Fold cross validation input data is divided into ‘K’ number of folds, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method. * This significantly reduce",2025-11-18T17:20:54.034865,,
How to handle missing data?,,medium,case,data_science,DS_Interview_Notebook,"Data can be missing because of mannual error or can be gennualy missing. * Delete low quality records completely which have too much missing data * Impute the values by educated guess,2025-11-18T17:20:54.034877,,,,
What is the difference between Bar graph and histogram?,,medium,mixed,data_science,DS_Interview_Notebook,* Bar graph is used for descreate data where as histogram is used for continuous data. * In bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale). * In bar graph the order of the bars can be changed and in histogram order remains same.,,,,,
What is the Box and Whisker plot? When should use it?,,hard,mixed,data_science,DS_Interview_Notebook,"* Box and whisker plots are ideal for comparing distributions because the centre, first quartile (Q1), median, third quartile (Q3), and “maximum”). * median (Q2/50th Percentile): the middle v",2025-11-18T17:20:54.034926
What is outlier? How to handle them?,,hard,mixed,data_science,DS_Interview_Notebook,"* An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. * Data points above and below 1.5*IQR,2025-11-18T17:20:54.035002,,,,
"If deleting outliers is not an option, how will you handle them?",,hard,mixed,data_science,DS_Interview_Notebook, can be fit by non-linear model. * Try normalizing the data, this way the extreame datapoints are pulled to the similar range. * We can use algorithms which are less affected by outliers. * We can also create separate model to handle the outlier data points.",2025-11-18T17:20:54.035017,,
You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?,,hard,ml,data_science,DS_Interview_Notebook,"* First of all model performace is not directly proportional to the number of predictors, MAE, R-squared, Adjusted R-squared, and RMSE. * MSE penalizes large errors, "
Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.,,hard,coding,data_science,DS_Interview_Notebook,"* This is quadratic equation, when a < 0, then function has maximum value) * To find the slope of the function, lets take derivative of it f'(x)= -8x + 4 * At maximum point, slope will be 0 -8x + 4 = 0 x = 0.5 * Now lets put 0.5 in equation to find the maximum values f(0,5) = -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14 * This functiona will have concave shape. So the maximum point is (0.5
Below is the output of a correlation matrix from your Exploratory data. Is using all the features in a model appropriate for predicting/inferencing Y?,,hard,ml,data_science,DS_Interview_Notebook,"![]( * We can see from above correlation matrix that there is high correlation(.98) between X1 and X2, similarly there is high correlation(.75) between X2 and X3 * All the variables are correlated to each other. In regression this would result in multicollinearity. We can try methods such as dimension reduction, feature selection, stepwise regression to choose the correct input variables for predictiong Y * Second part of question is - should we use ",2025-11-18T17:20:54.035114,
What is stepwise regression?,,hard,ml,data_science,DS_Interview_Notebook,"Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, then step by step we reduce the regressor variables and find the model with the least",2025-11-18T17:20:54.035140,,,
You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket.,,hard,mixed,data_science,DS_Interview_Notebook,"* Questions like this will test your out of the box thinking * Step1: Fill 5 lts bucket and empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket. * Step2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it. * Now fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it,2025-11-18T17:20:54.035166,,,,
Lis the differences between supervised and unsupervised learning,,hard,ml,data_science,DS_Interview_Notebook,"| Supervised learning | Unsupervised leanring |:- |:- Uses labeled data as input | Uses unlabeled data as input Supervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism Common supervised learning algorithms are decision tree, support vector machine etc | K Means clustering, hierarchical clustering etc Reference: ",2025-11-18T17:20:54.035180,,
Explain the steps in making decision tree?,,hard,mixed,data_science,DS_Interview_Notebook,![]( Below are the common steps in decision tree algorithm * Take the entire data as input * At the root node decision tree selects feature to split the data in two major categories. * Different criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems. * Features are selected for spliting based on highest information gain. * After every split we get decision rules and sub trees. * This process will c,,,,,
How do you build random forest model?,,hard,ml,data_science,DS_Interview_Notebook,Ranodm forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. ![]( * Select few random sub sample from given dataset * Construct a decision tree for every sub sample and predict the result. * Perform the voting on prediction from each tree. * At the end select the most voted result as final prediction. * Reference: ,,,,,
How do Random Forest handle missing data?,,hard,case,data_science,DS_Interview_Notebook,"Random Forests inherently have two primary ways of handling missing data: 1. **During Training (Building the Trees):** * **For Numerical Features:** Missing values can be imputed using simple strategies like mean or median. * **For Categorical Features:** A new ""missing"" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process. 2. **During Prediction (Making New Predictions):** * **""Sur",,,,,
What is model overfitting? How can you avoid it?,,hard,ml,data_science,DS_Interview_Notebook,"Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, reducing the number of features will help. We can manually",2025-11-18T17:20:54.035337,,,
There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?,,hard,mixed,data_science,DS_Interview_Notebook,"To find the heavier ball among 9 balls using a balance scale, Group B, and Group C. - Weigh Group A against Group B. 2. **Analyzing the First Weighing**: - **Case 1**: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C. - *",2025-11-18T17:20:54.035383
"Difference between univariate, bivariate and multivariate analysis?",,easy,mixed,data_science,DS_Interview_Notebook,2025-11-18T17:20:54.035397,,,,
What are feature selection methods to select right variables?,,hard,mixed,data_science,DS_Interview_Notebook,"Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out. ## Filter Methods * Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable,2025-11-18T17:20:54.035473,,,,
"In you choice of langauge: Write a program that prints the numbers from 1 to 50. But for multiples of three print ""Fizz"" instaed of the number and for the multiples of five print ""Buzz"". For the numbers which are multiples of both three and five print ""FizzBuzz"".",,easy,coding,data_science,DS_Interview_Notebook,,,,,,
You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?,,hard,case,data_science,DS_Interview_Notebook,"* There are multiple ways to handle missing values in the data * If dataset is huge we can simply remove the rows containing the missing data * If dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean,2025-11-18T17:20:54.035505,,,,
"For the given point how will you caluclate the Euclidean distance, in Python?",,easy,mixed,data_science,DS_Interview_Notebook,2025-11-18T17:20:54.035518,,,,
What is the angle between the hour and minute hands of clock when the time is half past six?,,easy,mixed,data_science,DS_Interview_Notebook,![Clock_Puzzle]( Reference: ,,,,,
How should you maintain your deployed model?,,hard,ml,data_science,DS_Interview_Notebook,### Monitor Constant monitoring of all the models is needed to determine the performance accuracy of the models ### Evaluate Evaluation metric of the current model is calculated to determine if new algorithm is needed. ### Compare The new models are compared against each other to determine which model performs the best. ### Rebuild The best performing model is re-built on the current set of data. Reference: ,,,,,
What are recommender systems?,,hard,mixed,data_science,DS_Interview_Notebook,* The purpose of a recommender system is to suggest relevant items or services to users. * Two major categories of recommender systems are collaboarative filtering and cotent based filtering methods ### Collaborative Filtering * It is based on the past interactions recorded between users and items in order to produce new recommendations. * e.g. Music service recommends track that are often played by other users with similar interests ### Content Based Filtering * Unlike collaborative methods tha,,,,,
"'People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?",,medium,ml,data_science,DS_Interview_Notebook,2025-11-18T17:20:54.035585,,,,
"If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?",,hard,stats,data_science,"* Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events. * Trick here is to know the probability of not raining on Saturday and Sunday. * If we subtract the intersection(∩) of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend. ``` = Total probability - (Probability that it will not rain on Saturday) ∩ (Probability that it will not rain ",2025-11-18T17:20:54.035610,,
How can you select K for K-Means?,,hard,mixed,data_science,DS_Interview_Notebook,"There are two ways to select the number of clusters in case K-Means clustering algorithm ### Visualization * To find the number of clusters manually by data visualization is one of the most common method. * Domain knowledge and proper understanding of given data also help to make more informed decisions. * Since its manual exercise there is always a scope for ambiguous observations,2025-11-18T17:20:54.035647,,,,
"Explain dimensionality reduction, and its benefits?",,hard,mixed,data_science,DS_Interview_Notebook,2025-11-18T17:20:54.035665,,,,
How can you say that the time series data is stationary?,,hard,case,data_science,DS_Interview_Notebook,"For accurate analysis and forecasting, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time. ![Stationarity]( Reference: ",2025-11-18T17:20:54.035684,,,
How can you calculate the accuracy using confusion matrix?,,easy,mixed,data_science,DS_Interview_Notebook,Accuracy = (True Positive + true Negative) / Total Obervations,,,,,
Write the equations for the precision and recall?,,easy,mixed,data_science,DS_Interview_Notebook,Precision = True Positive / (True Positive + False Positive) Recall = True Positive /(Total Positive + False Negative),,,,,
"If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?",,hard,mixed,DS_Interview_Notebook,"* There are three colors of socks- Red, Blue and White. No of socks is irrelevant here. * Suppose in our first pull we picked Red color sock * In second pull we picked Blue color sock * And in third pull we picked White color sock. * Now in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4! * Reference: "
Write a SQL query to list all orders with customer information,,easy,mixed,data_science,DS_Interview_Notebook,![SQL_Join](,,,,,
Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?,,medium,ml,data_science,DS_Interview_Notebook,``` - K-means clustering - Linear regression - K-NN - Decision tress ``` Using KNN we can compute the missing variable value by using the nearest neighbors.,,,,,
"Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes? Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out",,medium,mixed,data_science,"* We have two ropes A and B * Ligt A from both the end and B from one end * When A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining * Now light the other end of B also, it will now burnout in 15 minutes * This we got 30 + 15 = 45 minutes * Reference: ",2025-11-18T17:20:54.035756,,
"After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?",,hard,ml,data_science,DS_Interview_Notebook, meaning it doesn't make any assumptions about the underlying distribution of your data. This i",2025-11-18T17:20:54.035790,,,
Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?,,hard,mixed,data_science,DS_Interview_Notebook,"In this scenario, group A with coupon type 1, group B with coupon type 2). This controlled experiment ensures that any differences in purchase behavior can be directly attributed to the presence and type of coupon. * **Measures Impact:** Y",2025-11-18T17:20:54.035870,,
Explain feature scaling,,hard,mixed,data_science,DS_Interview_Notebook,"* Feature scaling is one of the most important data preprocessing step in machine learning * If we are **changing the range of the features then its called 'scaling'** and if we are **changing the distribution of the features then its called 'normalization/standardization'** ## Scaling * This means that you're transforming your data so that it fits within a specific scale, you can help compare different variables on equal footing. * Scaling is requir",2025-11-18T17:20:54.035957,,,
Difference between standardisation and normalization?,,hard,mixed,data_science,DS_Interview_Notebook,"**Standardization** * **What it does:** * Centers the data around zero (mean = 0) * Scales the data to have a standard deviation of one (std = 1) * **Transformation:** * `Z = (X - mean) / std_dev` * **When to use it:** * Algorithms that are sensitive to the scale of features (e.g., logistic regression, support vector machines). * When you assume your data follows a normal (Gaussian) distribution (though not strictly required). * When outliers are present, as standardization is",2025-11-18T17:20:54.036008,
What is meant by Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"* Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting. * In Machine learning,2025-11-18T17:20:54.036101,,,,
How to detect Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"* Results are too good too true * In general,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out. * At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. * So, it is advised that before the testing, the prior documented results are weighed against the expected results. "
How to fix the problem of Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage: * Select the features such a way that they do not contain information about the target variable, we should try to set aside a validation set in addition to training and test sets if possible. * The purpose",2025-11-18T17:20:54.036211,,,
Explain normal distribution of data,,hard,stats,data_science,DS_Interview_Notebook,"Data can be distributed (spread out) in different ways, and it gets close to a ""Normal Distribution"" like this: ![]( * The Normal Distribution has: - mean = median = mode - symmetry about the center - 50% of values less than the mean and 50% greater than the mean Refe",2025-11-18T17:20:54.036273,,,
What does it mean when distribution is left skew or right skew?,,hard,stats,data_science,DS_Interview_Notebook,"In a **right-skewed** distribution, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards. In a **left-skewed** distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example cou",2025-11-18T17:20:54.036294
What does the distribution looks like for the average time spend watching youtube per day?,,hard,stats,data_science,DS_Interview_Notebook,"The distribution of average time spent watching YouTube per day is likely to be right-skewed. This means that most people watch YouTube for a relatively short amount of time each day, indicating the presence of these high-usage viewers",2025-11-18T17:20:54.036308,,,
Expalin covariance and correlation,,hard,mixed,data_science,DS_Interview_Notebook,"* Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables. * “Covariance” indicates the **direction of the linear relationship between variables**. “Correlation” on the other hand measures both the **strength and direction of the linear relationship between two variables**. * In case of High correlation,2025-11-18T17:20:54.036332,,,,
What is regularization. Why it is usefull?,,hard,mixed,data_science,DS_Interview_Notebook,* Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting. * The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values. * There are two types of regularization as follows: - L1 Regularization or Lasso Regularization. L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weight,,,,,
What are confouding varaiables?,,hard,mixed,data_science,DS_Interview_Notebook,"* In statistics,2025-11-18T17:20:54.036368,,,,
Explain ROC curve and AUC,,hard,mixed,data_science,DS_Interview_Notebook,"**ROC Curve (Receiver Operating Characteristic Curve)** and **AUC (Area Under the Curve)** are tools used to evaluate the performance of a classification model, such as ""spam"" and ""not spam."" ### ROC Curve: 1. **What is the ROC Curve?** - The ROC Curve is a graph that shows the trade-off between the **True Positive Rate (TPR)** and the **False Positive Rate (FPR)** of a model at various thresholds. - **True Positi",2025-11-18T17:20:54.036440,,,
Explain Precision-Recall Curve,,hard,mixed,data_science,DS_Interview_Notebook,"The **Precision-Recall Curve** is a tool used to evaluate the performance of a classification model, the precision is 8 out of 10, or 80%. 2. **Recall**: - Measures how well the model finds all the",2025-11-18T17:20:54.036502,,
What is TF-IDF?,,hard,mixed,data_science,DS_Interview_Notebook,**TF-IDF (Term Frequency-Inverse Document Frequency)** is a method used in text analysis to determine how important a word is in a specific document compared to a whole collection of documents (called a corpus). It helps in identifying words that are most relevant to the content of a document. ### Key Concepts: 1. **Term Frequency (TF)**: - This measures how often a word appears in a document. A higher term frequency means that the word is more significant within that document. - **Simple Exampl,,,,,
Python or R- which one would you prefer for text analytics?,,medium,mixed,data_science,DS_Interview_Notebook,We will prefer python for following reasons * We can use pandas library which has easy to use data structures and high performance data analysis tools * R is more suitable for ML than text analytics * Python is faster for all types of text analytics.,,,,,
What are Eigenvectors and Eigenvalues?,,hard,mixed,data_science,DS_Interview_Notebook,"* In linear algebra, when a linear transformation is applied to it, only changes in scale (gets stretched or shrunk) but not in direction. * The **eigenvalue** associated with that eigenvector is the factor by which it is scaled. **Why are they important?** Eigenvectors and eigenvalues reveal the underlying structure and behavior of linear transformations. They have numerous applications across various fields: * **Image compression:** Eigenvectors can",2025-11-18T17:20:54.036607,,
Explain the scenario where both false positive and false negative are equally important,,hard,mixed,data_science,DS_Interview_Notebook,"1. **Medical Diagnosis (e.g., invasive procedures, and potential side effects from treatment. * **False Negative:** A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis. 2. **Fraud Detection** * **False Positive:** A legitimate transaction is flagged as fraudulent. This inco",2025-11-18T17:20:54.036632,
Why feature scalling is required in Gradient Descent Based Algorithms,,hard,ml,data_science,DS_Interview_Notebook,"* Machine learning algorithms like linear regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below: ![Gradient descent formula]( * The presence of feature value X in the formula will affect the step size of the gradient descent. * The difference in ranges of features will cause different step sizes for each feature. * To ensure that the gradient descent moves smoot",2025-11-18T17:20:54.036660,,
Why feature scaling not required in tree based algorithms,,hard,ml,data_science,DS_Interview_Notebook,"Imagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light). * **Tree-based algorithms work like this:** They make decisions based on *thresholds* or *cut-offs* for each feature (like color or weight). They ask questions like: ""Is this fruit red?"" or ""Is this fruit heavier than 1 pound?"". * **Feature scaling doesn't matter here:** * **Color:** It doesn't matter if we represent ""red"" as the number 1 and ""not r",,,,,
"Explain the difference between train, validation and test set",,medium,stats,data_science,DS_Interview_Notebook,2025-11-18T17:20:54.036773,,,,
What is Naive Bayes algorithm?,,hard,ml,data_science,DS_Interview_Notebook,"The Naive Bayes algorithm is a simple but surprisingly effective classification algorithm in machine learning. It's based on Bayes' Theorem, ""If an email *is* spam, how likely is it to contain these words?"" It then does the same for non-spam emails. Finally, it compares these probabilities to make its best guess about ",2025-11-18T17:20:54.036819,
What is the difference between MLOps and DevOps?,,hard,mixed,data_science,DS_Interview_Notebook,"* MLOps & DevOps have a lot of things in common. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code. * Reference: ",2025-11-18T17:20:54.036837,,,
What are the risks associated with Data Science & how MLOps can overcome the same?,,hard,case,data_science,DS_Interview_Notebook,"In Data Science, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns
What are the differences between XGBoost and Random Forest Model,,hard,ml,data_science,DS_Interview_Notebook,"| Feature | XGBoost | Random Forest | |------------------------------|----------------------------------------------|---------------------------------------------| | **Technique** | Boosting Technique: Builds trees sequentially,2025-11-18T17:20:54.036909,,,,
Please explain p-value to someone non-technical,,hard,mixed,data_science,DS_Interview_Notebook,"A p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance. Imagine you're playing a game of chance, so you decide to test it. **The ""normal"" assumption (null hypothesis):** The coin is fair, and there's a 50/50 chance of getting heads or tails. **Your experiment:** You flip the coin 100 times and get 60 heads. Hm",2025-11-18T17:20:54.036966,,
"Average comments per month has dropped over three-month period, despite consistent growth after a new launch. What metric would u investigate?",,hard,mixed,data_science,DS_Interview_Notebook, I would examine:** 1. **Engagement per User**: Assess if the average number of comments per active user has decreased. Even with user growth, fewer comments per user could explain the overall decline. 2. **Content Type Analysis**: Identify if the types of content driving growth are different from those that typically generate comments. Growth could be from content that attracts more views or rea",2025-11-18T17:20:54.037005,,
A PM tells you that a weekly active user metric is up by 5% but email notification open rate is down by 2%. WHat would you investigate to dignose this problem?,,hard,mixed,data_science,DS_Interview_Notebook,"Email open rate is calculated by dividing the number of emails opened by the number of emails sent minus any bounces. A good open rate is between 17-28%2. Email notification open rate is a type of email open rate that measures how many users open an email that notifies them about something. Weekly active user metric (WAU) is a measure of how many users are active on a website or app in a given week. It can be influenced by many factors, retention, engagement and churn. ",2025-11-18T17:20:54.037046,,
Explain data drift problem in machine learning,,hard,ml,data_science,DS_Interview_Notebook,**Data drift** is a common problem in machine learning that occurs when the statistical properties of the input data change over time. This change can lead to a decrease in the performance of machine learning models because the model is no longer receiving the same kind of data it was trained on. ### Key Points to Explain Data Drift: 1. **Definition of Data Drift**: - **Data drift** refers to any change in the distribution of data that a machine learning model was trained on compared to the data,,,,,
Explain transformer architecture,,hard,mixed,data_science,DS_Interview_Notebook,"The Transformer architecture is a neural network model designed for natural language processing tasks, summarization, and text generation. It was introduced in the paper ""Attention is All You Need"" by Google in 2017 and has since become the foundation for many advanced NLP models, including BERT and GPT. Key Components of the Transformer: **1. Self-Attention Mechanism:** The core innovation of the Transformer is its self-attention mechanism. This mechanism allows the model to w",2025-11-18T17:20:54.037226,
Explain the difference between prediction and forecasting,,hard,mixed,data_science,DS_Interview_Notebook,"* If you think of it like a detective story, prediction might be used to identify any current anomalies or issues in our system based on the data we're receiving. Foreca",2025-11-18T17:20:54.037256,,,
References,,easy,mixed,data_science,DS_Interview_Notebook,*  *  *  *  *  *  * ,,,,,
What is padding,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Sigmoid Vs Softmax,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is PoS Tagging,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is tokenization,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is topic modeling,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is back propagation,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the idea behind GANs,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the Computational Graph,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is sigmoid What does it do,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is Named-Entity Recognition,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain the masked language model,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How do you preprocess text in NLP,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How do you extract features in NLP,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How is wordvec different from Glove,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What Are the Different Layers on CNN,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What makes CNNs translation invariant,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How is fastText different from wordvec,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is backward and forward propagation,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are Syntactic and Semantic Analysis,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is a local optimumWhat is a local optimum,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain gates used in LSTM with their functions,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is ReLU How is it better than sigmoid or tanh,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is transfer learning have you used it before,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is multi-task learning When should it be used,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Difference between convex and non-convex cost function,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why do we remove stop words When do we not remove them,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain the difference between an epoch a batch and an iteration,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the difference between NLP and NLU,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
For online learning which one would you prefer SGD or Adagrad and why,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What Is a Multi-layer Perceptron MLPWhat Is a Multi-layer Perceptron MLP,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Is it always bad to have local optimaIs it always bad to have local optima,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"In node2vec, what does embedding represent topological similarity or nearness",,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,2025-11-19T09:28:09.061331,,,,
What do you understand by Boltzmann Machine and Restricted Boltzmann Machines,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to compute an inverse matrix faster by playing around with some computational tricks,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
For infrequent/rare words which among CBOW and SkipGram should be used for wordvec training,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is pooling in CNN Why do we need it,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Describe the structure of Artificial Neural Networks & RNN(recurrent neural network),,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to Select a Batch Size Will selecting a batch size produce better or worse results?,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are N-grams How can we use them,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How large should be N for our bag of words when using N-grams,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How can you use neural nets for text classification and computer vision,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Do gradient descent methods always converge at the same point,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is gradient descent How does it work,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are autoencoders Explain the different layers of autoencoders and mention three practical usages of them,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is vanishing gradient descent,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
difference between Vanishing gradient Vs Exploding gradient,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to handle dying node problems in case of ReLU activation function,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the use of the leaky ReLU function,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are the different Deep Learning Frameworks,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is a dropout layer and how does it help a neural network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain why dropout in a neural network acts as a regularizer,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to know whether your model is suffering from the problem of Exploding Gradients,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to handle exploding gradient problem,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How Does an LSTM Network Work,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What problem does Bi-LSTM solve instead of only LSTM,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What happens to the predictions of a CNN if an image is rotated,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How does CNN help in translation and rotation invariance of images,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Define Term Freuency & Inverse Document Freuency Tf-idf and how to use it for converting text to vector,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are three primary convolutional neural network layers How are they commonly put together,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Describe the architecture of a typical Convolutional Neural Network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"What do you mean by Dropout and Batch Normalization, When and why use",,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,2025-11-19T09:28:09.061421,,,,
What is the difference between online and batch learning,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Is dropout used on the test set,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is an activation function and discuss the use of an activation function,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain three different types of activation functions,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the range of activation functions,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why is Rectified Linear Unit a good activation function,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why don't we use the Relu activation function in the output layer,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What can go wrong if we use a linear activation instead of ReLU,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"Give examples in which a many-to-one RNN architecture is appropriate, Give examples in which a many-to-one RNN architecture is appropriate",,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,2025-11-19T09:28:09.061449,,,,
What is RNN and How does an RNN work,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why Sigmoid or Tanh is not preferred to be used as the activation function in the hidden layer of the neural network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"difference between various Activation functions such as Sigmoid , tanh, Softmax, ReLU, Leaky ReLU",,medium,deep_learning,Sandy1811/DS-Interview-FAANG,,2025-11-19T09:28:09.061460,
Why Tanh activation function preferred over sigmoid,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are word embeddings Why are they useful,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
what is WordVec,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are some advantages of using character embeddings instead of word embeddings,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"How do you get sentence meanings from word embeddings, considering the position of words in the sentence",,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,2025-11-19T09:28:09.061474,,,,
Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is bag of words How we can use it for text vectorization,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the main difference between Adam and SGD,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are the advantages and disadvantages of SGD over gradient descent,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"What is the difference between stochastic gradient descent SGD and gradient descent GD, Batch gradient descent, Stochastic gradient descent, Mini-batch gradient descent , what are the pros and cons for each of them",,medium,deep_learning,Sandy1811/DS-Interview-FAANG,,2025-11-19T09:28:09.061496,
When would you use GD over SDG and vice-versa,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How would you choose the number of filters and the filter size at each CNN layer,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How can we use CNN for text classification,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are some advantages in using a CNN (convolutional neural network rather than a DNN (dense neural network in an image classification task,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Describe two ways to visualize features of a CNN in an image classification task,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why do segmentation CNNs typically have an encoder-decoder style / structure,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is a convolutional layer & Why do we actually need convolutions Can we use fully-connected layers for that,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are the advantages of parameter sharing in case of convolution,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why do we use convolutions for images rather than just Fully Connected layers,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why would you use many small convolutional kernels such as x rather than a few large onesWhy would you use many small convolutional kernels such as x rather than a few large ones,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why we generally use Softmax non-linearity function as the last operation in-network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How does BatchNormalization differ in training and inferencing,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How does batch size affect training of neural networks,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
"When using mini batch gradient descent, why is it important to shuffle the data",,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,2025-11-19T09:28:09.061543,,,,
Give a simple mathematical argument why a mini-batch version of such ML algorithm might be computationally more efficient than a training with full data set,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
On a simplified and fundamental scale what makes the newly developed BERT model better than traditional NLP models,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How would you initialize weights in a neural network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why weights are initialized with small random numbers in a neural network What happens when weights are all or constant values,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Suppose you have a NN with layers and ReLU activations What will happen if we initialize all the weights with the same value,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is backpropagation How does it work Why do we need it,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why large filter sizes in early layers can be a bad choice How to choose filter size,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
which one is more powerful a layer decision tree or a -layer neural network without any activation function --> Hint non-linearity,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Both decision trees and deep neural networks are non-linear classifier ie they separates the space by complicated decision boundary Why then it is so much easier for us to intuitively follow a decision tree model vs a deep neural network,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
If you could take advantage of multiple CPU cores would you prefer a boosted-tree algorithm over a random forest,,medium,ml,deep_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are MSE and RMSE,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Explain DBSCAN algorithm,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are dummy variables,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is anomaly detection,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is Bayesian inference,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the R-Suared value,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What about ordinal features,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Loss functions in regression,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Undersampling vs Oversampling,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is reinforcement learning,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Do we call Knn a lazy algorithm,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Define a Monte Carlo simulation,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Does Kmeans and Kmeans++ is same,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is pruning in Decision Tree,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How does an XGB control overfitting,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the class imbalance problem,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why is lightGBM prone to overfitting,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Name any one distance based algorithm,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the objective function for Knn,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the standard error of the mean,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are some disadvantages of K-means,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is data augmentation Give examples,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is Euclidean and Manhatten distance,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the role of gamma in RBF kernels,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How would you handle an imbalanced dataset,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Is it a good idea to combine multiple trees,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How do support vector machine algorithms work,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the significance of Residual Networks,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What does it mean to have low MAE and high MSE,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are the disadvantages of linear regression,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is a recommendation engine How does it work,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is K-means How can you select K for K-means,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Does Radial basis kernel function is there in SVM,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is linear regression Why is it called linear,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Is pruning always a good method to construct a tree,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the difference between bagging and boosting,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Which algorithm uses margin to classify the classes,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What algorithm can be used to summarize twitter feed,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How do you generate arbitrary or random shape clusters,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How to compute standard error of median in a simple way,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How does GBDTs decide to split a node What does it minimize,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the difference between R-suare and Adjusted R-suare,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
How is matrix factorization useful in recommendation systems,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are the approximation methods in Reinforcement Learning,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What is the difference between an error and a residual error,,easy,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Why does training an SVM takes a long time How can I speed up,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Difference between bagging boosting and the relation to bayes theorem,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
Which algorithm takes the data to the next dimension and then classify,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
What are categorical variables and what do we do with categorical variables,,medium,ml,machine_learning,Sandy1811/DS-Interview-FAANG,,,,,,
[Paste question here],,easy/medium/hard,coding/stats/ml/case/behavioral,,manual_templates/leetcode_manual_template.txt,,,,,,
[Question title/description] [Full question text if available],"[Company Name or ""General""]",easy/medium/hard,sql/python,,manual_templates/stratascratch_manual_template.txt,,,,,,
"Count the number of movies per genre Write a query to find the number of movies in each genre. Return the genre name and count, ordered by count descending.",Netflix,medium,sql,,manual_templates/stratascratch_manual_template.txt,2025-11-17T11:27:21.133229,,,,
You are given a train data set having 1000 columns and 1 million rows based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
You are given a data set on cancer detection. You've built a classification model and achieved an accuracy of 96%. Why shouldn't you be happy with your model performance? What can you do about it?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
You are working on a time series data set. You built a decision tree model but later tried a time series regression model and got higher accuracy. Can this happen? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
"You are assigned a new project helping a food delivery company save money. The company's delivery team can't deliver food on time, so customers get unhappy and receive free food. Which machine learning algorithm can save them?",,hard,case,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133368,,,,
You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
"You are given a data set with many variables, some highly correlated. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?",,hard,case,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133390,,,,
"After spending several hours, you built 5 GBM models thinking boosting would do magic. Unfortunately, none performed better than benchmark. You decided to combine those models but ensembled models didn't improve accuracy. Where did you miss?",,hard,case,,,2025-11-17T11:27:21.133406,,,
You have built a multiple regression model. Your model R² isn't as good as you wanted. You remove the intercept term and model R² becomes 0.8 from 0.3. Is it possible? How?,,hard,case,,manual_templates/manual_scenario_questions.txt,,,,,,
"Your manager informed that your regression model is suffering from multicollinearity. How would you check if he's true? Without losing information, can you still build a better model?",,hard,case,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133434,,,,
"Is rotation necessary in PCA? If yes, why? What will happen if you don't rotate the components?",,medium,ml,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133449,,,,
"Why is Naive Bayes so 'naive'? Explain prior probability, likelihood and marginal likelihood in context of Naive Bayes algorithm.",,medium,ml,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133459,,,,
When is Ridge regression favorable over Lasso regression?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,,,,,
"Both being tree based algorithms, how is random forest different from Gradient boosting algorithm (GBM)?",,medium,ml,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133474,,,,
You've built a random forest model with 10000 trees. Training error is 0.00 but validation error is 34.23. What is going on? Haven't you trained your model perfectly?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,,,,,
You've got a data set where p (no. of variables) > n (no. of observations). Why is OLS a bad option? Which techniques would be best to use? Why?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,,,,,
What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,,,,,
"You are given a data set consisting of variables having more than 30% missing values. Out of 50 variables, 8 have missing values higher than 30%. How will you deal with them?",,medium,ml,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133519,,,,
How is kNN different from k-means clustering?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
How is True Positive Rate and Recall related? Write the equation.,,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
What is the difference between covariance and correlation?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
"Is it possible to capture the correlation between continuous and categorical variable? If yes, how?",,medium,stats,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133551,,,,
Running a binary classification tree algorithm is easy. But how does tree splitting take place? How does the tree decide which variable to split at the root node and succeeding nodes?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
"We know that one hot encoding increases the dimensionality of a data set, but label encoding doesn't. How?",,medium,stats,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133575,,,,
What do you understand by Type I vs Type II error?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
What is convex hull? (Hint: Think SVM),,medium,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?,,easy,stats,,manual_templates/manual_scenario_questions.txt,,,,,,
"While working on a data set, how do you select important variables? Explain your methods.",,medium,ml,,manual_templates/manual_scenario_questions.txt,2025-11-17T11:27:21.133631,,,,
What cross validation technique would you use on a time series data set? Is it k-fold or LOOCV? Explain the forward chaining strategy.,,medium,ml,,manual_templates/manual_scenario_questions.txt,,,,,,
"Write a function to find two numbers in an array that sum to a target value",,,,"Easy",Coding,"arrays,,hash-table,two-pointers","Array length: 2 <= n <= 10^4; Values: -10^9 <= nums[i] <= 10^9; Exactly one solution exists",
"Reverse a string in-place without using built-in reverse functions",,,,"Easy",Coding,"strings,,two-pointers","1 <= string length <= 10^5; String contains ASCII printable characters","Input: 'hello'; Output: 'olleh'",
"Implement a function to count word frequency in a text file",,,,"Easy",Coding,"hash-table,,strings,file-processing","File size: up to 10MB; Words separated by spaces/punctuation",
"Convert temperature between Fahrenheit and Celsius",,,,"Easy",Coding,"math,,conversion","Temperature range: -273.15°C to 10^6°C","Input: 32°F; Output: 0°C; Formula: C = (F - 32) * 5/9",
"Implement binary search on a sorted array",,,,"Easy",Coding,"arrays,,binary-search,divide-and-conquer","Array is sorted; 1 <= array length <= 10^6; -10^9 <= nums[i] <= 10^9",
"Solve the coin change problem using greedy algorithm",,,,"Easy",Coding,"greedy,,dynamic-programming,arrays","Coins: [1,
"Implement concurrent graph BFS with goroutines",,,,"Medium",Coding,"graphs,,bfs,concurrency,channels",
"Create HTTP authentication middleware",,,,"Medium",Coding,"web,,middleware,authentication,http",
"Implement a bank account system with error handling",,,,"Medium",Coding,"oop,,error-handling,concurrency","Support deposits,
"Create a polymorphic shape calculator",,,,"Medium",Coding,"oop,,interfaces,polymorphism","Support Circle,
"Implement SQL database CRUD operations",,,,"Medium",Coding,"database,,sql,crud","Support SELECT,
"Build a simple microservice with gRPC",,,,"Medium",Coding,"grpc,,microservices,protobuf","Define service in .proto; Implement server and client; Handle errors",
"Optimize code for better performance",,,,"Medium",Coding,"optimization,,profiling,algorithms","Reduce time complexity; Memory optimization; Use appropriate data structures",
"Implement circuit breaker pattern",,,,"Medium",Coding,"design-patterns,,resilience,fault-tolerance","States: Closed,
"Implement string pattern matching (KMP algorithm)",,,,"Medium",Coding,"strings,,pattern-matching,algorithms","Text length: 1 <= n <= 10^6; Pattern: 1 <= m <= n",
"Implement generic data structures in Go",,,,"Medium",Coding,"generics,,data-structures,type-parameters","Support any comparable type; Implement Stack,
"Implement context management for cancellation",,,,"Medium",Coding,"context,,concurrency,cancellation","Handle timeouts,
"Build a chat server using channels",,,,"Advanced",Coding,"concurrency,,channels,networking","Handle multiple clients; Broadcast messages; Client disconnect",
"Create RESTful API for book management",,,,"Advanced",Coding,"rest-api,,http,crud,json",
"Build concurrent web content aggregator",,,,"Advanced",Coding,"concurrency,,http,goroutines,sync",
"Create file processing pipeline",,,,"Advanced",Coding,"concurrency,,pipelines,channels","Read → Transform → Write; Handle large files; Memory efficient",
"Implement OAuth2 authentication flow",,,,"Advanced",Coding,"oauth,,authentication,security,jwt",
"Solve longest increasing subsequence (DP)",,,,"Advanced",Coding,"dynamic-programming,,arrays,algorithms","Array length: 1 <= n <= 2500; Values: -10^4 <= nums[i] <= 10^4",
"Implement shortest path algorithm (Dijkstra)",,,,"Advanced",Coding,"graphs,,shortest-path,algorithms,priority-queue",
"Build regex text processor",,,,"Advanced",Coding,"regex,,strings,parsing","Support pattern matching,
"Implement cache with eviction policies",,,,"Advanced",Coding,"cache,,lru,lfu,data-structures",
"Implement rate limiter",,,,"Advanced",Coding,"rate-limiting,,concurrency,algorithms","Support token bucket,
"Implement merge sort algorithm",,,,"Easy",Coding,"sorting,,divide-and-conquer,recursion","Array length: 0 <= n <= 10^5; Values: -10^9 <= nums[i] <= 10^9",
"Find all pairs with given sum in array",,,,"Medium",Coding,"arrays,,hash-table,two-pointers","Array length: 1 <= n <= 10^4; May have duplicates",
"Detect cycle in linked list",,,,"Medium",Coding,"linked-list,,two-pointers,cycle-detection","List length: 0 <= n <= 10^4; May contain cycle",
