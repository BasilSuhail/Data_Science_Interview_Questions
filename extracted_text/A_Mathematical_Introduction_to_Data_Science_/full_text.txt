
================================================================================
PAGE 1
================================================================================

Yi Sun
Rod Adams
A Mathematical
Introduction
to Data Science


================================================================================
PAGE 2
================================================================================

A Mathematical Introduction to Data Science


================================================================================
PAGE 3
================================================================================

Yi Sun • Rod Adams
A Mathematical Introduction
to Data Science


================================================================================
PAGE 4
================================================================================

Yi Sun Rod Adams
Department of Computer Science Department of Computer Science
University of Hertfordshire University of Hertfordshire
Hertfordshire, UK Hertfordshire, UK
ISBN 978-981-96-5638-7 ISBN 978-981-96-5639-4 (eBook)
https://doi.org/10.1007/978-981-96-5639-4
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore
Pte Ltd. 2025
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore
If disposing of this product, please recycle the paper.


================================================================================
PAGE 5
================================================================================

Preface
This book is written primarily as a text for a one-semester Data Science and
Analytics course: Foundations of Data Science. We hope the book will also
introduce this area to people who are not students but have some mathematical
knowledge and a willingness to learn more. The reader is assumed proficient in
handling numbers in various formats, including fractions, decimals, percentages,
and surds. They should also have a knowledge of introductory algebra, such as
manipulating simple algebraic expressions, solving simple equations, and graphing
elementary functions, along with a basic understanding of geometry including
angles, trigonometry, and Pythagoras’ theorem. This book introduces the reader to
the fundamental mathematical and statistical expertise required to understand the
principles of many algorithms used in Data Science.
As with all mathematical textbooks, the worked examples are very important,
and the exercises for you, the reader, are even more important. You cannot really
understand mathematics without seeing and doing examples yourself. By doing
examples, you have to keep looking back to find relevant equations, pieces of text,
or worked examples that will allow you to complete the example. This is the way
you learn mathematics.
A note on numerical answers in this book. You may not get exactly the same
answer as we have. We have often used Python to do our calculations and it will
probably be working with more decimal places than you might be using, so do
not worry if your answers are slightly different. As a rule of thumb to get a result
correct to two decimal places you need to work with at least three decimal places.
Brief solutions to all exercises are given at the end of the book. Fuller solutions can
be found by following this link: sn.pub/5m5zwx.
Chapter 1 presents the general procedures of Data Science, summarises three
case studies used throughout the book, and introduces data types.
Chapter 2 provides the knowledge of basic set theory and functions to set up the
foundation for later chapters.
Chapter 3 covers the linear algebra knowledge (vectors and matrices) used in the
subsequent chapters.
v


================================================================================
PAGE 6
================================================================================

vi Preface
Chapter 4 focuses on two widely used algorithms in Data Science, Principal
Component Analysis (PCA) and Singular Value Decomposition, and shows how
these two algorithms work.
Chapters 5 and 6 introduce the basic knowledge of calculus (differentiation and
integration) and the main optimisation ideas for finding the minimum value of an
objective function.
Chapters 7, 8, and 9 reveal principles behind three methods: Principal Com-
ponent Analysis, Simple Linear Regression, and training simple artificial Neural
Networks using knowledge built up in the proceeding chapters.
Chapters 10, 11, and 12 introduce basic knowledge of probability and statistics.
These topics underpin lots of scientific disciplines that deal with vast amounts of
data, by considering the probability distributions associated with the data and our
confidence in our analysis. In particular, it builds the foundations to extend the
material on the linear regression algorithm of Chap.8.
Chapter 13 revisits the linear regression model of Chap.8 under a probability and
statistical framework. Specifically, the chapter presents the method of Maximum
Likelihood Estimation.
Chapter 14 discusses some important issues surrounding data analysis which
motivates the introduction of two final algorithms that can improve model gener-
alisation, namely Ridge Regression and early stopping.
The overall structure of the book can be divided as follows:
Part 1 Introduction: Chapter 1
Part 2 Mathematical Knowledge (I): Chapters 2–6
Part 3 Algorithms (I): Chapters 7–9
Part 4 Mathematical Knowledge (II): Chapters 10–12
Part 5 Algorithms (II): Chapter 13
Part 6 Conclusion: Chapter 14
By the end of this textbook, you have met many mathematical concepts and
techniques in linear algebra, calculus, probability and statistics. Also, several Data
Science algorithms, with and without enhancements, have been introduced and
illustrated. These include Principal Component Analysis, Singular Value Decom-
position, Linear Regression in two and more dimensions, Simple Neural Networks,
Maximum Likelihood Estimation, Logistic Regression, and Ridge Regression.
For any comments and questions, please send an email to mathsfds2025@gmail.
com.
Hertfordshire, UK Yi Sun
January 2025 Rod Adams


================================================================================
PAGE 7
================================================================================

Acknowledgements The book was originally based on the teaching materials we
developed for the Foundations of Data Science module in the Department of Com-
puter Science at the University of Hertfordshire. However, we have significantly
expanded its content, the details of the techniques and algorithms, and the number of
exercises and examples. We want to express our gratitude for the valuable feedback
and suggestions from our students, colleagues, and all tutors in the module team.
We are especially grateful for the insights provided by our reviewers. We also thank
our editor, Nick Zhu, who has been very supportive throughout the process, and the
entire Springer Nature publishing team for their excellence, in particular, for their
help with final proof reading and editing and putting the book into the correct format
for publication.
In addition, over the years, we have absorbed many teaching ideas and useful
bits of information derived from a wide spectrum of materials, including traditional
textbooks and digital resources available on the internet. It is important to acknowl-
edge that these elements have become so ingrained in our collective expertise that
we cannot specifically acknowledge these since we can no longer determine their
origin. Although we have conscientiously referenced sources from which we have
drawn in the References section and which also serve as the primary repositories for
further exploration, we recognise that inadvertent omissions may occur. Therefore,
we apologise for any such oversights and express our gratitude for the vast body of
knowledge that has contributed to our understanding and teaching practices.
vii


================================================================================
PAGE 8
================================================================================

Contents
1 Introduction ................................................................. 1
1.1 The Procedures of Data Science..................................... 2
1.2 Supervised Learning and Unsupervised Learning .................. 3
1.2.1 Supervised Learning........................................ 3
1.2.2 Unsupervised Learning..................................... 5
1.3 Case Studies .......................................................... 7
1.3.1 Case Study 1: Potential Enhancement Ratio
Prediction Using Linear Regression ....................... 7
1.3.2 Case Study 2: Data Visualisation Using Principal
Component Analysis ....................................... 10
1.3.3 Case Study 3: A Simple Two-Layer Neural Network..... 14
1.4 Types of Data ......................................................... 17
1.4.1 Organised (Structured) and Unorganised
(Unstructured) Data ........................................ 17
1.4.2 Quantitative and Qualitative ............................... 18
1.4.3 The Four Levels of Measurement.......................... 19
2 Sets and Functions.......................................................... 23
2.1 Sets.................................................................... 23
2.1.1 Sets and Subsets ............................................ 23
2.1.2 Venn Diagrams.............................................. 26
2.1.3 Basic Set Operations ....................................... 26
2.1.4 Sets Written in Comprehension............................ 30
2.2 Binary Relations...................................................... 32
2.2.1 Binary Relations............................................ 33
2.3 Functions ............................................................. 34
2.3.1 Graph of a Function ........................................ 36
2.3.2 Common Functions with One Variable.................... 37
2.3.3 Properties of a Function.................................... 41
2.3.4 Inverse Functions ........................................... 43
2.3.5 Composition of Functions.................................. 45
2.3.6 Functions of Two or More Variables....................... 46
ix


================================================================================
PAGE 9
================================================================================

x Contents
3 Linear Algebra.............................................................. 47
3.1 Vectors ................................................................ 47
3.1.1 Vectors in Physics .......................................... 48
3.1.2 Vector Addition............................................. 49
3.1.3 Scalar-Vector Multiplication .............................. 49
3.2 The Dot Product of Two Vectors .................................... 50
3.2.1 Dot Product: Algebra Definition........................... 50
3.2.2 Norm ........................................................ 51
3.2.3 Vector Magnitude and Direction in R2 .................... 52
3.2.4 Dot Product: Geometric Definition ........................ 53
3.2.5 Unit Vector.................................................. 55
3.3 Matrices............................................................... 55
3.3.1 Matrix Addition............................................. 56
3.3.2 Scalar Multiplication ....................................... 57
3.3.3 Matrix Multiplication....................................... 58
3.3.4 Matrices as Linear Transformations ....................... 62
3.3.5 Representations of Simultaneous Equations .............. 64
3.3.6 Multiplying a Matrix by Itself ............................. 66
3.3.7 Diagonal and Trace......................................... 67
3.3.8 Diagonal Matrices .......................................... 68
3.3.9 Determinants................................................ 68
3.3.10 Identity and Inverse Matrices .............................. 71
3.3.11 Matrix Transposition ....................................... 72
3.3.12 Case Study 1 (Continued).................................. 75
3.3.13 Orthogonal Matrix.......................................... 76
3.4 Linear Combination.................................................. 78
3.4.1 Vector Spaces ............................................... 78
3.4.2 Linear Combinations and Span ............................ 80
3.5 Linear Dependence and Independence .............................. 83
3.5.1 Linear Dependence and Independence .................... 83
3.5.2 Basis of a Vector Space..................................... 87
3.6 Connection to Matrices .............................................. 87
3.6.1 Determinants and Singular Matrices....................... 87
3.6.2 Rank......................................................... 88
4 Matrix Decomposition ..................................................... 91
4.1 Eigendecomposition.................................................. 91
4.1.1 Computing Eigenvalues and Eigenvectors ................ 92
4.1.2 Diagonalisation ............................................. 97
4.2 Principal Component Analysis ...................................... 99
4.2.1 Mathematics Behind PCA.................................. 99
4.2.2 The Definition of PCA ..................................... 101
4.2.3 PCA in Practice............................................. 102
4.2.4 Case Study 2: Continued (1) ............................... 104


================================================================================
PAGE 10
================================================================================

Contents xi
4.2.5 A Principal Component Analysis on the Sparrow
Dataset ...................................................... 106
4.3 Singular Value Decomposition ...................................... 109
4.3.1 Intuitive Interpretations..................................... 109
4.3.2 Properties of the SVD ...................................... 111
4.3.3 Find a Singular Value Decomposition of a Matrix........ 111
4.3.4 Case Study 2: Continued (2) ............................... 112
4.3.5 An Example of the Interpretation of SVD on a
Small Dataset ............................................... 114
4.3.6 An Example of Image Compression Using SVD ......... 117
4.4 The Relationship Between PCA and SVD.......................... 120
5 Calculus...................................................................... 121
5.1 Limits of Functions .................................................. 121
5.1.1 Left-and Right-Hand Limits............................... 122
5.1.2 Theorems on Limits ........................................ 123
5.1.3 Continuity................................................... 127
5.2 Derivatives............................................................ 127
5.2.1 Derivatives of Some Elementary Functions ............... 131
5.2.2 Rules for Differentiation ................................... 131
5.2.3 The Second Derivative ..................................... 137
5.3 Finding Local Maxima and Minima Using Derivatives ............ 138
5.4 Integrals............................................................... 142
5.4.1 First Fundamental Theorem of Calculus .................. 143
5.4.2 Indefinite Integrals.......................................... 144
5.4.3 Second Fundamental Theorem of Calculus ............... 145
5.4.4 Integrals of Some Elementary Functions.................. 145
5.4.5 Two Properties of Integrals................................. 147
5.5 Further Integration Techniques ...................................... 148
5.5.1 Integration by Substitution................................. 148
5.5.2 Integration by Parts ......................................... 152
6 Advanced Calculus ......................................................... 155
6.1 Partial Derivatives.................................................... 155
6.1.1 The First Partial Derivatives ............................... 155
6.1.2 The Second Partial Derivatives ............................ 158
6.1.3 Differentiation of Composite Functions with
Two Variables............................................... 159
6.1.4 Gradient ..................................................... 161
6.1.5 Jacobian Matrix............................................. 163
6.1.6 Hessian Matrix.............................................. 164
6.2 Applications of Partial Derivatives .................................. 166
6.2.1 Local Maxima and Minima ................................ 166
6.2.2 Method of Lagrange Multipliers for Maxima
and Minima ................................................. 169
6.2.3 Gradient Descent Algorithm ............................... 173


================================================================================
PAGE 11
================================================================================

xii Contents
6.3 Double Integrals...................................................... 176
6.3.1 Integration of Double Integrals Using Polar
Coordinates ................................................. 181
7 Algorithms 1: Principal Component Analysis........................... 185
7.1 Revisit Principal Component Analysis .............................. 185
7.2 Preliminary Knowledge.............................................. 186
7.3 Problem Setting ...................................................... 191
7.4 The Formulation of Principal Component Analysis................ 192
7.4.1 The First Principal Component ............................ 192
7.4.2 The Second Principal Component ......................... 193
7.4.3 Data Normalisation......................................... 194
7.5 Case Study 2 from Chap.1: Continued.............................. 204
8 Algorithms 2: Linear Regression ......................................... 207
8.1 Simple Linear Regression Algorithm ............................... 207
8.2 Least-Squares Estimation ............................................ 208
8.2.1 Deriving the Estimates Using the Least-Squares
Objective Function ......................................... 210
8.3 Linear Regression with Multiple Variables ......................... 216
8.4 Numerical Computation: Case Study 1 from
Chap.1—Continued.................................................. 221
8.5 Some Useful Results ................................................. 224
8.5.1 Residuals.................................................... 224
8.5.2 The Coefficient of Determination.......................... 225
9 Algorithms 3: Neural Networks........................................... 227
9.1 Training a Neural Network by Gradient Descent................... 227
9.2 A Simple One-Layer Neural Network............................... 228
9.2.1 Linear Activation Function................................. 229
9.2.2 Logistic Sigmoid Activation Function..................... 232
9.3 A Simple Two-Layer Neural Network: Case Study 3
from Chap. 1.......................................................... 237
9.3.1 The Feed-Forward Propagation............................ 237
9.3.2 The Error Back-Propagation ............................... 239
9.4 The Delta Rule ....................................................... 247
9.5 Implementation Details .............................................. 248
9.5.1 Bias.......................................................... 248
9.5.2 Stochastic Gradient Descent and Batch ................... 249
9.6 Deep Neural Networks............................................... 249
10 Probability................................................................... 251
10.1 Preliminary Knowledge: Combinatorial Analysis .................. 252
10.1.1 Factorial Notation .......................................... 252
10.1.2 Binomial Coefficients ...................................... 252
10.1.3 Permutation and Combination ............................. 253
10.2 Probability............................................................ 256


================================================================================
PAGE 12
================================================================================

Contents xiii
10.2.1 Axiomatic Probability Theory ............................. 256
10.3 Discrete Random Variables .......................................... 260
10.4 Continuous Random Variables....................................... 263
10.5 Mean and Variance of Probability Distributions .................... 268
10.5.1 Mean ........................................................ 268
10.5.2 Variance ..................................................... 275
10.6 Special Univariate Distributions..................................... 279
10.6.1 Discrete Random Variables ................................ 279
10.6.2 Continuous Random Variables............................. 286
11 Further Probability......................................................... 293
11.1 The Law of Large Numbers and the Central Limit Theorem ...... 293
11.1.1 The Law of Large Numbers................................ 294
11.1.2 Central Limit Theorem..................................... 295
11.2 Multiple Random Variables.......................................... 299
11.2.1 Joint Probability Distributions: Discrete Random
Variables .................................................... 299
11.2.2 Joint Probability Distributions: Continuous
Random Variables .......................................... 305
11.2.3 Multinomial Distribution................................... 314
11.2.4 Multivariate Normal Distribution.......................... 316
11.3 Conditional Probability and Corresponding Rules ................. 319
11.3.1 Conditional Probability..................................... 319
11.3.2 Conditional Means and Conditional Variances............ 324
11.3.3 Mutual Exclusivity ......................................... 326
11.3.4 The Multiplication Rule.................................... 327
11.3.5 Independence ............................................... 327
11.3.6 The Law of Total Probability .............................. 328
11.4 Bayes’ Theorem...................................................... 330
12 Elements of Statistics....................................................... 335
12.1 Descriptive Statistics................................................. 335
12.1.1 Measures of Centre ......................................... 336
12.1.2 Measures of Variation ...................................... 339
12.1.3 The Range and the Interquartile ........................... 342
12.2 Elementary Sampling Theory........................................ 344
12.2.1 Random Sampling with and Without Replacement....... 344
12.2.2 Sampling Distributions of Means .......................... 344
12.2.3 Sampling Distributions of Proportions .................... 347
12.2.4 Standard Errors ............................................. 349
12.2.5 Degrees of Freedom ........................................ 349
12.2.6 Two Specific Sampling Distributions...................... 350
12.3 Inference.............................................................. 354
12.3.1 Point Estimation ............................................ 354
12.3.2 Interval Estimation ......................................... 355
12.3.3 Testing Hypothesis ......................................... 364


================================================================================
PAGE 13
================================================================================

xiv Contents
13 Algorithms 4: Maximum Likelihood Estimation and Its
Application to Regression.................................................. 379
13.1 Maximum Likelihood Estimation ................................... 379
13.2 Revisiting Linear Regression ........................................ 384
13.2.1 Linear Regression with Maximum Likelihood
Estimation................................................... 384
13.2.2 Sampling Distribution of the Linear Regression
Estimators................................................... 394
13.3 The Logistic Regression Algorithm ................................. 406
14 Data Modelling in Practice ................................................ 415
14.1 Data Pre-Processing.................................................. 415
14.1.1 Questions to Ask When Pre-Processing the Data......... 415
14.1.2 A Simple Feature Selection Method....................... 420
14.2 Model Selection ...................................................... 422
14.2.1 Data Splitting ............................................... 422
14.2.2 Model Evaluation........................................... 422
14.2.3 Understanding Bias-Variance Trade-Off .................. 428
14.2.4 Underfitting and Overfitting................................ 433
14.3 Ridge Regression..................................................... 434
14.3.1 The Closed-Form Solution................................. 434
14.3.2 Bias and Variance of Ridge Regression Coefficients ..... 436
14.4 Early Stopping........................................................ 441
Solutions........................................................................... 443
References......................................................................... 471
Index............................................................................... 473


================================================================================
PAGE 14
================================================================================

Chapter 1
Introduction
The total amount of data created, captured, copied, and
consumed globally is forecast to increase rapidly, reaching 149
zettabytes in 2024. Over the next 5 years up to 2028, global data
creation is projected to grow to more than 394 zettabytes.
Statista Research Department [1]
What is a Zettabyte? A zettabyte is a value of 10 to the power of 21, or if we
write it down to show all digits of it, we have 1,000,000,000,000,000,000,000
bytes. According to the figure reported by the Department of Economic and Social
Affairs, United Nations, the global population was projected to reach 8 billion,
8,000,000,000, on 15 November 2022. Imagine if everyone, including newborn
babies, takes an equal number of bytes, then this would mean each of us can have
125 billion bytes, that is, 125GB (1GB = 109 bytes). If the file size for 1 hour of
.
4K video is roughly 20GB, each of us will have a6.25-hour video to watch.
.
We live in the age of data. We access data every day: messages we send
via our mobile phones, news we hear on the radio, movies we see on TV, and
account statements we receive from the bank. These are all data—a collection of
information.
Data Science is an interdisciplinary field that uses principles and methods from
mathematics, statistics, computer science, and domain knowledge to tackle data.
It involves data engineering, which builds up the pipeline to collect and use the
data; data analytics, which analyses data to answer questions and draw conclusions;
machine learning, which gives computers the ability to learn from data without
explicit instructions; and more. Therefore, Data Science is usually used as a broad
term that includes collecting and pre-processing the data, understanding the data by
extracting useful information, and creating algorithms and predictive models.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 1
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_1


================================================================================
PAGE 15
================================================================================

2 1 Introduction
1.1 The Procedures of Data Science
Quite often, people working in a problem domain raise questions and ask for help
from data scientists. Solutions to questions are not necessarily apparent to data
scientists due to a lack of domain knowledge and the complexity of real-world
applications.Datascientistsmustcommunicate withclientsefficientlytounderstand
the essence of the problem and the challenges they face. A good data scientist
can quickly grasp what their clients want by listening to them, learning from the
background information, and explaining ideas to their clients using a layperson’s
terms.
Once the two parties agree on the problems they are going to work on, further
discussions about the data that can be used to solve the problem are needed,
including the volume of the available data and the features (also called attributes
of the data) that can characterise the data. Rather than saving data in a CSV file
or a relational database, more and more data are stored in cloud data warehouses.
Data scientists need to know how to export raw data from data sources, such as web
pages, emails, and SQL servers.
After obtaining the raw data, data scientists need to do data exploration in order
to understand the relationships among the data better. They may convert the data to a
table format where each row represents an observation and each column represents
a feature. They then often need to consider whether all those features are significant
or related to the task they will be dealing with. They may apply feature selection or
feature extraction methods to the data. There are many algorithms to deal with this
task involving mathematics knowledge. For example, in Chap.4, we will introduce
the principal component analysis method that can be used for data visualisation. The
same method can be employed to perform feature extraction too. When doing data
exploration, the data scientist also needs to know at least some basic statistics to
understand, for example, what boxplots and histogram plots tell about the data.
Once the data is thoroughly investigated, data scientists are ready to apply
existing computational algorithms to the data and/or to create a new algorithm to
tackle the problem. This is the stage of modelling. It aims to produce a trained
model with the existing data that can either reveal the natural structure in the
dataset or make the most accurate predictions for any unseen data. The data scientist
also needs to consider what the most suitable performance metrics to use are. The
knowledge of optimisation, differentiation, probability, and statistics is very helpful
in understanding the principles behind algorithms at this stage.
A further application of some domain knowledge may be used to improve per-
formance. Finally, data scientists need to visualise results using suitable statistical
figures and graphs.
We have just gone through the procedures of general Data Science [2], including
the following key steps:
• Identifying the questions we want to answer.
• Collecting the data.


================================================================================
PAGE 16
================================================================================

1.2 SupervisedLearningandUnsupervisedLearning 3
• Exploring the data to understand the natural structure and relationship among the
data.
• Modelling the data.
• Presenting and explaining the results.
It is essential to understand that the whole procedure works like a spinning wheel
rather than flowing in a linearly top-down mode. For example, when modelling the
data, if the data scientist realises that more features are needed, they can go back to
the second step to collect more data attributes. Alternatively, they may remove some
features and redo the modelling.
1.2 Supervised Learning and Unsupervised Learning
This book focuses on the fundamental knowledge of mathematics, probability, and
statistics, which are needed for exploring data, modelling data, and presenting
results. To start with, we introduce two main techniques for analysing data:
supervised learning and unsupervised learning.
1.2.1 Supervised Learning
In supervised learning, we have a training set of data and a test set of data. The
training set will be used to train a predictive model. The test set will determine how
well the trained model performs on this new unseen data. The training procedure is
also called learning. The type of data used here contains a set of input values and
a set of output (or target) values. Supervised learning aims to infer a function that
maps the relation between the input values and the output values in the training set.
The computer can then use this function to estimate the test data’s output when given
unseen test data, which has its own input values. Depending on the output value
types, there are two categories for supervised learning. These are classification and
regression. If the target value is discrete or categorical, we say it is a classification
problem; if the target value is continuous, we say it is regression.
Classification aims to find decision boundaries to distinguish patterns from
different classes given in the input data. The computer then uses these decision
boundaries to predict the class label for new unseen data by seeing which side of the
decision boundary the data lies in. On the other hand, regression aims to estimate a
curve, a line, or a function learned from training examples so that when given unseen
data, the computer can substitute the feature values into the function to estimate the
target value.


================================================================================
PAGE 17
================================================================================

4 1 Introduction
Fig. 1.1 An example of Class A Class B Test Data
supervised
9 9
learning—classification
8 8
7 7
6 6
5 5
4 4
3 3
2 2
1 1
0 0
-1 -1
0 2 4 6 0 2 4 6
Example 1.1 Figure 1.1 shows an example of supervised learning. The left
panel shows the training set, including five data points. There are two classes:
Class A, represented by circles, and Class B, represented by plus signs. The
learning process is to find a decision boundary between these two classes. In
this case, a straight line can be drawn between the two classes of data that
will act as a decision boundary. A neural network may be the technique to
use to find the best line or decision boundary. Neural networks are covered
in Chap.9. In the right panel, the computer is given a new unseen test point
denoted as a cross sign in black. It should be able to estimate the test data’s
class label using the decision boundary learned from the training set. This is a
classification problem.
Example 1.2 Assume we have a heart disease training dataset. It includes
500 patient records with 14 patient attributes and 1 target value. This target
value refers to the presence of heart disease in the patient. It is a binary value:
0 means no presence; 1 means there is presence. The 14 attributes may include
age, gender, smoker or non-smoker, the measured serum cholesterol, and so
on. Thus, we have 500 rows, each representing a patient, and 14 columns,
each being a feature, plus a column indicating the corresponding target value.
The aim is to find the relation between those 14 attributes and the target from
500 patient records so that when a new patient record with those 14 features
is given, the computer can predict whether the patient has heart disease by
applying the estimated function. In this example, the target values are discrete,
a binary value, so this is a classification problem.


================================================================================
PAGE 18
================================================================================

1.2 SupervisedLearningandUnsupervisedLearning 5
Example 1.3 Suppose an estate agent wants to predict house prices for
Hertfordshire. He has 3000 house sale records with 9 features: the income
of the householder of the house, the interest rates, the age of the house, the
number of bedrooms in the house, the type of house, the population of the
local area, the price that the house sold at last time, the postcode of the place,
and the garage type. A model is trained to find a relation between these nine
features and house prices. This is a regression problem, since house prices are
continuous.
The aim is to use the trained model to predict the price for any house not
included in those 3000 houses.
1.2.2 Unsupervised Learning
Unsupervised learning is different. Rather than finding a decision boundary or a
regression function, it is used to find any natural structure inherent in the dataset.
This natural structure may include clusters (data that group together) and outliers
among the data (ones that differ considerably from the bulk of the data). It works by
using data attribute information without considering data label information or target
values, whether or not such information exists.
Example 1.4 Suppose we have 15 data points with two different attributes,
as shown in Table 1.1. It is not easy to see if there are any natural clusters
among the data, or, if there are any, how many clusters exist, even though this
is a small dataset with only two dimensions.
However, the question can be answered easily if we can visualise the
data as shown in Fig.1.2. Data visualisation is a common application of
unsupervised learning. It helps us to understand the underlying distribution
of the data. In this example, we know there are four clusters in the data.


================================================================================
PAGE 19
================================================================================

6 1 Introduction
Table 1.1 A small dataset
1.1 2
including 15 data points with
11 1
2a ttributes
10 1
10.5 2
9 1
1 1
9 10
1 9
2.1 2.2
8.9 9.2
2 9
1 8
2 1
10 9.5
1.5 10
Fig. 1.2 An example of 12
unsupervised learning: the
scatter plot of data shown in
10
Table 1.1
8
6
4
2
0
0 2 4 6 8 1 0 12
Exercise
1.1 Incorporation of certain chemicals into a drug delivery vehicle may lead
to the enhancement of drug release and a more rapid clinical response. Such
chemicals have been labelled as drug delivery enhancers. Their enhanced
ability is measured as an enhancement ratio. You need to develop learning
algorithms to address each of the following two problems.
1. You have some chemical compounds. You want to predict the enhancement
ratio value for each of these chemicals.
(continued)


================================================================================
PAGE 20
================================================================================

1.3 CaseStudies 7
2. You have some chemical compounds. You want to decide for each
chemical if it is a potential drug delivery enhancer.
Should you treat these as classification or as regression problems?
a. Treat both as classification problems.
b. Treat both as regression problems.
c. Treat Problem 1 as a classification problem and Problem 2 as a
regression problem.
d. Treat Problem 1 as a regression problem and Problem 2 as a classifica-
tion problem.
Next, we will introduce three case studies. Each of them applies an algorithm,
which will be developed in detail and illustrated using worked examples in
Chaps.7, 8, and 9, respectively. These case studies illustrate some typical problems.
The mathematics is introduced so you can see how the mathematics naturally arises
when you are characterising and solving the problems. You are not expected to
understand all the mathematics yet, but it allows you to see why we need a book
like this to deal with Data Science.
1.3 Case Studies
1.3.1 Case Study 1: Potential Enhancement Ratio Prediction
Using Linear Regression
Suppose a pharmaceutical researcher wants to study the relationship between the
molecular weight and the enhancement ratio of chemicals to identify compounds
with potential as transdermal enhancers based on the value of the enhancement
ratio.1
The researcher has the molecular weight (MW) and enhancement ratio value
for three chemicals, as shown in the first three rows in Table 1.2. He wants
to apply a computational method to the three chemicals to find the relationship
between those two attributes. He then wants to use the learned relationship to
estimate the enhancement ratio value of the fourth chemical in Table 1.2.T hisi sa
supervised learning problem. We call MW the input tothecomputationalmodeland
1 Transdermal enhancers are chemicals incorporated into drug delivery vehicles leading to
enhancement of drug release through the uppermost layer of the skin, the stratum corneum, thus
resulting in a more rapid clinical response. To find potential transdermal enhancers, researchers
need to measure the enhancer ratio of each tested chemical by doing experiments in the lab, which
are time-consuming and expensive [3].


================================================================================
PAGE 21
================================================================================

8 1 Introduction
Table 1.2 A small dataset of
Chemicals index MW Enhancement ratio
compounds with molecular
1 295 10
weight values shown against
their corresponding 2 305 30
enhancement ratio values 3 300 20
4 301 ?
the enhancementratio the target. The first three chemicals are training examples,
.
including the input and the target. The fourth chemical is test data, for which the
researcher knows the input and wants to estimate its target value. The researcher
considers estimating a linear line that fits the training examples best. As we know
from secondary maths, a linear function passing through the origin is given by
y = ax, (1.1)
.
where x is the input, a is the slope of the line, and y is the output. Readers who
have forgotten it may view Sect.2.3.2.1 of Chap.2. In this case study, the line
needs to be estimated using the first three compounds in Table 1.2, with MW as
the input x andenhancementratioas the output y. Other physio-chemical features
.
may be used together with MW to estimate the enhancement ratio (in practice, there
would usually be several more features). That is, the researcher may have more than
one physio-chemical feature as the input. Therefore, we rewrite Eq. (1.1) in a more
general way as follows:
y=aX. (1.2)
.
A careful reader will notice that we have used the bold font in Eq.(1.2). That is
because we use bold capital letters to denote a matrix and bold little case letters
to denote a vector. The basic knowledge of vectors and matrices is introduced in
Chap.3.
Now the question is how to find a, the gradient or slope, in Eq.(1.2). One way to
.
do it is to use the least-squares regression method, which is a simple but widely used
technique in Data Science. The least-squares regression method aims to estimate
a by minimising differences between the estimates of training examples and their
.
actual target values. It is an optimisation problem.
For now, all you need to know is that
a=(XTX) −1XTy. (1.3)
.
Equation (1.3) s hows a is calculated via the matrix and vector multiplication
.
involving the inverse (denoted as −1) and transformation (denoted as T) of a matrix.
. .
The explanation of how we get Eq.(1.3) is presented in Chap.8.
To apply Eq.(1.3), the researcher needs to collect the data into the matrix Xand
.
vector y. Usually, data scientists normalise or re-scale feature values first. This is
.


================================================================================
PAGE 22
================================================================================

1.3 CaseStudies 9
important when there are many features with different ranges, especially those with
a large magnitude.
From Table 1.2, we hav e
⎡ ⎤ ⎡ ⎤
295 10
. r
aw_X=⎣
305
⎦ andy=⎣
30
⎦
,
300 20
where the first three molecular weight values in Table 1.2 have been assigned to
the inputraw_X, and their corresponding enhancement ratio values are assigned to
.
the output y.B othraw_Xand yare called column vectors. If there had been more
. . .
features than just the molecular weight,raw_Xwould have had more columns, one
.
for each feature. It would then have been a proper matrix. The mean value ofraw_X
.
(cid:2)
is 300. Let us subtract the mean value fromraw_Xand add a new column with 1s.
. .
We obtain the following:
⎡ ⎤
1−5
. X
=⎣
1 5
⎦
.
1 0
Xis now a matrix with three-row vectors and two-column vectors in it. The reason
.
(cid:2)
we add 1sinto Xand the idea behind the least-squares regression algorithm will be
. .
introduced in Chap.8.
Once the line is fitted to the data, that is, vector a in Eq.(1.2) is estimated,
.
the pharmaceutical researcher can substitute the known MW value of the fourth
chemical compound to Eq. (1.2) to obtain its estimated y value for the enhancement
ratio. To assess the model performance, he can compare the estimated value with the
lab-measured value for this chemical. If he does this, the fourth chemical compound
is called test data.
As a practitioner, the pharmaceutical researcher only needs to collect X and y,
. .
substitute them to Eq.(1.3) to estimate afirst, and then to obtain the estimation of
.
enhancement ratio by substituting a and the value of a new input into Eq.(1.2).
.
The knowledge he needs will be taught in Chaps. 2 and 3. If he is lucky, the data
scientist may have pre-calculated a for him using data that he has supplied to the
.
data scientist. However, as data scientists, we do not stop there. We go further. We
want to know how a is obtained and how the least-squares technique works, since
.
these will help us understand how to adjust a model when necessary. To do that,
knowledge of calculus, shown in Chaps.5 and 6, is needed.
In addition, rather than obtaining a single estimated enhancement ratio value
for a chemical compound, can we determine a range, with a certain confidence
level, for the estimate? We may answer this question using probabilistic models.
For instance, we can apply the maximum likelihood method covered in Chap.13
to estimate model parameters and model predictions of a simple linear regression
model and derive confidence intervals using statistical principles like standard error


================================================================================
PAGE 23
================================================================================

10 1 Introduction
calculation. It is built up on the knowledge of probability and statistics, which is
introduced in Chaps.10, 11, and 12.
In this case study, acan be obtained by applying Eq.(1.3) for the three chemicals
.
shown in Table 1.2. If the researcher can collect more chemicals or three different
chemicals, he may get different avalues. Which estimated ais the most suitable one
. .
to use? We will discuss the related issues and model selection in Chap.14.
Remark 1.1 The maximum likelihood method mentioned in Case Study 1 deals
with a regression problem. It can also be applied to classification problems. For
a classification task, it can provide the probability that a pattern belongs to each
class. The class with the highest probability would be the estimated class for that
pattern. (cid:2)
.
1.3.2 Case Study 2: Data Visualisation Using Principal
Component Analysis
Usually, before training a computational model on a dataset, we want to investigate
the underlying distribution of the data, the relationships among data attributes, and
the correlation between each data attribute and the data targets. This investigation
is called data exploration. One of the data exploration methods is data visualisation.
For example, a scatter plot of an attribute against another attribute may be used to
observe relationships among data points and to detect whether there are clusters in
the data.
Let us use the Iris dataset. The dataset contains 3 classes of 50 data items each,
where each class refers to a type of Iris plant [4], namely, Setosa, Versicolour, and
Virginica, respectively. The dataset includes four features: sepal length, sepal width,
petal length, and petal width in centimetres. Figure 1.3 shows scatter plots of one
feature against another. Plots along the main diagonal are histogram plots of the data
in the corresponding feature, since otherwise, they would just be a comparison of a
feature with itself. Each scatter plot shows the correlation between the two features
involved. It also displays clustering information: the class Setosa (represented by
circles) is separated from the other two classes in all scatter plots, and there is
some overlap between classes Versicolour (represented by squares) and Virginica
(represented by triangles).
Are there any visualisation methods that consider all features in one single plot
panel? The answer is yes, and the classical principal component analysis (PCA)
is one of these methods. It is a widely used method for data visualisation and
data dimensionality reduction. PCA is an unsupervised learning method. Figure 1.4
shows a PCA plot of the Iris data in the coordinate system constructed by the
first two principal components. It is also a scatter plot, and it presents similar
clustering information. Looking at the figure, readers who do not know the principal
component analysis method may ask the following questions:


================================================================================
PAGE 24
================================================================================

1.3 CaseStudies 11
Fig. 1.3 Scatter plots of the Iris dataset, with circles representing the class Setosa, squares
representing the class Versicolour, and triangles representing the class Virginica
Fig. 1.4 A PCA visualisation
plot of the Iris dataset, where
the first two principal
components capture about
95.8% of the total variance in
the data


================================================================================
PAGE 25
================================================================================

12 1 Introduction
• What are those principal components (PCs)?
• What is the relationship between those PCs and the original four features in the
dataset?
• Why is it necessary to report the variance percentage value (shown in the figure
caption)?
• How is the variance percentage value calculated?
• How is the position of each data item in the coordinate plane determined?
It is not easy to see what the PCA has done with this dataset. Now let us see another,
simpler, example, which we refer to as a toy dataset. It is a much smaller dataset X,
.
including just five data points and just two features, and is shown in the following
matrix:
⎡ ⎤
15
⎢ ⎥
⎢22⎥
⎢ ⎥
.
X =⎢
⎢
33⎥
⎥
.
⎣44⎦
51
The average value of each column vector is the same, 3. We remove the average
value of each column. That is, we subtract the mean value from each element in the
matrix, and we get the following matrix:
⎡ ⎤
− 2 2
⎢ ⎥
⎢−1−1⎥
⎢ ⎥
. n ewX=⎢ ⎢ 0 0 ⎥ ⎥ .
⎣ 1 1 ⎦
2 −2
The left panel of Fig.1.5 shows the five data points in the x − y Cartesian
.
coordinate system after removing the average value. The first column of newX is
.
the vector x which is plotted on the horizontal (or x-axis), and the second column
1.
is x and is plotted on the vertical (or y-axis). The right panel of the figure shows
2.
projectionsofthosedatapointsinthePCAcoordinatesystem,withthefirstprincipal
component (PC1) plotted horizontally and the second principal component (PC2)
plotted vertically. The PCA projection plot seems to result from the axes in the
original Cartesian coordinate system having been rotated, so that the largest distance
among the data is displayed along the horizontal axis.
For now, all readers need to know are:
1. The PCA has been performed on the data using the features only, excluding the
target value. The class label information (or the target value) shown in the Iris
dataset is used only for colouring classes in the plot.
2. The projection, that is,the position of each data along each PC axis, isdetermined
by a linear combination of the original features. As will be shown later in Chap.7,


================================================================================
PAGE 26
================================================================================

1.3 CaseStudies 13
3
2
1
0
-1
-2
-3
-2 0 2
x
1
x 2
3
2
1
0
-1
-2
-3
-2 0 2
PC1
2CP
Fig. 1.5 The example of the toy dataset. The left panel shows the scatter plot of the data in the
original x −y. Cartesian coordinate system; the right panel shows the scatter plot of the data
projected onto the PCA space, where PC1 denotes the first principal component axis, and PC2
denotes the second principal component axis
for the Iris dataset, this linear combination is:
projection=c ×sepal.length+c ×sepal.width+c ×petal.length
. 1 2 3
+c ×petal.width,
4
wherec ,c ,c , and c are coefficients that need to be determined when doing the
1 2 3. 4.
PCA analysis. The four attribute (or feature) values are normalised values, that is,
each attribute has had its mean value subtracted and usually has been divided by its
standard deviation (this is not necessary for this toy dataset since the two standard
deviations are the same).
Remark 1.2 Normalisation is an important pre-processing step when analysing
data to make all attributes have the same magnitude. This is useful when doing
a distance-based calculation, since it avoids those attributes with large magnitudes
dominating the distance. Normalisation may change the range of the data, but it does
not change the data’s structure and trend. An example can be seen in Fig.1.6, where
the original data with two attributes is shown in the left panel, and the normalised
data having a zero mean and unit variance for each attribute is shown in the right
panel. (cid:2)
.
Chapter 4 describes how PCA is carried out after introducing the relevant linear
algebra knowledge in Chaps.3 and 4. Readers should be able to fully understand
the idea behind PCA used in Chap.7 after learning further knowledge regarding the
relevant aspects of calculus in Chaps.5 and 6.


================================================================================
PAGE 27
================================================================================

14 1 Introduction
Fig. 1.6 A comparison of the data structure without data normalisation (left panel) and with data
normalisation (right panel)
Fig. 1.7 An illustration of a
simple two-layer
feed-forward neural network
1.3.3 Case Study 3: A Simple Two-Layer Neural Network
Inspired by the biological information processing mechanism of the brain, the neural
network (NN) with artificial neurons was first proposed in the 1940s. Since then,
many different types of NN have been developed. Especially after 2010, with the
growth of computing power, the increased requirements of processing a massive
amount of data, and the need to achieve better solutions to optimisation problems,
the deep neural network (DNN) has rapidly developed to deal with different types
of data such as time series data, text, and images. Even with the development of
DNN, the basic building blocks of DNN are still similar to the traditional NN; they
all have activation functions and layers.
There are many different sorts of neural networks, each doing a different job
and having different complexity and depth. However, the basic element of a neural
network is the neuron, or unit, which has n inputs, and usually, each input has a
weight w associated with it. This is illustrated inFig.1.7. The input of this neuron,
or unit, is then the weighted sum of the input values and the weights. From the


================================================================================
PAGE 28
================================================================================

1.3 CaseStudies 15
figure, we see that this weighted sum is:
(cid:8)
x w +x w +···+x w = x w =x·w.
. 1 1 2 2 n n i i
i
This uses the scalar product of vectors that will be introduced in Chap.3. Figure 1.7
could be a node that collects the inputs, so the x are the input values, or any other
i.
unit, where the x are the outputs from previous units.
i.
A neural network is usually arranged in layers, going from the input layer, which
takes in the input values, to the output layer, which gives the output(s). Any units
in layers in between are called hidden units. A network with inputs, a hidden layer,
and an output layer is referred to as a two-layer neural network. The input layer is
not usually counted as a layer, since it just contains the inputs and does not have
adjustable weights. The input values are said to be fed-forward to give the output
values of the network.
Having taken a weighted sum of its inputs, each neural unit performs some
activation function on its input to transform the input to create an output value.
Sample activation functions are the threshold, a linear function, a logistic sigmoid
function, and a hyperbolic tangent. The same happens at each layer until output
values(s) are produced. The output values are then compared to some target values,
and the difference is called the error.
In this book, we are going to concentrate on networks where the weights are
trained by gradient descent using some form of propagation of the error back
through the network. Hence, we are only interested in the last three of the above
activation functions, since they are differentiable, and hence, we can use gradient
descent learning to train the network. Figure 1.8 illustrates the different sorts of
activation functions introduced here. The concept of differentiable and gradient will
be introduced in Chaps.5 and 6.
The case study in this section illustrates artificial neural networks using a simple
example ofa two-layer neural network (NN) withonly twohidden units initsmiddle
layer. Of course, such simple neural networks have many limitations on what they
can represent. However, we use this example to illustrate an activation function in
y
y
y
1 1
0 x 0 x
0 x
-1
Fig. 1.8 Activation functions: (a) linear, (b) logistic sigmoid, (c) hyperbolic tangent


================================================================================
PAGE 29
================================================================================

16 1 Introduction
Fig. 1.9 An illustration of a feed-forward simple two-layer neural network
operation and how information is propagated through the layers and how the error
is fed back to update the weights.
Figure 1.9 shows the architecture of a two-layer neural network used in this
example, where we consider that each input (training) example x has only two
.
attributes, or features, x and x . Squares in Fig.1.9 represent two input features,
1. 2.
forming the neural network’s input layer. Suppose each input example has two
targets, denoted as t and t . y and y are the outputs or predictions of the neural
1. 2. 1. 2.
network for the given x, and they form the output layer of the neural network. The
.
two-layer in the name means there are two layers of adaptive weights. The nodes
in between two weight layers are called hidden units. Each input in the input layer
is connected to hidden units via weights of the first layer. Each hidden unit is a
linear combination of the input attributes. Usually, an activation function, which is
most often a non-linear function and can transform the total input, is applied to each
hidden unit to simulate the complexity of the brain.
We follow the notations used in [5] for weights. That is, we denote each weight
(l)
asw , where (l)denotes the lth layer, j the jth hidden unit in the corresponding
ji. .
(1)
layer, and i the ith node of the immediate layer to the left. Forexample,w denotes
21.
the weight going from the first input feature x to hidden unit 2 in the first layer. The
1.
training of this neural network aims to adjust weight values to reduce the error, that
is, the difference between the targets and the predictions or outputs. We show how
to use and train this simple neural network in Chap.9.
Remark 1.3 The simple two-layer neural network shown in Case Study 3 can
be used in a supervised learning task. It can be used for both regression and
classification problems. (cid:2)
.
We have focused on approaches that can be applied to understand the data and
make predictions for unseen data. As we can see, these approaches need mathemat-
ical and statistical knowledge almost everywhere. In the following chapters, we will
equip our readers with the essential skills for data analysis.
However, before we start, let us have a look at data types. You may have noticed
that data are in a format as shown in Tables 1.1 or 1.2. However, how do we deal
with data in free forms, such as audio signals, email, and survey comments with
some numerical scores? In the final section of this chapter, we will briefly discuss
data types.


================================================================================
PAGE 30
================================================================================

1.4 TypesofData 17
1.4 Types of Data
Looking into the types of data is one of the most important steps you need to take to
perform Data Science. It will help you to understand the data and choose the correct
class of algorithms that can be used to analyse the data.
1.4.1 Organised (Structured) and Unorganised (Unstructured)
Data
When given a dataset, the first question you need to ask yourself is whether it is
structured data or unstructured data. Structured data is usually organised using a
table method; unstructured data exists as a free entity and does not follow any
standard format. Most data analysis algorithms are built with structured data in
mind.
Let us have a look at an organised/structured data example. Table 1.3 shows 15
sample rows of the Iris dataset mentioned in Case Study 2 in Sect.1.3.2 of this
chapter. As you can see, the data is sorted into a row and column structure. Each
row represents a single observation; each column represents either a feature or class
information. This data set has four attributes or features. They are all continuous
Table 1.3 Examples of data items from the Iris dataset, illustrating feature values and correspond-
ing class labels
sepal.length sepal.width petal.length petal.width Variety
5.1 3.5 1.4 0.2 Setosa
4.9 3 1.4 0.2 Setosa
4.7 3.2 1.3 0.2 Setosa
4.6 3.1 1.5 0.2 Setosa
5 3.6 1.4 0.2 Setosa
.
.
..
7 3.2 4.7 1.4 Versicolor
6.4 3.2 4.5 1.5 Versicolor
6.9 3.1 4.9 1.5 Versicolor
5.5 2.3 4 1.3 Versicolor
6.5 2.8 4.6 1.5 Versicolor
.
.
..
6.7 3 5.2 2.3 Virginica
6.3 2.5 5 1.9 Virginica
6.5 3 5.2 2 Virginica
6.2 3.4 5.4 2.3 Virginica
5.9 3 5.1 1.8 Virginica


================================================================================
PAGE 31
================================================================================

18 1 Introduction
values. The last column, with a head denoted as variety, gives the class label
information for each plant, indicating which category the plant belongs to.
Examples of unstructured data include genetic sequences and molecular structure
graphs. These are unstructured data, since we cannot form features of the sequence
using a row-column format without taking a further look. Feedback left for a product
review on Amazon and messages on Twitter are also unstructured free text. Images
are another type of unstructured data. To read the information saved in an image, we
need to open the file with an image viewer.
Exercise
1.2 Is the following data structured or unstructured?
(1) Speech signals,
(2) Emails,
(3) Medical X-ray,
(4) Student ID numbers.
Remark 1.4 Most real-world data are unstructured data. To apply most data
analysis algorithms, we must first convert unstructured data to structured data using
pre-processing techniques.
For example, consider speech signals. People may decompose each signal into a
set of signals with different frequencies, and then values related to the amplitude of
frequencies can be used as signal features. How to decompose the signal is not our
focus here. However, it is essential to know that converting data from unstructured
to structured is a crucial step in data analysis.
As another example, let us consider text data. We have many options to transform
the free text into a structured format. We could apply new features that describe the
data. For instance, we can define a set of words or phrases first and then count the
particular words or the specific phrases appearing in each file. This way, we can use
the features defined here to convert them into structured data. Of course, we may
also have a topic as a class label for each text file. Then, we can put all of them into
one big table: each row representing one text, such as a tweet, columns showing
the counts of specific words or phrases, and one column indicating the class label
information. (cid:2)
.
1.4.2 Quantitative and Qualitative
Another classification of data is quantitative and qualitative. Quantitative data can
be described using numbers, including discrete and continuous data. Discrete data
has limited values, while continuous data can take on any value between two values.


================================================================================
PAGE 32
================================================================================

1.4 TypesofData 19
For example, the number of PC labs in a university is discrete, since it is a whole
integer value. In contrast, the average number of hours a student sleeps daily is a
continuous value, such as 5 or 7.5 hours, or any value in between.
Qualitative data is also called categorical data and is non-numerical in nature.
However, qualitative data may appear as a number, though they cannot be used
meaningfully in the computation. For example, Level 4, Level 5, Level 6, and
Level 7 denote modules running for the first year, the second year, the third year of
undergraduate courses, and the postgraduate course, respectively. It does not make
sense if you do an addition between a Level 6 module and a Level 7 module.
Remark 1.5 To tell if a number is quantitative or qualitative, ask yourself whether
it still makes sense after adding them together. (cid:2)
.
Exercise
1.3 Is the following data qualitative or quantitative?
(1) Book title,
(2) Welcome to Year 1,
(3) Maximum daily temperature,
(4) Car registration number.
1.4.3 The Four Levels of Measurement
A more detailed classification of data types is the four levels of measurement. These
include the nominal level, the ordinal level, the interval level, and the ratio level.
This detailed classification allows us to recognise what mathematical operations
can be applied for each level.
1.4.3.1 The Nominal Level
At the nominal level, the data is described by name or category, for example, hair
colour, gender, and house types, such as terrace houses, semi-detached houses, and
bungalows. At this level, possible mathematical operations to the data include set
membership and equality. For example, suppose a colour set has three colours:
green, yellow, and red. If a student’s hair colour is black, then the colour black is
not a member of that colour set. We will introduce the knowledge of sets in Chap.2.


================================================================================
PAGE 33
================================================================================

20 1 Introduction
1.4.3.2 The Ordinal Level
The ordinal level gives us a rank order. For example, the customer rating of a product
or a book and the award someone receives after completing a maths competition.
Possible mathematical operations that can be applied at this level are ordering and
comparison, whilst we cannot do addition and other computations.
Exercise
1.4 A group of students is asked the following questions. Is the answer
collected from each of the following nominal or ordinal?
(1) Are you an international student?
(2) What is your gender: male, female, prefer not to say, other?
(3) How many countries have you visited?
(4) What is your preferred contact method: email or telephone?
(5) What is your usual lunchtime: 11a.m.–12p.m., 12p.m.–1p.m., 1p.m.–
2p.m., or later than 2p.m.?
1.4.3.3 The Interval Level
Data measured at the interval level is like the ordinal level, placing numerical values
in order. Unlike the ordinal level, however, the interval level has a known and equal
distance between each value. For instance, consider the Celsius temperature. The
difference between 10 and 30 degrees is a measurable 20 degrees, as is the difference
between 40 and 60 degrees. However, the interval level data does not have a natural
zero. For example, if we consider the Celsius temperature at zero degrees, then
Celsius zero does not mean the absence of temperature.
More complicated mathematical operations are allowed at the interval level.
Compared with the ordinal level, we can do addition and subtraction at this level
besides ordering and comparison.
1.4.3.4 The Ratio Level
The ratio level allows us to multiply and divide too. Data has a clear definition of
zero at this level. For example, students’ marks for assessments in numerical values
are at the ratio level. We can have zero marks in the final score, and it makes sense
to say 90 out of 100 marks is twice as much as 45.
Remark 1.6 Let us consider two continuous number lines: one for the interval level
and the other for the ratio level. Data indicate positions along each line. There is no
actual zero position along the interval level line. In other words, the zero position


================================================================================
PAGE 34
================================================================================

1.4 TypesofData 21
is arbitrary along the line. For example, zero degrees Celsius and zero degrees
Fahrenheit are different temperatures. Zero degrees Celsius is the freezing point
of water, while zero degrees Fahrenheit is colder than that, and the freezing point of
water in the Fahrenheit scale is 32 degrees Fahrenheit. On the contrary, zero grams
and zero pounds mean the same thing along the ratio level line. There are no values
less than zero on the ratio level line. (cid:2)
.
Example 1.5 Calculating the increase or decrease in percentage terms is not
useful at the interval level. Suppose the temperature increases from 10 degrees
Celsius to 15 degrees Celsius. The increase in percentage terms is 15−10 =
10
50%. If we convert the temperatures to Fahrenheit, we have (10× 9)+32=
. 5
50 and (15× 9)+32 = 59, respectively. The increase in percentage terms
. 5 .
is 59−50 = 18%. It does not make sense to say 15 degrees Celsius is 50%
50 . .
warmer than 10degrees Celsius,whileweget only18%warmer inFahrenheit.
.
Example 1.6 Calculating the increase or decrease in percentage terms is
valid at the ratio level. Suppose weight increases from 10 grams to 15 grams.
The increase in percentage terms is 15−10 = 50%. If we convert the unit to
10 .
pounds, we have 10×0.0022 = 0.022 pounds and 15×0.0022 = 0.033
. .
pounds, respectively. The increase in percentage terms is 0.033−0.022 = 50%.
0.022 .
It does make sense to say 15 grams is50% heavier than 10 grams, since we
.
also get50%heavier in pounds.
.
Remark 1.7 Quantitative data includes the interval level and ratio level, while
qualitative data includes the nominal level and the ordinal level. (cid:2)
.
Understanding the data type and its measurement level will help us to select
models or statistical procedures to analyse the data. For example, for continuous
or ordinal data with a large number of categories, say the number of categories
greater than 4, we may use regression models, including ordinary linear regression
and neural networks and the Gaussian normal distribution to analyse the data. For
nominal or ordinal data (usually with a small number of categories, say 2, 3, or
4), we can use the Chi-square statistical test to examine whether the observed
values follow the assumed theoretical distribution, or we can use logistic regression
to make predictions on unseen data. Chapter 8 explains how linear regression
works; Chap.9 introduces the principle behind the traditional neural networks. Data
following a univariate Gaussian distribution and multivariate Gaussian distributions
are described separately in Chaps.10 and 11. The primary statistical analysis
techniques and the Chi-square test are presented in Chap.12. More linear regression
and the logistic regression model will be explained in Chap.13.


================================================================================
PAGE 35
================================================================================

22 1 Introduction
Exercise
1.5 Identify the level of measurement for the following:
(1) Military title: Lieutenant, Captain, Major.
(2) Categorisation of property: Flats, Detached, Semi-detached, Terraced,
End-of Terrace, Cottage, Bungalows.
(3) A list of temperatures in degrees Celsius for last week.
(4) Heights of a group of Year 6 students.
(5) Calendar years.
(6) Temperature in Kelvin scale.


================================================================================
PAGE 36
================================================================================

Chapter 2
Sets and Functions
Basic set theory and functions are the foundation of later chapters. Although we
assume that readers are familiar with the rudiments of basic set theory and basic
notions of functions, let us refresh our memory and define notations in this chapter.
We focus on functions with one variable in this chapter.
2.1 Sets
In this section, we will introduce set membership, how to find the cardinality of a
set, and how to represent sets using a Venn diagram. In addition, we will discuss
four basic normal set operations: set union, set intersection, set subtraction, and set
complement. Moreover, we will show how to write sets in comprehension and define
what a binary relation is.
2.1.1 Sets and Subsets
Definition 2.1 (Sets) A set is a collection of objects. The objects are known as the
elements of the set or its members.
For small sets, we can define a set by writing out the names of all the elements of
the set, separating them by commas, and enclosing the whole list in curly brackets.
For example: {apple, pear, orange, melon}. An empty set with no elements can be
represented by {} or ∅. Sets have the following two properties:
.
• Sets are not ordered. For example, {a, b, c} is a representation of the same set as
{c, a, b} .
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 23
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_2


================================================================================
PAGE 37
================================================================================

24 2 SetsandFunctions
• There are no repeats. For example, {a, b, c, b} is a representation of the same set
as {a,b ,c } .
Two sets are equal if they have the same members. The equal sign, =, can be
.
used when two sets are equal. For example,{1,2}={2,1}. The not equal sign, (cid:3)=,
. .
can be used when two sets are not equal.
Definition 2.2 (Cardinality) The Cardinality of a finite set is the number of
elements in the set.
It is denoted using the symbol #, or with a vertical bar on each side of the name of
.
the set. For example:
• #{1,2,3}=3 .
.
• |{3,3, 7, 2,1}| =4.
.
2.1.1.1 Infinite Sets
Sets can be infinite as well as finite. For example, three infinite sets that we will use
are:
• N: the set of natural numbers. In this book, it includes 0, that is, it includes all
.
non-negative numbers,{0,1,2,...}.
.
• Z: the set of integers,{...,−3,−2,−1,0,1,2,...}.
. .
• R: the set of real numbers, that is, any decimal number.
.
2.1.1.2 Intervals
A set that contains all the real numbers between two given numbers is called an
interval. For instance, all the real numbers between 2 and 3, including both numbers,
are a closed interval and denoted: [2,3]. If we do not include both the endpoints,
.
then it is an open interval and denoted: (2,3) or ]2,3[. Of course, we can include
. .
one endpoint and not the other, giving mixed intervals, that is,[2,3)and(2,3].
. .
2.1.1.3 Set Membership
The symbol ∈denotes is a member of a set. For example,
.
apple ∈{apple,pear,orange,melon}
.
is a true statement, since apple is a member of the given set, while strawberry ∈
{apple,pear,orange,melon} is a false statement, since strawberry is not a
.
member of the given set. (cid:3)∈ denotes is not a member of a set. So strawberry (cid:3)∈
.
{apple,pear,orange,melon}is a true statement.
.


================================================================================
PAGE 38
================================================================================

2.1 Sets 25
Example 2.1
If the real numberx ∈[2,3]then2≤x ≤3.
. .
If the real numberx ∈(2,3)then2 <x <3.
. .
If the real numberx ∈(2,3]then2 <x≤3.
. .
If the real numberx ∈[2,3)then2≤ x <3.
. .
Definition 2.3 (Subsets) The set A is a subset of the set B if and only if either the
set A is empty or every element of A is also anelementofB.A ⊆ B means A is a
.
subset of B.
Definition 2.4 (Proper Subset) The set A is a proper subset of the set B if and
only if A is a subset of B but not equal to B. A ⊂ B means A is a proper subset
.
of B.
Note that both ⊂ and ⊆ can be used with a line through them to denote their
. .
opposite. For example,{1,2,3,4}(cid:3)⊂{1,2,4}.
.
Exercises
2.1 What is the value (True or False) of each of the following statements?
(1) {4,8,−1}={4,8, 4, −1,−1 }.
.
(2) {x, y, z, z} ={z,y,x}.
. .
(3) {}={0}.
.
(4) #{x, y, z, z}=4.
.
(5) {}⊂{1, 2,4} .
.
(6) [1,2]=[1, 2) .
.
(7) (1,2) ⊂[1,2] .
.
2.2 Write down the value of each of the following.
(1) #{a, b, c, d,e,f}.
.
(2) #{}.
.
(3) #{{}}.
.
Definition 2.5 (Power Sets) The power set of a set S is the set containing all
possible subsets of S. The cardinality of the power set of a set S is2#S.
.


================================================================================
PAGE 39
================================================================================

26 2 SetsandFunctions
Example 2.2 GivenA ={3,4,5}with a cardinality of 3, the power set of A
.
is
{{},{3},{4},{5},{3,4},{3,5},{4,5},{3,4,5}},
.
and its cardinality is23 =8.
.
Exercise
2.3 Do the following:
(1) Write down the power set of{−1,1}. What is its cardinality?
.
(2) Write down the power set of{0,1,2,3}. What is its cardinality?
.
(3) What is the cardinality of the power set of{a,b,c,d,e,f,g,h}?
.
2.1.2 Venn Diagrams
Venn diagrams are a way of describing sets and how they are related to one another
in pictures. Each set is represented as a circle within a universe and contains values
written inside the circle’s boundary. The overlapping part of the two circles shows
elements shared by both sets.
Example 2.3 Suppose universe = 1{1,12,13,14,15,16,17,18,19,20},
.
SetA = {13,14,16,17,19}, andSetB = {12,13,15,16}. Figure 2.1 shows
. .
the relationship betweenSetAandSetB.
. .
Another example is the classification of numbers using sets considered in
Sect.2.1.1.1. In Fig .2.2, Zand Nare represented as circles within the universe R.
. . .
2.1.3 Basic Set Operations
Definition 2.6 (Set Union) Two sets may be joined together to form a new set
containing all of the elements in one or the other or both of them. This operation is
known as set union and is denoted using the symbol ∪.
.


================================================================================
PAGE 40
================================================================================

2.1 Sets 27
Fig. 2.1 An example of a Venn diagram used to visualise the relationships and intersections
among two data sets
Fig. 2.2 A Venn diagram showing the relationships among Z .(integers), N .(natural numbers), and
R .(real numbers)
Example 2.4 The union of two sets A and B in Fig.2.1 is A ∪ B =
{12,13,14,15,16,17,19}, whose elements are highlighted using under-
.
scores in Fig.2.3.
Definition 2.7 (Set Intersection) Two sets may be joined together to form a new
set containing only the elements in both of them. This operation is known as set
intersection and is denoted using the symbol ∩.
.
Example 2.5 The intersection of two sets A and B in Fig.2.1 is A∩B =
{13,16}, whose elements are highlighted using underscores in Fig.2.4.
.


================================================================================
PAGE 41
================================================================================

28 2 SetsandFunctions
Fig. 2.3 A Venn diagram illustrating the union of setsA∪B., representing all elements that belong
to A, B, or both, highlighted using underscores
Fig. 2.4 A Venn diagram illustrating the intersection of sets A ∩ B., representing comments
elements that belong to both A and B, highlighted using underscores
Definition 2.8 (Set Subtraction or Difference) Two sets may be joined together
to form a new set containing only the elements in the first but not the second. This
operation is known as set subtraction and is denoted using the symbol \.
.
Example 2.6 The set formed by subtracting set B from set A in Fig.2.1 is
A\B = 1{4,17,19}, whose elements are highlighted using underscores in
.
Fig.2.5.


================================================================================
PAGE 42
================================================================================

2.1 Sets 29
Fig. 2.5 A Venn diagram illustrating the set differenceA\B., representing elements that belong to
A, but not to B, highlighted with underscores
Definition 2.9 (Set Complement) The complement of a set A is the set of all
elements in the universe but not in A .Adenotes the set formed from the complement
.
of set A.
Example 2.7 The complement of set A in Fig.2.1 is A =
{11,12,15,18,20}, whose elements are highlighted using underscores
.
in Fig.2.6.
Fig. 2.6 A Venn diagram illustrating the complement of set A, A., representing elements that do
not belong to the set A, highlighted with underscores


================================================================================
PAGE 43
================================================================================

30 2 SetsandFunctions
Fig. 2.7 The Venn diagram referenced in Exercise 2.4, representing the sets and their relationships
Exercise
2.4 Given a Venn diagram shown in Fig.2.7, find the following:
(1) A∪ B.
.
(2) C ∩ B.
.
(3) A∪ B.
.
(4) A\(B ∩ C).
.
(5) A∪ B ∪ C.
.
(6) A∩ B ∩ C.
.
(7) (A∪ B)\ C.
.
(8) (A∪ B)\ C.
.
2.1.4 Sets Written in Comprehension
So far, we have written sets in extension. That is to list all of the values in the set
separated by commas within a pair of curly brackets. There is another way to write
sets. That is to give a typical element and a condition for its inclusion in the set. It
is called sets written in comprehension.


================================================================================
PAGE 44
================================================================================

2.1 Sets 31
Example 2.8 Suppose we haveS ={x ∈N| x <5},where the curly brackets
.
tell us that this is a set. Inside the brackets, there are two sections. The left of
the vertical bar gives us the signature of the values in the set. A signature tells
us which universe the value named by x is drawn from. In this example, it
is drawn from the universe of values N. The right of the vertical bar states
.
the condition that each member of the set must satisfy. If we write this set in
extension, we haveS = {0,1,2,3,4}. The vertical bar |(sometimes written
. .
as a colon:) is usually read as such that. So, set S can be described as the set
of all numbers x inNsuch thatx <5 .
. .
The condition in a set comprehension expression is a truth-valued expression. All
values that make this expression true are members of the set. All values that make
the expression false are not members of the set (they are in its complement).
2.1.4.1 Using Logic
We can define more complex conditions using operations from logic:
• AND is represented by the symbol ∧.
.
• OR is represented by the symbol ∨.
.
• NOT is represented by the symbol ¬.
.
Example 2.9 Let Z = x{ ∈ N| x >10 ∧ x <16}. If we write set Z in
.
extension, we haveZ ={11,12,13,14,15}.
.
Exercise
2.5 Write each of the following sets in an extension.
(1) A ={y ∈ N|¬(y >10)}.
.
(2) B ={x ∈N|(x < 12)∧ (x ≥6)}.
.
(3) C ={z ∈ N|(z < 8)∧ (z<5)}.
.
(4) D ={y ∈ N|(y < 8)∨ (y <5)}.
.
(5) E ={x ∈ N|¬(x ≥ 12)∧ (x >3)}.
.


================================================================================
PAGE 45
================================================================================

32 2 SetsandFunctions
2.2 Binary Relations
Definition 2.10 (Cartesian Product Sets) Given two sets A and B, the set that
contains all ordered pairs (x,y) such that x belongs to A and y belongs to B is
.
called the Cartesian Product. It is denoted as A×B, which can be expressed as
.
follows:
A×B ={(x,y)|x ∈A∧y ∈B}.
.
A and B may be subsets of different universes. If either A or B is an empty set,then
∅×B =A ×∅=∅.I fA(cid:3)=B and both A and B are not the emptyset,thenA×B
. . .
is not equivalent toB×Abecause the inside of each pair is ordered.
.
Example 2.10 IfA ={a,b}andB ={0,1,2}, then
. .
A×B ={(a,0),(a,1),(a,2),(b,0),(b,1),(b,2)};
.
B×A ={(0,a),(0,b),(1,a),(1,b),(2,a),(2,b)}.
.
Remark 2.1 Although the inside of each pair in a Cartesian product set is ordered,
it should be remembered that the actual set, like all sets, is not ordered.
So ifA ={a,b}andB ={1}, then:
. .
A×B ={(a,1),(b,1)}={(b,1),(a,1)}(cid:3)={(1,a),(1,b)}=B×A.
.
(cid:2)
.
Exercise
2.6 Find the value (True or False) of each of the following:
(1) (2,8) ∈{(8,1), (2,10), (1,10), (8,2)}.
.
(2) (3,7) ∈{(1,3), (3,7), (7,3), (1,7)}.
.
(3) {(1,4), (0,2), (10,9)}={(4,1), (2, 0), (9,10)}.
.
(4) {(1,4), (0,2), (10,9)}={(10,9), (0,2), (1,4)}.
.
2.7 Write each of the following Cartesian products as a single set in
extension:
(1) {2,3,5}×{0, 1} .
.
(2) {0,1}×∅.
.


================================================================================
PAGE 46
================================================================================

2.2 BinaryRelations 33
We can form the Cartesian product of any number n ∈ N of sets and whose
.
elements will be n-tuples. For example, we can use it to model customer accounts
in the following way:
A×B×C ={(a,b,c)|a ∈A∧b∈B∧c∈C},
.
where A represents customer account numbers, B customer names, and C customer
addresses.
2.2.1 BinaryRelations
Definition 2.11 (Relation) Given two sets A and B, a relation from A to B is a
subset of theCartesianProductA×B.
.
Any subset of the setA×B can be considered as a relation. That could be the
.
empty set, the entire set,A×B, or anything in between.
.
Definition 2.12 (Binary Relation) A binary relation relates values from one uni-
verse to the values of another. The from-universe is called the source; the to-universe
is called the target.
2.2.1.1 Kinds of Relation
Figure 2.8 shows four types of relation. In each panel, the left rectangle shows the
source universe, and the right shows the target universe. The oval in the source
includes input values of a relation, called the domain of the relation, while the oval
in the target includes output values of a relation, called the range of the relation.
Panel (a) shows a one-to-one relation, where each value in the domain has only
one corresponding value in the range.
Panel (b) presents a many-to-one relation, where two (can be more) values in the
domain have the same value in the range.
Panel (c) displays a one-to-many relation, where one value in the domain has two
(can be more) different values in the range.
Panel (d) represents a many-to-many relation, where a value in the domain may
have more than one output in the range, and a value in the range may have more
than one corresponding value in the domain.
Remark 2.2 Note that not all points are necessarily in the ovals in Fig.2.8, since
a relation is any subset of the whole Cartesian product, source× target. Hence,
.
the domain is a subset of the source, and the range is a subset of the target. That is,
domain ⊆ source and range ⊆ target. Note the illustrated relations in Fig.2.8
. .
also do not include every possible pair from the domain to the range. Again, this is


================================================================================
PAGE 47
================================================================================

34 2 SetsandFunctions
Fig. 2.8 An illustration of four kinds of relations: (a) one-to-one, (b) many-to-one, (c) one-to-
many, and (d) many-to-many
because the relation is a subset of the whole Cartesian product, so the relation does
not have to contain all pairs. (cid:2)
.
2.3 Functions
Many-to-one and one-to-one relations are very common in computing and have a
special status, and they are called functions.
Figure 2.9 presents examples of relations, some of which are functions and others
are not. The left column represents the input values (possible domain), while the
right column represents the possible output values.
Panels (a) and (b) are two functions, since all the elements in the domain have
just one corresponding image, or output, in the target universe and so construct the
range of actual values in the target. Values in the right column do not all have a value
in the domain with their image in the target universe.
Panel (c) illustrates a case of a relation that is not a function. Every element
in the possible domain should have an image in the range by applying the given
function. But 3 in green does not have a related value in the range. Panel (d) is
another example of a relation that is not a function, since 3 has two images in the
range, b and d, indicating a one-to-many r elation.
So, in summary, a function has to have an image for every value in the domain,
and it has to have just one image. The set of images in the target universe is called
the range.


================================================================================
PAGE 48
================================================================================

2.3 Functions 35
Fig. 2.9 An illustration
depicting various relations,
distinguishing between those
that are functions and those
that are not
Exercise
2.8 Given the domain as{0,1,2}, are the following relations also functions
.
(assume the target universe includes{0,1,2,3,4,5,6,7,8}):
.
(1) {(0,1), (2,2), (1,3) }?
.
(2) {(0,2), (1,3) }?
.
(3) {(0,7), (1,5), (2,6), (0,6)}?
.
(4) {(1,4), (0,4), (2,4) }?
.
Definition 2.13 (Function) Let x represent the elements of the domain (denoted
as D), y represent the elements of the range (denoted as W), and f symbolise the
function, then wehavey = f(x). It can be written as follows:
.
W ={y|y = f(x)∧x ∈D}.
.


================================================================================
PAGE 49
================================================================================

36 2 SetsandFunctions
f( x ) is also called the image of x with respect to the function f. The domain
.
variable x is called the independent variable, while the range variable y is called
the dependentvariable.
Example 2.11 GivenW ={y|y =x3∧x ∈[−3,3]}, we know that f(x)=
.
x3, the domain is−3≤x ≤3, and the corresponding range is−27≤x ≤27.
. . .
2.3.1 Graph of a Function
The plot of pairs(x,f(x))in a coordinate system is the graph off(x).
. .
Example 2.12 Figure 2.10 is a pictorial representation of the function {y|y =
x3∧x ∈[−3,3]}.
.
Fig. 2.10 An example of the 30
graph of a function
20
10
0
-10
-20
-30
-3 -2 -1 0 1 2 3


================================================================================
PAGE 50
================================================================================

2.3 Functions 37
2.3.2 Common Functions with One Variable
2.3.2.1 Linear Function
A linear function with one independent variable has the following form:
y = f(x)=a +a x,
. 0 1
where a is the intercept on the vertical axis in the graph (the constant term) and a
0. 1.
is the slope of the line in the graph (the coefficient). Ifa =0, theny =a , that is,
1 . 0.
the line is horizontal. Ifa = 0, the line will pass through the origin. Ifa (cid:3)= 0and
0 . 0 .
a (cid:3)=0, there are two cases witha >0anda <0, respectively.
1 . 1 . 1 .
Example 2.13 Figure 2.11 shows two linear functions,y =5−2x and y =
.
5+2x. Both functions have the same intercept, but one has a negative slope
.
(solid line), and the other has a positive slope (dashed line).
2.3.2.2 Polynomial Function
These are functions built out of non-negative integer powers of the independent
variable.
Fig. 2.11 Two linear 15
functions with the same
intercept: a dashed line with a
positive slope and a solid line
with a negative slope 10
5
0
-5
-6 -4 -2 0 2 4 6


================================================================================
PAGE 51
================================================================================

38 2 SetsandFunctions
25
100
20
50
15
0
10
-50
5
0 -100
-6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6
(a) (b)
Fig. 2.12 Examples of two simple polynomial functions
Example 2.14 Figure 2.12 illustrates two simple polynomial functions. Panel
(a) shows f(x) = x2 and panel (b) depicts f(x) = x3, both defined for
. .
x ∈[−5,5].
.
To have a more complicated polynomial function, we can start with the building
blocks, such as 1,x,x2,x3, and so on, and we can multiply these basic functions
.
by numbers and then add a finite number of them together. For example, f(x)=
7x2+4x3−2.
.
2.3.2.3 Exponential Function
The exponential function is defined asf(x)=ax, where the basea >0 anda (cid:3)=1.
. . .
The domain of the function is (−∞,∞); the range of the function is (0,∞). T he
. .
graph of the function alwayspasses(0,1)sincea0 =1.
. .
Example 2.15 Figure 2.13 shows two exponential functions with a domain
of[−5,5]. Panel (a) displaysf(x)=2x with a base equal to 2, and panel (b)
. .
presentsf(x)=2 −x =(1)x with a base of 1.
2 . 2.
There is a horizontal asymptote aty = 0. The curve does not touch the x-axis,
.
no matter what it looks like on the graph. In fact, the graph off(x)=2x is just the
.
reflection off(x)=2 −x in the y-axis. A common base isa =e =2.71828,u sing
. .
the irrational number e, giving thefunctionf(x)=ex.
.


================================================================================
PAGE 52
================================================================================

2.3 Functions 39
30 30
25 25
20 20
15 15
10 10
5 5
0 0
-5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5
(a) (b)
Fig. 2.13 Examples of two simple exponential functions
Exponential functions are important in the Data Science field. When we intro-
duce probability distributions in Chap.10, we will see that a Gaussian distribution
of a random variable is, in fact, a member of the exponential function family. In
addition, there are many applications using a type of function that is a combination
of exponential functions, for example,
ex−e−x
and
ex+e−x
. These are special
2 . 2 .
functions: hyperbolic functions. Readers are referred to [6] to find more details.
2.3.2.4 Logarithmic Function
A logarithmic function is denoted as f(x) = log x, where a is a constant and
a .
a >0 , buta (cid:3)=1. The domain of a logarithmic function is(0,∞). The graph of ax
. . . .
is symmetric to the graph of log x about the line of y = x (see Fig.2.14). If the
a . .
basea =e=2.71828, then we denote log x =log x as lnx.
. a e . .
The logarithmic function is also important in the Data Science field. We will
discuss a cost function or error function defined in a log probability format in
.
Chap.13.
2.3.2.5 Trigonometric Functions
The variable x in these functions is generally expressed in radians (π radi-
.
◦
ans = 180 ). Figure 2.15 shows sinx and cosx, respectively, in the domain
. . .
[−8radians,8radians]. The relations between sinx and cosx can be summarised
. . .
as:
π
sinx =cos( −x), (2.1)
.
2


================================================================================
PAGE 53
================================================================================

40 2 SetsandFunctions
Fig. 2.14 The graphs of ex . 5
and lnx.are symmetrical
about the line ofy=x. 4
3
2
1
0
-1
-2
-3
-4
-5
-10 -5 0 5 10
1 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0 0
-0.2 -0.2
-0.4 -0.4
-0.6 -0.6
-0.8 -0.8
-1 -1
-8 -6 -4 -2 0 2 4 6 8 -8 -6 -4 -2 0 2 4 6 8
(a) (b)
Fig. 2.15 Panel (a)s hows thesinx.function, while panel (b)s hows thecosx.function
π
cosx =sin( −x), (2.2)
.
2
sin2x+cos2x =1. (2.3)
.
Equations (2.1), (2.2), and (2.3) are trigonometric identities involving trigonometric
functions. There are quite a lot of them, but it is not necessary to remember all the
identities. However, it is important to know that these identities are useful when we
need to simplify trigonometric functions, and sometimes, they can be used to solve
certain types of integrals. We will introduce integrals in Chaps.5 and 6.


================================================================================
PAGE 54
================================================================================

2.3 Functions 41
2.3.3 Properties of a Function
Definition 2.14 (Bounded Functions) If there is a constant M such that f(x)≤
M for all x in an interval, then M is called an upper bound of the function. On the
.
other hand, if there is a constant M such that f(x)≥ M for all x in an interval,
.
then M is called a lower bound of the function. Usually, to indicate that a function
is bounded both above and below by M,wewrite|f(x)| ≤M, where M is a non-
.
negative real valueand|f(x)| means the absolute value of f(x).
. .
Example 2.16 f( x) = cosx is bounded in the interval (−∞,∞) since for
. .
allx ∈ R,|cosx | ≤1is valid. HereM = 1. Of course, M can be any value
. . .
not less than 1 in this ex ample.
Example 2.17 Let us consider f(x) = 1. First, suppose x ∈ (0,1). We
x. .
notice that however big a value M takes, we can always find a small value
approaching zero for x, sothat 1 is greater than M. Therefore, the function is
x.
unbounded, since there does not exist an M, so that |1| ≤M is valid in the
x .
interval(0,1). Next, supposex ∈ (1,3). Then, the function is bounded. For
. .
example, takingM =1, then|1|≤1 is valid to all x in the interval(1,3).
. x . .
Example 2.18 Other bounded function examples are the sigmoid functions.
One common sigmoid function, defined by f(x) = 1 = ex , i s
1+e−x ex+1.
bounded inside the interval(0,1)(see Fig.2.16, where the domain is defined
.
as[−7.8,7.8]).
.
Definition 2.15 (Monotonic Functions) Given a functionf(x)in an interval, for
.
any two points x and x in the interval, if we have:
1. 2.
• x <x and f(x ) ≤ f(x), then the function is monotonic increasing. If
1 2. 1 2 .
f(x )<f(x ), then the function is called strictly increasing.
1 2 .
• x < x and f(x ) ≥ f(x), then the function is monotonic decreasing. If
1 2. 1 2 .
f(x )>f(x ), then the function is called strictly decreasing.
1 2 .


================================================================================
PAGE 55
================================================================================

42 2 SetsandFunctions
Fig. 2.16 A plot of the 1
sigmoidfunction
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-8 -6 -4 -2 0 2 4 6 8
Example 2.19 f( x) = x 2 is monotonic increasing in the interval of[0,∞)
. .
and monotonic decreasing in (−∞,0]. It is not monotonic in the domain of
.
(−∞,∞) (see Fig.2.12a). However, f(x)= x3 is monotonic increasing in
. .
the domain of(−∞,∞)(see Fig.2.12b).
.
Definition 2.16 (Odd and Even Functions) A function f(x) is called odd if
.
f(−x) = −f(x) for all x in the domain. A function f(x) is called even if
. .
f(−x)= f(x) for all x in the domain.
.
An odd function is symmetrical about the origin of the coordinate system. An
even function is symmetrical about the y-axis of the coordinate system. A function
does not have to be even or odd.
Example 2.20 f( x) = x 3 is an odd function, because f(−x) = (−x)3 =
.
−x3 =−f(x), and it is symmetrical about the origin (see Fig.2.12b). f(x)=
.
x2 is an even function, because f(−x) = (−x)2 = x2 = f(x), and it is
. .
symmetrical about the y-axis (see Fig.2.12a).


================================================================================
PAGE 56
================================================================================

2.3 Functions 43
Exercise
2.9 Is each of the following functions even, odd, or neither even nor odd?
[Hint: some can be answered by looking at the figures given above.]
(1) x5− 2x3+ 3 x.
.
(2) x3− 2x + 1.
.
(3) ex.
.
(4) ln x.
.
(5) sinx.
.
(6) cos x.
.
(7) Sigmoid: 1 .
1+e−x.
(8) Hyperbolic Cosine:
ex+e−x
.
2 .
(9) Hyperbolic Tangent:
ex−e−x
.
ex+e−x.
Definition 2.17 (Period of a Function) Given a functionf(x), if there exists l (cid:3)=
.
0, so thatf(x+l)= f(x)is valid for any x value in the domain, then the function
. .
is a periodic function and l is called the period of thefunction.
Example 2.21 Functions sinxand cosxare periodic functions with a period
. .
of 2π (see Fig.2.15).
.
Here are two more trigonometric identities:
sin(x+2π)=sin x, (2.4)
.
cos(x+2π)=cosx. (2.5)
.
2.3.4 Inverse Functions
Definition 2.18 (Inverse Functions) Suppose y = f(x). If the relation between
.
the domain and range values is one-to-one, then a new functionf
−1
can be created
.
by interchanging the domain and range of f .f
−1
is called the inverse function. It
.
can be denoted as x = f −1(y). However, it is usually convenient to rename the
.
domain variable as x and the range variable as y, giving thenotationy = f −1(x).
.
The inverse function is symmetric about the liney =x with the original function.
.


================================================================================
PAGE 57
================================================================================

44 2 SetsandFunctions
Example 2.22 Supposey = x2 withx ∈ (−∞,∞)andy ∈ 0[,∞). There
. . .
is no inverse function here since, for instance, both 22 and(−2)2 are equal to
. .
4, and therefore the function is not one to one. This means the inverse relation
is one to many and so not a function. √
If we limit the domain to bex ∈ 0[,∞), the inverse function isy = x.
. .
On the other hand√, if we limit the domain to be x ∈ (−∞,0]
.
, the i nverse
functionisy =− x.
.
Example 2.23 The logarithmic function f(x) = lnx is the inverse of the
.
exponential functionf(x)=ex (see Fig.2.14).
.
Example 2.24 Inverse trigonometric functions.
By convention, we denote the inverse of sinx asarcsin(x), and the inverse
. .
of cosx asarccos(x).
. .
Trigonometric functions are periodic and so not one to one, hence to define
an inverse function we have to restrict the domain of the original function so
that it is one to one. This can be done by restricting the domain to[−π,+π]
2 2 .
for sinx and restricting the domain to [0,π] for cosx. The domain of x in
. . .
these two inverse functions for real results is,therefore,[−1,1].
.
2.3.4.1 How to Find the Inverse Function
We can apply the following procedure to find the inverse function:
• Step 1—sety = f(x);
.
• Step 2—make x the s ubject;
• Step 3—replace y with x toobtainf
−1(x).
.
Example 2.25 Supposef(x)= 6x−3, then this function is one to one and
.
sof
−1(x)exists.
Findf
−1(x).
. .
Solution
• Step 1—sety = f(x), that isy =6x−3.
. .
(continued)


================================================================================
PAGE 58
================================================================================

2.3 Functions 45
Example 2.25 (continued)
• Step 2—make x the subject, then we havex = y+3.
6 .
• Step 3—replace y with x toobtainf
−1(x),
then we havef
−1(x)= x+3.
. 6 .
Exercise
2.10 Find the inverse function for each of the following functions.
(1) f( x)= x 3+10;
.
(2) f( x)= 3si nx ;
.
(3) f( x)= 4 + ln(x+1) ;
.
(4) f( x)= 3x .
3x+1.
2.3.5 Composition of Functions
Definition 2.19 (Composite Functions) Let f and g befunctions.f ◦g (read as
.
f composite g) is called a composite function, denotedasf ◦g = f(g(x)), where
.
the range values ofg(x)are the domain values of f.
.
That is, to obtain the composition of functions f and g, f ◦ g, we need to first
.
apply the function g to x and then apply function f tog(x). Similarly, to find the
.
composition of functions f and g,g◦f, we need to first apply the function f to x
.
and then apply functiongtof(x)
.
Example 2.26 Letf(x)=5x+2 andg(x)=x2.F indf ◦g.
. . .
Solution
f ◦g = f(g(x))= f(x2)=5x2+2.
.


================================================================================
PAGE 59
================================================================================

46 2 SetsandFunctions
Exercise
2.11 Findg◦f for the following given functions g and f .
.
(1) f( x)= 5x+2 andg(x)=x2.
. .
(2) f( x)= 2x andg(x)=sinx.
. .
(3) f( x)= e x andg(x)=x2.
. .
(4) f( x)= e x andg(x)=lnx.
. .
(5) f( x)= cosx andg(x)=x3.
. .
Remark 2.3 The order in the composition of functions is important because, in
general,f ◦g(x)is not the same asg◦ f(x).
. .
Remark 2.4 Two functions cannot always be composited to obtain a new function.
For example, let f(x) = arcsinx and g(x) = 2 + x2. The composition of the
. .
functions f and g, f ◦ g(x), does not exist, because the range value for any x in
.
thedomain(−∞,∞)ofg(x)is a value equal or greater than 2, which cannot be an
. .
input tof(x)=arcsinx (see Example 2.24 in Sect.2.3.4 of this chapter). (cid:2)
. .
2.3.6 Functions of Two or More Variables
Functions may have more than one independent variable. The domain of a function
of two or more variables is a set of n-tuples, while the range is one-dimensional with
an interval of numbers.
√
Example 2.27 Let f(x,y) = x3 +2 y with two independent variables x
.
and y, whose domain can be written as follow s:
{(x,y)|−∞ <x <∞∧y ≥0}.
.
We conclude this chapter here. Readers interested in gaining a deeper funda-
mental understanding of sets and functions are encouraged to explore classical
textbooks, such as Chapter 2 of [7].


================================================================================
PAGE 60
================================================================================

Chapter 3
Linear Algebra
In this chapter, we introduce vectors and matrices. We show how to do the basic
operations on them and why we need to use both vectors and matrices. In this book,
we only consider vectors and matrices that consist of finite real numbers, and not,
for instance, complex numbers.
3.1 Vectors
The input in most machine learning applications is an ordered list of numbers; for
instance, as indicated in Case Study 3 in Chap.1, most neural networks consist of
one or more layers, and the initial input data goes to the set of first-layer units or
neurons. This input could be:
• an ordered list of features of the skin that may determine the ability of the skin
to absorb medical drugs (such as Nicotine).
• the list of pixel values from scanning a picture.
In all cases, this list of ordered values is a vector, and knowledge of how to
manipulate vectors is essential for a full understanding of the operation of machine
learning techniques.
Definition 3.1 (Vector) A vector is an ordered list of numbers and subscripts. Each
subscript denotes the position of the value in the list. Such a list of values, denoted
asx=(x , x , ··· , x), where d is the number of elements in the list, is called a
1 2 d .
linear array or vector.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 47
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_3


================================================================================
PAGE 61
================================================================================

48 3 LinearAlgebra
Example 3.1 Five students’ Maths grades are listed as follows:
82, 90, 65, 78, 46. We can denote all the values in the list using only one
.
symbol, for instance, xwith different subscripts, that is,x , x , x , x , x .
. 1 2 3 4 5.
Sometimes a vector is written vertically. For example,
⎡ ⎤
2
⎣ ⎦
. 1 .
5
Note that we use both a pair of square brackets and round brackets to denote
vectors and matrices in this book.
3.1.1 Vectors in Physics
Vectors can be represented by arrows having appropriate lengths and directions and
emanating from some given reference point. In Fig.3.1, the reference point is(0,0),
.
and the ending point is the vectorw = (4,3), whose magnitude is denoted as||w||
. .
and θ is the angle from the positive horizontal axis to the vector measured in an
.
anticlockwise direction. In Fig.3.1, the vector has two elements or components and
is referred to as a vector in R2, where Ris the field of real numbers. It is one of the
. .
infinite number of possible vectors in R2. In general, a vector has d elements over
.
the field of real numbers inRd. The field of real numbers means we can do addition
.
and scalar multiplication of vectors as with all real numbers.
Fig. 3.1 An illustration of a
specified vector(4,3).,w here
(cid:2)w(cid:2) .denotes the length of the
vector, and the arrow
indicates its direction from
the origin


================================================================================
PAGE 62
================================================================================

3.1 Vectors 49
3.1.2 Vector Addition
Considering two vectors in Rd: x = (x , x , ··· , )x and w =
. 1 2 d .
(w , w , ··· , w), their sum is given by
1 2 d .
x+w=(x +w , x +w , ··· , x+w ). (3.1)
. 1 1 2 2 d d
Example 3.2 Suppose we have two vectors: a = (2,4) and b = (5,1).
. .
According to Eq.(3.1), a + b = (2 + 5,4 + 1) = (7,5) and a − b =
.
(2−5,4−1)=(−3,3).
.
Figure 3.2 shows aand bin a plane. The reference point is(0,0)for both
. . .
vectors. For a, the ending point is(2,4).F or b, the ending point(5,1). If we
. . . .
draw aparallelogramwithaand bas its two sides, thena+bactually is the
. . .
longer diagonal line;a−b, in fact, can be considered asa+(−b), that is the
. .
shorter diagonal line.
3.1.3 Scalar-Vector Multiplication
The scalar product of a vector with a real number k is given by:
kx=(kx , kx , ··· , kx), (3.2)
. 1 2 d
Fig. 3.2 An example of vectors


================================================================================
PAGE 63
================================================================================

50 3 LinearAlgebra
where k is any real number. When we multiply a vector with a scalar, we actually are
compressing or stretching the vector. For instance, if k is 2, then we are doubling the
length of the vector. We can also change the direction of the vector to the opposite
direction by multiplying by a negative real number.
Exercise
3.1 Compute the following:
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
2 3 2 3
(1) ⎣ 1 ⎦ + ⎣ 6 ⎦ , . and ⎣ 1 ⎦−⎣ 6 ⎦ . .
5 1 5 1
⎡ ⎤ ⎡ ⎤
2 2
(2) 2× ⎣ 1 ⎦ , . and −2×⎣ 1 ⎦ . .
5 5
3.2 The Dot Product of Two Vectors
3.2.1 Dot Product: Algebra Definition
Definition 3.2 (Dot Product) Considering arbitrary vectors
w=(w , w , ··· , w)andx=(x , x , ··· , x)in Rd,
1 2 d . 1 2 d . .
the dot product (also referred to as an inner product) of wand xis denoted and
. .
defined by
w·x=<w,x>=w ·x +w ·x +···+w ·x . (3.3)
. 1 1 2 2 d d
The use of wand xis not entirely a coincidence, since it is the type of calculation
. .
that occurs in machine learning, such as in a neural network. If xis the input vector
.
to a neural network, then each element is multiplied by a corresponding weight from
a weight vector w, and then all are added together to get a net input. The dot product
.
exactly represents this operation.
Example 3.3 Suppose we have two vectors: a = (2,4) and b = (5,1).
. .
According to Eq.(3.3), we havea·b=2×5+4×1=14.
.


================================================================================
PAGE 64
================================================================================

3.2 TheDotProductofTwoVectors 51
Exercise
3.2 Compute the dot product for the following vectors:
(1) m= (−2,−1 )andn=(1,3).
. .
(2) u= (−2,−1,3 )andv=(1,3,−2).
. .
(3) s= (−2,2 )andt=(3,3).
. .
(4) a= (4,3, 5 )andb=(−4,−3,5).
. .
3.2.2 Norm
Definition 3.3 (Norm) The norm or length of a vector w in Rd, denoted by
. .
(cid:2)w(cid:2), is defined to be the non-negative square root of w · w. That is, if w =
. .
(w , w , ··· , w), then
1 2 d .
(cid:6)
√
(cid:2)w(cid:2)= w·w= w2+w2+···+w2. (3.4)
. 1 2 d
Because a norm is the length of a vector, we can use it to measure the distance
between two points.
Example 3.4 Continue Example 3.2.
In Fig.3.3, the distance between two points P and Q is measured as the norm
−→ −−→
ofa−b, where aisOP and bisOQ. That is
. . . . .
(cid:7) √
(cid:2)QP(cid:2)= (2−5)2+(4−1)2 =3 2.
.
−−→
The distance between O and M is measured as thenormofOM. That is
.
(cid:7) √
(cid:2)OM(cid:2)= (7−0)2+(5−0)2 = 74.
.
Considering arbitrary vectors w = (w , w , ··· , w) and x =
1 2 d .
(x , x , ··· , x) in Rd, in general, the distance between these two vectors is
1 2 d . .
defined as follows:
(cid:7)
d(w,x)= (w −x )2+(w −x )2+···+(w −x )2. (3.5)
. 1 1 2 2 d d


================================================================================
PAGE 65
================================================================================

52 3 LinearAlgebra
Fig. 3.3 An illustration of
distances between two points
Exercise
3.3 Compute the distance between the following vectors:
(1) w= (1,10,3,2) andz=(5,4,−1,0).
. .
(2) a = (4,3,5) andb=(−4,−3,5).
. .
3.2.3 Vector Magnitude and Direction in R2
.
Suppose we have
√
a vector u(cid:6) = (x
1
,y
1
)
.
(see Fig.3.4). Its magnitude is its norm,
that is, (cid:2)u(cid:2) = u·u = x2+y2; its direction is given by θ = tan −1(y1).
1 1. x1 .
Each component of the vector can be obtained as follows: x = (cid:2)u(cid:2)cos(θ) and
1 .
y =(cid:2)u(cid:2)sin(θ), respectively. Note that we define θ so that0≤ θ <2π.
1 . . .
Fig. 3.4 An illustration of a
vector, with the point marking
the vector’s endpoint


================================================================================
PAGE 66
================================================================================

3.2 TheDotProductofTwoVectors 53
Example√ 3.5 Supp√ose we have a √vector: a = (2,4),
.
then its magnitude is
(cid:2)a(cid:2) = a·a = 22+42 = 2 5; and its direction is θ = tan −1(4) =
. 2 .
1.1071radians.
.
In reve√rse, the components can be foun√d from the magnitude and direction
asx =2 5cos(1.1071)=2andy =2 5sin(1.1071)=4.
1 . 1 .
Exercise
3.4 Suppose we have two vectors:u=(2,−3)andv=(5,4).
. .
(1) Compute the direction of each vector.
(2) Compute the length of each vector.
(3) Compute the distance between uand v.
. .
3.2.4 Dot Product: Geometric Definition
Definition 3.4 (Dot Product) Considering two vectors wand x, their dot product
. .
can also be defined as follows:
w·x=(cid:2)w(cid:2)(cid:2)x(cid:2)cosθ. (3.6)
.
where θ is the angle between wand x.
. . .
This definition of dot product can be proved to be equivalent to the previous one,
Definition 3.2. A detailed explanation can be viewed in [8].
Remark 3.1 Definition 3.4 can be used to measure the similarity between two
vectors in terms of the direction of the vectors.
• When cosθ = 0, that is, two vectors are at right angles to each other, referred
.
to as being orthogonal to each other, we havew·x = 0. Intuitively, it says two
.
vectors have zero similarity.
• When cosθ =1, that is, two vectors are pointing in the same direction, we have
.
w·x=(cid:2)w(cid:2)(cid:2)x(cid:2). This is the largest value one can get forw·x.
. .
• When cosθ =−1, that is, two vectors are opposed to each other, we have w·x=
.
−(cid:2)w(cid:2)(cid:2)x(cid:2). This is the most negative value one can get forw·x.
. .
(cid:2)
.


================================================================================
PAGE 67
================================================================================

54 3 LinearAlgebra
Fig. 3.5 An illustration of
twov ectorsi nE xamp le3.6
Example 3.6 Calculate the dot product of the two vectors: a = (2,4) and
.
b=(5,1)in Example 3.3 using Eq.(3.6).
.
√ √ √ √
Solution (cid:2)a(cid:2)= 22+ 42 = 20and (cid:2)b(cid:2)= 52+12 = 26.
. .
Since the direction of a is θ = tan −1(4) and the direction of b is θ =
. a 2 . . b
tan −1(1), the angle between aand bisθ = θ −θ ≈ 0.9098radians (see
5 . . . a b .
Fig.3.5). The cosof0.9098radians is about0.6139. Thus, we have
. . .
√ √
(cid:2)a(cid:2)(cid:2)b(cid:2)cosθ = 20× 26×0.6139≈14.
.
This is the same answer as we got before in Example 3.3.
Exercise
3.5 Compute the dot product for the following vectors using both Eqs.(3.3)
and (3.6), and check that they are the same.
(1) u= (2,2 )andv=(3,3).
. .
(2) u= (2,2 )andw=(−2,2).
. .
(3) u= (2,2 )ands=(−2,−2).
. .
(4) u= (2,2 )andt=(0,5).
. .


================================================================================
PAGE 68
================================================================================

3.3 Matrices 55
3.2.5 Unit Vector
Definition 3.5 (Unit Vector ) For any non-zero vector uin Rd, the vectoruˆ = u
. . (cid:2)u(cid:2).
is a unit vector in the same direction as u. The process of finding uˆ from uis called
. . .
normalising u.
.
Example 3.7 For a given vector u = (2,3,1), its unit vector can be
.
calculated as follows:
u (2,3,1) 2 3 1
uˆ = = √ =(√ ,√ ,√ ).
. (cid:2)u(cid:2) 22+32+12 14 14 14
Exercise
3.6 Calculate the unit vector for each of the following vectors:
(1) w= (2,1) .
.
(2) s= (3, 1) .
.
(3) t= (3,1,−1) .
.
(4) v = (−1,2,4,1) .
.
3.3 Matrices
Definition 3.6 (Matrix) A matrix is a rectangular arrangement of numbers made
up of rows and columns. A matrix with m rows and n columns is calledanm×n
.
matrix. Each element in a matrix ( M) is identified by two indices: the first one
.
indicates the specific row, and the second indicates the column.
A matrix is usually labeled with a (bold) capital letter. For example,
(cid:8) (cid:9)
M M M
M= 1,1 1,2 1,3 .
.
M M M
2,1 2,2 2,3
Mis a rectangular matrix, andM represents the element in the first row and the
. 1,2.
second column of M. A matrix with the same number of rows and columns is called
a square matrix.


================================================================================
PAGE 69
================================================================================

56 3 LinearAlgebra
Example 3.8 Five students’ Maths grades are listed as follows:
82, 90, 65, 78, 46.
.
Their corresponding English grades are listed as follows:
76, 78, 60, 50, 60.
.
The matrix looks like
⎡ ⎤
8276
⎢ ⎥
⎢9078⎥
⎢ ⎥
.
M =⎢
⎢
6560⎥
⎥
,
⎣7850⎦
4660
where each column corresponds to the specific subject marks and each row
represents one student’s marks for two subjects.
Example 3.9 In terms of neural networks, if there are multiple input units,
then each unit will have a weight vector w. So, we will need a compact method
.
to represent this collection of weights, and matrices represent just what is
needed. Also, as we shall see in Sect.9.2 of Chap.9, the multiplication of a
vector and a matrix of these weights is just the operation we need to represent
the complete operation of finding the inputs to the first layer of the neural
network.
3.3.1 Matrix Addition
The sum of two matrices M and N, where M and N must be the same size, is the
. . . .
matrix obtained by adding corresponding elements from Mand N.
. .


================================================================================
PAGE 70
================================================================================

3.3 Matrices 57
Example 3.10
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
24 14 2+ 14+4 38
+ = = .
. 01 10 0+ 11+0 11
3.3.2 Scalar Multiplication
The product of the matrix Mby a scalar k writtenk·Mor simplykMis the matrix
. . .
obtained by multiplying each element of Mby k.
.
Example 3.11
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
12 5× 15×2 5 10
5× = = .
. 35 5× 35×5 1525
Exercise
3.7 Let
⎡ ⎤ ⎡ ⎤
− 3 10 −12
. U
=⎣
9 .06
⎦
,
V=⎣
1 0
⎦
.
1 −5 0 1
Find
(1) U+V.
.
(2) 2U−4V.
.
(3) −3U+2V.
.


================================================================================
PAGE 71
================================================================================

58 3 LinearAlgebra
Fig. 3.6 An illustration of
matrix multiplication:
C=AB.
3.3.3 Matrix Multiplication
The product of two matrices Aand Bis somewhat complicated. Each element of the
. .
resultant matrix (C = AB) is the dot product of a row from the first matrix Aand
. .
a column from the second matrix B. The first index of the element of the resultant
.
matrix tells us which row we need to use from the first matrix A, and the second
.
index tells us which column we need to use from the second matrix B. Figure 3.6
.
shows an example ofC = AB, where Ahas two rows and four columns and Bhas
. . .
four rows and three columns. c is the dot product of the first row of A and the
1,2. .
second column of B.
.
Example 3.12 Suppose we have two matrices
⎡ ⎤
(cid:8) (cid:9)
12
230 ⎣ ⎦
. and 73 ,
145
05
the following shows how we compute the matrix multiplication:
⎡ ⎤
(cid:8) (cid:9) (cid:8) (cid:9)
12
230 ⎣
73
⎦= 2×1+3×7+0× 0 ×22+3×3+0×5
145 1×1+4×7+5× 0 ×12+4×3+5×5
05
.
(cid:8) (cid:9)
23 13
= .
29 39


================================================================================
PAGE 72
================================================================================

3.3 Matrices 59
Fig. 3.7 This is an example
where the two matrices
cannot be multiplied
Remark 3.2 Note that to calculate the multiplication of two matrices, the number
of columns of the first matrix must equal the number of rows of the second matrix.
In Fig.3.7, the first matrix has two columns, while the second matrix has three rows.
Since they are not equal, one cannot calculate the matrix multiplication of these two
matrices. (cid:2)
.
Remark 3.3 If two matrices can be multiplied, the final resultant matrix has the
same number of rows as the first matrix and the same number of columns as the
second matrix. (cid:2)
.
Matrices and vectors can also be multiplied together, providing they have
appropriate sizes. For instance, a matrix with two columns can be multiplied by a
two-component vector written vertically (which can be thought of as a matrix with
just one column).
(cid:8) (cid:9) (cid:8) (cid:9)
87 3
Example 3.13 Ifyouwanttomultiplythematrix bythevector ,
96 . −4 .
we get:
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
87 3 8×3+7×(−4) −4
= = .
. 96 −4 9×3+6×(−4) 3
Exercise
3.8 Compute the following:
(cid:8) (cid:9) (cid:8) (cid:9)
10 3 2 10 7 0
(1) + .
.
1 2 5 1669
⎡ ⎤
3 2
(2) 3× ⎣ 0 1 ⎦ . .
5 −1
(continued)


================================================================================
PAGE 73
================================================================================

60 3 LinearAlgebra
(cid:8) (cid:9)(cid:8) (cid:9)
9 −2 2
(3) .
.
2 6 1
⎡ ⎤
(cid:8) (cid:9)
2 2
(4)
10 3 2 ⎣
1 8
⎦
. .
1 2 5
25
3.3.3.1 Properties of Matrix Multiplication
LetA,B, and Caren×nmatrices.
. . .
• Associative property of multiplication
(AB)C=A(BC). (3.7)
.
Example 3.14 Suppose we have
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
2 2 .06 21 7 3 2−1
A=⎣ 10. 1 8 ⎦ . ,B=⎣ 50 3 ⎦ . andC=⎣ 4− 2 10 ⎦ . .
3 2 10 16−2 7 0. 5 5
We can compute the following:
⎡ ⎤⎡ ⎤ ⎡ ⎤
2 2 .06 21 7 14.6 5. 6 18.8
. A B=⎣ 10. 1 8 ⎦⎣ 50 3 ⎦=⎣ 10. 5 49−8.7 ⎦ .
3 2 10 16−2 26 63 7
⎡ ⎤⎡ ⎤ ⎡ ⎤
14.6 5. 6 18.8 3 2−1 197. 8 2.47 135.4
. ( AB)C=⎣ 10. 5 49−8.7 ⎦⎣ 4− 2 10 ⎦=⎣ 166.6 −81.35 436 ⎦ .
26 63 7 7 0. 5 5 379 −70.5 639
Then we compute the following:
⎡ ⎤⎡ ⎤ ⎡ ⎤
21 7 3 2−1 59 5. 5 43
. B C=⎣ 50 3 ⎦⎣ 4− 2 10 ⎦=⎣ 36 11. 5 10 ⎦ .
16−2 7 0. 5 5 13−1149
⎡ ⎤⎡ ⎤ ⎡ ⎤
2 2 .06 59 5. 5 43 197. 8 2.47 135.4
. A (BC)=⎣ 10. 1 8 ⎦⎣ 36 11. 5 10 ⎦=⎣ 166.6 −81.35 436 ⎦ .
3 2 10 13−1149 379 −70.5 639
That is, we have(AB)C=A(BC).
.


================================================================================
PAGE 74
================================================================================

3.3 Matrices 61
• Distributive properties
C(A+B)=CA+CB. (3.8)
.
Example 3.15 Continue with matrices in Example 3.14. We hav e
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
2 2 .06 21 7 4 3 .76
. A
+B=⎣
10.1 8
⎦+⎣
50 3
⎦=⎣
60.1 11
⎦
.
3 2 10 16−2 4 8 8
⎡ ⎤⎡ ⎤ ⎡ ⎤
3 2−1 4 3 .76 20 1.2 3.86
. C (A+B)=⎣ 4−2 10 ⎦⎣ 60.1 11 ⎦=⎣ 44 91.8 8.84 ⎦ .
7 0.5 5 4 8 8 5161.05 98.7
In addition, we can compute and get the following:
⎡ ⎤⎡ ⎤ ⎡ ⎤
3 2−1 2 2 .06 5 .24.87
. C A=⎣ 4−2 10 ⎦⎣ 10.1 8 ⎦=⎣ 36 27.8 8.64 ⎦ ,
7 0.5 5 3 2 10 29.5 24.05 58.2
⎡ ⎤⎡ ⎤ ⎡ ⎤
3 2−1 21 7 15 −3 29
. C B=⎣ 4−2 10 ⎦⎣ 50 3 ⎦=⎣ 8 64 2 ⎦ ,
7 0.5 5 16−2 21.5 37 40.5
and
⎡ ⎤
20 1.2 3.86
. C
A+CB=⎣
44 91.8 8.84
⎦
.
5161.05 98.7
That is, we haveC(A+B)=CA+CB.
.
(A+B)C=AC+BC. (3.9)
.


================================================================================
PAGE 75
================================================================================

62 3 LinearAlgebra
Example 3.16 Continue with matrices in Example 3.14 again. We have
calculated(A+B)in Example 3.15. So, we can compute
.
⎡ ⎤⎡ ⎤ ⎡ ⎤
4 3 .76 3 2−1 77.2 .58 64
. ( A+B)C=⎣ 60.1 11 ⎦⎣ 4−2 10 ⎦=⎣ 95.4 17.3 50 ⎦ .
4 8 8 7 0.5 5 100 −4 116
In addition, we can compute and get the following:
⎡ ⎤⎡ ⎤ ⎡ ⎤
2 2 .06 3 2−1 18.2 0.3 21
. A C =⎣ 10.1 8 ⎦⎣ 4−2 10 ⎦=⎣ 59.4 5.8 40 ⎦ ,
3 2 10 7 0.5 5 87 7 67
⎡ ⎤⎡ ⎤ ⎡ ⎤
21 7 3 2−1 59 5.5 43
. B C=⎣ 50 3 ⎦⎣ 4−2 10 ⎦=⎣ 36 11.5 10 ⎦ ,
16−2 7 0.5 5 13−1149
and
⎡ ⎤
77.2 .58 64
. A
C+BC=⎣
95.4 17.3 50
⎦
.
100 −4 116
That is, we have(A+B)C=AC+BC.
.
3.3.4 Matrices as Linear Transformations
Now you know how to do matrix addition and matrix multiplication. But what is a
matrix? Why do you need to use matrices? There are several different very useful
properties of matrices. In this book, let us consider two of them. First, let us consider
matrices that model linear transformations. We will consider linear transformations
in just two dimensions. (cid:8) (cid:9) (cid:8) (cid:9)
1 2
Suppose you want to rotate the triangle with vertices u = , v = and
. .
3 3
(cid:8) (cid:9)
w= 1 through90 ◦ anticlockwise about the origin as shown in Fig.3.8.
. .
1


================================================================================
PAGE 76
================================================================================

3.3 Matrices 63
Fig. 3.8 This is an example
of matrices used to represent
linear transformations
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
1 −3 2 −3
As you can see in Fig.3.8, has transformed to , to and
. . . .
3 1 3 2
(cid:8) (cid:9) (cid:8) (cid:9)
1 −1
to . It looks as if all three points follow the following rule:
. .
1 1
(cid:8) (cid:9) (cid:8) (cid:9)
x −y
istransformedto .
.
y x
This can be expressed as a matrix equation:
(cid:8) (cid:9) (cid:8) (cid:9)(cid:8) (cid:9)
x (cid:8) 0−1 x
= ,
. (cid:8)
y 1 0 y
(cid:8) (cid:9)
0−1
◦
where the matrix represents a90 anticlockwise rotation about the origin.
. .
1 0
If we wish to have a more general rotation with any degree ( θ) about the origin, we
.
can use the following matrix:
(cid:8) (cid:9)
cos(θ)−sin(θ)
.
.
sin(θ) cos(θ)
Matrices can also represent other linear transformations, such as reflections.


================================================================================
PAGE 77
================================================================================

64 3 LinearAlgebra
Fig. 3.9 Vector u.is rotated
by45◦ .in the anticlockwise
direction around the origin
(cid:8) (cid:9)
10
Example 3.17 Suppose there is a vector u = . You wish to rotate u
. .
0
◦
through 45 in the anticlockwise direction around the origin. This is illustrated
.
in Fig.3.9, where the origi(cid:8)nal v(cid:9)ector is along the horizontal axis, and the new
vector is located at u (cid:8) = 7.1 after the rotation. The new position keeping
.
7.1
one decimal place is calculated as follows:
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
u (cid:8) = cos(45 ◦ )−sin(45 ◦ ) 10 = 7.1 .
. ◦ ◦
sin(45 ) cos(45 ) 0 7.1
3.3.5 Representations of Simultaneous Equations
Let us look at the second useful property of matrices by considering the following
example. Suppose we have two TV producers (A and B) that send different
proportions of TVs they produce to three warehouses (1, 2, and 3), as shown in
Fig.3.10.
We can model this as a matrix shown as follows:
(cid:8) (cid:9)
0.50.5 0.0
U= ,
.
0.30.3 0.4


================================================================================
PAGE 78
================================================================================

3.3 Matrices 65
Fig. 3.10 Two TV producers
(A and B) that send different
proportions of TVs they
produce to three warehouses
(1,2 ,a nd 3)
Fig. 3.11 The three
warehouses then forward TVs
in different proportions to
four shops (W,X,Y, andZ.)
where each row represents one of the two producers, each column represents one of
the three warehouses, and each element in the matrix is the proportion of the TVs
each producer sends to the specific warehouse.
The three warehouses then forward TVs in different proportions to four shops
(W,X,Y, and Z), as shown in Fig.3.11. Similarly, we can model this as a matrix
.
as well:
⎡ ⎤
0.30.7 0.00.0
. V
=⎣
0.00.4 0.60.0
⎦
.
0.00.0 0.50.5
Notice that the number of columns in the first matrix, U, is equal to the number of
.
rows in the second one, V, so that they can be multiplied together. This allows us to
.
calculate the proportions of the TVs that would be sent to the shops in the simplified
situation where the two producers send TVs directly to the shops. The following


================================================================================
PAGE 79
================================================================================

66 3 LinearAlgebra
shows the product of these two matrices:
⎡ ⎤
(cid:8) (cid:9) (cid:8) (cid:9)
0.30.7 0.00.0
. U V= 0.50.5 0.0 ⎣ 0.00.4 0.60.0 ⎦= 0.15 0.55 0.30 0.00 .
0.30.3 0.4 0.09 0.33 0.38 0.20
0.00.0 0.50.5
3.3.6 Multiplying a Matrix by Itself
Definition 3.7 (Square Matrix) A square matrix is a matrix with the same number
of rows as columns.
Ann×nsquare matrix is called a square matrix of order n. For example,
.
(cid:8) (cid:9)
15
A= isasquarematrixoforder2,
.
24
and
⎡ ⎤
1012
. B
=⎣
8 35
⎦
isasquarematrixoforder3.
6 01
3.3.6.1 Multiplying a Matrix by Itself
Only square matrices can be multiplied by themselves, since the number of columns
of the first mat(cid:8)rix m(cid:9)ust equal the number of rows of the second matrix. Let A be a
15
square matrix . We hav e
.
24
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
15 15 1125
A2 =AA= = ,
.
24 24 1026
(cid:8) (cid:9)(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
15 15 15 61155
A3 =AAA= = ,
.
24 24 24 62154
.
.
..
.


================================================================================
PAGE 80
================================================================================

3.3 Matrices 67
3.3.7 Diagonal and Trace
Suppose we have
⎡ ⎤
a ···a
11 1n
.
A = ⎢ ⎣ . .
.
... . .
.
⎥ ⎦.
a ···a
n1 nn
Definition 3.8 (Diagonal) The diagonal of a matrix consists of the elements with
the same subscripts, that is,a , a , ··· , a .
11 22 nn .
Example 3.18 Taking the matrix A defined in Sect.3.3.6, its diagonal ele-
.
ments are 1 and 4.
Definition 3.9 (Trace) The trace of a square matrix A, denoted astr(A), is the sum
. .
of the diagonal elements,thatis,tr(A)=a +a +···+a .
11 22 nn.
Example 3.19 Again, considering thematrix AinSect.3.3.6,itstrace,tr(A),
. .
is1+4=5.
.
Exercise
3.9 Given
⎡ ⎤ ⎡ ⎤
1 5 0 1012
. A
=⎣
2 410
⎦ andB=⎣
8 35
⎦
,
−12 3 6 01
compute:
(1) tr(A).
.
(2) tr(B).
.


================================================================================
PAGE 81
================================================================================

68 3 LinearAlgebra
3.3.8 Diagonal Matrices
Definition 3.10 (Diagonal Matrix) A diagonal matrix is a matrix in which the
entries outside the main diagonal are all zeros.
Let us have a look at the following two examples.
⎡ ⎤ ⎡ ⎤
1000 100
. A
=⎣
0300
⎦ andB=⎣
030
⎦
.
0090 009
Ais called a rectangular diagonal matrix, where the number of rows is not equal to
.
the number of columns. Bis called a symmetric diagonal matrix, which is a square
.
matrix.
3.3.9 Determinants
Definition 3.11 (Determinant) Each n-square matrix A = a[ ], where i =
ij .
1, ··· ,nand j = 1, ··· ,n, is assigned a special scalar called the determinant
. .
of A, denoted by det(A)or |A|or
. . .
a ···a
11 1n
.
. .
.
... . .
.
.
a ···a
n1 nn
Note that ann×narray of scalars enclosed by straight lines called a determinant
.
of order n, is not a matrix but denotes the determinant of A. A general way to
.
calculate the determinant of order n can be learned from [8]. This book covers how
to compute determinants of order 1, 2, and 3 only.
3.3.9.1 Determinants of Order 1, 2, and 3
• Determinant of order 1
The determinant of order 1 is defined as follows:
|a |=a .
. 11 11
That is, the determinant of a1×1matrix is that number itself.
.


================================================================================
PAGE 82
================================================================================

3.3 Matrices 69
• Determinant of order 2
The determinant of order 2 is defined as the product of elements along the main
diagonal minus the product of elements along the reverse diagonal. That is
a a
11 12 =a a −a a . (3.10)
. 11 22 12 21
a a
21 22
(cid:8) (cid:9)
3−1
Example 3.20 SupposeM= . Compute the determinant of M.
. .
4 11
Solution
3−1
=3×11−(−1)×4=37.
.
4 11
Exercise
3.10 Compute the determinant of the following matrices.
(cid:8) (cid:9)
13 1
(1) N= .
−4 2 .
(cid:8) (cid:9)
1 1
(2) U= .
.
5 2
(cid:8) (cid:9)
10 4
(3) V= .
.
5 2
• Determinant of order 3 ⎡ ⎤
a a a
11 12 13
Considering a 3×3 . matrix A = ⎣ a 21 a 22 a 23 ⎦ . , its determinant is defined as
a a a
31 32 33
follows:
a a a
11 12 13
. det(A) =|A|=a 21 a 22 a 23
a a a
31 32 33
=a a a +a a a +a a a −a a a
11 22 33 12 23 31 13 21 32 13 22 31
−a a a −a a a . (3.11)
11 23 32 12 21 33


================================================================================
PAGE 83
================================================================================

70 3 LinearAlgebra
Fig. 3.12 An illustration of
the calculation of the
determinant of order 3
Looking at Eq.(3.11), we can see six terms on the right-hand side of the equation.
Each of these terms is a product of three elements. It may be easier to see which
three elements from Fig.3.12. Here, we copy the matrix first and add the first
two columns after the third column. Then, we draw the main diagonal line for
every three columns, and similarly, we can draw the reverse diagonal line. The
first three terms in the equation are the products along the main diagonals, and
the last three are the products along the reverse diagonals. We add up the first
three products and subtract the last three products.
⎡ ⎤
3 − 1 10
Example 3.21
SupposeX=⎣
4 2.50
⎦
. . Compute the determinant of X . .
2.5 − 2 6
Solution
det(A)=3×2×6+(−1)×0.5×2.5+10×4×(−2)−10×2×2.5
.
−3×0.5×(−2)−(−1)×4×6
=−68.25.
Exercise
3.11 Compute the determinant of the following matrices.
⎡ ⎤
3 1 .50
(1) W= ⎣ − 4 2 1 ⎦ 0 . .
3 . 02 6
⎡ ⎤
1 1 0
(2) X= ⎣ 5 2 3 ⎦ . .
− 4100.1


================================================================================
PAGE 84
================================================================================

3.3 Matrices 71
3.3.9.2 What Is the Determinant for?
As you will see soon, the determinant can help us find the inverse of a matrix. In
addition, it can tell us information about the matrix that is useful in solving systems
of linear equations.
3.3.10 Identity and Inverse Matrices
3.3.10.1 Identity Matrices
Definition 3.12 (Identity Matrix) Square matrices with all zeroes except 1s on the
main diagonal are identity matrices.
For example,
⎡ ⎤
⎡ ⎤
(cid:8) (cid:9) 1000
100 ⎢ ⎥
. I 2 = 10 , I 3 =⎣ 010 ⎦ , andI 4 =⎢ ⎣ 0100⎥ ⎦,
01 0010
001
0001
where we us eIto denote each identity matrix and a value as the subscript denotes
.
the size of the matrix.
Multiplying a matrix Aby an identity matrix Iequals the original matrix A. That
. . .
is,
AI=IA=A.
.
3.3.10.2 Inverse Matrices
Given a matrix A, can we find an inverse matrixA −1such thatAA −1 =A −1A=I?
. . .
Sometimes we can, but only square matrices can have inverses, and not all square
matrices do. Finding inverses is complicated, so we shall only consider inverses for
2×2 . matrices in this book. (cid:8) (cid:9)
a b
Given a2×2matrixM= , the determinant of this matrix is calculated as
. .
c d
follows:
det(M)= ad−bc.
.


================================================================================
PAGE 85
================================================================================

72 3 LinearAlgebra
If the determinant is not zero, then the inverse exists, and it can be calculated by
swapping the elements on the main diagonal, changing the signs of the elements on
the reverse diagonal, and then dividing by the determinant, as shown:
(cid:8) (cid:9) (cid:8) (cid:9)
M −1 = 1 d −b = 1 d −b .
. det(M) − c a ad−bc − c a
(cid:8) (cid:9)
15
Example 3.22 If A = , then its determinant is 4−10 = −6. So the
. .
24
inverse matrixis
(cid:8) (cid:9)
A −1 = 1 4 −5 .
. −6 − 2 1
You can now confirm thatAA −1 =A −1A=I.
.
Exercise
3.12 Compute the inverse for the following matrices when it exists.
(cid:8) (cid:9)
−4 −3
(1) A= .
.
2 6
(cid:8) (cid:9)
2 1
(2) B= .
.
10 5
(cid:8) (cid:9)
13 1
(3) C= .
−4 2 .
(cid:8) (cid:9)
1 0
(4) I= .
.
0 1
3.3.11 Matrix Transposition
In some circumstances, you need to flip a matrix around its main diagonal, that is,
to exchange rows for columns.
Definition 3.13 (Matrix Transposition) The transpose of a matrix A, written as
.
AT, is formed by swapping the rows and columns.
.
If Aism×n, then AT isn×m.
. . . .


================================================================================
PAGE 86
================================================================================

3.3 Matrices 73
⎡ ⎤
14
Example 3.23 Suppose a matrix of size 3 by 2 is A = ⎣ 25 ⎦ . , then its
36
(cid:8) (cid:9)
123
transpose is a matrix of size 2 by 3, that is,AT = .
.
456
Exercise
3.13 Find the transpose of each of the following matrices:
⎡ ⎤
1 2 10
(1) A= ⎣ 4 5− 1 ⎦ . .
70−3
(cid:8) (cid:9)
−1 4− 13
(2) B= .
.
0 5 8
⎡ ⎤
10
(3) C=
⎢
⎢ ⎣
−2 ⎥
⎥ ⎦. .
23
−1
(cid:12) (cid:13)
(4) D= 1, 0, −0.7,10 . .
Remark 3.4 If AT = A, then A must be square, and it is called a symmetric
. .
matrix. Iis a symmetric matrix. (cid:2)
. .
3.3.11.1 Properties
Let A, B, and Care matrices . It can be shown that:
. . .
(1) (AT )T = A.
.
(2) (A + B)T = AT +BT.
.
(3) (AB)T =BTAT .
.
(4) (ABC)T = CTBTAT.
.
(5) If Ais a square matrix, then det(A)=det(AT).
. .


================================================================================
PAGE 87
================================================================================

74 3 LinearAlgebra
Example 3.24 We will illustrate the second and third of properties shown in
Sect.3.3.11.1 usin(cid:8)g 2 b(cid:9)y 2 matrice(cid:8)s. (cid:9)
a b e f
SupposeA= , andB= .
. .
c d g h
(cid:8) (cid:9)
a+ e b+f
Then, A + B = , and its transpose is (A + B)T =
c+ g d+h .
(cid:8) (cid:9)
a+ e c+g
.
b+ f d+h .
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
a c e g a+ e c+g
Also,AT +BT = + = .
b d f h b+ f d+h .
Thus, we obs(cid:8)erve that(A+B)T (cid:9) =AT +BT . . (cid:8) (cid:9)
ae+bg af +bh ae+bg ce+dg
Next,AB= , and(AB)T = .
ce+dg cf +dh . af +bhcf +dh .
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
e g a c ae+bg ce+dg
Also,BTAT = = .
f h b d af +bhcf +dh .
Thus, we observe that(AB)T =BTAT.
.
In the case of vectors, the transpose of a vector just turns a column (vertical)
vector into a row (horizontal) vector and a row vector into a column one.
⎡ ⎤
1
Example 3.25 Ifa=⎣ 2 ⎦ . , thenaT =(1,2,3). .
3
In this case, the dot producta·a=aTa=14.
.
Remark 3.5 In practice, the standard vector is assumed to be a column vector. So
technically, a row vector is always the transpose of a vector, that is aT, where ais a
. .
column vector. (cid:2)
.
Example 3.26 Suppose Ais a matrix of sizem×n, Band Care matrices of
. . . .
n×n. ProveA(B+C)AT =ABAT +ACAT.
. .
Solution Applying Eqs.(3.8) and (3.9), we have
A(B+C)AT =(AB+AC)AT
.
=ABAT +ACAT.


================================================================================
PAGE 88
================================================================================

3.3 Matrices 75
Example 3.27 Suppose the inverse of Aexists. Prove
.
(A −1)T =(AT) −1. (3.12)
.
We need to show that (A −1)T acts like the inverse of AT. We will make
. .
use of the third property of transposition, namely,(AB)T =BTAT.F irst:
.
AT(A −1)T =(A −1A)T =IT =I.
.
Then:
(A −1)TAT =(AA −1)T =IT =I.
.
So multiplying AT on either side by(A −1)T gives the identity matrix I.T his
. . .
proves that(A −1)T =(AT) −1.
.
3.3.12 Case Study 1 (Continued)
⎡ ⎤ ⎡ ⎤
1−5 10
In Sect.1.3.1 of Chap.1, we have X = ⎣ 1 5 ⎦ . and y = ⎣ 30 ⎦ . , and we need to
1 0 20
compute the coefficients of the least-squares regression method using the following
equation:
a=(XTX) −1XTy.
.
To compute a, we need to obtain the transpose of Xand the inverse ofXTXfirst
. . .
and then substitute them to the equation as shown as follows:
• Swap rows and columns of Xto obtain the transpose of X:
. .
(cid:8) (cid:9)
1 11
XT = .
. −550
• Compute the matrix multiplication ofXTX:
.
⎡ ⎤
(cid:8) (cid:9) 1−5 (cid:8) (cid:9)
. XTX= − 1 5 1 5 1 0 ⎣ 1 5 ⎦= 3 05 0 0 .
1 0


================================================================================
PAGE 89
================================================================================

76 3 LinearAlgebra
• Compute the inverse ofXTX:
.
Since the determinant ofXTXis 150, the inverse ofXTXexists and equals:
. .
(cid:8) (cid:9)
(XTX) −1 = 1 500 .
.
150 0 3
• Substitute(XTX) −1, XT, and yinto the equation of a, and obtain the following:
. . . .
⎡ ⎤
(cid:8) (cid:9)(cid:8) (cid:9)
10
. a = 15 1 0 5 0 0 3 0 − 1 5 1 5 1 0 ⎣ 30 ⎦
20
⎡ ⎤
(cid:8) (cid:9)
10
= 1 50 5050 ⎣ 30 ⎦
150 −1515 0
20
(cid:8) (cid:9) (cid:8) (cid:9)
1 3000 20
= = .
150 300 2
Hence, we get the vector awith intercept 20 and gradient 2.
.
3.3.13 Orthogonal Matrix
Orthogonal has been mentioned when we introduce the concept of the dot product
in Sect.3.2.4, where it states that two vectors are orthogonal to each other when
their dot product equals zero.
Definition 3.14 (Orthonormal Vectors) Suppose we havem−vectors x ,x , ···,
. 1 2
x . These vectors are orthonormal if
m.
• each vector has unit norm, that is (cid:2)x(cid:2)=1 , i=1, 2, ··· , m.
i .
• they are mutually orthogonal, that isx ·x =0,i fi (cid:9)= j,i,j =1, 2, ··· , m.
i j . .
Example 3.28 Given the following three vectors,
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
0 −1 1
. x 1 =⎣ 0 ⎦ , x 2 = √ 1 ⎣ 1 ⎦ , x 3 = √ 1 ⎣ 1 ⎦ ,
2 2
1 0 0
the norm of each of these vectors is equal to 1, and they are mutually
orthogonal to each other. That is, their dot products are all equal to zero. A
(continued)


================================================================================
PAGE 90
================================================================================

3.3 Matrices 77
Example 3.28 (continued)
matrix, denoted as A, including these three columns, is an orthogonal matrix
.
as shown as follows:
⎡ ⎤
0 √−1 √1
⎢ 2 2⎥
.
A =⎣0 √1 √1 ⎦.
2 2
1 0 0
Note that sincex ·x = 1fori = 1, 2, 3andx ·x = 0for i (cid:9)= j,i,j =
i i . . i j .
1, 2, 3thenATA=AAT =I, as you can check for yourself.
. .
Exercise
(cid:14) (cid:15) (cid:14) (cid:15)
3.14 Let mT = √2 ,0,−√1 , mT = −√1 ,0,−√2 , and mT =
(cid:12) (cid:13) 1 5 5 2 5 5 . 3
0,1,0 . . Are these three vectors orthogonal to each other?
Definition 3.15 (Orthogonal Matrix) A square matrix A with orthonormal
.
columns is called orthogonal. This is equivalent to the following definition:
ATA=AAT =I,
.
where Iis the identity matrix.
.
Remark 3.6 SinceAA −1 =IifA −1exists (see Sect.3.3.10.2), andAAT =I(see
. . .
Definition 3.15), we obtainAT =A −1, when Ais a square matrix. (cid:2)
. . .
Remark 3.7 Orthogonal vectors (matrix) are useful to build up a new coordinate
system. (cid:2)
.
Exercise
3.15 For the following matrices,
(cid:16) (cid:17)
√1 √3
• Q= 10 10 and
− √3 √1 .
(cid:16) 10 1(cid:17)0
√4 √3
• Q= 5 5 ,
− √3 √4 .
5 5
(continued)


================================================================================
PAGE 91
================================================================================

78 3 LinearAlgebra
compute:
(1) QT .
.
(2) Q
−1.
.
(3) Is Qan orthogonal matrix?
.
3.4 Linear Combination
3.4.1 Vector Spaces
To motivate this section, let us just start by considering vectors in R2. Without going
.
into all the details, it is fairly obvious that:
• For any two vectorsx,w∈R2then their sumx+wis also ∈R2[called Closure].
. . .
• There is a “zero vector”0 = (0,0)T ∈ R2 that, when added to any vector, does
.
not change it [called Identity].
• For any vector, there is a vector called its inverse, that is: forv ∈ R2 then −vis
. .
∈R2so thatv+(−v)=0.
. .
The operation of vector addition is associative and communative; that is, it does not
matter what order you add them to; you always get the same result.
Also, in terms of scalar multiplication:
• Multiplying any vector v ∈ R2 by a scalar gives a result which is also ∈ R2
. .
[Closure again].
• Multiplying any vector by the scalar 1 leaves it unchanged [Identity again].
• Multiplying a vector by one scalar then another gives the same result as
multiplying the scalars together first, that is,k(jx)=(kj)x[Associative again].
.
• Distributivity:k(x+w)=kx+kwand(k+ j)x=kx+jx.
. .
A bit more thought says that the same applies to R3and in fact to any Rn.
. .
To generalise these ideas, we can define a Vector Space:
Definition 3.16 (Vector Spaces) Let V be a non-empty set with two operations:
• vector addition: this assigns to any x,w ∈ V a sum x + w ∈ V. So, vector
. .
addition is closed. Vector addition is also associative and communicative and has
an identity and inverse.
• scalar multiplication: this assigns to any x ∈ V, a product kx ∈ V, where
. .
k is a scalar. So, scalar multiplication is closed. Scalar multiplication is also
Associative and Distributive and has an Identity.
Lots of structures are vector spaces. For example, R2, R3, Rn, matrix space, poly-
. . .
nomials and function space, and many more are all vector spaces. The advantage of


================================================================================
PAGE 92
================================================================================

3.4 LinearCombination 79
defining an abstract vector space is that once you have proved something for vector
spaces in general, then it applies to all the particular vector spaces.
Definition 3.17 (Subspaces) Suppose W is a subset of a vector space V. Then W
is a subspace of V if the following two conditionshold:
• the zero vector belongs to W.
• forx,w∈W
.
1. the sumx+w∈W.
.
2. the multiplekx∈W, where k is a scalar.
.
Example 3.29 Consider the vector space R3. L etW consist of all vectors
.
whose elements areequalinR3, such as(4,4,4). That is W is the line through
. .
the origin O andthepoint(1,1,1). ClearlyO =(0,0,0)belongs to W, since
. .
all entries in O are equal. Further, suppose we have two arbitrary vectors in
W,x =[a,a,a]andy =[b,b,b]. Then, for any real value scalar, k, we hav e
. .
x+y =[a+b,a+b,a+b]∈W andkx= (ka,ka,ka)∈ W.Thus, W is a
. .
subspaceofR3.
.
(cid:18)(cid:8) (cid:9) (cid:19)
x
Example 3.30 SupposeS = 1 ∈R2|x ≥0 .I sS a subspaceof R2?
1 . .
x
2
Let us check the conditions.
(cid:8) (cid:9)
0
• S contains the zero vector .
.
0
(cid:8) (cid:9) (cid:8) (cid:9)
a c
• Supposeu= andv= are two vectors in S .W eh av e
. .
b d
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
a c a+c
+ = .
. b d b+d
Sincea ≥0andc≥0,w eh avea+c≥0. Therefore, the sum ofu+v
. . . .
is a vector of S.
• However, when multiplying any vector, u, inS , by −1, the direction of
. .
the resultant vector, v = −u, is opposite to u. The vector v is not in the
. . .
area covered by S anymore. In Fig.3.13, the vector (2,1) is an element
.
of S, since x = 2 > 0. After multiplying by −1, the resultant vector,
1 . .
(−2,−1), is not an element of S since the valueofx = −2 < 0 .S oS is
. 1 .
not a subspaceofR2.
.


================================================================================
PAGE 93
================================================================================

80 3 LinearAlgebra
Fig. 3.13 An example of a vector space (with the shaded region representingx1 ≥0.) that is not a
subspace of R2 ., as explained in Example 3.30
Exercise
3.16
⎡ ⎤
(cid:18) (cid:19)
x
1
(1) SupposeS 1 = ⎣ x 2 ⎦∈R3 . .I s S 1. a subspace of R3 . ?
0
⎡ ⎤
(cid:18) (cid:19)
x
1
(2) SupposeS 2 = ⎣ 0 ⎦∈R3 . .I s S 2. a subspace of R3 . ?
0
(cid:18)(cid:8) (cid:9) (cid:19)
x
(3) SupposeS = 1 ∈R2 .I s S a subspace of R2?
3 . 3. .
0
3.4.2 Linear Combinations and Span
Definition 3.18 (Linear Combination) vis a linear combination of a set of vectors
.
u ,u , ···,u if there is a solution to the vector equation
1 2 d.
v=x u +x u +···+x u , (3.13)
. 1 1 2 2 d d
wherex ,x , ··· ,x are unknown scalars.
1 2 d.


================================================================================
PAGE 94
================================================================================

3.4 LinearCombination 81
Fig. 3.14 An illustration of a
linear combination of two
vectors, as explained in
Example 3.31
(cid:8) (cid:9)
4
Example 3.31 See Fig.3.14. We can express v = in R2 as a linear
. .
5
(cid:8) (cid:9) (cid:8) (cid:9)
2 0
combination of the vectorsu = andu withx = 2andx = 1.5,
1 . 2 . 1 . 2 .
1 2
that is,
v=2u +1.5u .
. 1 2
vis the longer diagonal of the parallelogram constructed by2u and1.5u .
. 1. 2.
3.4.2.1 Solving Simultaneous Equations to Find Linear Coefficients
Suppose we want to expressvT = (4,−2,2) in R3 as a linear combination of the
. .
vectorsuT =(1,2,1),uT =(2,1,2), anduT =(−1,1,1). We seek scalars x , x ,
1 . 2 . 3 . 1. 2.
and x , such thatv=x u +x u +x u , that is,
3. 1 1 2 2 3 3.
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
4 1 2 −1
.
⎣−2 ⎦=x
1
⎣
2
⎦+x
2
⎣
1
⎦+x
3
⎣
1
⎦
.
2 1 2 1


================================================================================
PAGE 95
================================================================================

82 3 LinearAlgebra
Since there are three unknown variables, we need to solve three equations:
⎧
⎪⎪⎨x
1
+2x
2
−x
3
=4
. ⎪⎪⎩ 2x 1 +x 2 +x 3 =−2 .
x +2x +x =2
1 2 3
We can solve simultaneous equations by elimination and substitution. The solution
isx = −5, x = 7, andx =−1, so we can express vas follows:
1 3 2 3 . 3 . .
−5 7
v= u + u −u .
. 1 2 3
3 3
3.4.2.2 Span
The next question is: if given a set of vectors, how big is the space of the set of all
linear combinations of the vectors? This is called the span of the set of vectors.
(cid:8) (cid:9)
1
Example 3.32 Describe the space formed by thespan(u), whereu= .
. .
1
(cid:8) (cid:9) (cid:8) (cid:9)
1 x
Solution Any linear combination of isx uwhich is: 1 , where x ∈
. 1 . . 1
1 x
1
R. Therefore,span(u)scales up or scales down along one line, where the two
. .
elements in the vector are equal to each other. So the span is a line.
(cid:8) (cid:9)
1
Example 3.33 Describe the space formed by thespan(u,v), whereu= ,
. .
1
(cid:8) (cid:9)
−2
andv= .
−2 .
(cid:8) (cid:9) (cid:8) (cid:9)
1 −2
Solution Any linear combination of and isx u+x v which is:
1 . −2 . 1 2 .
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
1 −2 x −2x
x +x = 1 2 , where x , x ∈ R. Therefore, span(u,v)
1 1 2 −2 x −2x . 1 2 . .
1 2
scales up or scales down along one line, where both elements in the vector are
equal tox −2x . Again, the span is a line.
1 2.


================================================================================
PAGE 96
================================================================================

3.5 LinearDependenceandIndependence 83
(cid:24)(cid:8) (cid:9) (cid:8) (cid:9)(cid:25)
1 0
Example 3.34 Describe the space formed by thespan , .
.
1 2
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
1 0 1 0
Solution Any linear combination of and is: x + x =
. . 1 2
1 2 1 2
(cid:8) (cid:9) (cid:24)(cid:8) (cid:9) (cid:8) (cid:9)(cid:25)
x 1 0
1 , wherex , x ∈ R. Therefore,span , = R2, that is,
x +2x . 1 2 . 1 2 .
1 2
any vector including two elements with arbitrary real numbers. So the span is
the whole ofR2.
.
Definition 3.19 (Spanning Sets) Let V be a vector space. Vectorsu ,u , ···,u
1 2 d.
are said to form a spanning set of V if every v ∈ V is a linear combination of
.
the vectorsu ,u , ···,u . That is, the spanning set ofu ,u , ···,u is the whole
1 2 d. 1 2 d.
of V.
(cid:8) (cid:9) (cid:8) (cid:9)
1 0
In Example 3.34,u = , andv = ∈ R2 form a spanning set for R2.O f
. . .
1 2
course, there are many vectors in R2
.
that would(cid:8) fo(cid:9)rm a span(cid:8)nin(cid:9)g set for R2
.
. T he
1 0
simplest example might be the two vectors:u= andv= .
. .
0 1
3.5 Linear Dependence and Independence
3.5.1 Linear Dependence and Independence
Intuitively, vectors are linearly dependent if one of them “depends” on the others;
that is, it is a linear combination of the others. Vectors are linearly independent
if none of them depends on the others; that is, there cannot exist any linear
combination of some of the vectors that add up to another one.
Definition 3.20 (Linear Dependence and Independence) Consider the vector
equation
x u +x u +···+x u =0, (3.14)
. 1 1 2 2 d d
whereu ,u and u are vectors with n elements,andx ,x , ··· ,x are scalars. The
1 2. d. 1 2 d.
vectors u ,u , ···,u are called linearly independent if x = 0 , x = 0, ···,
1 2 d. 1 2 .
x = 0 is the only solution to Eq.(3.14) The vectors u ,u , ···,u are linearly
d . 1 2 d.
dependent if not allx ,x , ··· ,x are zeros.
1 2 d.


================================================================================
PAGE 97
================================================================================

84 3 LinearAlgebra
Remark 3.8 Whenu ,u , ···,u are linearly independent,
1 2 d.
• x = 0 ,x = 0, ··· ,x = 0 is the only solution to Eq.(3.14). On the other
1 2 d .
hand, whenu ,u , ···,u are linearly dependent,x =0 , x =0, ··· ,x =0
1 2 d. 1 2 d .
is not the only solution.
• Any of these vectors cannot be expressed as a linear combination of other
elements. That is to say, for example, one cannot rewrite Eq.(3.14) as u =
1
−x2u − ··· − xdu since x = 0. On the other hand, if all the x are not
x1 2 x1 d. 1 . i.
zero, then Eq.(3.14) can be rearranged to express one of the vectors in terms of a
linear combination of the others, so that they are linearly dependent. For example,
supposex (cid:9)= 0, then we can write thatu = −x1u − x3u −···− xdu , that
2 . 2 x2 1 x2 3 x2 d.
is, u is a linear combination of the others.
2.
• Two vectors uand ware linearly dependent if and only if one is a multiple of the
. .
other. For example, suppose we haveu=(1,−3)andw=(3,−9). uand ware
. . . .
linearly dependent sincew=3u. (cid:2)
. .
Example 3.35 Let us consider a 3-dimensional Cartesian coordinate system
⎡ ⎤
1
⎣ ⎦
with X, Y, and Z axes. We have three vectors: the first oneis 0 . , the second
0
⎡ ⎤ ⎡ ⎤
0 0
⎣ ⎦ ⎣ ⎦
one 1 . , and the third one 0 . . They are the unit vector along X, Y, and Z
0 1
axis lines. Now the question is: Can we find a solution for a, b, and c, so that
the following linear system is val id?
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
1 0 0 0
. a
⎣
0
⎦+b ⎣
1
⎦+c ⎣
0
⎦=⎣
0
⎦
.
0 0 1 0
The only solution to the above simultaneous equations is a = b = c = 0.
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ .
1 0 0
⎣ ⎦ ⎣ ⎦ ⎣ ⎦
Therefore, 0 . , 1 . and 0 . are linearly independent.
0 0 1


================================================================================
PAGE 98
================================================================================

3.5 LinearDependenceandIndependence 85
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
1 1 4
⎣ ⎦ ⎣ ⎦ ⎣ ⎦
Example 3.36 Suppose we have three vectors: 1 . , 3 . and 9 . , and
0 2 5
consider the following:
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
1 1 4 0
. a
⎣
1
⎦+b ⎣
3
⎦+c ⎣
9
⎦=⎣
0
⎦
,
0 2 5 0
which can be written as follows:
⎧
⎪⎪⎨a+b+4c=0
. ⎪⎪⎩
a+3b+9c=0 .
2b+5c=0
If we subtract the first equation from the second equation, we obtain 2b +
5c=0, which is identical to the third one. The solution to these simultaneous
.
equations must satisfy 2b +5c = 0. That is, there are many solutions. For
.
example,a =3 , b=5 , c=−2ora =−6 , b=−10 , c=4. In fact, when
. .
a = b = c = 0, the linear system is also valid. However, this is not the only
. ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
1 1 4
⎣ ⎦ ⎣ ⎦ ⎣ ⎦
solution. Therefore, 1 . , 3 . and 9 . are linearly dependent.
0 2 5
Exercise
3.17 Determine whether or not the following vectors are linearly depen-
dent:
(cid:8) (cid:9) (cid:8) (cid:9)
2 4
(1) and .
. .
0 1
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
1 2 −1
(2)
⎣
0
⎦
. ,
⎣
1
⎦
. , and
⎣
1
⎦
. .
2 0 −2
(cid:8) (cid:9) (cid:8) (cid:9)
2 5
(3) and .
. .
3 7.5


================================================================================
PAGE 99
================================================================================

86 3 LinearAlgebra
Fig. 3.15 An example of two
vectors linearly independent
of each other but not
perpendicular, as shown in
Example 3.37
Remark 3.9 Orthogonal vectors are a special case of linear independence.
But being linearly independent does not mean the vectors are orthogonal to each
other. (cid:2)
.
(cid:8) (cid:9) (cid:8) (cid:9)
2 3
Example 3.37 Vectors and are not perpendicular, since the dot
2 . −1 .
product of them is 4 and not equal to zero. That they are not perpendicular
can also be seen in Fig.3.15. However, the only solution to the following:
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
2 3 0
x +x = ,
. 1 2 2 −1 0
which can be written as simultaneous equations as follows:
(cid:26)
2x +3x =0
1 2
.
2x −x =0
1 2
(cid:8) (cid:9) (cid:8) (cid:9)
2 3
is x = 0 and x = 0. Therefore, vectors and are linearly
1 . 2 . 2 . −1 .
independent of each other.


================================================================================
PAGE 100
================================================================================

3.6 ConnectiontoMatrices 87
3.5.2 Basis of a Vector Space
We now revisit spanning sets for vector spaces. We saw at the end of Sect.3.4.2.2
of this chapter an example of two vectors that represented a spanning set for R2. We
.
now define the basis for a vector space.
Definition 3.21 (Basis) A setS of vectors u ,u , ···,u is a basis of a vector
1 2 d.
space V if it has the following two pr operties:
• S is linearly independent;
• S spans V .
This definition says that a basis is a spanning set, and any element in this set
cannot be expressed as a linear combination of other elements. In other words, they
are linearly independent. Intuitively, the basis is the smallest set of vectors that will
generate the whole vector space.
In R2
.
, we need two non-parallel vectors. In R3
.
, we need three vectors n(cid:8)ot (cid:9)in the
1
same plane. In fact, there is a standard basis for R2, namely, the easiest: and
. .
0
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
(cid:8) (cid:9)
1 0 0
0 . . Similarly in R3 . ,t hes tandard basisis ⎣ 0 ⎦ . , ⎣ 1 ⎦ . , and ⎣ 0 ⎦ . .
1
0 0 1
Remark 3.10 Case Study 2
In Sect.1.3.2 of Chap.1, we see that the PCA projection plot seems to result from
the axes in the original Cartesian coordinate system being rotated. In fact, principal
component analysis computes the orthonormal basis, each data projection being
a vector that lies in a vector space spanned by this basis. Each element of this
orthonormal basis can capture the most crucial information, the variance in the given
dataset. We will discuss this further in Chap.4. (cid:2)
.
3.6 Connection to Matrices
3.6.1 Determinants and Singular Matrices
A set of n vectors of length n is linearly independent if the matrix with these vectors
⎡ ⎤
100
⎣ ⎦
as columns has a non-zero determinant. For instance, thedeterminantof 010 . is
001
1. As said in Sect.3.3.10.2, such matrices have an inverse. (In this case, the inverse
is the same matrix.)
On the other hand, the set of n vectors of length n is linearly dependent if the
determinantiszero.


================================================================================
PAGE 101
================================================================================

88 3 LinearAlgebra
We already know that vectors in the following matrix are linearly dependent:
⎡ ⎤
114
⎣ ⎦
. 139 ,
025
since it was shown in Example 3.36 of Sect.3.5.1. And the determinant of it is:
⎡ ⎤
(cid:24) (cid:25)
114
. det ⎣ 139 ⎦ =0.
025
Definition 3.22 (Singular) A square matrix with linearly dependent columns is
known as singular.
Since such matrices have a zero determinant, they do not have inverses. So,
singular matrices do not have inverses.
Remark 3.11 So why is studying singularity and determinants important in Data
Science? This is because many algorithms used in Data Science assume that all
features of the data are linearly independent. (cid:2)
.
3.6.2 Rank
Definition 3.23 (Rank) The maximal number of linearly independent columns of
a matrix is called its rank.
Example 3.38 The rank of the matrix, including three vectors shown in
Example 3.36 of Sect.3.5.1, is in fact:
⎡ ⎤
(cid:24) (cid:25)
114
. r ank ⎣ 139 ⎦ =2.
025
This can be shown by taking any two columns u and u and solving the
i. j.
equationx u +x u =0. The only solution is alwaysx =x =0. So, any
i i j j . i j .
two columns are linearly independent, and so the rank is 2.
.


================================================================================
PAGE 102
================================================================================

3.6 ConnectiontoMatrices 89
To find the rank of a general matrix, one can use Gaussian Elimination, which
is beyond the content covered in this book. Readers who want to learn more about
it can refer to [8]. Note that the rank denoted as r must be less than or equal to
the smallest of the two dimensions of the matrix, thatis,r ≤ min(m,n)for matrix
.
M m×n. .
Exercise
⎡ ⎤
214
3.18
LetA=⎣
321
⎦
. .
535
(1) Find det(A).
.
(2) Are the columns of Alinearly independent?
.
(3) Does Ahave an inverse?
.
(4) What is the rank of A?
.


================================================================================
PAGE 103
================================================================================

Chapter 4
Matrix Decomposition
Matrix decomposition is also called matrix factorisation. Unfortunately, many
matrix operations cannot be solved efficiently. However, in the same way, that
integers can be decomposed into prime factors to make calculations simpler, and
we can use matrix decomposition to reduce a matrix into parts that make it
easier to calculate more complex matrix operations. Such parts are, for instance,
diagonal matrices and triangular matrices (which only have values in the main
diagonal and either the top right half or bottom left half of the matrix). There
are many types of matrix decomposition, and in this chapter, you will learn two
different matrix decomposition methods: eigendecomposition and singular value
decomposition. In addition, you will learn an application of eigendecomposition—
principal component analysis.
4.1 Eigendecomposition
We first define eigenvectors and their corresponding eigenvalues of a square matrix
A.
Definition 4.1 (Eigendecomposition) LetA∈Rn×nbe a square matrix. Then λ∈
.
R is an eigenvalue of A and u (the non-zero column vector) is the corresponding
. . .
eigenvector of Aif
.
Au=λu. (4.1)
.
Remark 4.1 Looking at Eq.(4.1), the left-hand side is a matrix multiplication
and represents a linear transformation of u; the right-hand side is just a scalar
.
multiplication. A scalar multiplication is just an elongation or shrinking of a vector
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 91
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_4


================================================================================
PAGE 104
================================================================================

92 4 MatrixDecomposition
along its own line. So after the linear transformation, the vector u is still along
.
the original line, either in the same or opposite direction (when λ takes a negative
.
value); the length of the vector is changed if the absolute value of the scalar λdoes
.
not equal 1. (cid:2)
.
The equationAu = λuhas non-zero solutions for the vector uif and only if the
. .
matrixA−λIhas a zero determinant, that is, det(A−λI)=0.
. .
To see why it needs det(A−λI)=0, let us rewrite Eq.(4.1) as follows:
.
Au =λu⇔Au−λu=0⇔Au−λIu=0⇔(A−λI)u=0. (4.2)
.
Looking at(A−λI)u=0, then(A−λI)is just a matrix formed by subtracting two
. .
matrices. Now suppose the matrix(A−λI)is invertible. This means det(A−λI)is
. .
non-zero and(A−λI) −1exists.
.
Then if we multiply (A − λI) −1 on both sides of (A − λI)u = 0, we obtain
. .
(A−λI) −1(A−λI)u=(A−λI) −10⇒Iu=0⇒u=0.
.
This contradicts Definition 4.1, which says uis non-zero. Therefore, we cannot
.
assume(A−λI)is invertible, which means det(A−λI)=0.
. .
Remark 4.2 det(A − λI ) is called the characteristic polynomial of A. It is a
. .
polynomial of degree n in λ and has n roots. det(A − λI) = 0 is called
. .
the characteristic equation of A. Sometimes finding the roots of the charac-
.
teristic equation is just referred to as finding the roots of the characteristic
polynomial. (cid:2)
.
4.1.1 Computing Eigenvalues and Eigenvectors
Suppose A is an n-square matrix. The following shows the procedure to compute
.
eigenvalues and eigenvectors:
• Step 1: Find the characteristic polynomial of A.
.
• Step 2: Find the roots of the characteristic equation of Ato obtain the eigenvalues
.
of A.
.
• Step 3: Repeat the following two steps for each eigenvalue λ:
.
1. Form the matrixM=A−λI.
.
2. Find the solution ofMu=0.
.
These non-zero vectors uare linearly independent eigenvectors of A. Each of them
. .
has its corresponding eigenvalue λ.
.


================================================================================
PAGE 105
================================================================================

4.1 Eigendecomposition 93
Example 4.1 Find eigenvalues and eigenvectors of the following2×2matrix
.
(cid:2) (cid:3)
4 2
A= .
. 3−1
Solution
• Step 1: Find the characteristic polynomial of A.
(cid:2) (cid:3)
4−λ 2
A−λI= .
. 3 −1−λ
|A−λI|=(4−λ)(−1−λ)−6=λ2−3λ−10.
.
• Step 2: Find the roots of the characteristic equation of A to obtain the
.
eigenvalues of A.
.
Set λ2−3λ−10=0.
.
(λ−5)(λ+2)=0.
.
The rootsλ =5andλ =−2 are the eigenvalues of A.
1 . 2 . .
• Step 3 (1): Form the matrixM=A−λIforλ =5.
. 1 .
(cid:2) (cid:3) (cid:2) (cid:3)
4− 5 2 − 1 2
M= = .
. 3 −1−5 3 −6
• Step 3 (2): Find the solution ofMu=0.
.
(cid:2) (cid:3)(cid:2) (cid:3) (cid:2) (cid:3)
− 1 2 u 0
Mu= 1 = .
. 3 −6 u 0
2
(cid:4)
−u +2u =0
1 2 →u −2u =0.
. 1 2
3u −6u =0
1 2
The system has only one free variable. Any non-zero so(cid:2)lu(cid:3)tion of this one
6
variable is an eigenvector ofλ = 5. For example,u = . Another way
1 . .
3
to do it is to find a unit eigenvector. From u −2u = 0, w e h ave u =
1 2 . 1
(continued)


================================================================================
PAGE 106
================================================================================

94 4 MatrixDecomposition
Example 4.1 (continued)
(cid:5)
2u . According to the definition of a unit vector, we have u2+u2 =
2. (cid:6) (cid:7)1 2
(cid:5) (cid:5)
√2
(2u )2+u2 = 5u2 = 1. Possible solutions are u = 5 , or u =
2 2 2 . √1 .
(cid:6) (cid:7) 5
−√2
5 . Note these two vectors are just in opposite directions.
−√1 .
5
• Repeat Steps 3 (1) and 3 (2) forλ =−2.
2 .
(cid:2) (cid:3) (cid:2) (cid:3)
4−(−2) 2 62
M= = .
. 3 −1−(−2) 31
(cid:2) (cid:3)(cid:2) (cid:3) (cid:2) (cid:3)
62 u 0
Mu= 1 = .
.
31 u 0
2
(cid:4)
6u +2u =0
1 2 →3u +u =0.
. 1 2
3u +u =0
1 2
Again the system has only one free variable, and any non-zero so(cid:2)lutio(cid:3)n
1
of this variable is an eigenvector of λ = −2 . For example, u = ,
2 . −3 .
(cid:6) (cid:7) (cid:2) (cid:3)
√1 −1
whose unit vector is u = 10 ; o r u = , whose unit vector is
−√3 . 3 .
(cid:6) (cid:7) 10
−√1
u= 10 .
√3 .
10
Example 4.2 Find eigenvalues and eigenvectors of the following3×3matrix
.
⎡ ⎤
322
. A
=⎣
232
⎦
.
223
(continued)


================================================================================
PAGE 107
================================================================================

4.1 Eigendecomposition 95
Example 4.2 (continued)
Solution
• Step 1: Find the characteristic polynomial of A
.
⎡ ⎤
3−λ 2 2
. A−λI=⎣ 2 −3λ 2 ⎦ .
2 2−λ3
Applying the method described in Sect.3.3.9 of Chap.3, we can produce
Fig.4.1.F romF ig .4.1, we can obtain the following:
|A−λI|=(3−λ)3+2×23−3×(2×2×(3−λ)) =−λ3+9λ2−15λ+7.
.
• Step 2: Find the roots of the characteristic equation of A to obtain the
.
eigenvalues of A.
.
Set −λ3+9λ2−15λ+7=0.
.
(λ−1)(λ−1)(λ−7)=0.
.
The rootsλ =λ =1andλ =7are the eigenvalues of A.
1 2 . 3 . .
• Step 3 (1): Form the matrixM=A−λIforλ =λ =1.
. 1 2 .
⎡ ⎤ ⎡ ⎤
3− 1 2 2 222
. M =⎣ 2 −3 1 2 ⎦=⎣ 222 ⎦ .
2 2−13 222
• Step 3 (2): Find the solution ofMu=0.
.
⎡ ⎤⎡ ⎤ ⎡ ⎤
222 u 0
1
. M u=⎣ 222 ⎦⎣ u 2 ⎦=⎣ 0 ⎦→u 1 +u 2 +u 3 =0.
222 u 0
3
⎡
Any
⎤
non-zero solution is an eige⎡nvec⎤tor of λ = 1
.
. For example, u =
√1
1
⎢ 2⎥
⎣−1 ⎦
.
, whose unit vector
isu=⎣√−1⎦
.
.
2
0 0
(continued)


================================================================================
PAGE 108
================================================================================

96 4 MatrixDecomposition
Example 4.2 (continued)
• Repeat Steps 3 (1) and 3 (2) forλ =7.
3 .
⎡ ⎤ ⎡ ⎤
3− 7 2 2 − 4 2 2
. M =⎣ 2 −3 7 2 ⎦=⎣ 2 − 4 2 ⎦ .
2 2−73 2 2−4
⎡ ⎤⎡ ⎤ ⎡ ⎤
− 4 2 2 u 0
1
. M u=⎣ 2 − 4 2 ⎦⎣ u 2 ⎦=⎣ 0 ⎦ .
2 2−4 u 0
3
⎧
⎪⎪⎨ −4u
1
+2u
2
+2u
3
=0
. ⎪⎪⎩ 2u 1 −4u 2 +2u 3 =0 →u 1 =u 2 =u 3 .
2u +2u −4u =0
1 2 3
⎡ ⎤
1
Any non-zero solution is an eigenvector ofλ 3 =7 . . For example,u=⎣ 1 ⎦ . ,
⎡ ⎤ 1
√1
⎢ 3⎥
whose unit vector
isu=⎢
⎣
√1 ⎥
⎦. .
3
√1
3
Remark 4.3 In many applications of eigendecomposition, eigenvectors are unit
vectors, which means that their length or magnitude is equal to 1. (cid:2)
.
Fig. 4.1 An illustration of
calculating the determinant of
A−λI.in Example 4.2


================================================================================
PAGE 109
================================================================================

4.1 Eigendecomposition 97
Exercises
4.1 Compute the eigenvalues and eigenvectors for the following matrices.
(cid:2) (cid:3)
3 1
(1) A= .
.
3 5
(cid:2) (cid:3)
2 1
(2) B= .
.
4 5
(cid:2) (cid:3)
−4 −3
(3) C= .
.
2 3
(cid:2) (cid:3)
1 −3
(4) D= .
.
2 6
⎡ ⎤
−1 2 2
(5) E= ⎣ 2 2 2 ⎦ . .
−3 6− 6
(cid:2) (cid:3)
0 2
4.2 Can you find eigenvalues and eigenvectors for the matrixF= ?
−20 .
4.1.2 Diagonalisation
Finally, we can decompose a matrix Ainto simpler parts. Ifn×nmatrix Ahas n
. . .
eigenvectorsu ,u , ···,u with associated eigenvaluesλ ,λ , ··· ,λ, then Acan
1 2 n. 1 2 n. .
be written in a diagonalised form
A=UDU −1, (4.3)
.
where U = u[ ,u , ···,u ], and D are a diagonal matrix with λ ,λ , ··· ,λ as
1 2 n . . 1 2 n.
the main diagonal elements.
If A can be expressed this way, it is said to be diagonalisable. Since it is not
.
always possible to find eigenvectors and eigenvalues for a matrix, then not all
matrices are diagonalisable.
Remark 4.4 Since each u is a column vector, then U = u[ ,u , ···,u ] is just
i. 1 2 n .
a normal matrix with each of its columns equal to one of the u in turn. It is just
i.
another way of describing a matrix. (cid:2)
.
To prove that A can be diagonalised in the way described in Eq.(4.3), first we
.
multiply Ufrom right on both sides of Eq.(4.3) and then multiplyU
−1from
left on
. .
both sides, we have
D=U −1AU. (4.4)
.


================================================================================
PAGE 110
================================================================================

98 4 MatrixDecomposition
Now we have to show thatU −1AUis the diagonal matrix withλ ,λ , ··· ,λ on
. 1 2 n.
the main diagonal.
Proof First note that U
−1u
is the ith column of U
−1U.
That is U
−1u
is the ith
i. . i.
column of the identity matrix Iwith a size ofn×nsinceU −1U=I. Also remember
. . .
thatU = u[ ,u , ···,u ]and that from the definition of eigenvectors that Au =
1 2 n . i
λu. Then:
i.
U −1AU =U −1A[u ,u , ···,u ]
1 2 n
=U −1[Au ,Au , ···,Au ]
1 2 n
=U −1[λ u ,λ u , ··· ,λu ]
1 1 2 2 n n
. =[λ 1 U −1u 1 ,λ 2 U −1u 2 , ··· ,λ n U −1u n ] (4.5)
⎡ ⎤
λ 0 ... 0
1
⎢ ⎥
⎢0 λ 2 ... 0 ⎥
=⎢
. . .
⎥.
⎣ . . . ⎦
. . .
0 0... λ
n
(cid:7)(cid:8)
Example 4.3 Let us revisit Example 4.1. After performing t(cid:6)he eigend(cid:7)ecom-
√2 √1
position, one possible solution for U = u[ ,u ] is U = 5 10 . The
1 2 . √1 √−3 .
(cid:6) (cid:7)5 10
√−3 √−1
determinant of U equals to √−7 and U −1 = 1 10 10 . Substituting
. 50 . √−7 √−1 √2 .
50 5 5
U
−1,
A, and Uinto Eq.(4.4), we have
. . .
(cid:6) (cid:7)(cid:2) (cid:3)(cid:6) (cid:7) (cid:2) (cid:3)
1
√−3 √−1
4 2
√2 √1
5 0
D= 10 10 5 10 = ,
. √−7 √−1 √2 3−1 √1 √−3 0−2
50 5 5 5 10
where the two elements along the main diagonal are the eigenvalues of A.
.
Remark 4.5 If then×nmatrix Ais symmetric, then it is diagonalisable. Moreover,
. .
its eigenvectors corresponding to different eigenvalues are orthogonal to each other,
whichisaspecialcaseoflinearindependence. Hence, UinEq.(4.3)isanorthogonal
.
matrix. (cid:2)
.


================================================================================
PAGE 111
================================================================================

4.2 PrincipalComponentAnalysis 99
Exercise
4.3 Diagonalisation. Find a matrix U, so thatD=U −1AUis diagonal.
. .
(cid:2) (cid:3)
3 1
(1) A= (The first matrix in Exercise 4.1).
.
3 5
(cid:2) (cid:3)
3 2
(2) A= . Since Ais symmetric, check whether Uis orthogonal.
. . .
2 6
4.2 Principal Component Analysis
Principal Component Analysis (PCA) is widely used in many Data Science appli-
cations. It extracts important information from the data. This important information
relates to the total variation contained in the data. One can use PCA to compress the
size of the dataset by keeping only the information relating to the most variance in
the data. One can also use PCA to visualise the structure of the data.
4.2.1 Mathematics Behind PCA
First, we need to define some concepts from Statistics. In each case, we will give
the definitions in their general form but give an example in terms of a small number
that might be easier to visualise.
Suppose X is a data matrix including n data observations with d dimensions
.
(or variables, features, or attributes). Each element of X is denoted as x , where
. i,j.
i = 1,...,nand j = 1,...,d. For example, we could have 6 data points with 3
. .
features (dimensions), for instance, height, width, and depth.
• Define the sample mean for each dimension
(cid:18)n
1
x¯ = x . (4.6)
. j i,j
n
i=1
In our example
(cid:18)6
1
x¯ = x .
. j i,j
6
i=1
So x¯ would be the mean height.
1.


================================================================================
PAGE 112
================================================================================

100 4 MatrixDecomposition
• Define the sample standard deviation for each dimension
(cid:19)
(cid:20)
n (x −x¯ )2
s(x )= i=1 i,j j . (4.7)
. j n−1
The sample standard deviation can be considered as an average distance from the
centre of the data within a specific dimension.
In our example,
(cid:19)
(cid:20)
6 (x −x¯ )2
s(x )= i=1 i,1 1 .
. 1 6−1
is the average distance in the data to the mean height.
The squared standard deviation is called variance:
var(x )=(s(x ))2. (4.8)
. j j
• The degree to which a pair of variables is linearly related is referred to as the
correlation between the two variables. Here we define the sample covariance,
which measures the correlation between two dimensions (the hth dimension and
kth dimension) in the data.
(cid:20)
n (x −x¯ )(x −x¯ )
cov(x ,x )= i=1 i,h h i,k k . (4.9)
. h k n−1
The sign of a covariance value can tell us whether two features are positively
correlated or negatively correlated.
In our example, the correlation between the 2nd (width) and 3rd (depth) over
all six data points is:
(cid:20)
6 (x −x¯ )(x −x¯ )
cov(x ,x )= i=1 i,2 2 i,3 3 .
. 2 3 6−1
• A particular correlation coefficient is the Pearson correlation coefficient: r
cov(x ,x )
r = √ h k . (4.10)
.
var(x )var(x )
h k
Pearson correlation coefficient has a value between − 1 and 1. It usually
.
evaluates the linear relationship between two continuous variables. If we want
to check the strength of the correlation between two features, then we need to
consider the absolute value of the Pearson correlation coefficient.


================================================================================
PAGE 113
================================================================================

4.2 PrincipalComponentAnalysis 101
• The covariance matrix is ad×d square matrix, (cid:2)given by:
. .
⎡ ⎤
cov(x ,x ) cov(x ,x ) ···cov(x ,x )
1 1 1 2 1 d
⎢ ⎢cov(x 2 ,x 1 ) cov(x 2 ,x 2 ) ···cov(x 2 ,x d ) ⎥ ⎥
. (cid:2) =⎢ ⎣ . . . . . . ··· . . . ⎥ ⎦ .
cov(x ,x )cov(x ,x ) ···cov(x ,x )
d 1 d 2 d d
For example, for our data, the covariance matrix is a3×3matrix:
.
⎡ ⎤
cov(x ,x ) cov(x ,x ) cov(x ,x )
1 1 1 2 1 3
. (cid:2)
=⎣
cov(x 2 ,x 1 ) cov(x 2 ,x 2 ) cov(x 2 ,x 3 )
⎦
.
cov(x ,x )cov(x ,x )cov(x ,x )
3 1 d 2 3 3
The covariance matrix is symmetrical, where cov(x ,x ) = cov(x ,x ).
h k k h .
Each element of the main diagonal is the variance of a specific dimension.
For our data, for example, cov(x ,x ) = cov(x ,x ) since the correlation
1 2 2 1 .
between height and width is the same as the correlation between width and
height.
Also, on the main diagonal, the first number is
(cid:20) (cid:20)
6 (x −x¯ )(x −x¯ ) 6 (x −x¯ )2
cov(x ,x )= i=1 i,1 1 i,1 1 = i=1 i,1 1 .
. 1 1 6−1 6−1
This is the variancevar(x )of the heights.
1 .
• The covariance matrix is symmetric, so we can do eigendecomposition on the
covariance matrix, since there exist u eigenvectors of (cid:2)such that
i. .
(cid:2)u =λ u .
. i i i
Recall that if the matrix is symmetric, then eigenvectors corresponding to
different eigenvalues must be orthogonal to each other.
4.2.2 The Definition of PCA
The d principal components ofdataX(n×d) are thed eigenvectors u ,u ,...,u
. . 1 2 d.
corresponding to the d ordered eigenvaluesλ ≥ λ ≥ ...≥ λ of the covariance
1 2 d.
of X, (cid:2).
. .
Remember that there are d dimensions (or features) of each data item. Suppose
there are just three dimensions, like height, width, and depth. These can be plotted
in a three-dimensional space using height as the x-axis, width as the y-axis, and
depth as the z-axis. What we are doing with PCA is replacing these three mutually
perpendicular axes with three different mutually perpendicular ones. The first of


================================================================================
PAGE 114
================================================================================

102 4 MatrixDecomposition
Fig. 4.2 An illustration of
projecting the data onto u1.
these new axes, u , will be the direction of the most variance in the data, the second
1.
axis, u , will have the next most variance and so on.
2.
So the first principal component of the data X is the vector u , such that the
. 1.
projection of the data onto u , that is, Xu , has the largest variance, subject to
1. 1.
the normalising constraint uTu = 1. This normalising constraint means that the
1 1 .
principal component is a unit eigenvector, and its variance can be measured by the
corresponding eigenvalue.
The second principal component is always orthogonal to the first component,
and the third principal component is orthogonal to both the first two, etc. Up to d
principal components can be found by this method. Projections of the data on each
principal component are obtained as linear combinations of the original variables,
thatis,Xu .
i.
Remark 4.6 When calculating Xu , each data vector x in the matrix X forms a
1. i. .
dot product with u . AlsouTu = 1, that is the vector u is a unit vector of length
1. 1 1 . 1.
1. So using the second definition of dot product, the geometric definition, we get
x·u =(cid:10)x(cid:10)cosθ, where θis the angle between them. Looking at Fig.4.2, which as
i 1 i . . −→ −→
a two-dimensional representation only has u and u , we see that cosθ =OA/OB
−→ −→ 1. 2. .
and so OA = OBcosθ = (cid:10)x(cid:10)cosθ. This can be seen as laying the length of x
i . i.
onto the vector u , and this is what is meant by projecting the data onto u . (cid:2)
1. 1. .
4.2.3 PCA in Practice
The following procedure shows how to perform PCA in a real-world setting:
1. Pre-process the given dataset with n data points and d attributes: For example,
normalise the data, so that they have zero means and unit standard deviations.
We useXto denote the normalised data matrix with a size of n by d .
.
2. Calculate the covariance matrix. The size of the covariance matrix is d by d.


================================================================================
PAGE 115
================================================================================

4.2 PrincipalComponentAnalysis 103
3. Compute the eigenvectors u and eigenvalues λ of the covariance matrix. The
i. i.
size of the eigenvector matrix is d by d, where each column vector is an
eigenvector. The number of eigenvalues is d .
4. Select k principal components, where k ≤ d. If we are trying to illustrate the
.
data then we usually just pick k = 2, since it is easy to plot and visualise.
.
Usually these are the first two principle components, since we want to visualise
the greatest variance. If we are trying to compress the data, we may use any value
k <d and again pick the first k components so as to retain the most var iation.
.
5. Derive the new dataset. That is, project the normalised data onto the selected
k principal components. This step involves a very basic operation: matrix
multiplication.
projected_data=normalised_data×selected_principal_components.
.
For example, the projections along the first principal component are given by:
. p rojected_data=X n×d u d×1 =u 11 x ,1 +u 21 x ,2 +···+u d1 x ,d ,
whereu ,u ,...andu are elements of u andx ,x ,...,and x denote
11 21 . d1. 1. ,1 ,2 . ,d.
the individual attributes of the data, that is the column vectors of the data in the
matrix X. It can be further written as follows:
.
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
x x x
11 12 1d
projected_data=u ⎢ ⎣ . . ⎥ ⎦+u ⎢ ⎣ . . ⎥ ⎦+···+u ⎢ ⎣ . . ⎥ ⎦.
. 11 . 21 . d1 .
x x x
n1 n2 nd
As can be seen, the projection of each data point on the first principal component
is a weighted sum of all attributes or features, where the weights are elements in
the corresponding eigenvector. In general, projections of data points on a specific
principal component are a linear combination of all attributes or features of the
dataset.
Since PCA is a method to extract the total variation information of the data, it is
important to report how much selected principal components have captured this
information. We have:
• Total variation in the original data istr((cid:2))(cid:20). .
• Total variation of principal components is λ .
i.
(cid:12)
• When performing a PCA, the variation information of (cid:2) is kept in λs, so we
. .
hav e
(cid:18)
λ = tr((cid:2)).
. i
• (cid:20)λi is the amount of information contained in the ith principal component.
λj .


================================================================================
PAGE 116
================================================================================

104 4 MatrixDecomposition
• (λ1 +λ(cid:20)2 +...+λk) is proportion information in first k principal components;
λj .
• We can use PCA to do feature extraction. That is to select the first k principal
components. When doing feature extraction, we want (λ1 +λ(cid:20)2 +...+λk) large but
λj .
also want k small.
4.2.4 Case Study 2: Continued (1)
⎡ ⎤
15
⎢ ⎥
⎢22⎥
⎢ ⎥
Consider a small data setX=⎢
⎢
33⎥
⎥
.
.
⎣44⎦
51
This data has five data points, each with dimension 2, so n is 5 and d is 2.
• Pre-process the given dataset. In this example, we remove the mean value from
each dimension. Since x¯ 1 = 1+2+3 5 +4+5 = 3 . and x¯ 2 = 5 ⎡ +2+ 5 3+4+ ⎤ 1 = 3 . , t he
− 2 2
⎢ ⎥
⎢−1−1⎥
⎢ ⎥
datasets having zero means are shown as follows:newX=⎢ ⎢ 0 0 ⎥ ⎥. .
⎣ 1 1 ⎦
2 −2
Here the variance of both columns is the same (and is 2.5). In this case, since
.
the variance is the same, there is no need to normalise by dividing by the standard
deviation. This would not be the case with realistic examples. More will be said
about this in Chap.7, Sect.7.4.3.
• Calculate the covariance matrix using Eqs.(4.7), (4.8), and (4.9). We have:
var(newx )=(s(newx ))2
. 1 1
(cid:19)
(cid:21) (cid:22)
(−2)2+(−1)2+0+12+22 2
=
5−1
=2.5,
var(newx )=(s(newx ))2
. 2 2
(cid:19)
(cid:21) (cid:22)
(2)2+(−1)2+0+12+(−2)2 2
=
5−1
=2.5,


================================================================================
PAGE 117
================================================================================

4.2 PrincipalComponentAnalysis 105
and
cov(newx ,newx )=cov(newx ,newx )
. 1 2 2 1
(cid:20)
5 (x −0)(x −0)
= i=1 i,1 i,2
5−1
(−2)×(2)+(−1)×(−1)+0×0+1×1+2×(−2)
=
4
=−1.5.
(cid:2) (cid:3)
2.5 −1.5
The covariance matrix is (cid:2) = . Since the data is two-
−1. 5 .25 .
dimensional, the size of the covariance matrix is 2 by 2.
• Compute the eigenvectors and eigenvalues of the covariance matrix. We obtain
the following:
– λ
1
= 4;uT
1
=[ √1
2
√−1
2
]
.
.
– λ
2
= 1;uT
2
=[ √1
2
√1
2
].
.
The sum of eigenvalues 4 + 1 = 5 equals the sum of elements along the
.
covariance matrix 2.5+2.5 = 5. Readers are encouraged to check the results
.
by following the steps shown in Sect.4.1.1 of this chapter.
• Select principal components. We shall use both the first principal component and
the second principal component to visualise the data in this example. The first
principal component is uT = [√1 √−1] having the largest eigenvalue of 4. The
1 2 2 .
first principal component captures λ1 = 4 = 80%of the total variation in
λ1 +λ2 4+1 .
the dataset.
• Derive the new dataset using XU, where U is the matrix formed with columns
. .
equal to the two eigenvectors u and u .
1. 2.
Doing this for each data point, in turn, we can see where each point is moved
to in the PCA space. So taking the first data point [−2,2] on the first principal
.
component, we have
(cid:6) (cid:7)
√1
1 1 4
projected_data_pc1 =[−2,2]× 2 =−2× √ +2 ×−√ =−√ .
. −√1
2 2 2
2
To project the data point[−2,2]on the second principal component, we have
.
(cid:6) (cid:7)
√1
1 1
projected_data_pc2 =[−2,2]× 2 =−2× √ +2× √ =0.
. √1
2 2
2


================================================================================
PAGE 118
================================================================================

106 4 MatrixDecomposition
Therefore, the projection of data point[−2,2]in the PCA space is[−√4 ,0].
. .
2
Projections of the other four data points can be calculated in the same way. The
final PCA plot is shown in Fig.1.5 of Chap.1. This PCA plot has captured all
variation information in the original dataset.
This example is unrealistic, since it only has two dimensions. It was picked, so
that all the stages could be calculated by hand and the working can be explained. In
the next section, we illustrate a realistic data set.
4.2.5 A Principal Component Analysis on the Sparrow Dataset
In this example, we demonstrate PCA on a female sparrows dataset. The data
includes 49 sparrows with five body measurements which are total length, alar
extent, length of beak and head, length of humerus and length of keel of sternum.
After a severe storm, about half of the 49 birds died. The researcher wanted to know
whether they could find any support for Charles Darwin’s theory of natural selection
[9].
First, we normalise the data, so that each feature has a zero mean and a unit
standard deviation. We then obtain the covariance matrix of the normalised data,
equivalent to the correlation-coefficient matrix, which is shown as follows:
⎡ ⎤
1.0000 0.7350 0.6618 0.6453 0.6051
⎢ ⎥
⎢0.7350 1.0000 0.6737 0.7685 0.5290⎥
⎢ ⎥
. (cid:2) =⎢ ⎢ 0.6618 0.6737 1.0000 0.7632 0.5263⎥ ⎥ .
⎣0.6453 0.7685 0.7632 1.0000 0.6066⎦
0.6051 0.5290 0.5263 0.6066 1.0000
As can be seen,tr((cid:2))=5.0000. After doing the eigendecomposition, we have the
.
following eigenvalues:
• λ(cid:20)1 = 3.6160, λ
2
= 0.5315, λ
3
= 0.3864, λ
4
=0.3016,λ
5
=0.1645
.
,
• λ = 5.0000,
i .
and eigenvectors are shown in Table 4.1.
Table 4.1 Eigenvectors and
u1. u2. u3. u4. u5.
eigenvalues of the covariance
0.4518 −0.0507 −0.6905 0.4204 0.3739
matrix of the sparrow dataset
0.4617 0.2996 −0.3405 −0.5479 −0.5301
0.4505 0.3246 0.4545 0.6063 −0.3428
0.4707 0.1847 0.4109 −0.3883 0.6517
0.3977 −0.8765 0.1785 −0.0689 −0.1924
λi. 3.6160 0.5315 0.3864 0.3016 0.1645


================================================================================
PAGE 119
================================================================================

4.2 PrincipalComponentAnalysis 107
Fig. 4.3 A PCA visualisation
plot of the sparrow dataset,
where the first two principal
components capture about
82.95% of the total variance
in the data
Figure 4.3 shows the first principal component plotted against the second
principal component. The difference along the first principal component (denoted
as PC1) axis between the rightmost point and the leftmost point is about 8, and the
difference along the second principal component (denoted as PC2) axis between the
highest point and the lowest point is greater than 4 but much less than the 8 along
the PC1 axis. In fact, from Table 4.1, it can be seen that the variance along PC1
is3.616, and the variance along PC2i s0.5315. The first two principal components
. .
have captured (3.6160+0.5315)/5.0 = 82.95% of the total variation among the
.
data.
PCA is often used to help visualise the most important aspects of multiple
dimensional data, since multiple dimensional data is not visualisable in itself. It can
often show relationships that are not obvious in the original data. For the sparrow
dataset, we can see that the two classes cannot be linearly separated in the PCA
space. However, it does illustrate that there are some outliers in the non-survival
class.
Projections (denoted asproj_pc1) along PC1 are calculated using the following
.
equation:
proj_pc1=Xu
. 1
⎡ ⎤
0.4518
⎢ ⎥
⎢0.4617⎥
⎢ ⎥
=[x
1
,x
2
,x
3
,x
4
,x
5
]⎢
⎢
0.4505⎥
⎥
(4.11)
⎣0.4707⎦
0.3977
= 0.4518x +0.4617x + 0.4504x + 0.4707x + 0.3 977x .
1 2 3 4 5
wherex ,x , ···,x are the five columns of X.
1 2 5. .


================================================================================
PAGE 120
================================================================================

108 4 MatrixDecomposition
Projections (denoted asproj_pc2) along PC2 are calculated using the following
.
equation:
proj_pc2=Xu
. 2
⎡ ⎤
−0.0507
⎢ ⎥
⎢ 0.2996 ⎥
⎢ ⎥
=[x
1
,x
2
,x
3
,x
4
,x
5
]⎢
⎢
0.3246 ⎥
⎥
(4.12)
⎣ 0.1847 ⎦
−0.8765
=−0.0507x + 0.2.996x + 0.3246x +0.1847x − 0.8765 x .
1 2 3 4 5
wherex ,x , ···,x are the five columns of X.
1 2 5. .
Researchers can trace back to the original data based on the PCA projection plot
to look into more details. For instance, the lowest point in Fig.4.3 clearly looks like
an outlier. So we can look atproj_pc1being positive with a value just above zero
.
and proj_pc2 being negative and less than −2.5. In fact, this specific sparrow
. .
has a total length of 162 millimeters (mm), an alar extent of 239 mm , ale ngth of
beak and headof30.3mm, al ength of humersof18.0mm, and a length of keel of
. .
stermum of23.1mm. After removing the mean and converting each feature to the
.
unit standard deviation, we havex ≈1.11,x ≈−0.46,x ≈−1.47,x ≈−0.84,
1 . 2 . 3 . 4 .
and x ≈ 2.32. It tells us that this sparrow’s total length and length of keel of
5 .
stermum are greater than their corresponding mean feature values, while the other
three features are less than the corresponding mean feature values. Researchers may
further work out why this sparrow is an outlier by comparing its body structural
information with other sparrows’ body structural information.
Exercise
4.4 Do a principal component⎡ analysis⎤ on the following small dataset Y
.
3 3
⎢ ⎥
⎢ 0 0 ⎥
⎢ ⎥
involving five data points:Y=⎢
⎢
−3−3⎥
⎥
.
.
⎣− 1 1⎦
1 −1
(1) Compute the mean value of each variable.
(2) Compute the standard deviation of each variable.
(3) Compute the covariance between two variables.
(4) Write down the covariance matrix.
(5) Find the eigenvalues and eigenvectors of the covariance matrix.
(6) Find the percentage of variance captured by each principal component.
(7) Compute the projection for the first data point[3,3]in the PCA space.
.


================================================================================
PAGE 121
================================================================================

4.3 SingularValueDecomposition 109
4.3 Singular Value Decomposition
Singular value decomposition (SVD) is one of the most known and widely used
matrix decomposition methods. SVD can be used as a method for dealing with large,
high-dimensional data and finding important dimensions in the data.
Definition 4.2 (Singular Value Decomposition) Let M be the real n×d matrix
. .
that we want to decompose. The SVD theorem states:
. M n×d =U n×n S n×d VT d×d , (4.13)
where
• U is a column-orthonormal matrix; the columns of the U matrix are called the
. .
left-singular vectors of M;
.
• Sis
.
– an×d diagnonal matrix;
.
– the diagonal values in the S matrix are known as the singular values of the
.
original matrix M;
.
– the singular values are stored in descending order along the main diagonal in
S;
.
– the number of non-zero values in Sis equal to the rank of matrix M.
. .
• Vis a column-orthonormal matrix; the columns of Vare called the right-singular
. .
vectors of M.
.
The SVD described in Eq.(4.13) is called a full SVD. Some of the non-zero singular
values may be significant, while others may be very small and not significant.
The singular value decomposition can also be done as follows:
. M n×d =U n×k S k×k VT k×d , (4.14)
wherek ≤min(n,d). This is often called compact SVD, or economy SVD.
.
If k = r . , where r is the rank of M . , then M n×d = U n×k S k×k VT k×d. ; otherwise,
ifk < r . , thenM n×d ≈ U n×k S k×k VT k×d. which is useful in data compression as we
will see later (Sect.4.3.6 of this chapter).
Note that the SVD of a matrix is not a unique solution.
4.3.1 Intuitive Interpretations
(cid:2) (cid:3) (cid:2) (cid:3)
0−2 −→ 1 −→
LetM = , and(cid:14)pqr denotes a triangle constructed withop = , oq =
. . .
2 0 3
(cid:2) (cid:3) (cid:2) (cid:3)
2 −→ 1
, and or = (see the red triangle in Fig.4.4a).
. .
3 1


================================================================================
PAGE 122
================================================================================

110 4 MatrixDecomposition
Fig. 4.4 An illustration of how SVD works. (a) The original triangle, (cid:14)pqr., is shown in red,
and its transformed version, after the linear transformation defined by matrix M., is shown in blue.
(b) The triangle is shown in black after the linear transformation defined by matrix VT .. (c) The
triangle is shown in green after the linear transformation defined bymatrixSVT .. (d) The triangle
is shown in blue after the linear transformation defined bymatrixUSVT .. This is the same as the
transformation defined by M.shown in (a)
After doing SVD on M
.
(S(cid:6)ect.4.3.3 o(cid:7)f this c
(cid:2)
hapte
(cid:3)
r shows a po(cid:6)ssible met(cid:7)hod to
−√1 √1 √1 √1
20
perform SVD), we haveU= 2 2 ,S= , andVT = 2 2 .
√1 √1 . 02 . √1 −√1 .
2 2 2 2
As mentioned in Sect.3.3.4 of Chap.3, a vector can be linearly transformed
through matrix multiplication. Figure 4.4a shows the triangle(cid:14)pqr in red and the
.
triangle (in blue) after the linear transformation given by matrix M, which is the
.
result of Mmultiplying each edge of(cid:14)pqr. As can be seen, the triangle has been
. .
rotated and made bigger. Panel (b) displays, in black, the resultant triangle after
rotating the triangle(cid:14)pqr via a linear transformation given by matrix VT, that is,
. .
VT multiplies each edge of(cid:14)pqr. As can be seen, the size of the triangle remains
. .
the same. The black triangle is then stretched after another linear transformation
given by S; that is, Smultiplies each edge of the black triangle, as shown in panel
. .
(c) in green. This time there is no rotation involved. In panel (d), a further rotation
has happened, and the final triangle is shown in blue, which is the result after U
.
is multiplied by the green triangle. The size of the blue triangle is the same as


================================================================================
PAGE 123
================================================================================

4.3 SingularValueDecomposition 111
the green one. The blue triangles in panels (a) and (d) are the same. This shows
that the linear transformation represented by multiplying by Mcan be decomposed
.
into three simpler, linear transformations represented by multiplying by matrix VT,
.
matrix S, and matrix Uin that order.
. .
4.3.2 Properties of the SVD
IfM=USVT, where the size of Sisn×d, then
. . .
1. UTU = I. This can be obtained by Definition 4.2, since UT has rows which are
. .
orthonormal and U has columns which are the same orthonormal vectors and
.
matrix multiplication multiplies rows from the first matrix by columns of the
second.
2. VTV= I. This can be obtained by Definition 4.2 for the same reason.
.
3. MT = VSTUT. This can be obtained by applying the fourth property of
.
Sect.3.3.11.1 of Chap.3.
4. US(:, i) = MV(:,i), where(:,i)is the ith column of each matrix. This can be
. .
obtained by multiplying the right-hand side of both sides of Eq.(4.13)b y Vand
.
considering each column separately.
4.3.3 Find a Singular Value Decomposition of a Matrix
Let Mbe a matrix. One can find a singular value decomposition on Mby following
. .
the procedure shown as follows:
1. ComputeA=MTM.
.
2. Find the eigenvalues and eigenvectors of A:
.
a. sort the eigenvalues in descending order;
b. the square roots of the eigenvalues are the singular values;
c. the corresponding unit eigenvectors are the right-singular vectors Vof M.
. .
3. Find the left-singular vectors Uone column at a time by using the property US(:
.
,i)=MV(:,i).
.
Alternatively, one can do the singular value decomposition on M starting with
.
the calculation ofA = MMT. In this way, the unit eigenvectors obtained from the
.
eigendecomposition are the left-singular vectors of M. To find the right-singular
.
vectors V, one can use UTM = SVT, which can be obtained by multiplying the
. .
left-hand side of both sides of Eq.(4.13) of this chapter by UT. We can then find VT
. .
one row at a time.


================================================================================
PAGE 124
================================================================================

112 4 MatrixDecomposition
4.3.4 Case Study 2: Continued (2)
Find a singular value decomposition of
⎡ ⎤
− 2 2
⎢ ⎥
⎢−1−1⎥
⎢ ⎥
. n ewX=⎢ ⎢ 0 0 ⎥ ⎥ .
⎣ 1 1 ⎦
2 −2
Note thatnewXis a5×2matrix, son=5andd =2. We are actually going to find
. . . .
the compact or economy version of SVD, using Eq.(4.14) of this chapter, where
k =d =2, where 2 is the rank ofnewX.
. .
Solution (Note we have the particular matrixnewXinstead of Mas used in the
. .
theory):
1. ComputenewXTnewX.
.
(cid:2) (cid:3)
10 −6
A=newXTnewX= .
. − 6 10
2. Do the eigendecomposition on A by following the procedure introduced in
.
Sect.4.1.1 of this chapter, and we ob(cid:6)tain λ(cid:7)1 = 16 . , λ 2(cid:6) = 4(cid:7). , and two possible
−√1 √1
corresponding eigenvectors arev = 2 andv = 2 , respectively.
1 √1 . 2 √1 .
2 2
a. Sort the eigenvalues in descending order.
b. The squar√e roots of the eigenv√alues are the singular values. In this case, they
ares = 16 = 4ands = 4 = 2, and they are elements along the main
1 . 2 .
diagonal of the singular-value matrix:
(cid:2) (cid:3)
40
S= .
.
02
.
c. The corresponding unit eigenvectors are the right-singular vectors of M. Then
.
Vis the matrix with these two eigenvectors as columns, that is
.
(cid:6) (cid:7)
−√1 √1
V= 2 2 .
. √1 √1
2 2


================================================================================
PAGE 125
================================================================================

4.3 SingularValueDecomposition 113
3. Find the left-singular vectors U by using the property US(:,i)= newXV(:,i);
. .
that is, we find Uone column at a time:
.
⎡ ⎤ ⎡ ⎤
− 2 2 √1
⎢ ⎥(cid:6) (cid:7) ⎢ 2 ⎥
1 1
⎢
⎢
−1−1⎥
⎥
−√1 ⎢
⎢
0 ⎥
⎥
. u 1 = s newXv 1 = 4 ⎢ ⎢ 0 0 ⎥ ⎥ √1 2 =⎢ ⎢ 0 ⎥ ⎥ ,
1 ⎣ 1 1 ⎦ 2 ⎣ 0 ⎦
2 −2 −√1
2
⎡ ⎤ ⎡ ⎤
− 2 2 0
1 1 ⎢ ⎢ ⎢ −1−1 ⎥ ⎥ ⎥ (cid:6) √1 (cid:7) ⎢ ⎢ ⎢ −√1 2 ⎥ ⎥ ⎥
. u 2 = s newXv 2 = 2 ⎢ ⎢ 0 0 ⎥ ⎥ √1 2 =⎢ ⎢ 0 ⎥ ⎥ .
2 ⎣ 1 1 ⎦ 2 ⎣ √1 ⎦
2
2 −2 0
Thus
⎡ ⎤
√1 0
⎢ 2 ⎥
⎢ 0 −√1 ⎥
⎢ 2⎥
. U =[u 1 ,u 2 ]=⎢ ⎢ 0 0 ⎥ ⎥.
⎢ ⎥
⎣ 0 √1 ⎦
2
−√1 0
2
To conclude, we have found a singular value decomposition ofnewX:
.
⎡ ⎤ ⎡ ⎤
. n ewX= ⎢ ⎢ ⎢ ⎢ ⎢ − − 0 1 2 − 0 1 2 ⎥ ⎥ ⎥ ⎥ ⎥ = ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ √ 0 0 1 2 − 0 0 √1 2 ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ (cid:2) 4 0 0 2 (cid:3)(cid:6) − √1 √1 2 √ √ 1 1 2 (cid:7) T .
⎣ 1 1 ⎦ ⎣ 0 √1 ⎦ 2 2
2
2 −2 −√1 0
2
Remark 4.7 If, instead of finding the compact or economy SVD, we had found the
full SVD, then Swould bea5×2matrix:
. .
⎡ ⎤
40
⎢ ⎥
⎢02⎥
⎢ ⎥
.
S=⎢
⎢
00⎥
⎥
,
⎣00⎦
00


================================================================================
PAGE 126
================================================================================

114 4 MatrixDecomposition
and Uwould be a5×5matrix. But since the last three rows of Sare all zeros, the last
. . .
three columns of Uare irrelevant. Hence, finding the compact or economy SVD with
.
k =d =r =2, wherer =2is the rank ofnewX, finds all the relevant information
. . .
of the full SVD, since we can only get 2 non-zero singular values anyway. (cid:2)
.
Exercise
4.5 Find a singular value decomposition fork =d =r =2of
.
⎡ ⎤
3 3
⎢ ⎥
⎢ 0 0 ⎥
⎢ ⎥
.
Y =⎢
⎢
−3−3⎥
⎥
.
⎣− 1 1⎦
1 −1
4.3.5 An Example of the Interpretation of SVD on a Small
Dataset
Results of SVD can provide insights to explain concepts included in a dataset.
Suppose four children have ranked five books written by Louis Sachar and Philip
Pullman, respectively. The ranking is shown in Table 4.2, where the maximum score
is 5:
We have the ranking matrix shown as follows:
⎡ ⎤
55400
⎢ ⎥
. M
=⎢
⎣
44300⎥
⎦,
00055
00033
Table 4.2 The rankings of five books reviewed by four children
Louis Sacha Philip Pullman
Holes Small steps Fuzzy mud Serpentine Northern lights
Mary 5 5 4 0 0
Jack 4 4 3 0 0
Tim 0 0 0 5 5
Ann 0 0 0 3 3


================================================================================
PAGE 127
================================================================================

4.3 SingularValueDecomposition 115
where each row shows rankings for all five books by one child and each column
shows rankings for one specific book reviewed by all four children. We perform an
SVD using Eq.(4.14) of this chapter and setk = 2(note that the rank of Mis 3).
. .
Keeping two decimal places for each value, we have
⎡ ⎤
0.79 0 (cid:2) (cid:3)(cid:2) (cid:3)
⎢ ⎥
. M≈USVT =⎢ ⎣ 0.62 0 ⎥ ⎦ 10. 3 0 0.62 0.62 0.48 0 0 . (4.15)
0 .806 0 .28 0 0 0.7100.71
0 .501
In this example, the concepts are two authors. The U matrix connects children
.
to the author they like, where the first column corresponds to Louis Sachar and
the second column corresponds to Philip Pullman. For example, Jack likes Louis
Sachar’s books and has not ranked Philip Pullman’s books. Scores corresponding to
Jack in the Umatrix are in the second row with values of0.62and 0, respectively.
. .
Scores corresponding to Mary are 0.79 and 0, respectively, shown in the first row
.
of U. Mary has not ranked Philip Pullman’s books either. The score from Mary to
.
Louis Sachar is higher than the one from Jack to Louis Sachar, because Mary has
given a higher rank to each of those three books written by Louis Sachar than Jack
has.
The matrix Stells us the strength of concepts in the dataset.10.3is the strength
. .
of Louis Sachar, while 8.2is the strength of Philip Pullman. The information about
.
Louis Sachar is stronger, because there is more information in the dataset about
books written by Louis Sachar.
Finally, the V matrix connects books to authors. The first three books in the
.
first row are written by Louis Sachar, and the last two books in the second row
are written by Philip Pullman. Interestingly, one cannot compare values in the V
.
across two authors. For example, it does not make sense to compare 0.62 to 0.71.
. .
However, one may compare values within each specific author. For instance, 0.62
.
is bigger than 0.48, and it suggests that both Holes and Small Steps have a better
.
ranking than Fuzzy Mud overall for Louis Sachar in this small review dataset.
Now let us swap the first two rows of M and keep values in N as shown as
. .
follows:
⎡ ⎤
44300
⎢ ⎥
.
N=⎢
⎣
55400⎥
⎦.
00055
00033


================================================================================
PAGE 128
================================================================================

116 4 MatrixDecomposition
After performing an SVD on N, we hav e
.
⎡ ⎤
−0.62 0 (cid:2) (cid:3)(cid:2) (cid:3)
. N ≈USVT = ⎢ ⎢ ⎣ −0.79 0 ⎥ ⎥ ⎦ 10. 3 0 −0.62 −0.62 −0.48 0 0 .
0 .860 0 .28 0 0 .071 00.71
0 .510
Comparing with the SVD output in the mathematical expression (4.15), one can
see that S is the same, and the absolute values of the two VT matrices are equal.
. .
However, the first two rows of the U matrix have been swapped, and the signs of
.
values in these two rows have been changed.
Furthermore, let us swap the first column and the last column of M and keep
.
values in Zas shown as follows:
.
⎡ ⎤
05405
⎢ ⎥
. Z
=⎢
⎣
04304⎥
⎦.
50050
30030
After performing an SVD on Z, we hav e
.
⎡ ⎤
0.79 0 (cid:2) (cid:3)(cid:2) (cid:3)
⎢ ⎥
. Z ≈USVT =⎢ ⎣ 0. 0 62 −0 0 .86 ⎥ ⎦ 10 0 . .2 3 8 0 −0 0 .7 . 1 620 0 0.48 0 0 −0. 0 7 . 1 62 0 .
0 −0.51
Comparing with the SVD output in the mathematical expression (4.15), one can see
that Sis the same, and the absolute values of the two Umatrices are equal. But the
. .
two VT matrices are different. The absolute values seem the same; however, the first
.
column and the last column have been swapped, and the signs of some values have
been changed.
Remark 4.8 Uis a matrix that holds important information about rows of a given
.
data matrix; VT is a matrix that holds important information about columns of the
.
given data matrix. (cid:2)
.
4.3.5.1 One More Property of SVD
IfM n×d = U n×n S n×d VT d×d. , thens i = |M| v i || . , where s i. is the ith value along the
main diagonal of S, and v is the ith column of V.
. i. .


================================================================================
PAGE 129
================================================================================

4.3 SingularValueDecomposition 117
For example, if we calculate Mv , where v as shown in the mathematical
1. 1.
expression (4.15,w eh av e
⎡ ⎤
⎡ ⎤ 0.62 ⎡ ⎤
55400 ⎢ ⎥ 8.12
. M v 1 = ⎢ ⎢ ⎣ 4 0 4 0 3 0 0 5 0 5 ⎥ ⎥ ⎦ ⎢ ⎢ ⎢ ⎢ 0 0 . . 6 4 2 8 ⎥ ⎥ ⎥ ⎥ = ⎢ ⎢ ⎣ 6 0 .4 ⎥ ⎥ ⎦.
⎣ 0 ⎦
00033 0
0
√
ThenormofMv is 8.122+6.42 ≈10.3,whichisapproximatelyequaltothefirst
1. .
element of S in the mathematical expression (4.15). The approximation is caused
.
by the fact that we have kept only two decimal places in (4.15). As mentioned
previously, s is the strength of the first author in the data matrix.Mv shows each
1. 1.
child’s overall rating to the first author. It is an average of all five books weighted
by the first row in VT. Therefore,||Mv ||may be considered as a score over all four
. 1 .
children, weighted by the first row of VT, which connects books to Louis Sachar.
.
4.3.6 An Example of Image Compression Using SVD
Let us do image compression using SVD on a cat image1 as shown in Fig.4.5. The
size of the imagen×dis668×640. That is, the image hasn=668row andd =640
. . . .
column pixels. An image can be treated as a matrix of pixels with corresponding
colour values and can be decomposed using SVD with a smaller number of singular
values that retain only the essential information that comprises the image which
results in a smaller image file size.
Figure 4.6 shows the sorted singular values after performing a full SVD on the
cat image. As can be seen, singular values decrease dramatically from the first one
to the 50th and converge to about 0 after the 100th singular value.
We can compress the image by using a smaller number (k) of singular values on
the right-hand side of Eq.(4.14) in this chapter to reconstruct the image. Figure 4.7
shows three compressed images with the number of singular values equal to 50, 20,
and 5, respectively.
The quality of a compressed image can be measured using the following equation
(assuming the size of the original image isn×d.):
.
s2+···+s2
1 k ×100%, (4.16)
. s2+···+s2
1 d
that is, the sum of the squares of the retained singular values divides by the sum
of the squares of all of the singular values. In this example, the image quality is
1 This image was sourced from Pexels: https://www.pexels.com/search/cats/.


================================================================================
PAGE 130
================================================================================

118 4 MatrixDecomposition
Fig. 4.5 The
black-and-white version of
the original colour image of a
cat
Fig. 4.6 A plot of singular
values sorted in descending
order
99.37%,98.73%, and95.57%fork =50, 20, and5, respectively, calculated from
. . . .
Python programming. The compressed image withk =5is not good, as can be seen
.
in Fig.4.7, although a percentage value of95.57seems a lot.
.
The compression ratio of an image can be calculated using the following
equation:
n×d
. (4.17)
. k×(n+1+d)


================================================================================
PAGE 131
================================================================================

4.3 SingularValueDecomposition 119
Fig. 4.7 The original image of a cat and its reconstructions using different numbers of singular
values
The top line is the size of the original matrix =n×d. The bottom line is the sum of
.
three small matrices after SVD, namely,n×k+k×k+k×d. However, since the
.
singular values are kept in the main diagonal of S, one can just save the top k values
.
along the main diagonal rather than thewholek×k matrix. Hence the bottom line
.
isn×k+k×1+k×d =k×(n+1+d).
.
Remark 4.9 When performing an image compression task, one needs to consider
both the image quality and the compression ratio. That is to have a compression ratio
as large as possible while keeping the compressed image as good as the original
one. (cid:2)
.
Example 4.4 Compute the compression ratio of the cat image for k = 50
.
(the second image in the first row of Fig.4.7).
Solution Compression ratio= 668×640 ≈6.5 3.
50×(668+1+640) .


================================================================================
PAGE 132
================================================================================

120 4 MatrixDecomposition
Exercise
4.6 Compute the compression ratio in Example 4.4 withk =20.
.
4.4 The Relationship Between PCA and SVD
IfM=USVT, then columns of Vare principal directions (or axes). Singular values
. .
are related to the eigenvalues of the covariance matrix via the following equation,
where n is the number of data points:
λ =s2/(n−1), (4.18)
. i i
Now let us compare results obtained in Sects.4.2.4 and 4.3.4 of this chapter.
As can be seen, the two columns of Vin the SVD calculation are equal to the two
.
eigenvectors inthePCAcalculation. Ifwesubstitutes =4ands =2toEq.(4.18),
1 . 2 .
we have 42 =4and 22 =1, respectively, which are eigenvalues of PCA in Case
5−1 . 5−1 .
Study 2 (Sect.4.2.4).
Exercise
4.7 Exercises 4.4 and 4.5 work on the same data matrix using PCA and
SVD, respectively. Apply Eq.(4.18) to results obtained from Exercise 4.5,
and compare these eigenvalues with what you have obtained in Exercise 4.4.


================================================================================
PAGE 133
================================================================================

Chapter 5
Calculus
This chapter introduces calculus. Calculus deals with the way in which quantities
grow or change in relationship with each other. This chapter includes finding the
derivative of a function, finding an integral, and some applications of derivatives,
such as finding the local minimum and maximum of a function. Many readers will
have covered this material before; this chapter will, therefore, represent a reminder
for such readers. Doing the many exercises will help with that revision. For others,
the many examples and exercises will aid in the learning process.
5.1 Limits of Functions
The principles behind both differentiation and integration in calculus are based on
the concept of limits. So, before introducing the derivative of a function, we need to
have an understanding of limits.
The limiting value of something is the value you get as you approach it ever and
ever closer. To find the limiting value of a function of x at apointx , if such exists,
0.
you need to look at the value of the function as you approach ever closer to the point.
If the limiting value of the function A exists, then we need to show that the function
getsc losert oA asx approachesx . This is formalised in the following definition.
0.
Definition 5.1 (Limits) Letf(x)be a function defined at all values of x with the
.
possible exception of x = x . If for any positive number (cid:2) (however small), there
0. .
exists a positive number δ so that whenever 0 < |x −x | < δ, the function f(x)
. 0 . .
satisfies|f(x)−A| <(cid:2).W es ay A isthelimitoff(x)as x approaches x (x →x )
. . 0. 0.
and denote it as
lim f(x)= A.
.
x→x0
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 121
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_5


================================================================================
PAGE 134
================================================================================

122 5 Calculus
Fig. 5.1 An example of a
function,f(x)= x
x
2
−
−
2
4 ., that
is undefinedatx=2.
(indicated by the hollow
circle) but well-behaved
nearby
Table 5.1 Asx→1.,w e
x 0 0.9 0.99 0.999 0.9999
havef(x)=2x→2..
f(x) 0 1.8 1.98 1.998 1.9998
Why do we need the concept of limits? Some functions are not defined at a point
butarewell-behavednearby.Forexample,seeFig.5.1,wherefunctionf(x)= x2−4
x−2.
is undefined atx = 2, since this value makes the bottom line zero, and sof(x)is
. .
undefined. However, as can be seen, as x → 2, we have f(x)= 4. This means
. .
when x is near 2 but not equal to it, the valuesoff(x)are near 4. That is,
.
lim f(x)=4.
.
x→2
How near can it be? The answer is that it can be as near as we want it to be. For
example, iff(x)=2x, then asx →1,w eh avef(x)→2, as shown in Table 5.1.
. . .
5.1.1 Left-and Right-Hand Limits
Let us consider f(x)= |x−3| . The function is undefined at x = 3 (see Fig.5.2).
x−3 . .
Suppose we imagine that x is moving. Then, it can approach 3 either from the right
or from the left. We indicate these by writingx →3 + andx →3 − , respectively. In
. .
this example, asx → 3 − ,w eh avef(x) = −1. On the other hand, asx → 3 + , we
. . .
havef(x)=1. We can write these as
.
lim f(x) =−1, and lim f(x)=1.
.
x→3− x→3+
We say that
lim f(x)=A , if f(x)→A asx →x − ,
. x→x − 1 1 0
0


================================================================================
PAGE 135
================================================================================

5.1 LimitsofFunctions 123
Fig. 5.2 An illustration of the limits of the functionf(x)= | x x − − 3 3| . asx → 3., approaching from
the right (dotted circle) and from the left (solid circle)
and
lim f(x)=A , if f(x)→A asx →x + .
. x→x + 2 2 0
0
IfA =A =A, then
1 2 .
lim f(x)= lim f(x)= A,
.
x→x − x→x +
0 0
that is, it does not matter which side x approaches x from, then the limit of the
0.
function exists and we say that
lim f(x)= A.
.
x→x0
This can be seen in the example above,f(x)= x2−4, where the valuef(x)=4
x−2. .
is obtained if you approach 2 from either side.
5.1.2 Theorems on Limits
Supposeg(x)andh(x)are two functions. If
. .
lim g(x)=Aand lim h(x)= B,
.
x→x0 x→x0


================================================================================
PAGE 136
================================================================================

124 5 Calculus
then
•
lim (g(x)±h(x))=A±B = lim g(x)± lim h(x).
.
x→x0 x→x0 x→x0
•
lim (g(x)h(x))=AB = lim g(x) lim h(x).
.
x→x0 x→x0 x→x0
•
lim
g(x)
=
A
=
lim x→x0 g(x)
, ifB (cid:3)=0.
.
x→x0 h(x) B lim x→x0 h(x)
Sometimes it happens that as x → x , the limit of either g(x) or h(x), or both
0. . .
does not exist. Finding the limits of such functions is beyond the scope of this book.
Readers may want to view details from [6].
Example 5.1 Find the limit for the following function asx →1:
.
3x−1.
.
Solution
lim(3x−1)= lim 3x− lim 1=3 lim x− lim 1=3×1−1=2.
.
x→1 x→1 x→1 x→1 x→1
Example 5.2 Find the limit for the following function asx →2:
.
(x3)(x2).
.
Solution
lim((x3)(x2))= lim x3 lim x2=(lim x)3(lim x)2=23×22=8×4=32.
.
x→2 x→2 x→2 x→2 x→2
(continued)


================================================================================
PAGE 137
================================================================================

5.1 LimitsofFunctions 125
Example 5.2 (continued)
Note that
(x3)(x2)=x5,
.
and
lim(x5)=(lim x)5 =25 =32,
.
x→2 x→2
which gives us confidence that the rule is correct.
Example 5.3 Find the limit for the following function asx →1:
.
x3−1
.
. x2−4x+2
Solution
lim
x3−1
=
lim x→1 (x3−1)
. x→1x2−4x+2 lim x→1 (x2−4x+2)
=
lim x→1 x3−lim x→1 1
lim x→1 x2−4lim x→1 x+lim x→1 2
=
(lim x→1 x)3−1
(lim x→1 x)2−4×lim x→1 x+2
13−1
=
12−4×1+2
=0.
It can be seen that one can substitute the value of x into a rational function to
0.
find the limit of the function. However, if a rational function’s denominator equals
zero or approaches ∞ after substituting, it needs to be treated differently. Let us
.
consider the following two examples.


================================================================================
PAGE 138
================================================================================

126 5 Calculus
Example 5.4 Find the limit for the following function asx →3:
.
x−3
.
. x2−9
Solution When x → 3, both limits of the numerator and the denominator
.
are zeros, so we cannot take the limit of the numerator and denominator
separately. Instead, we can cancel the common factorx−3from the numerator
.
and the denominator. Therefore,
lim
x−3
= lim
x−3
= lim
1
=
lim x→3 1
=
1
.
. x→3x2−9 x→3(x−3)(x+3) x→3x+3 lim x→3 (x+3) 6
Example 5.5 Find the limit for the following function asx →∞:
.
2x3+3x2+1
.
. 6x3+4x2−2
Solution Whenx → ∞, both the numerator and the denominator approach
.
∞. Therefore, we cannot apply theorems of limits directly. Instead, let us
.
divide the numerator and the denominator by x3 first, then we can find the
.
limit:
2x3+3x2+1 2+ 3 + 1 1
lim = lim x x3 = .
. x→∞6x3+4x2−2 x→∞6+ 4 − 2 3
x x3
Exercise
5.1 Find the following limits.
(1)
lim(x2−3),
.
x→2
(2)
(cid:2) (cid:3)(cid:2) (cid:3)
1 1
lim 1+ 3− ,
. x→∞ x x3
(continued)


================================================================================
PAGE 139
================================================================================

5.2 Derivatives 127
(3)
x−2
lim √ ,
. x→2 x+2
(4)
8x3−1
lim ,
.
x→1
6x2−5x+1
2
(5)
4x3−2x2+x
lim .
. x→0 3x2+2x
5.1.3 Continuity
Let f(x) be defined for all values of x, near x = x as well as x = x . The
. 0. 0.
functionf(x) . is called continuous atx = x 0. , if lim x→x0 f(x)= f(x 0 ) . . Consider
the following:
(cid:4)
x3, x(cid:3)=1
f(x)= and lim f(x)=1.
.
0 , x=1 x→1
f(x) . is not continuous atx =1 . sincef(1)=0 . and lim x→1 f(x)=1(cid:3)= f(1) . .
5.2 Derivatives
We can apply the concept of limit in many applications.
For example, the position of an object with uniform motion (constant velocity)
along the x-axis at time t is x = f(t). Its velocity is the ratio of the distance it
.
travels to the time it takes, which is the same at all points. However, if the motion is
not uniform, its velocity is different at different time intervals. For any time interval,


================================================================================
PAGE 140
================================================================================

128 5 Calculus
Fig. 5.3 An illustration of the slope of a secant line, represented by the dashed line
denoting the starting time point as t , when the object is at point x , the velocity can
0. 0.
be found by calculating the following:
x−x f(t)− f(t )
0 = 0 , (5.1)
. t −t t −t
0 0
which is the average velocity within the time interval. If t → t and the limit of
0.
Eq.(5.1) exists, then
f(t)− f(t )
0
lim
. t→t0 t −t
0
is the velocity of the object at the instant in timet =t .
0.
Let us consider another example. The slope of the line joining two points on a
curve, called a secant line, as shown in Fig.5.3, can be calculated as follows:
changein f(x) f(x )− f(x)
slope= = 2 1 . (5.2)
. changeinx x −x
2 1
Ifx →x and the limit of Eq.(5.2) exists, then
1 2.
f(x )− f(x)
2 1
lim
. x1 →x2 x 2 −x 1
is the slope of the line just touching the curve, called the tangent, at the point
(x ,f(x )).
2 2 .


================================================================================
PAGE 141
================================================================================

5.2 Derivatives 129
In the above two examples, we tried to calculate the limit of the rate of change
of a function: the change of the dependent variable related to the change of the
independent variable, which can be written in general as follows:
f(x +(cid:4)x)− f(x)
0 0
lim , (5.3)
.
(cid:4)x→0 (cid:4)x
where (cid:4)xandf(x +(cid:4)x)− f(x)are the increase of the independent variable and
. 0 0 .
the dependent variable of the function y = f(x), respectively. This limit is called
.
the derivative of the functionf(x)at x .
. 0.
Definition 5.2 (Derivative and Differential) The ratio defined in Eq.(5.3) i s
calledthederivativeofthefunctiony = f(x)atitsdomainvalue x .Ifthederivative
. 0.
can be formed at each point of a subdomain of f , then f is said to be differentiable
(cid:6)
at a general point x on that subdomain, and a newfunctionf has been constructed,
.
denoted asf (cid:6) (x), dy,o r df(x). That is,
. dx. dx .
dy df(x) f(x+(cid:4)x)− f(x)
f (cid:6) (x)= = = lim .
.
dx dx (cid:4)x→0 (cid:4)x
It is also convenient to define the differential, particularly in applications where
a linear approximation to a function is required. The differential, dy, or principal
part of the change in a function with respect to changes in the independent variable,
is defined as:
dy =f (cid:6) (x)dx,
.
which is the differential of y orf(x).
.
Remark 5.1 Note that in generaldy (cid:3)=(cid:4)y, where(cid:4)y = f(x+(cid:4)x)− f(x). dy
. . dx.
is not actually a fraction at all. It is the limit of the fraction (cid:4)y as(cid:4)x →0. (cid:2)
(cid:4)x. . .
Example 5.6 Let f(x)= 6x +5, and use the derivative definition to find
.
(cid:6)
f (x)at any point x.
.
Solution
f(x+(cid:4)x)=6(x+(cid:4)x)+5=6x+6(cid:4)x+5
.
f(x+(cid:4)x)− f(x)=6(cid:4)x
.
6(cid:4)x
lim =6.
.
(cid:4)x→0 (cid:4)x
(continued)


================================================================================
PAGE 142
================================================================================

130 5 Calculus
Example 5.6 (continued)
Since we know from Sect.2.3.2.1 of Chap.2 that y = f(x)= 6x +5 is a
.
straight line and that its gradient is 6 everywhere, then the result that f (cid:6) (x)=
6, as found here, is correct.
.
Example 5.7 Letf(x) =x||. Computef (cid:6) (x)atx = 0using the definition
. . .
of derivative.
Solution
f(0+(cid:4)x)− f(0) |(cid:4)x|−0 |(cid:4)x|
lim = lim = lim .
.
(cid:4)x→0 (cid:4)x (cid:4)x→0 (cid:4)x (cid:4)x→0 (cid:4)x
f(0+(cid:4)x)− f(0)
If(cid:4)x <0, lim =−1;
.
(cid:4)x→0− (cid:4)x
f(0+(cid:4)x)− f(0)
if(cid:4)x >0, lim =1.
.
(cid:4)x→0+ (cid:4)x
f(0+(cid:4)x)− f(0) f(0+(cid:4)x)− f(0)
Since lim (cid:3)= lim ,
.
(cid:4)x→0− (cid:4)x (cid:4)x→0+ (cid:4)x
the derivative off(x) =x|| atx = 0does not exist. There is no one unique
. .
tangent forf(x) =|x| atx =0. For example, some are shown as dotted lines
. .
in Fig.5.4.
Fig. 5.4 An illustration of
f(x) =|x| .. Dotted lines are
three examples of possible
tangents passing through
x=0.


================================================================================
PAGE 143
================================================================================

5.2 Derivatives 131
Exercise
5.2 From the definition of the derivative, find whetherf (cid:6) (x)exists atx =0.
. .
If it exists, calculatef (cid:6) (x =0). Also, calculatef (cid:6) (x)at a general point x.
. .
(1) f( x)= x 2.
.
(2) f( x)= x 2+x.
.
5.2.1 Derivatives of Some Elementary Functions
This is not a Mathematics textbook, so we just list the derivatives of some of
the most common functions. In the following, we assume each function is a
differentiable function of x or θ, where θ is measured in radians. c and a denote
. .
constants.
d
c=0 (5.4)
.
dx
d
axc =caxc−1 (5.5)
.
dx
d
sinθ =cosθ (5.6)
.
dθ
d
cosθ =−sinθ (5.7)
.
dθ
d
eax = aeax (5.8)
.
dx
d 1
lnx = (5.9)
.
dx x
Readers may read [6] to find derivatives for more trigonometrical and hyperbolic
functions.
5.2.2 Rules for Differentiation
Again, we just list these rules. Iff(x),g(x), andh(x)are differentiable functions,
. . .
then the following differentiation rules are valid.


================================================================================
PAGE 144
================================================================================

132 5 Calculus
• Addition Rule
d d d
(g(x)+h(x))= g(x)+ h(x).
.
dx dx dx
•
d d d
(g(x)−h(x))= g(x)− h(x).
.
dx dx dx
•
d d
Cg(x)=C g(x),
.
dx dx
where C is any constant.
• Product Rule
d d d
(g(x)h(x))= g(x) h(x)+h(x) g(x).
.
dx dx dx
• Quotient Rule
d g(x) h(x) d g(x)−g(x) d h(x)
= dx dx .
. dx h(x) (h(x))2
• Ify = g(x), andx =g −1(y), then dy and dx are related by
. . dx. dy.
dy 1
= .
. dx dx
dy
That is, the derivative of an inverse function is the reciprocal of the derivative of
the function.
• Chain Rule
In calculus, the chain rule is a formula for computing the derivative of the
composition of two or more functions. For two functions: If y = f(u)and
.
u= g(x), then
.
dy dy du
= .
.
dx dudx
This generalises to three or more functions, for instance: If y = f(z),z=
g(w)andw =h(x), then
. .
dy dy dz dw
= .
.
dx dzdw dx


================================================================================
PAGE 145
================================================================================

5.2 Derivatives 133
Example 5.8 Letf(x)=x2.F indf (cid:6) (x).
. .
Solution Applying Eq.(5.5), we have:
f (cid:6) (x)=2x2−1 =2x.
.
Example 5.9 Leth(x)=3x2+5x. Find d h(x).
. dx .
Solution Applying the addition rule and Eq.(5.5), we have:
d d d
h(x)= (3x2)+ 5x =6x+5.
.
dx dx dx
Example 5.10 Leth(x)= x(3x+5).F indh (cid:6) (x).
. .
Solution Consider f(x)= x and g(x) = 3x +5 and applying the product
. .
rule, Eq.(5.5) and the addition rule, we have:
d d
h (cid:6) (x)= f(x) g(x)+g(x) f(x)
.
dx dx
=x×3+(3x+5)×1
=6x+5.
Sinceh(x)= x(3x+5)=3x2+5x, this is the same as Example 5.9 and
.
has the same solution. This gives us confidence that the product rule is correct.
Example 5.11 Leth(x)=excosx.F ind d h(x).
. dx .
Solution Consider f(x)= ex andg(x) = cosx, then applying the product
. .
rule and Eqs.(5.7), with independent variable x rather than θ, and (5.8), we
.
have:
d d d
h(x)= f(x) g(x)+g(x) f(x)
.
dx dx dx
(continued)


================================================================================
PAGE 146
================================================================================

134 5 Calculus
Example 5.11 (continued)
= ex × (−sinx)+ cos x × ex
= e x(cosx−sinx).
Example 5.12 Leth(x)=tanx = sinx.F ind d h(x).
cosx. dx .
Solution Consider f(x) = sinx and g(x) = cosx, then applying the
. .
quotient rule and Eqs.(5.6) and (5.7), we have:
d g(x) d f(x)− f(x) d g(x)
h(x)= dx dx
. dx (g(x))2
cosx×cosx−sinx×(−sinx)
=
cos2x
cos2x+sin2x
=
cos2x
1
=
cos2x
=sec2x.
Example 5.13 Let σ(x) be a sigmoid function, defined as: σ(x) = 1 .
. 1+e−x.
Find d σ(x).
dx .
Solution Applying the quotient rule, we have:
d (1+e −x)×0−1×(−e −x)
σ(x)=
. dx (1+e−x)2
e
−x
=
(1+e−x)2
1 (1+e −x)−1
= ×
1+e−x 1+e−x
= σ(x)(1−σ(x)).
(continued)


================================================================================
PAGE 147
================================================================================

5.2 Derivatives 135
Example 5.13 (continued)
Note that in line one of the above, we have applied (1) the derivative of a
constant is zero; and (2) Eq.(5 .8). Figure 5.5 shows a sigmoid function and its
derivative in the domain of[−10,10]. The function itself is bounded between
.
0 and 1, and its derivative is symmetrical about the vertical axis of x = 0.
.
Values of the derivative are convergent to 0 as x approacheseither∞or −∞.
. .
Example 5.14 Lety =lnx andx =ey.F ind dy and dx.
. . dx. dy.
Solution Using Eqs.(5.9) and (5.8), we have: dy = 1 and dx = ey =
dx x. dy
x. Since the functions y = lnx and x = ey are inverse functions (see
. . .
Example 2.23 in Sect.2.3.4 of Chap.2), we have shown that
dy 1
= .
. dx dx
dy
Example 5.15 Lety =sin −1x, andy ∈[−π,π].F ind dy.
. 2 2 . dx.
Solution We can apply the rule of calculating the derivative for an inverse
function in this case. Sincex =siny and dx = d siny =cosy,w eh ave :
. dy dy .
dy 1 1 1
= = (cid:5) = √ .
.
dx cosy 1−(siny)2 1−x2
Fig. 5.5 An illustration of a
sigmoid function (solid line)
and its derivative (dash-dotted
line)


================================================================================
PAGE 148
================================================================================

136 5 Calculus
Example 5.16 Letf(x)=ln(cosx).F ind dy.
. dx .
Solution Let y = lnz and z = cosx. Applying the chain rule, we have
. .
dy = 1, dz =−sinx , and
dz z. dx .
dy dy dz 1 sinx
= = ×(−sinx) =− =−tanx.
.
dx dzdx z cosx
Example 5.17 Find the derivative off(x)=max(0,x).
.
Solution If x ≤ 0, the function value is 0. The derivative of any constant
.
value is 0. Ifx >0 , the function value is the value of x. The derivative of x
.
is 1.Figure5.6 shows the function (the left panel) and its derivative (the right
panel) in the domain of[−5,5]. This function is widely used as an activation
.
function in neural networks, and its name is the Rectified Linear Unit, or
ReLU for short. Technically, the derivative is undefined when the input is 0. In
practice, if we assume that the derivative is zero here, there are no problems.
Many real-world applications using neural networks have empirically proved
that models with ReLU as the activation function are easier to train and can
have a better performance.
Fig. 5.6 An illustration of a ReLU function (on the left) and its derivative (on the right)


================================================================================
PAGE 149
================================================================================

5.2 Derivatives 137
Exercises
5.3 Find d f(x)of the following functions.
dx .
(1) f (x)= x+6.
.
(2) f (x)= x 6.
.
(3) f (x)= 10 e x.
.
(4) f (x)= 5ln(x).
.
(5) f (x)= ln(x)sinx.
.
(6) f (x)= ex .
cosx .
(7) f (x)= e 10x+1.
.
(8) f (x)= 4(e x )2+5.
.
(9) f (x)= e 3xsin5x.
.
(10) f (x)= ln(8x) .
e (x2) .
5.2.3 The Second Derivative
In general, the derivative y (cid:6) or f (cid:6) (x) of a function y = f(x)in an interval is still
. . .
a function of x. For example, see the derivative in Figs.5.5 and 5.6, respectively. If
f (cid:6) (x)is also differentiable in the interval, we call the derivative ofy (cid:6) = f (cid:6) (x)the
. .
second-order derivative ofy = f(x), denoted as y (cid:6)(cid:6) ,f (cid:6)(cid:6) (x), or d2y.That is,
. . . dx2 .
d2y d dy
y (cid:6)(cid:6) =(y (cid:6) ) (cid:6) or = .
. dx2 dxdx
Similarly, the nth derivative off(x)if it exists, is denoted asy(n),f(n)(x), or dny.
. . . dxn.
This book considers the first and the second derivative of a function only.
Example 5.18 Find the second derivative of the following function:
y =5x+8.
.
Solution
y (cid:6) =5 , y (cid:6)(cid:6) =(y (cid:6) ) (cid:6) =0.
.


================================================================================
PAGE 150
================================================================================

138 5 Calculus
Example 5.19 Find the second derivative of the following function:
y =sin(ωx).
.
Solution
y (cid:6) =ωcos(ωx), y (cid:6)(cid:6) =(ωcos(ωx)) (cid:6) =−ω2sin(ωx) =−ω2y.
.
Exercise
5.4 Find the second derivative of the following functions.
(1) y = x3lnx.
.
(2) y = ae − αx, express the answer in terms of y.
.
(3) y = ae −αx + beαx, express the answer in terms of y.
.
5.3 Finding Local Maxima and Minima Using Derivatives
There is a close relationship between the function monotony and the sign of its
derivative. Suppose functiony = f(x)is continuous in the interval of[a,b]and is
. .
differentiable in(a,b). Recall that the derivative of a function at a specific point can
.
be considered as the slope of the tangent line of the function passing through that
point. Iff (cid:6) (x)>0for allx ∈(a,b), theny = f(x)monotonically increases in the
. . .
interval of[a,b](Fig.5.7a). On the other hand, iff (cid:6) (x)<0for allx ∈(a,b), then
. . .
y = f(x)monotonically decreases in the interval of[a,b](Fig.5.7b).
. .
We can use the relationship between the function monotony and the sign of its
derivative to find extreme points, the local maxima and minima, of a function. Look-
ing at Fig.5.8, we see function f(x) has four local minima, f(x ),f(x ),f(x ),
. 1 4 6 .
andf(x ), respectively, and three local maxima,f(x ),f(x ), andf(x ), respec-
8 . 2 5 . 7 .
tively, in the interval of [a,b]. Among them, the local maximum value f(x ) is
. 2 .
smaller than the local minimum valuef(x ). In fact,f(x )is the overall minimum,
6 . 1 .
andf(x )is the overall maximum of the function in the interval of[a,b]. We can
7 . .
also see that all tangents at either local minima or local maxima are horizontal.
However, when a tangent is horizontal, the corresponding function value is not
necessarily a local minimum or a local maximum, such as the point (x ,f(x )).
3 3 .
(x ,f(x ))is called a point of inflection. Any point(x,f(x))at whichf (cid:6) (x) = 0
3 3 . . .
is called a critical point.


================================================================================
PAGE 151
================================================================================

5.3 FindingLocalMaximaandMinimaUsingDerivatives 139
Fig. 5.7 An illustration of the relationship between the sign of the derivative and the monotonicity
of a function, where the solid line represents the function and the dashed line represents the tangent
line
Fig. 5.8 An illustration of local maxima and minima of a function
Now let us have a look at Fig.5.9. The top panel shows a diagram of a function,
the middle shows the first derivative of the function, and the bottom shows the
second derivative. We can see that the local minimum point of the function has a
zero first derivative and a positive second derivative; the local maximum point of
the function has a zero first derivative and a negative second derivative.
The general idea to find local maxima and minima of a functiony = f(x)using
.
derivatives is:
• Step 1: to find critical values x using theconditionf (cid:6) (x)=0.
.
• Step 2: to determine the exact nature of the function at a critical point(x,f(x)),
.
(cid:6)(cid:6)
f (x)needs to be calculated.
.


================================================================================
PAGE 152
================================================================================

140 5 Calculus
Fig. 5.9 An illustration of
the relationship between local
maxima or minima and their
corresponding first and
second derivatives
(cid:6)(cid:6)
– Iff (x)>0, the critical point is a local minimum point.
.
(cid:6)(cid:6)
– Iff (x)<0, the critical point is a local maximum point.
.
– Iff (cid:6)(cid:6) (x)=0, it needs further investigation.
.
Example 5.20 Find the local minima and maxima of function f(x)=(x2−
1)3−1.
.
Solution
• Find critical values using the conditionf (cid:6) (x)=0.
.
(cid:6)
– First, find f (x) by applying the chain rule (see Sect.5.2.2 of this
.
chapter). Setu=(x2−1), then we have:
.
df(x) df(u)du
= =3u22x =6x(x2−1)2.
.
dx du dx
(continued)


================================================================================
PAGE 153
================================================================================

5.3 FindingLocalMaximaandMinimaUsingDerivatives 141
Example 5.20 (continued)
– Set6x(x2−1)2 =0,w eh ave:x =−1,x =0, andx =1.
. 1 . 2 . 3 .
Substitute these values to the function, we obtain three critical points:
(−1,−1),(0,−2), and(1,−1).
. . .
• Calculatef (cid:6)(cid:6) (x), that is, calculate the derivative of(6x(x2−1)2). Consider
. .
g(x) = 6x and h(x) = (x2 − 1)2, and apply the product rule to
. .
d (g(x)h(x))and the chain rule toh(x), then we have:
dx . .
f (cid:6)(cid:6) (x)=6(x2−1)2+6x·2(x2−1)·2x
.
=30x4−36x2+6
=6(5x2−1)(x2−1).
• Substitutex =−1,x =0, andx =1intof (cid:6)(cid:6) (x)=6(5x2−1)(x2−1),
1 . 2 . 3 . .
separately.
– Sincef (cid:6)(cid:6) (0) = 6 > 0,f(x)has the minimum value atx = 0, which is
. . .
f(0) =−2.
.
– Since f (cid:6)(cid:6) (−1) = 0 and f (cid:6)(cid:6) (1) = 0, each critical point needs further
. .
investigation. When taking a close value from the left side of −1,f or
.
example, −1.01, w e h ave f (cid:6) (−1.01 ) <0; taking a close value from
. .
the right side of −1, for example, −0.9, we havef (cid:6) (−0.9)<0. Since
. . .
(cid:6)
there is no sign change tof (x), then we conclude there is no maximum
.
or minimum atx =−1. Similarly, there is no maximum or minimum at
.
x =1. Both points are, in fact, points of inflection with zero gradients.
.
Remark 5.2 A point of inflection is a point where the gradient line at a point is
above the curve on one side and below the curve on the other side of the point. Or
where the curve changes from being concave downward to being concave upward
or vice versa. At these points f (cid:6)(cid:6) (x) = 0. If f (cid:6) (x) = 0 as well, then we have
. .
a point of inflection with a zero gradient like point (x ,f(x )) on Fig.5.8. Other
3 3 .
points of inflection are like the point wheref (cid:6)(cid:6) (x) = 0butf (cid:6) (x) (cid:3)= 0on Fig.5.9.
. .
This is where the middle vertical dash-dotted line goes down from the middle of the
upward-sloping part of the graph in the top part of the figure to the bottom part of
the figure, where it shows that f (cid:6)(cid:6) (x) = 0 (in fact the gradient itself, f (cid:6) (x) has a
. .
maximum at that point as seen in the middle part of the Fig.5.9). (cid:2)
.


================================================================================
PAGE 154
================================================================================

142 5 Calculus
Exercise
5.5 Find any local maximums and minimums of the following functions.
(1) f( x)= x 3−3x2− 24x+3.
.
(2) f( x)= 4x 3− 3x2+1.
.
(3) f( x)= 24x − 2x 3.
.
(4) f( x)= 4x 2− 1.
x .
(5) f( x)= x 4−4x3− 2x2+12x+4.
.
(6) forx ∈[0,2π ]:f(x)=ex(cosx+sinx).
. .
(7) f( x)= xe −x.
.
(8) f( x)= x 2e −x.
.
5.4 Integrals
Earlier, we introduced how to find the derivative of a differentiable function. In this
section, we will discuss the inverse operation of finding derivatives. That is, given
a functionf(x), and we will see how we can find a differentiable functionF(x)so
. .
that the derivative ofF(x)equalsf(x). First, we consider the area under a curve by
. .
summing up (integrating) small areas. This again will draw on the concept of limits.
Consider the area of the region A shown in Fi g.5.10. That is the area underf(x)
.
bounded in the interval of[a,b]. Suppose we divide the interval into n sub-intervals
.
by inserting arbitrarily n − 1 . points x 1 , x 2 , ..., x n−1. , and the length of each
subinterval is:
. (cid:4) x 1 =a−x 1 ,(cid:4)x 2 =x 2 −x 1 ,...,(cid:4)x n =b−x n−1 .
Fig. 5.10 An illustration of
the definite integral off(x).
over the interval[a,b] .,
represented as the shaded
area under the curve


================================================================================
PAGE 155
================================================================================

5.4 Integrals 143
Randomly choose a point in each subinterval ξ i (x i−1 ≤ ξ i ≤ x i ) . , where i =
1,...,n,x = a and x = b, and take the product of the function value at ξ and
0 . n . i.
the length of its corresponding subinterval (cid:4)x . This product represents the area
i.
of a rectangle of height f(ξ ) and width (cid:4)x and will be an approximation to the
i . i.
area under the curve in that subinterval(cid:4)x . Then, the sum of these products can be
i.
written as:
(cid:6)n
S = f(ξ )(cid:4)x . (5.10)
. i i
i=1
Definition 5.3 (Definite Integrals) Let the number of subintervals n increase so
that the lengths (cid:4)x → 0. Denote λ = max{(cid:4)x ,(cid:4)x , ··· ,(cid:4)x}. If the sum of
i . 1 2 n .
Eq.(5.10) approaches a limit that does not depend on how we divide the interval,
then we denote this limit by the following:
(cid:7)
b (cid:6)n
f(x)dx= lim f(ξ )(cid:4)x . (5.11)
. i i
a
λ→0
i=1
This is called the definite integral off(x)between a and b. Finding the summation
.
is known as integration. f(x) is called the integrand; a and b are the limits of
.
integration or the endpoints of integration; dx tells us x is the variable of integration.
Geometrically, the integral that is the limit of the sum (Eq.5.10) represents the
total area of all rectangles (defined by subintervals) under a bounded function. For
(cid:8)
example, 4 xdx = 8. This can be viewed in Fig.5.11. The line represents the
0 .
function off(x)= x. The area in the interval of[0,4]under the line off(x)= x
. . .
is a triangle, whose area is computed as 1 ×4×4=8.
2 .
5.4.1 First Fundamental Theorem of Calculus
For f continuouson[a,b], define a function F by the follow ing:
.
(cid:7)
x
F(x)= f(t)dt forx in[a,b], (5.12)
.
a
then F is differentiable on(a,b)andF (cid:6) (x)= f(x). That is, differentiating F gives
. .
us back the originalfunctionf(x). We callF(x)an antiderivative off(x).
. . .


================================================================================
PAGE 156
================================================================================

144 5 Calculus
Fig. 5.11 An illustration of the definite integral under a line off(x)=x.in the interval of [0, 4]
Example 5.21 Suppose f(x)= x5. It means F (cid:6) (x) = d F(x)= f(x)=
. dx
x5. Can you think of some functions whose derivatives are x5?
. .
Consider d c = 0 and d axc = caxc−1 (Eqs.5.4 and 5.5). Some possible
dx . dx .
functions can beF(x)= x6 ,F(x)= x6 +100, andF(x)= x6 +34.5. In
6 . 6 . 6 .
fact, any function of x of theformF(x)= x6 +C forsomeconstantC is an
6 .
antiderivative off(x)=x5.
.
Remark 5.3 Not all functions f(x) have antiderivatives F(x) in terms of ele-
. .
mentary functions (polynomials, exponentials, logarithms, trigonometric functions,
etc.), but the definite integral, understood as the limit of a summation, can still
exist. (cid:2)
.
5.4.2 Indefinite Integrals
(cid:8)
The family of all antiderivatives off(x), denoted as f(x)dx, are called indefinite
. .
integrals.
(cid:8)
b
Remark 5.4 A defi(cid:8)nite integral, written as
a
f(x)
.
, is a number. An indefinite
integral, written as f(x)dx, is a family of functions. It is a family of functions,
.
because the constant C can take any value. (cid:2)
.


================================================================================
PAGE 157
================================================================================

5.4 Integrals 145
5.4.3 Second Fundamental Theorem of Calculus
If f is continuous on[a,b] and F is any of the family of antiderivative of f with
.
respect to x, then
(cid:7)
b
f(x)dx= F(b)− F(a). (5.13)
.
x=a
Remark 5.5 Care should be taken when finding an area using the equation
(cid:7)
b
f(x)dx= F(b)− F(a). (5.14)
.
x=a
When f(x) is below the axis for all of the interval [a,b], then F(b)− F(a) is
. . .
negative, so if the function f(x) is both above and below the axis in the interval
.
[a,b], then F(b)− F(a) is the difference between the upper area and the lower
. .
area. This could be zero if the areas are identical. (cid:2)
.
5.4.4 Integrals of Some Elementary Functions
We use a, c, and C to denote constants in the following set of formulas. These can
all be checked by differentiating the right-hand side to get back to the left-hand side.
(cid:7)
adx= ax+C, (5.15)
.
(cid:7)
axc+1
axcdx= +C, (c(cid:3)=−1), (5.16)
. c+1
(cid:7)
a
dx=aln|x|+C, (5.17)
.
x
(cid:7)
ceax
ceaxdx= +C, (5.18)
.
a
(cid:7)
cax
caxdx= +C, (5.19)
.
lna


================================================================================
PAGE 158
================================================================================

146 5 Calculus
(cid:7)
acosxdx=asinx+C, (5.20)
.
(cid:7)
asinxdx =−acosx+C. (5.21)
.
Readers may find integral formula for more elementary functions in [6].
Example 5.22 Find the following integral:
(cid:7)
6x2dx.
.
Solution Applying Eq.(5.16), we have:
(cid:7)
6x3
6x2dx= +C =2x3+C.
.
3
Example 5.23 Find the following integral:
(cid:7)
dx
√ .
.
x 3x
Solution Applying Eq.(5.16), we have:
(cid:7) (cid:7)
. x d √ 3 x x = x −4 3 dx = − x
−
4 3
4
+
+1
1 +C =−3x −1 3 +C. =−√ 3 3 x +C.
3
Exercise
5.6 Calculate the following integrals.
(cid:8)
(1) (cid:8) e2x dx, .
(2) (cid:8) −8x3dx, .
(3) (cid:8) x 6 dx . .
(4) (cid:8) 3e
−x
dx, .
(5) 5sinx dx,
.


================================================================================
PAGE 159
================================================================================

5.4 Integrals 147
5.4.5 Two Properties of Integrals
Iff(x)andg(x)are integrable in[a,b], then
. . .
(cid:7) (cid:7) (cid:7)
b b b
(f(x)+g(x))dx = f(x)dx+ g(x)dx, (5.22)
.
a a a
(cid:7) (cid:7)
b b
cf(x)dx =c f(x)dx, cisaconstant. (5.23)
.
a a
Similarly, for indefinite integrals, the following two properties are valid:
(cid:7) (cid:7) (cid:7)
(f(x)+g(x))dx = f(x)dx+ g(x)dx, (5.24)
.
(cid:7) (cid:7)
cf(x)dx =c f(x)dx, cisaconstant. (5.25)
.
Example 5.24 Find the following integral:
(cid:7)
(x−1)3
dx.
. x2
Solution
(cid:7) (cid:7)
(x−1)3 x3−3x2+3x−1
dx= dx
. x2 x2
(cid:7) (cid:7) (cid:7) (cid:7)
dx dx
= xdx−3 dx+3 −
x x2
x2 1
= −3x+ 3ln|x|+ +C.
2 x
Example 5.25 Find the following integral:
(cid:7)
10xexdx.
.
(continued)


================================================================================
PAGE 160
================================================================================

148 5 Calculus
Example 5.25 (continued)
Solution
(cid:7) (cid:7)
(10e)x 10xex
10xexdx= (10e)x = +C = +C.
. ln(10e) ln10+1
Exercise
5.7 Calculate the following integrals.
(cid:8)
(1) (4sinx +ex )d x,
(cid:8) .
π
(2) 3 (θ +cos θ)d θ,
(cid:8)θ=0 .
(3)
ex+e−x
dx
2 .
5.5 Further Integration Techniques
In many real-world applications, we need to integrate more complex functions. The
methods shown in the previous two sections are not enough. We will introduce two
more new techniques in this subsection. More examples can be found in [6].
5.5.1 Integration by Substitution
This method transforms an integral over one variable x to an integral over a different
variable u by making a substitution. Thatis:
(cid:7) (cid:7)
x2 u2
f(x)dx= g(u)du.
.
x1 u1
The idea behind this integration method is that by making a substitution, you
produce a new integral that is simpler to evaluate. This can often be achieved
by substituting for the “most difficult part” of the integral, or recognising that
the integrand is of the form that you would get having differentiated a composite
function using the chain rule.
The fo(cid:8)llowing shows the procedure when applying the substitution method to an
integral
x2f(x)dx:
x1 .
• Step 1: Think of a substitutionu=h(x)that will make the integral simpler.
.
• Step 2: Differentiate the substitutionu=h(x), and write dx in terms of du.
.


================================================================================
PAGE 161
================================================================================

5.5 FurtherIntegrationTechniques 149
• Step 3: For a definite integral, we must also determine the new limits of integra-
tion.
• Step 4: Make the following substitutions into the original integral:
– Replace x with the equation from Step 1.
– Replace dx with the equation from Step 2.
– Replace limits x and x with the new limits from Step 3.
1. 2.
• Step 5: Do the new integral in terms of u.
• Step 6: Write the answer in terms of x.
Example 5.26 Perform the following integral:
(cid:7)
(2x+5)4dx.
.
Solution This integral could be done by expanding out the bracket, but it will
give a more convenient solution if we replace the “difficult bit” in the bracket.
So setu=2x+5.
.
Differentiate the equationu = 2x+5, and obtain du = 2or equivalently
. dx .
dx= du.
2 .
Substitutingdx= du andu=2x+5into the original integral gives
2 . .
(cid:7) (cid:7) (cid:7)
du u4 u5 (2x+5)5
(2x+5)4dx= u4 = du= +C = +C.
.
2 2 2(5) 10
Example 5.27 Perform the following integral:
(cid:7)
(cid:5)
x3 x4+3 dx.
.
Solution The key to this substitution is recognising that this is the sort of
result that you could get by using the chain rule of differentiation. The
“difficult bit” in the bracket, when differentiated, gives the other part of the
integrand (apart from a constant). That is, d(x4+3) =4x3. So setu=x4+3.
dx . .
(continued)


================================================================================
PAGE 162
================================================================================

150 5 Calculus
Example 5.27 (continued)
Differentiate the equationu=x4+3, and obtaindx= du.
. 4x3.
Substitutingdx= du andu=x4+3into the original integral gives:
4x3. .
(cid:7) (cid:7)
(cid:5) √ du
x3 x4+3dx= x3 u
4x3
(cid:7) √
u
= du
4
= u
1
2
+1
+C
. 4(1 +1)
2
3
=
u2
+C
6
=
(x4+3)2 3
+C.
6
Example 5.28 Evaluate:
(cid:7)
π
2 cos5xsinxdx.
.
0
Solution Again, −sinx is what you get when differentiating cosx, so the
. .
chain rule is implicated. Hence,wesetu=cosx.
.
Differentiate the equationu=cosx, and obtaindx =−du .
. sinx.
Whenx =0,u=1; whenx = π,u=0.
. . 2. .
Substituting dx = −du , u = cosx and new limits into the original
sinx. .
integral gives:
(cid:7) (cid:7) (cid:7) (cid:9)
. π 2 cos5xsinxdx =− 0 u5du= 1 u5du= u6(cid:9) (cid:9) (cid:9) 1 = 1 .
6 6
0 1 0 0


================================================================================
PAGE 163
================================================================================

5.5 FurtherIntegrationTechniques 151
Example 5.29 Perform the following integral:
(cid:7)
x+2
√ dx.
. x+3
Solution Here there is n√o obvious chain rule, but the denominator is the
“difficult bit”. So tryu= x+3.
.
Henceu2 =x+3
.
andx =u2−3√. and obtaindx=2udu
.
.
Substitutingdx=2udu andu= x+3into the original integral gives:
. .
(cid:7) (cid:7)
x+2 u2−1
√ dx= 2udu
. x+3 u
(cid:7)
= (2u2−2)du
2u3
= −2u+C
3
= 2 (x+3) 3
2
−2(x+3) 1
2
+C.
3
You can check the result is correct by differentiating the answer! Note that
the substitution ofu=x+3also works as you can check.
.
Exercise
5.8 Calculate the following integrals.
(cid:8) (cid:5)
(1) (cid:8) x (2 − x2)dx . .
(2) dx .
(cid:8)
(x−1)2 .
π
(3) 8 sin(4x)dx .
(cid:8)0 .
(4)
x2
dx.
(cid:8) (1+x3)2 .
(5) 1 x dx.
(cid:8)
0 (1+x)3 .
π
(6) 2 cos x dx.
(cid:8)π
6
sin3x .
(7) x dx.
(cid:8) 1+x2 .
(8)
xex2
dx.
.


================================================================================
PAGE 164
================================================================================

152 5 Calculus
5.5.2 Integration by Parts
The formula of integration by parts can be derived from the product rule for
differentiation (see Sect.5.2.2 of this chapter). Suppose u and v are functions of
x and differentiable:
d dv du
(uv)=u +v .
.
dx dx dx
Integrating the above equation with respect to x, we get:
(cid:7) (cid:7)
dv du
uv = u dx+ v dx.
.
dx dx
Rearranging this gives us the integration by parts formula:
(cid:7) (cid:7)
dv du
u dx=uv− v dx. (5.26)
.
dx dx
The key to this method of integration is to treat the integral as a product. One
part of the product is represented by dv on the left-hand side of Eq.(5.26) and is
dx.
integrated to give v on the right-hand side. This part needs to be something that you
can, therefore, integrate. The other part of the integral is represented by u and is
differentiated,giving du on the right-hand side. In this way, only part of the original
dx.
integral is integrated, and hopefully, the new integral is made simpler.
Example 5.30 Perform the following integral:
(cid:7)
xsinxdx.
.
Solution The integrand is a product, both of which can be integrated sepa-
rately.Butif uistakenas x,thenthenewintegrandwillcontainthedifferential
of x, which is just 1, and so will be simpler. Hence consider u = x and
.
dv =sinx. Then we have du =1andv =−cosx. Applying the integration
dx . dx . .
by parts formula (Eq.5.26 )g ives:
(cid:7) (cid:7)
xsinxdx= x(−cosx)− (−cosx)1dx
(cid:7)
. =−xcosx+ cosxdx
=−xcosx+sinx+C.


================================================================================
PAGE 165
================================================================================

5.5 FurtherIntegrationTechniques 153
Example 5.31 Perform the following integral:
(cid:7)
xlnxdx.
.
Solution The only part we can integrate is the x , s o w e s etu = lnx and
.
dv = x. Then we have du = 1 and v = 1x2. Applying the integration by
dx . dx x. 2 .
parts formula (Eq.5.26) gives:
(cid:7) (cid:7)
1 1 1 1
xlnxdx=lnx× x2− x2 dx= x2lnx
.
2 2 x 2
(cid:7)
1 1 1
− xdx= x2lnx− x2+C.
2 2 4
Example 5.32 Evaluate:
(cid:7)
π
2 x2cosxdx.
.
0
Solution Again, both parts of the integrand can be integrated, but using u
as x2 will lead to a simpler integral. So set u = x2 and dv = cosx. Then
. . dx .
du = 2x andv = sinx. Applying the integration by parts formula (Eq.5.26)
dx . .
gives:
(cid:7) (cid:9) (cid:7)
π (cid:9)π π
. 2 x2cosxdx=(x2sinx) (cid:9) (cid:9) 2 −2 2 xsinxdx.
0 0 0
Now, the new integral can again be evaluated using integration by parts. But,
in fact, we have just done it (Example 5.30). So we get:
(cid:9) (cid:9)
.
(x2sinx)
(cid:9)
(cid:9)
(cid:9)
π
2 −2(−xcosx+sinx)
(cid:9)
(cid:9)
(cid:9)
π
2 =
π2
−2.
4
0 0


================================================================================
PAGE 166
================================================================================

154 5 Calculus
Exercise
5.9 Calculate the following integrals.
(cid:8)
(1) (cid:8) xex dx. .
(2) (cid:8) x2 sinxdx. .
(3) x3 ln xdx.
(cid:8) .
2
(4) (cid:8) 1 ln xdx, . Hint: treat lnx . as1lnx. .
(5) (cid:8) x sin4xd x. .
(6) (cid:8) 2x ln5xd x. .
(7) sin2 xdx. Hint: treat sin2x as sinxsinx in the integration by parts
. . .
and then use cos2x =1−sin2x and rearrange!
.


================================================================================
PAGE 167
================================================================================

Chapter 6
Advanced Calculus
This chapter takes the study of calculus forward into more advanced topics involving
multiple variable functions. In general, most functions that are found in the machine
learning field are ones with many variables rather than just one. Quite often, we
are trying to maximise some value or minimise some error function, so the ability
to differentiate such functions and find their maxima and minima will be essential.
This leads us to the methods of partial differentiation that enable us to find gradients
in different planes as described in Sect.6.1. We also briefly look at multiple integrals
that will be needed when we look at probability distributions of multiple continuous
random variables in Chap.11.
6.1 Partial Derivatives
6.1.1 The First Partial Derivatives
In functions with two or more variables, the partial derivative is the derivative with
respect to one of those variables, keeping all other variables constant. For example,
consider a functionf(x,y)with two variables (such asf(x,y)=x2+2xy+y3).
. .
Partial derivatives of f(x,y) with respect to x and y are denoted by ∂f and ∂f,
. ∂x. ∂y.
respectively.
Let (cid:3)x = dx and (cid:3)y = dy be increments given to x and y of f(x,y),
. . .
respectively. (cid:3)f is then the subsequent incremental change of the function f . By
.
Eq.(5.3) (in Sect.5.2 of Chap.5), if the corresponding limits exist, we have
∂f f(x+(cid:3)x,y)−f(x,y)
= lim ,
.
∂x (cid:3)x→0 (cid:3)x
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 155
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_6


================================================================================
PAGE 168
================================================================================

156 6 AdvancedCalculus
and
∂f f(x,y+(cid:3)y)−f(x,y)
= lim .
.
∂y (cid:3)y→0 (cid:3)y
When evaluati(cid:2)ng these part(cid:2)ial derivatives at a particular point(x
0
,y
0
)
.
, they can
(cid:2) (cid:2)
be denoted as ∂f(cid:2) and ∂f(cid:2) , respectively.
∂x x y = = x y 0 0 . ∂y x y = = x y 0 0 .
Again, we can define the differential as we did for ordinary differentiation. The
following expression
∂f ∂f
df = dx+ dy (6.1)
.
∂x ∂y
is called the total differential of f , or the principal part of the change in the function
f with respect to changes in the independent va riables.
Remark 6.1 Note that in general,df (cid:3)=(cid:3)f.I f(cid:3)x = dxand(cid:3)y = dyare small,
. . .
then df is a close approximation of(cid:3)f. (cid:2)
. .
Thought of visually or graphically, afunction of two variables is a surfacein three
dimensions. Assumez = f(x,y). Then, ∂f keeps y constant, and so is a gradient
. ∂x.
(seeSect.6.1.4 of this chapter) on the curve where the surface meets a plane parallel
to thex−zplane. Similarly, ∂f keeps x constant, and so is a gradient on the curve
. ∂y.
where the surface meets a plane parallel to they−zplane. The same considerations
.
apply to more variables, but the graph is no longer possible to visualise.
Example 6.1 Supposef(x,y)=x2+2xy+y3. Find the partial derivatives
.
at the point of(2,1).
.
Solution Consider y as a constant:
∂f
=2x+2y.
.
∂x
Consider x as a constant,
∂f
=2x+3y2.
.
∂y
Substitute the point of (2,1) into the two partial derivative results, then we
.
have
(cid:2)
∂f(cid:2)
(cid:2) =2×2+2×1=6,
.
∂x
x=2
y=1
(continued)


================================================================================
PAGE 169
================================================================================

6.1 PartialDerivatives 157
Example 6.1 (continued)
and
(cid:2)
∂f(cid:2)
(cid:2) =2×2+3×12 =7.
.
∂y
x=2
y=1
Example 6.2 A cylinder is being made that is 30 cm radius and 60 cm high.
The tolerances in construction are that the radius is ±0.05%and the height
.
is ±0.01%. Find the approximate maximum error in volume to the nearest
.
integer and hence find the percentage error that this represents.
Solution Radius error: ±0.05%of 30 cm is ±0.015cm.
. .
Height error: ±0.01%of 60 cm is ±0.006cm.
. .
Set the cylinder’s radius, height, and volume as r, h, and V . We hav e
V = πr2h.
.
Let (cid:3)r, (cid:3)h, and(cid:3)V denote increments of r, h, and V . Applying Eq. (6.1), it
. . .
gives us the following:
∂V ∂V
(cid:3)V ≈ dV = dr+ dh=2πrh(cid:3)r+ πr2(cid:3)h.
.
∂r ∂h
To get an estimate of the maximum error, both (cid:3)r and (cid:3)hshould be positive
. .
(or negative).
Substitute r = 30, h = 60, (cid:3)r = 0.015, and (cid:3)h = 0.006, and then we
. . . .
have
(cid:3)V ≈2π ×30×60×0.015+π ×302×0.006=187(cm3).
.
The actual volume should be:
V = πr2h=π ×302×60=169,646(cm3).
.
So the error represents
187÷169,646×100%=0.11%.
.


================================================================================
PAGE 170
================================================================================

158 6 AdvancedCalculus
Exercises
6.1 Find the following partial derivatives:
(1) At the point(3,−1)for the functionf(x,y)=x3y+5x2y2+2xy3
. .
(2) At the point(1,π)for the functionf(x,y)=x2siny−3xcosy
2 . .
(3) At the point(0,2)for the functionf(x,y)=y3e2x +y2e3x +ye4x.
. .
6.2 A triangle is being stamped out of a sheet of metal. Its height is 5 cm, and
its base is 10 cm. The tolerances in this process are that the height is ±0.2%
.
and the base is ±0.1%. Find the approximate maximum error in the area to
.
three decimal places and hence the percentage error that this represents.
6.1.2 The Second Partial Derivatives
The second partial derivatives are the partial derivatives of the first derivative
function. For example, let us consider a functionf(x,y)with two variables. If its
.
first derivatives ∂f and ∂f are continuous and the partial derivatives of ∂f and ∂f
∂x. ∂y. ∂x. ∂y.
all exist, then the second derivatives off(x,y)can be denoted as follows:
.
• ∂2f —the partial derivative of ∂f with respect to x
∂x
2. ∂x.
• ∂2f —the partial derivative of ∂f with respect to y
∂y∂ x. ∂x.
• ∂2f —the partial derivative of ∂f with respect to y
∂y
2. ∂y.
• ∂2f —the partial derivative of ∂f with respect to x
∂x∂y . ∂y.
Remark 6.2 For most well-behaved functions (ones where the two second partial
derivatives involved are continuous), we have ∂2f = ∂2f . (cid:2)
∂y∂x. ∂x∂y. .
Example 6.3 Find the second partial derivatives of the function f(x,y) =
x3+6xy+3y3.
.
Solution Consider y as aconstant: ∂f =3x2+6y.
∂x .
Consider x as aconstant, ∂f =6x+9y2.
∂y .
Consider y as a constantin ∂f: ∂2f =6x.
∂x. ∂x2 .
Consider y as a constantin ∂f: ∂2f =6.
∂y. ∂x∂y .
Consider x as a constantin ∂f: ∂2f =6.
∂x. ∂y∂x .
Consider x as a constantin ∂f: ∂2f =18y.
∂y. ∂y2 .


================================================================================
PAGE 171
================================================================================

6.1 PartialDerivatives 159
Exercise
6.3 Find the first and second partial derivatives of the following functions:
(1) f (x, y) = 3x4y + 6x3y2− 4x2y3+xy4.
.
(2) f (x, y) = x2 sin y +6x3cosy.
.
(3) f (x, y) = e(x2+y 2).
.
(4) f (x, y) = (x3+ y3)ln(y3+x3).
.
(5) f (x, y) = x3+3 y.
x .
6.1.3 Differentiation of Composite Functions with Two
Variables
Let us consider functionz= f(u,v)with two variables u and v, where both u and v
.
are functions with one variable t,thatisu= ϕ(t)andv = ψ(t). If both u and v are
. .
differentiable functions of t, the function z is continuous, and the partial derivatives
exist with respect to u and v, and then the differentiation of the composite function
can be computedasfollows:
dz ∂zdu ∂zdv
= + . (6.2)
.
dt ∂u dt ∂v dt
t is often time and expresses the dependence of each of u and v on the passing
of time. Note that we can find dz as full differentiation since, in reality, z can be
dt.
expressed as a function of t .
Example 6.4 Supposez=sinucosv, whereu=et andv =lnt. Find dz.
. . . dt.
Solution
dz ∂zdu ∂zdv
= +
.
dt ∂u dt ∂v dt
sinu(−sinv)
=cosucosv(et)+
t
sin(et)sin(lnt)
=cos(et)cos(lnt)(et)− .
t
The same result would be found if you substituted t for u and v first, but the
differentiation ismorecomplicated.


================================================================================
PAGE 172
================================================================================

160 6 AdvancedCalculus
Similarly, consider both u and v as functions of two variables, s and t . I f t he
partial derivatives of functions u = ϕ(s,t)and v = ψ(s,t)exist with respect to
. .
s and t, and function z = f(u,v)is continuous, and the partial derivatives exist
.
with respect to u and v, then the differentiation of the composite function can be
computed as follow s:
∂z ∂z∂u ∂z∂v
= + , (6.3)
.
∂s ∂u∂s ∂v ∂s
∂z ∂z∂u ∂z∂v
= + . (6.4)
.
∂t ∂u ∂t ∂v ∂t
Example 6.5 Suppose z = eucosv, where u = st and v = s +t. F ind ∂z
. . . ∂s.
and ∂z.
∂t.
Solution
∂z ∂z∂u ∂z∂v
= +
.
∂s ∂u∂s ∂v ∂s
=eucosv·t −eusinv
=est(tcos(s+ t)−sin(s+t)).
This is the same result that you would get if you first substituted s and t for u
and v into the formula for z and then found the partialderivative ∂z. Usually,
∂s.
though, having lots of simple functions to differentiate is easier than having a
complicated composite function:
∂z ∂z∂u ∂z∂v
= +
.
∂t ∂u ∂t ∂v ∂t
=eucosv·s−eusinv
=est(scos(s+ t)−sin(s+t)).
Again, the same result would be found by substituting s and t for u andv as
before.


================================================================================
PAGE 173
================================================================================

6.1 PartialDerivatives 161
Exercise
6.4 Differentiation of composite functions:
(1) z = eulnv, whereu=sint andv =cost.F ind dz.
. . . dt.
(2) z = (1+ u2)sinv, whereu=s2+t2andv = st2.F ind ∂z and ∂z.
. . . ∂s. ∂t.
6.1.4 Gradient
Armed with the definitions of partial derivatives in coordinate directions, we can
define a vector representing the total gradient in the full space concerned.
Suppose a function f is differentiable in a region. The gradient of the function,
denoted by gradf, is a vector function where each element is a partial derivative
.
with respect to one of the variables. For example, the gradient off(x,y,z)can be
.
written as the following vector:
(cid:3) (cid:4)
∂f(x,y,z) ∂f(x,y,z) ∂f(x,y,z)
(gradf)T = , , .
.
∂x ∂y ∂z
Since the gradient is a vector, it can provide information on the magnitude and
direction of the vector. Suppose the gradient of a function f(x,y) is given by the
.
vector
(cid:3) (cid:4)
∂f(x,y) ∂f(x,y)
(gradf)T = , , (6.5)
.
∂x ∂y
then the magnitude is calculated as follows:
(cid:5)
(cid:6) (cid:7) (cid:6) (cid:7)
∂f(x,y) 2 ∂f(x,y) 2
|gradf |= + .
.
∂x ∂y
If ∂f(x,y) (cid:3)= 0, then the tangent of the angle θ from the x−axis to the gradient is
∂x . . .
given by
∂f(x,y)
tanθ = ∂y .
. ∂f(x,y)
∂x


================================================================================
PAGE 174
================================================================================

162 6 AdvancedCalculus
Fig. 6.1 A geometric description of the gradient vector for the functionf(x,y)=4x2+y2 .used
in Example 6.6
Example 6.6 Suppose f(x,y) = 4x2 + y2. Figure 6.1a shows function
.
values as heights above a grid in the x−y plane. Applying Eq.(6.5) f or
.
f(x,y),w eh ave(gradf)T = 8[x,2y] . Figure 6.1b shows the contour lines
. .
of the function for constant levels (heights, function values) over the interval
[−5, 5] for x and y, respectively. For example, the innermost oval line
.
contains all the pairs of points (x,y) that have the same function value of
.
1. The gradient at a general point is given by [8x,2y]. Each gradient vector
.
is perpendicular to the corresponding tangent line. Three particular gradient
vectors are plotted at the points (1,0), (0,1), and (1,1), respectively. The
2 . .
first two are shown as solid-line arrows perpendicular to the tangents of
f(x,y)=1, and the third as a dashed-line arrow perpendicular to the tangent
.
off(x,y)=5.
.
Remark 6.3 The gradient vector points in the direction of the maximum rate of
increase of the function. Or it points in the opposite direction of the maximum rate
of decrease of the function. Readers interested in this may want to learn more about
directional derivatives and the gradient in Section 2.E of [11]. (cid:2)
.


================================================================================
PAGE 175
================================================================================

6.1 PartialDerivatives 163
Exercise
6.5 Find the gradient vectors at the points indicated for:
(1) f (x, y) = x3y3 at points(1,1),(2,1),(1,2)
. . . .
(2) f (x, y) = x2 sin y +y2cosx, at points(0,1),(1,0),(π,π),(π,π)
. . . 2 2 . 4 4 .
6.1.5 Jacobian Matrix
Iff(x,y)andg(x,y)are differentiable in a region, the Jacobian matrix of f and g
. .
with respect to x and y can bedefinedby
(cid:8) (cid:9)
∂f ∂f
J = ∂x ∂y .
. ∂g ∂g
∂x ∂y
Similarly, if f(x,y,z), g(x,y,z), and h(x,y,z) are differentiable in a region,
. . .
the Jacobian matrix of f, g, and h with respect to x, y, and z can be definedby
⎡ ⎤
∂f ∂f ∂f
⎢∂x ∂y ∂z⎥
J =⎣∂g ∂g ∂g⎦.
. ∂x ∂y ∂z
∂h ∂h ∂h
∂x ∂y ∂z
Extensions are easily made. That is, each row of the Jacobian matrix includes partial
derivatives of a specific function with respect to all variables. A Jacobian matrix can
be either a rectangular matrix or a square matrix. Essentially, the Jacobian matrix
collects all the information about first derivatives together in one place and is useful
for translating between coordinate systems (see Sect.6.3.1 of this chapter) and tells
us about the local behaviour of a function in terms of its gradient.
Example 6.7 If f (x,y) = excosy and f (x,y) = exsiny, determine the
1 . 2 .
Jacobian matrix.
(continued)


================================================================================
PAGE 176
================================================================================

164 6 AdvancedCalculus
Example 6.7 (continued)
Solution Note that the question can also be written in vector form by
collecting the component functions together as define functionF :R2 →R2
.
given byF(x,y)=(f (x,y),f (x,y)).
1 2 .
(cid:8) (cid:9) (cid:3) (cid:4)
∂f1 ∂f1 excosy −exsiny
J = ∂x ∂y = .
. ∂f2 ∂f2 exsin y xecosy
∂x ∂y
Note that the determinant of J ise2xcos2y+e2xsin2y =e2x.
.
Example 6.8 Determine the Jacobian matrix of the function F : R3 → R2
.
given byF(x,y,z)=(4xy+3xz3,3xyz2).
.
Solution Setf (x,y,z)=4xy+3xz3 andf (x,y,z)=3xyz2:
1 . 2 .
(cid:8) (cid:9) (cid:3) (cid:4)
∂f1 ∂f1 ∂f1 4y+3z3 4x 9xz2
J = ∂x ∂y ∂z = .
. ∂f2 ∂f2 ∂f2 3yz2 3xz2 6xyz
∂x ∂y ∂z
Exercise
6.6 Determine the Jacobian matrix for the following:
(1) f (x, y) = x2siny andf (x,y)=y3sinx.
1 . 2 .
(2) F : R2 → R2given byF(r,θ)=(rcosθ,rsinθ).
. .
(3) F : R3 → R3given byF(r,θ,φ)=(rsinθcosφ,rsinθsinφ,rcosθ).
. .
6.1.6 Hessian Matrix
A Hessian matrix of a function is a square matrix of the second partial derivatives
of the function. It collects together all the information about the second derivatives
and tells us about the curvature of a function at a point. For a two-variable function
f(x,y), its Hessian matrix is defined by the following:
.
⎡ ⎤
(cid:3) (cid:4)
∂2f ∂2f
Hf(x,y)= H 11 (x,y) H 12 (x,y) =⎣ ∂x2 ∂x∂y⎦ .
. H (x,y) H (x,y) ∂2f ∂2f
21 22
∂y∂x ∂y2


================================================================================
PAGE 177
================================================================================

6.1 PartialDerivatives 165
Extensions are easily made to a function with more than two variables. Each row
of a Hessian matrix includes partial derivatives of the first derivative with respect
to a specific variable. For example, the Hessian matrix of a three-variable function
f(x,y,z)is defined by the following:
.
⎡ ⎤
⎡ ⎤ ∂2f ∂2f ∂2f
H 11 (x,y,z) H 12 (x,y,z) H 13 (x,y,z) ⎢ ∂x2 ∂x∂y ∂x∂z⎥
. H f(x,y,z)=⎣ H 21 (x,y,z) H 22 (x,y,z) H 23 (x,y,z) ⎦=⎢ ⎣ ∂ ∂ y 2 ∂ f x ∂ ∂ 2 y f 2 ∂ ∂ y 2 ∂ f z ⎥ ⎦,
H
31
(x,y,z) H
32
(x,y,z) H
33
(x,y,z) ∂2f ∂2f ∂2f
∂z∂x ∂z∂y ∂z2
where the first column includes partial derivatives of the first derivative ∂f, the
∂x.
second column comprises partial derivatives of ∂f, and the third column includes
∂y.
partial derivatives of ∂f.
∂z.
Example 6.9 Find the Hessian matrix of the functionf(x,y)=x2y+2xy3.
.
Solution
⎡ ⎤
(cid:3) (cid:4) (cid:3) (cid:4)
Hf(x,y)= H 11 (x,y) H 12 (x,y) =⎣ ∂ ∂ 2 x f 2 ∂ ∂ x 2 ∂ f y⎦= 2y 2x+6y2 .
. H (x,y) H (x,y) ∂2f ∂2f 2x+6y2 12xy
21 22
∂y∂x ∂y2
Exercises
6.7 Find the Hessian matrix of the following functions:
(1) f (x, y) = ex y2+eyx2.
.
(2) f (x, y, z) = x 3y2z−2xyz3.
.
6.8 Which of the following statements about the Jacobian matrix and the
Hessian matrix is correct?
(1) The Hessian matrix is always non-square, while the Jacobian matrix is
always square.
(2) The Jacobian matrix describes the rate of change of each output variable
with respect to the input variables. In contrast, the Hessian matrix helps
to understand the curvature of the function at a specific point.
(3) The Hessian matrix is always a diagonal matrix for any multivariable
function.
(4) The Jacobian matrix is the matrix of second-order partial derivatives of a
function.


================================================================================
PAGE 178
================================================================================

166 6 AdvancedCalculus
6.2 Applications of Partial Derivatives
We now come to discuss issues regarding the maxima and minima of multiple
variable functions, as indicated earlier.
6.2.1 Local Maxima and Minima
Section 5.3 of Chap.5 shows how to find the critical points for a function of one
variable. This section describes how to find maxima and minima for a function with
two variables,f(x,y). As before, these are local maxima and minima.
.
Necessary condition—supposef(x,y)has a relative extreme value at the point
.
(x ,y )and the partial derivatives off(x,y)at(x ,y )exist. Thus, we have
0 0 . . 0 0 .
(cid:2) (cid:2)
∂f(cid:2) ∂f(cid:2)
(cid:2) =0, and (cid:2) =0.
. ∂x x y = = x y 0 0 ∂y x y = = x y 0 0
Sufficient condition—suppose the first partia(cid:2)l derivatives a(cid:2)nd the second partial
(cid:2) (cid:2)
derivatives off(x,y)at(x ,y )exist, and ∂f(cid:2) =0, ∂f(cid:2) =0.
(cid:2) (cid:2) . 0 0 . (cid:2) (cid:2) ∂x x y = = x y 0 0 (cid:2) (cid:2) ∂x x y = = x y 0 0 .
SetA= ∂2f(cid:2) , B= ∂2f (cid:2) , andC = ∂2f(cid:2) .
∂x2 x y = = x y 0 0 ∂x∂y x y = = x y 0 0 . ∂y2 x y(cid:3) = = x y 0 0 . (cid:4)
A B
Then, the Hessian matrixHf(x,y)isHf(x,y)= .
. .
B C
Then,
• The function has a local maximum value at (x ,y ) if AC − B2 > 0 (i.e.,
0 0 . .
det(Hf(x,y))>0) andA<0.
. .
• The function has a local minimum value at (x ,y ) if AC − B2 > 0 (i.e.,
0 0 . .
det(Hf(x,y))>0) andA>0.
. .
• The function does not have an extreme value at (x ,y ) if AC −B2 < 0 (i.e.,
0 0 . .
det(Hf(x,y))<0);
.
• More investigation is needed ifAC−B2 =0(i.e., det(Hf(x,y))=0).
. .
Remark 6.4 IfAC−B2 >0andA<0, then necessarilyC <0 , and if AC−B2 >
. . .
0andA>0, then necessarilyC >0 . So the above conditions for local maximums
. . .
and minimums could have been written equivalently usingC <0 for a maximum
.
andC >0 for a minimum. (cid:2)
. .


================================================================================
PAGE 179
================================================================================

6.2 ApplicationsofPartialDerivatives 167
The general procedure of finding local maxima and minima of a function of
two variablesf(x,y)is
.
• Step 1: Find critical values(x,y)by solving simultaneous equations:
.
∂f(x,y) ∂f(x,y)
=0and =0.
.
∂x ∂y
• Step 2: Write the mathematical expression ofA,B,and C for the given f unction.
.
• Step 3: EvaluateA,B,and C using each pair of critical val ues.
.
• Step 4: Check the sign ofAC −B2 and decide whether the function has a rela-
.
tive extreme at the corresponding pair of critical values in terms of sufficient
conditions.
Example 6.10 Find the critical points of the following function:
f(x,y)=x3−y3+3x2+3y2−9x.
.
State whether the critical points are local maxima or minima.
Solution
• Step 1: Find critical values by solving simultaneous equations obtained
from the first partial derivatives:
⎧
(cid:16) (cid:16)
⎪⎪⎪⎪⎨ x
1
=−3
∂f(x,y) =3x2+6x−9=0 (x+3)(x−1)=0 x =1
∂x ⇒ ⇒ 2
. ∂f(
∂
x
y
,y) =−3y 2+6y =0 y(y−2)=0 ⎪⎪⎪⎪⎩ y
1
=0
y =2.
2
Therefore, the function has four critical points at(−3,0),(−3,2),(1,0),
. . .
and(1,2).
.
• Step 2: Write the mathematical expression of A, B, and C for the given
function:
∂2f(x,y) ∂2f(x,y)
A= =6x+6 , B= =0,
. ∂x2 ∂x∂y
∂2f(x,y)
C = =−6y+6.
∂y2
(continued)


================================================================================
PAGE 180
================================================================================

168 6 AdvancedCalculus
Example 6.10 (continued)
• Step 3: EvaluateA,B,and C using each pair of critical val ues:
.
– At point(−3,0),A =−12 , B=0, andC =6.
. .
– At point(−3,2),A =−12 , B=0, andC =−6.
. .
– At point(1,0),A=12 , B=0, andC =6.
. .
– At point(1,2),A=12 , B=0, andC =−6.
. .
• Step 4: Check the sign ofAC −B2, and decide whether the function has
.
a relative extreme at the corresponding pair of critical values in terms of
sufficient conditions:
– At point(−3,0),AC−B2 =(−12)×6−0<0.
. .
Therefore, the function does not have an extreme value at(−3,0).
.
– At point(−3,2),AC−B2 =(−12)×(−6)−0>0,andA<0.
. . .
Therefore,f(x,y)has a local maximum value of 31 at(−3,2).
. .
– At point(1,0),AC−B2 =12×6−0>0,andA>0.
. . .
Therefore,f(x,y)has a local minimum value of −5at(1,0).
. . .
– At point(1,2),AC−B2 =12×(−6)−0<0.
. .
Therefore, the function does not have an extreme value at(1,2).
.
Example 6.11 Find the critical points of the following function:
f(x,y)=2x2+3y2+3xy+3x+y.
.
State whether the critical points are local maxima or minima.
Solution
• Step 1: Find critical values by solving simultaneous equations obtained
from the first partial derivatives:
∂f(x,y) =4x+3y+3=0,
∂x
∂f(x,y) =6y+3x+1=0.
∂y .
Multiply the first equation by 2 and subtracting the second gives
5x +5 = 0, that is, x = −1. Substituting back in either equation gives
. .
y = 1.
3.
So the function has only one critical point at(−1,1).
3 .
• Step 2: Write the mathematical expression of A, B, and C for the given
function:
∂2f(x,y) ∂2f(x,y) ∂2f(x,y)
A= =4 , B= =3 , C= =6.
. ∂x2 ∂x∂y ∂y2
(continued)


================================================================================
PAGE 181
================================================================================

6.2 ApplicationsofPartialDerivatives 169
Example 6.11 (continued)
• Step 3: EvaluateA,B,and C for the critical value.
.
Nothing to do here.
• Step 4: Check the sign ofAC−B2, and decide whether the function has a
.
relative extreme at the critical value in terms of sufficient conditions.
At point(−1,1),AC−B2 =15>0,andA>0.
3 . . .
Therefore,f(x,y)has a local minimum value of −4 at(−1,1).
. 3 . 3 .
Exercise
6.9 Find the critical points of the following functions and state whether the
critical points are local maxima or minima:
(1) f (x, y) = 2x3+ 2y3− 3x2+ 3y2−12x−12y.
.
(2) f (x, y) = 6− x3− 4xy−2y2−x.
.
(3) f (x, y) = x2+ y2+(x +y+1)2.
.
(4) f (x, y) = 2x3+ 2y3+ 3y2− 9x2−36y+4.
.
6.2.2 Method of Lagrange Multipliers for Maxima and Minima
So far, we have discussed how to find the local maxima and minima of a
function with one or two variables. The only condition we have considered is
that these functions are defined within their domain. However, in many real-world
applications, it is possible to meet problems with other constraints, such as all the
solutions have to be on a plane or line. Converting a constraint problem to a non-
constrained problem is not always easy.
In this section, we introduce the method of Lagrange multipliers for local
maxima and minima created to deal with such constraint problems. This method was
proposed by Joseph-Louis Lagrange, an Italian mathematician and astronomer, later
naturalised French. Lagrange found that the relative extreme of a function under a
constraint is obtained when the gradient of the original function is parallel to the
gradient of the constraint condition function.
Suppose we wanted to find the local maxima and minima of a function z =
f(x,y) where x and y need to satisfy the constraint g(x,y) = 0. The Lagrange
. .
multiplier method operates using the following steps:
• First, it constructs a new function, that is,
F(x,y)= f(x,y)+λg(x,y),
.


================================================================================
PAGE 182
================================================================================

170 6 AdvancedCalculus
where λis a constant. This functionF(x,y)1 is called the Lagrangian.
. .
• Then, it calculates the first partial derivative of the functionF(x,y)with respect
.
to x and y and sets them to zero. Together with the constraint, these form a set of
simultaneous equations give nby
⎧
⎪⎪⎨ ∂f(x,y) +λ∂g(x,y) =0,
∂x ∂x
. ⎪⎪⎩
∂f(
∂
x
y
,y) +λ∂g(
∂
x
y
,y) =0,
g(x,y)=0.
The solutions of(x,y)from these simultaneous equations are the points at which
.
the function may have a relative extreme.
This method can be extended to functions with more than two variables and with
more than one constraint condition.
Example 6.12 Minimise the function f(x,y) = 4x2 + y2, subject to the
.
constraintg(x,y)=x+y−2=0.
.
Figure 6.2 shows the function and the constraint condition function. As
it shows, the plane of the constraint condition function intersects the function
not at the bottom of the surface. Therefore, the minimum value of the function
is not at the bottom of the surface anymore; rather, it is at the lowest point
where the plane intersects the surface.
Solution
• First, construct a new function, that is,
F(x,y)= f(x,y)+λg(x,y) =4x2+y2+λ(x+y−2).
.
• Then, calculate the first partial derivative of the function F(x,y) with
.
respect to x and y and set them both to zero. Together with the constraint,
these form a set of simultaneous equations give nby
⎧ ⎧
⎪⎪⎨ ∂f(x,y) +λ∂g(x,y) =8x+λ=0, ⎪⎪⎨x =0.4
∂x ∂x
. ⎪⎪⎩ ∂f( ∂ x y ,y) +λ∂g( ∂ x y ,y) =2y+λ=0, ⇒ ⎪⎪⎩ y =1.6
g(x,y)=x+y−2=0. λ =−3.2.
Substitutex =0.4andy =1.6into the function, and then we have f(x=
. .
0.4,y=1.6)=4×0.42+1.62 =3.2. The relative extreme of the function
.
(continued)
1 Or sometimes written asF(x,y)=f(x,y)−λg(x,y).. The sign in front of λ.is arbitrary. What
matters is that we need to be consistent throughout the derivation for a specific task.


================================================================================
PAGE 183
================================================================================

6.2 ApplicationsofPartialDerivatives 171
Example 6.12 (continued)
is 3.2 obtained at the point (0.4,1.6) subject to the constraint g(x,y) =
. .
x+y−2.
.
• From Fig.6.2, this is clearly a minimum, and Fig.6.3 shows t his a s w ell.
Figure6.3 shows the contour lines of the function. The second innermost
contour line tells us that all the points along this ellipse give a function
value of 3.2. The dash-dotted line is the constraint condition. As can be
.
seen, the dash-dotted line is a tangent of the contour line with a function
value of 3.2, and it touches this contour line at the point (x = 0.4,y =
.
1.6). The arrow shows the gradient direction at this point. It points in the
.
opposite direction of the maximum rate of decrease of the function and is
perpendicular to the tangent.
The gradient of f,gradf = 8[x,2y], at the point (x = 0.4,y= 1.6)
. .
can be calculated asgradf =[8×0.4,2×1.6]=[3.2,3.2].
.
Fig. 6.2 The visualisation of the graph of the function f(x,y) = 4x2 +y2 . together with the
constraintg(x,y)=x+y−2=0., examined in Example 6.12


================================================================================
PAGE 184
================================================================================

172 6 AdvancedCalculus
Fig. 6.3 Example 6.12:t he
contour lines and the gradient
at the extreme for the
functionf(x,y)=4x2+y2 .,
and the constraint
g(x,y)=x+y−2=0.
Example 6.13 Maximise the functionf(x,y)=xy, subject to the constraint
.
g(x,y)=2x+y−1=0.
.
Solution
• First, construct a new function, that is,
F(x,y)= f(x,y)+λg(x,y) =xy+λ(2x+y−1).
.
• Then, calculate the first partial derivative of the function F(x,y) with
.
respect to x and y and set them both to zero. Together with the constraint,
these form a set of simultaneous equations given by
⎧ ⎧
⎪⎪⎨ ∂f(x,y) +λ∂g(x,y) =y+2λ=0, ⎪⎪⎨x = 1
∂x ∂x 4
. ⎪⎪⎩ ∂f( ∂ x y ,y) +λ∂g( ∂ x y ,y) =x+λ=0, ⇒ ⎪⎪⎩ y = 1 2
g(x,y)=2x+y−1=0. λ =−1.
4
Substitute x = 1 and y = 1 into the function, and then we have f(x=
4. 2.
1,y= 1)= 1 =0.125. The relative extreme of the function is 1 obtained
4 2 8 . 8.
at the point(1,1)subject to the constraintg(x,y)=2x+y−1.
4 2 . .
(continued)


================================================================================
PAGE 185
================================================================================

6.2 ApplicationsofPartialDerivatives 173
Example 6.13 (continued)
• To check if it is a local maximum, try points just either side of(0.25,0.5)
.
on the line2x+y−1=0
.
f(0.249,0.502)=0.124998<0.125= 1 and
8.
f(0.251,0.498)=0.124998<0.125= 1, so it is a local maximum.
8.
Remark 6.5 To see whether the function has an extreme at the critical values and
which sort of extreme it is, we often need to make a judgment in terms of the nature
of the problem we are solving. (cid:2)
.
Exercise
6.10 Applying the Lagrange multiplier method to the following functions:
(1) Minimise the function f(x,y) = x2 +y2 +1 subject to the constraint
.
g(x,y)=x−y+1=0.
.
(2) Minimise the function f(x,y) = x3 + y3 subject to the constraint
.
g(x,y)=x+y−1=0.
.
6.2.3 Gradient Descent Algorithm
This section introduces how to find a minimum value of a function by using the
gradient descent algorithm. Earlier, we have seen in Fig.5.7 in Chap.5 that if a
function monotonically decreases, then the sign of its first derivative is negative;
otherwise, it is positive.
The general idea of a gradient descent algorithm is to update the values of
variables of a function iteratively to minimise the function. This is used extensively
in neural networks. Figure 6.4 displays two functions: one on the left in black has a
minimum value atx = x , and the other one on the right in brown has a maximum
0.
value atx =x . Let us have a look at the one with a minimum value first. When its
1.
x value is on the left-hand sideofx , x needs to move along the positive direction of
0.
the x-axis to reachx . The moving direction is opposite to the sign of its derivative,
0.
which is negative. When its x value is on the right-hand sideofx , x needs to move
0.
along the negative direction of the x-axis to reachx . Again, the moving direction
0.
is opposite to the sign of its derivative, which is positive. Therefore, the gradient
descent algorithm works as follows:
• Step 1: Initialise a value for x, denoted asxold; calculate the function value using
.
xold.
.


================================================================================
PAGE 186
================================================================================

174 6 AdvancedCalculus
Fig. 6.4 An illustration of the relationship between the signs of gradients and the extreme values
• Step 2: Update x value by moving x along the x-axis with the direction opposite
to the corresponding derivative sign. Thatis,
(cid:2)
d (cid:2)
xnew =xold −(cid:10) f(x)(cid:2) , (6.6)
.
dx x=xold
where (cid:10) is the learning rate, a positive scalar determining the size of the step x
.
moves, and the derivative of the function is evaluatedusingxold.
.
• Step 3: Calculate the function value usingxnew.
.
• Step 4: Assignxold =xnew.
.
• Step 5: Repeat Steps2−4until the function reaches its minimum or the iterations
.
satisfy some pre-set criterion.
This can be easily extended to functions with more than one variable. For
example, for a function with two variablesf(x,y), one can update variable values
.
as follows:
(cid:2) (cid:2)
∂f(x,y)(cid:2) ∂f(x,y)(cid:2)
xnew =xold −(cid:10) (cid:2) , and ynew =yold −(cid:10) (cid:2) . (6.7)
.
∂x
x=xold
∂y
x=xold
y=yold y=yold
Similarly, the gradient ascent algorithm updates the values of variables and the
function by moving x in the direction matching the sign of the corresponding
derivative (see the function on the right inFig.6.4). For a function withtwo variables
f(x,y), one can update variable values as follows:
.
(cid:2) (cid:2)
∂f(x,y)(cid:2) ∂f(x,y)(cid:2)
xnew =xold +(cid:10) (cid:2) , andynew =yold +(cid:10) (cid:2) . (6.8)
.
∂x
x=xold
∂y
x=xold
y=yold y=yold
Remark 6.6 When applying a gradient-based algorithm, the initial value of a
variable should be chosen carefully. In addition, the step size (cid:10) should be small to
.


================================================================================
PAGE 187
================================================================================

6.2 ApplicationsofPartialDerivatives 175
avoid going past the local minimum or the maximum. However, very small values
will take longer to calculate. (cid:2)
.
Example 6.14 Let f(x ,x ) = 4x2 + x2. Perform one iteration of the
1 2 1 2.
gradient descent algorithm. The initial values are xold = 3, and xold = 2.
1 . 2 .
Set the learning rate to(cid:10) =0.001.
.
Solution
• Substitutexold = 3, andxold = 2tof(x ,x ) = 4x2+x2, and we have
1 . 2 . 1 2 1 2.
4×32+22 =40.
.
• Compute partial derivatives of the function:
∂f ∂f
=8x , =2x .
. 1 2
∂x ∂x
1 2
• Update values for x and x : substitute initial values to Eq.(6.7), and we
1. 2.
have
(cid:2)
∂f (cid:2)
xnew =xold −(cid:10) (cid:2) =3−0.001×(8×3)=2.976,
. 1 1 ∂x 1 x1 =x 1 old
x2 =x
2
old
(cid:2)
∂f (cid:2)
xnew =xold −(cid:10) (cid:2) =2−0.001×(2×2)=1.996.
. 2 2 ∂x 2 x1 =x 1 old
x2 =x
2
old
Substitutex = 2.976, andx = 1.996intof(x ,x ) = 4x2+x2, which
1 . 2 . 1 2 1 2.
gives4×2.9762+1.9962 =39.41.Note that this is smaller than the initial
.
value of 40, so we are moving (slowly) toward the minimum. (If we had set
the learning rate, (cid:10), to0.01, we would getx =2.76andx =1.96, giving
. . 1 . 2 .
f(x ,x ) = f(2.76,1.96) = 34.31, which is a faster descent—though it
1 2 .
runs the risk of jumping right past the minimum.)
• Assignxold =2.976andxold =1.996, to complete the first iteration.
1 . 2 .
Remark 6.7 The gradient descent algorithm is really a process that requires a
computer that is programmed specifically to do this task. Indeed, work in neural
networks invariably does access a computer to do all the hard iterative calculations.
So, we will not attempt further iterations and examples in this book. Hopefully, the
basic idea of the iterative method is clear. (cid:2)
.


================================================================================
PAGE 188
================================================================================

176 6 AdvancedCalculus
Exercise
6.11 Which of the following statements is correct?
(1) The gradient descent algorithm follows the direction of the gradient to
maximise a function, whereas the gradient ascent algorithm follows the
gradient to minimise a function.
(2) Gradient descent and gradient ascent algorithms are used for uncon-
strainedoptimisation,whileLagrange multipliersareusedforconstrained
optimisation problems.
(3) Gradient descent is used to maximise functions, while Lagrange multipli-
ers are used to minimise functions.
(4) Lagrange multipliers iteratively adjust the variables to minimise or
maximise the objective function.
6.3 Double Integrals
In this book, we consider double integrals as an example of multiple definite
integrals.
A definite integral of a function of one variable gives the area “under” the curve
between two x value limits of integration, which define the “bottom” boundary of
the area. In the same way, a double definite integral of a function of two variables
gives the volume “under” the surface, where the limits of integration define the
area, or region, onthex−y plane that gives the “bottom” boundary of the volume.
.
Intuitively integrating by one variable gives the area, and integrating these “areas”
in the other direction gives the volume.
The easiest cases are where the bounding region in thex−yplane is a rectangle.
.
We start with a really easy one.
Example 6.15 Find
(cid:21) (cid:21)
1 2
6dxdy.
.
x=0 y=0
Solution The boundary of the region is the rectangle from x = 0 to x = 1
. .
and fromy =0toy =2. The “top” of the volume is the surfacef(x,y)=6.
. . .
(continued)


================================================================================
PAGE 189
================================================================================

6.3 DoubleIntegrals 177
Example 6.15 (continued)
Integrating over y first and then x,
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
1 2 1 2
6dxdy= 6dy dx
.
x=0 y=0 x=0 y=0
(cid:21) (cid:3) (cid:4)(cid:2) (cid:21) (cid:3) (cid:4)(cid:2)
1 (cid:2)2 1 (cid:2)1
. = 6y (cid:2) (cid:2) dx= 12dx= 12x (cid:2) (cid:2) =12.
x=0 y=0 x=0 x=0
In fact, f(x,y) = 6 is a horizontal plane at height 6. So, the volume is a
.
cuboid with base 1 by 2 and height 6. This has volume 12, so the integration
has “worked”.
Not surprisingly, you get the same result if you integrate in the other order:
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
2 1 2 1
6dxdy= 6dx dy
.
y=0 x=0 y=0 x=0
(cid:21) (cid:3) (cid:4)(cid:2) (cid:21) (cid:3) (cid:4)(cid:2)
2 (cid:2)1 2 (cid:2)2
. = 6x (cid:2) (cid:2) dy = 6dx= 6y (cid:2) (cid:2) =12.
y=0 x=0 y=0 y=0
It is generally true for rectangular regions that you can integrate in either order
since it is the same volume, and it does not matter which “areas” are summed in
the second integration. However, it is sometimes easier to define and integrate the
region in thex−yplane in one direction first rather than the other, especially when
.
we have non-rectangular regions.
However, we will do another, more complicated, rectangular region example
first. For these examples when integrating over one variable, we treat the other
independent variable as a constant.
Example 6.16 Find
(cid:21) (cid:21)
2 2
x+ydxdy.
.
x=1 y=0
Solution The boundary of the region is another rectangle from x = 1 to
.
x = 2 and from y = 0 to y = 2. The “top” face of the volume is now the
. . .
(continued)


================================================================================
PAGE 190
================================================================================

178 6 AdvancedCalculus
Example 6.16 (continued)
surfacef(x,y)=x+y:
.
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
2 2 2 2
x+ydxdy= x+ydy dx .
.
x=1 y=0 x=1 y=0
Since we are integrating over y first, we treat x as a constant. So w econtinue:
(cid:21) (cid:3) (cid:4)(cid:2) (cid:21) (cid:3) (cid:4)(cid:2)
.
2
xy+
y2 (cid:2)
(cid:2)
(cid:2)
2
dx=
2
2x+2dx= x2+2x
(cid:2)
(cid:2)
(cid:2)
2
=5.
x=1 2 y=0 x=1 x=1
Again, you can check if you get the same result if you integrate over x and y
in the other order.
Exercise
6.12 Calculate the following integrals that also have rectangular regions:
(cid:22) (cid:22)
2 1
(1) xydxdy.
(cid:22)x=0 y(cid:22)=0 .
(2) 1 2 2x2+ 3y2+1dxdy.
(cid:22)x=−1(cid:22) y=−2 .
π
(3) 2 2 x cos ydxdy.
x=0 y=0 .
More generally, regions over which the integration takes place are not rectangles.
Sometimes, one or more of the boundaries is a curve or a sloping line. Then, one
of the limits is expressed in terms of a variable since it is bounded by a curve or
sloping line. So, we need to look at the limits (or endpoints) of integration. If there
is an integration variable in the limits of integration, we must perform the integral
with the variable limit first. As before, when integrating over one variable, we treat
the other independent variable as a constant.
For our first example, we will integrate over the region that is a triangle bounded
by the x-axis, the linex = 1, and the liney = x (see Fig.6.5). If we integrate over
. .
y first, then the “areas” are between the x-axis, thatis,y = 0and the liney = x.
. .
So, the top y limit is x. We then integrate over x fromx =0tox =1.
. .


================================================================================
PAGE 191
================================================================================

6.3 DoubleIntegrals 179
Fig. 6.5 This figure shows
the region (shaded) used in
Example 6.17, over which the
double integration is
performed to form the base of
the volume
Example 6.17 Perform the following integral:
(cid:21) (cid:21)
1 x
1+x2+y2dxdy.
.
x=0 y=0
Solution The volume defined is that of a triangular-based shape bounded at
the “top” by the surface f(x,y) = 1+x2 +y2. We must integrate over y
.
first since the upper limit of the integral over y is a variablelimit(x). When
.
integrating over y, we treat the independent variable x as a constant:
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
1 x 1 x
1+x2+y2dxdy= 1+x2+y2dy dx
.
x=0 y=0 x=0 y=0
(cid:21) (cid:3) (cid:4)(cid:2)
=
1
y+x2y+
y3 (cid:2)
(cid:2) (cid:2)
x
dx
x=0 3 y=0
(cid:21) (cid:21) (cid:3) (cid:4)(cid:2)
. = 1 x+x3+ x3 −0dx= 1 x+ 4x3 dx= x2 + x4 (cid:2) (cid:2) (cid:2) 1 = 1 + 1 = 5 .
x=0 3 x=0 3 2 3 x=0 2 3 6
Integrating over this particular region can be thought of another way. We could
integrate over x first. This gives “areas” from theliney =xto the linex =1.
. .
So, the x limits are y and 1. We then integratey fromy = 0toy = 1. This
. .
gives the integ ral:
(cid:21) (cid:21)
1 1
1+x2+y2dxdy.
.
y=0 x=y
Now, you have to integrate over x first. We set this as an exercise (see below).


================================================================================
PAGE 192
================================================================================

180 6 AdvancedCalculus
It should be noted again that sometimes, it is easier to define the region in the
x−yplane in one direction first rather than the other when we have non-rectangular
.
regions, so the order of integration cannot be reversed easily by changing the limits.
Example 6.18 Perform the following integral:
(cid:21) (cid:21)
2 2x
xydxdy.
.
x=1 y=0
Solution This is an integration over the region bounded by the x-axis, the
lines x = 1 and x = 2, and the line y = 2x. It is a trapezium. The “top”
. . .
boundary of the volume is the surfacef(x,y)=xy.
.
We must integrate over y first since the upper limit of the integral over
y is a variable limit (x). When integrating over y, we treat the independent
.
variable x as a constant:
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7) (cid:21) (cid:3) (cid:4)(cid:2)
.
2 2x
xydxdy =
2 2x
xydy dx=
2 xy2 (cid:2)
(cid:2) (cid:2)
2x
dx
x=1 y=0 x=1 y=0 x=1 2 y=0
(cid:21) (cid:21) (cid:3) (cid:4)(cid:2)
. = 2 [2x3−0]dx= 2 2x3dx= x4 (cid:2) (cid:2) (cid:2) 2 = 1 (24−14)= 15 .
x=1 x=1 2 x=1 2 2
Now, consider a boundary for our integration region to be a semicircle above the
x-axis, centred at the origin of radius 1. This circle has equati√onx2+y2 = 1
.
. We
can form limits of an integral as y goes from y = 0 to y = 1−x2, and then x
. .
goesfrom −1to 1. This gives us the next example, where the “top” boundary of
.
the volume is the surface:f(x,y)=x2y.
.
Example 6.19 Perform the following integral:
√
(cid:21) (cid:21)
1 1−x2
x2ydxdy.
.
x=−1 y=0
Solution
√ √
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
1 1−x2 1 1−x2
x2ydxdy= x2ydy dx
.
x=−1 y=0 x=−1 y=0
(continued)


================================================================================
PAGE 193
================================================================================

6.3 DoubleIntegrals 181
Example 6.19 (continued)
√
(cid:21) (cid:3) (cid:4)(cid:2)
=
1 x2y2 (cid:2)
(cid:2) (cid:2)
1−x2
dx
x=−1 2 y=0
(cid:21) (cid:21) (cid:3) (cid:4)(cid:2)
. = 1 x2(1−x2) dx= 1 x2−x4 dx= x3 − x5 (cid:2) (cid:2) (cid:2) 1 = 2 .
x=−1 2 x=−1 2 6 10 x=−1 15
Exercise
6.13 Calculate the following integrals:
(cid:22) (cid:22)
y 1
(1) xydxdy .
(cid:22)x=0(cid:22)y=0 .
(2) 1 1 1+ x2+ y2dxdy.
(cid:22)y=0(cid:22)x=y .
(3) 2 2x 2x2y + 3xy2dxdy.
(cid:22)x=1(cid:22)y=0 .
(4) 1 1−x2 3x2y2dxdy.
x=0 y=0 .
6.3.1 Integration of Double Integrals Using Polar Coordinates
Some double integrals can be expressed in a simpler form if we can transform the
area from rectangular Cartesian coordinates (x,y) to polar coordinates (r,θ). As
. .
we saw in Example 6.19 above, the limits for integration in some exam√ples can
be really complicated when expressed in Cartesian coordinates (e.g.,y = 1−x2)
.
andpotentiallyleadtosomedifficultintegrals.However,theregioninExample6.19,
which is a half-disc centred on the origin of unit radius, can be easily expressed in
polar coordinates. It is
0≤r ≤1 and 0≤θ ≤ π.
.
So, the limits of integration would be simple.
To convert to polar coordinates, we need the following relationships between
Cartesian and polar coordinates:
(cid:16)
x =rcosθ,
.
y =rsinθ.


================================================================================
PAGE 194
================================================================================

182 6 AdvancedCalculus
We also need to replace the dxdy . In brief,d xdy essentially represents a small
area in the plane, denoted as dA. The Jacobian matrix J describes the change
in coordinates from (x,y) to (r,θ), that is, dA = dxdy =J||drdθ, where the
. . .
determinant of the Jacobian matrix represents the scaling factor by which areas are
scaled during the transformation.
The Jacobian matrix for this transformation is
(cid:3) (cid:4) (cid:3) (cid:4)
∂x ∂x cosθ −rsinθ
J = ∂r ∂θ = .
. ∂y ∂y sin θ corsθ
∂r ∂θ
The determinant of this Jacobian matrix is r. Therefore, the area element in
polar coordinates isrdrdθ, which accounts for the fact that the segments of a circle
.
increase in size as you move further from the centre.
So, the transformation formula is shown as follows:
(cid:21)(cid:21) (cid:21)(cid:21)
f(x,y)dxdy= f(rcosθ,rsinθ)rdrdθ. (6.9)
.
D D
Let us redo Example 6.19 from the previous section by converting to polar
coordinates in Example 6.20.
Example 6.20 Converting Example 6.19 to polar coordinates,
√
(cid:21) (cid:21) (cid:21) (cid:21)
1 1−x2 π 1
x2ydxdybecomes (rcosθ)2(rsinθ)rdrdθ.
.
x=−1 y=0 θ=0 r=0
Solution
(cid:21) (cid:21) (cid:21) (cid:6)(cid:21) (cid:7)
π 1 π 1
(rcosθ)2(rsinθ)rdrdθ= r4cos2θsinθdr dθ
.
θ=0 r=0 θ=0 r=0
(cid:21) (cid:3) (cid:4)(cid:2) (cid:21)
.
= π r5 (cid:2) (cid:2)
(cid:2)
1 cos2θsinθdθ= 1 π cos2θsinθdθ
θ=0 5 r=0 5 θ=0
(cid:3) (cid:4)(cid:2)
= 1 −cos3θ (cid:2) (cid:2) (cid:2) π = 2 .
5 3 15
θ=0
When integrating with respect to θ, we have used integration by substitution
.
withu = cosθ, so thatdu = −sinθdθ. We have obtained the same answer
. .
as before for this example.


================================================================================
PAGE 195
================================================================================

6.3 DoubleIntegrals 183
Now, to do an example that is impossible without converting to polar coordinates.
Example 6.21 Perform the following integral:
(cid:21)(cid:21)
e
−x2−y2
dxdy,
.
D
where D is a closed circular area with the origin as the centre and a as the
radius.
Solution If not expressed in polar coordinates, the integral would be
√
(cid:21) (cid:21)
a a2−x2
√ e −x2−y2 dxdy.
.
x=−a y=− a2−x2
This is impossible to integrate non-numerically, so we express D in the polar
coordinate system as follow s:
0≤r ≤ a, 0≤θ ≤2π.
.
Applying Eq.(6.9), we have
(cid:21)(cid:21) (cid:21)(cid:21)
e −x2−y2 dxdy= e −r2 rdrdθ
.
D D
(cid:21) (cid:3)(cid:21) (cid:4)
2π a
= e −r2 rdr dθ
θ=0 r=0
(cid:21) (cid:3) (cid:4)
2π 1 a
= − e −r2 dθ
θ=0 2 r=0
(cid:21)
1 2π
= (1−e −a2 ) dθ
2 θ=0
= π(1−e −a2 ).
In the first equation line of the above, we have used the general equation of
a circle centred at (0,0), that is, x2 +y2 = r2. In the second line, we have
. .
applied integration by substitution. That is, we setu = r2, and then we have
.
du=2rdr.
.


================================================================================
PAGE 196
================================================================================

184 6 AdvancedCalculus
Exercise
6.14 Convert the following to polar coordinates and hence evaluate the
following:
(cid:22)(cid:22)
(1)
ex2+y2
dxdy, where D is a closed circle area with the origin as the
D .
c(cid:22)e(cid:22)ntre and 2 as the radius.
(2) xydxdy ,where D is the area in the first quadrant between the circles
D .
with radius 1 and 3 centred at the origin, that is, a quarter of a ring around
t(cid:22)h(cid:22)e or igin.
(3) sin(x2+ y2)dxdy,where D isa closed circle area witht he origin as
D .
centreandoneastheradius.


================================================================================
PAGE 197
================================================================================

Chapter 7
Algorithms 1: Principal Component
Analysis
This chapter and the next two chapters, (8 and 9), represent the culmination of a
lot of mathematics theory, specifically linear algebra and calculus. The material has
been divided into three chapters to indicate the separate nature of each topic since
each chapter revisits and completes one of the case studies introduced in Chap.1.
Hence, these three chapters aim to show how we can apply the knowledge
introduced in previous chapters to formulate three widely used algorithms in the
Data Science field: principal component analysis, simple linear regression, and
simple two-layer neural networks trained by gradient descent.
This chapter deals with principal component analysis. In Chap.4, w e h ave
described the basic idea of principal component analysis (PCA). This chapter will
further help us understand the relationship between eigenvalues produced in the
PCA analysis and variances among the data projected in the PCA space.
7.1 Revisit Principal Component Analysis
In Sect.4.2 of Chap.4, you learned how to find the principal components for a set of
data points. The aim was to find the directions with the most variance in the data. If
all you want to do is to find principal components for data, then the work in Chap.4
is all you need, and this new section in this chapter is unnecessary for you. However,
this is a book giving the maths behind the algorithms, so we will now explain why
defining principal components as eigenvectors of the covariance matrix of data X,
.
called (cid:2), gives the directions of most variance. To do this, we need to find the
.
maximum value of the variance with various constraints, such as the direction being
a unit vector. Finding maximum values with a constraint means we will appeal to
the Lagrange multipliers method for maxima and minima as given in Sect.6.2.2
of Chap.6. Before going through the details, we need further knowledge regarding
vectors, matrices, and their differentiation, as given in the next subsection.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 185
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_7


================================================================================
PAGE 198
================================================================================

186 7 Algorithms1:PrincipalComponentAnalysis
7.2 Preliminary Knowledge
Apart from the basic maths knowledge introduced in Sect.3.3.11.1 of Chap.3
and Sect.4.2.1 of Chap.4, we need a bit more to deal with the reasons that
using eigenvectors and eigenvalues for principal component analysis does what we
require. After giving each result, we will illustrate that they are true with one or
more examples.
Suppose Xis an×d matrix and uis ad×1vector.
. . . .
• The variance of Xuis given by
.
var(Xu)=uTcov(X)u. (7.1)
.
If we denote the matrixcov(X)as (cid:2), then we have
. .
var(Xu)=uT(cid:2)u. (7.2)
.
This result looks at the variance of a matrix multiplied by a vector. This is important
since it is about projecting the data onto a vector—see Sect.4.2.2 of Chap.4.
Example 7.1 Let Xbe a5×2matrix of data and ua2×1vector given by
. . . .
⎡ ⎤ ⎡ ⎤
32 8
⎢ ⎥ ⎢ ⎥
⎢43⎥ (cid:8) (cid:9) ⎢11⎥
⎢ ⎥ 2 ⎢ ⎥
.
X =⎢
⎢
21⎥
⎥
andu=
1
, soXuis ⎢
⎢
5⎥
⎥
.
⎣22⎦ ⎣6⎦
42 10
We will show thatvar(Xu)is the same asuTcov(X)ufor this example.
. .
Xu is a set of numbers and has a mean of 8 and a variance of 6.5. S o
. .
var(Xu)=6.5.
.
Now for our data, the mean of the first column is 3, and the mean of the
second column is 2. Remember that the covariance of two sets of numbers has
been defined in Sect.4.2.1 of Chap.4. For example, for our data,
(cid:10)
5 (x −x¯ )(x −x¯ )
cov(x ,x )= i=1 i,1 1 i,2 2
. 1 2 5−1
0+1×1+(−1)×(−1)+0+0
=
4
2
= .
4
(continued)


================================================================================
PAGE 199
================================================================================

7.2 PreliminaryKnowledge 187
Example 7.1 (continued)
So, the covariance matrix is a2×2matrix:
.
(cid:8) (cid:9) (cid:8) (cid:9)
cov(x ,x )cov(x ,x ) 1 42
(cid:2) =cov(X)= 1 1 1 2 = .
.
cov(x ,x )cov(x ,x ) 4 22
2 1 2 2
Hence,uTcov(X)uis
.
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
(cid:11) (cid:12) (cid:11) (cid:12)
1 42 2 1 2
. 21 = 106 =6.5.
4 22 1 4 1
So, both are 6.5. Hence, we have demonstrated that var(Xu) =
.
uTcov(X)u=uT(cid:2)uas required.
.
Example 7.2 Let us try a more complicated example. So, let X be a 6×3
. .
matrix of data and ua3×1vector given by
. .
⎡ ⎤
234
⎢ ⎥
⎢101⎥ ⎡ ⎤
⎢ ⎥ 2
. X = ⎢ ⎢ ⎢ 4 1 3 2 2 2 ⎥ ⎥ ⎥ andu=⎣ 3 ⎦ ,
⎢ ⎥
1
⎣ ⎦
221
222
⎡ ⎤
17
⎢ ⎥
⎢3⎥
⎢ ⎥
⎢19⎥
then Xuis ⎢ ⎥with a mean of 12 and a variance of 32. So,var(Xu)=32.
. ⎢10⎥. .
⎢ ⎥
⎣ ⎦
11
12
Now for our data, the mean of all three columns is 2. The covariance matrix
is a3×3matrix:
.
⎡ ⎤ ⎡ ⎤
cov(x ,x )cov(x ,x )cov(x ,x ) 641
. (cid:2)
=cov(X)=⎣
cov(x
1
2 ,x
1
1 )cov(x
1
2 ,x
2
2 )cov(x
1
2 ,x
3
3 )
⎦= 1⎣
464
⎦
.
5
cov(x ,x )cov(x ,x )cov(x ,x ) 146
3 1 3 2 3 3
(continued)


================================================================================
PAGE 200
================================================================================

188 7 Algorithms1:PrincipalComponentAnalysis
Example 7.2 (continued)
So,uTcov(X)uis
.
⎡ ⎤⎡ ⎤ ⎡ ⎤
(cid:11) (cid:12) 641 2 (cid:11) (cid:12) 2
.
1
231
⎣
464
⎦⎣
3
⎦= 1
25 3020
⎣
3
⎦=32.
5 5
146 1 1
Hence,var(Xu)=uTcov(X)u=uT(cid:2)uas required since both are 32.
.
Exercise
7.1 Show thatvar(Xu)=uTcov(X)u=uT(cid:2)ufor
.
(1)
⎡ ⎤
34
⎢ ⎥
⎢42⎥ (cid:8) (cid:9)
⎢ ⎥ 1
.
X =⎢
⎢
23⎥
⎥
andu=
−1
.
⎣22⎦
44
(2)
⎡ ⎤
254
⎢ ⎥
⎢423⎥ ⎡ ⎤
⎢ ⎥ 2
. X = ⎢ ⎢ ⎢ 2 3 2 2 2 5 ⎥ ⎥ ⎥ andu=⎣ 3 ⎦ .
⎢ ⎥
1
⎣ ⎦
542
232
Now, suppose A is a d ×d symmetric matrix, x is a d ×1 vector with xT =
. . . .
(x , x , ··· , x), and αis a scalar.
1 2 d . .
• Let the scalar αbe defined by
.
α =xTAx,
.


================================================================================
PAGE 201
================================================================================

7.2 PreliminaryKnowledge 189
where Adoes not depend on x, and then in general, we have
. .
∂α
=(A+AT)x. (7.3)
.
∂x
Since Ais also symmetric, thenAT =Aso that
. .
∂α
=2Ax. (7.4)
.
∂x
The product xTAx is just a scalar value (i.e., not a vector or matrix), which is an
.
equation inx , x , ··· , x. So, ∂α is the gradient of the function as a vector as in
1 2 d. ∂x.
Sect.6.1.4 of Chap.6. Hence,
(cid:8) (cid:9)
∂α ∂α ∂α ∂α T
=gradα = , , ···, .
.
∂x ∂x ∂x ∂x
1 2 d
Of course, we are only interested in symmetric matrices since cov(X) is always
.
symmetric. However, let us start with a non-symmetric example (see Example 7.3).
(cid:8) (cid:9)
a b
Example 7.3 Letd =2,xT =(x , x ), andA= .
. 1 2 . .
c d
(cid:8) (cid:9)
(cid:11) (cid:12)
x
Then, α =xTAx= ax +cx bx +dx 1 = ax2+cx x +bx x +
1 2 1 2 x 1 1 2 1 2
2
dx2.
2 .
So, αis a scalar function of x and x . Hence,
. 1. 2.
(cid:13) (cid:14) (cid:8) (cid:9)
∂α ∂α 2ax +cx +bx
. ∂x = ∂ ∂ x α 1 = cx 1 +bx 2 +2dx 2 .
∂x2 1 1 2
Also,
(cid:8) (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
2 a +b c x 2ax +cx +bx
(A+AT)x= 1 = 1 2 2 .
. b+c 2d x cx +bx +2dx
2 1 1 2
So, ∂α is the same as(A+AT)xand the result is as required.
∂x. .
Now, let us see a symmetric example (see Example 7.4).


================================================================================
PAGE 202
================================================================================

190 7 Algorithms1:PrincipalComponentAnalysis
(cid:8) (cid:9)
a b
Example 7.4 Let A = . So, A is the same as in Example 7.3 except
. .
b d
thatc=b.
.
Then, looking at the result in Example 7.3 and puttingc=b, we hav e
.
α =xTAx= ax2+2bx x +dx2,
. 1 1 2 2
and so
(cid:13) (cid:14) (cid:8) (cid:9) (cid:8) (cid:9)(cid:8) (cid:9)
∂α ∂α 2ax +2bx a b x
. ∂x = ∂ ∂ x α 1 = 2bx 1 +2dx 2 =2 b d x 1 =2Ax.
∂x2 1 2 2
Hence, ∂α =2Axas required.
∂x .
Exercise
⎡ ⎤
a b c
7.2 Letd =3 . ,xT =(x 1 , x 2 , x 3 ) . , andA=⎣ b d e ⎦ , . which is symmetric.
c e f
Ifα =xTAx, show that ∂α =2Ax.
. ∂x .
Now, suppose xand aared×1vectors and β and γ are scalars.
. . . . .
• Let the scalar β be defined by
.
β =xTx,
.
then we have:
∂β
=2x. (7.5)
.
∂x
• Let the scalar γ be defined by
.
γ =xTa=aTx,
.
then we have:
∂γ
=a. (7.6)
.
∂x


================================================================================
PAGE 203
================================================================================

7.3 ProblemSetting 191
Example 7.5 Letd =2
.
. So, (cid:8)we (cid:9)havexT =(x
1
,x
2
)
.
andaT =(a
1
,a
2
)
.
.
(cid:11) (cid:12)
x
Then,β = (cid:13) xTx (cid:14) = x (cid:8) 1 x 2 (cid:9) x 1 2 =x 1 2+x 2 2 . ,
∂β
2x
and so ∂ ∂ β x = ∂ ∂ x β 1 = 2x 1 =2x . .
∂x2 2
Alsoγ =xTa(cid:13) =a(cid:14) Tx=
(cid:8)
x
1(cid:9)
a
1
+x
2
a
2.
,
∂γ
a
and so ∂ ∂ γ x = ∂ ∂ x γ 1 = a 1 =a. .
∂x2 2
Exercise
7.3 Whend = 3,xT = (x ,x ,x )andaT = (a ,a ,a ). Ifβ = xTxand
. 1 2 3 . 1 2 3 . .
γ =xTa, show that ∂β =2xand ∂γ =a.
. ∂x . ∂x .
7.3 Problem Setting
Recall that in Sect.4.2.2 of Chap.4, we have claimed that if the first principal
component of the data X is the eigenvector u of the covariance matrix of data
. 1.
X, then the projection of the data onto u is such that:
. 1.
• Xu has the largest variance,
1.
• where this is subject to the normalising constraintuTu =1.
1 1 .
We can re-express these using mathematical equations and prove that the direction
of the first principal component of the data is the direction with the largest
eigenvalue:
• First, the variance ofXu can be written asvar(Xu ), which is equal touT(cid:2)u
1. 1 . 1 1.
according to Eqs.(7.1) and (7.2), where (cid:2) = cov(X) is a symmetric matrix.
.
Hence,uT(cid:2)u is the objective function we wish to maximise.
1 1.
• We want to maximise the objective function subject to the constraint that uTu =
1 1
1. Hence, this is an optimisation problem with constraints.
.
Applying the Lagrange multiplier method (see Sect.6.2.2 of Chap.6), the new
objective function is shown as follows:
F =uT(cid:2)u −λ (uTu −1), (7.7)
. 1 1 1 1 1 1


================================================================================
PAGE 204
================================================================================

192 7 Algorithms1:PrincipalComponentAnalysis
where λ is a Lagrange multiplier. The task has been converted to maximising F
1. 1.
with respect to both u and λ .
1. 1.
Similarly, we can set up an objective function for all other principal components.
For example, for the second principal component ( u ), we need to maximise
2.
var(Xu )=uT(cid:2)u subject to certain constraints. However, this time, we have not
2 2 2.
only uTu = 1 but also uTu = 0, since we are looking for a coordinate system,
2 2 . 2 1 .
where axes are perpendicular to each other. Again, applying the Lagrange multiplier
method, the new objective function is shown as follows:
F =uT(cid:2)u −λ (uTu −1)−ρuTu , (7.8)
. 2 2 2 2 2 2 2 1
where λ and ρ are Lagrange multipliers.
2. .
We now have two objective functions, F and F , that we can maximise to get
1. 2.
the first and second largest variances.
7.4 The Formulation of Principal Component Analysis
Let us maximise each of the functions in turn.
7.4.1 The First Principal Component
To find the maximum of F , we calculate the partial derivative ∂F1 from Eq.(7.7):
1. ∂u1 .
∂F
1 =2(cid:2)u −2λ u , (7.9)
. 1 1 1
∂u
1
where we have applied Eqs.(7.4) and (7.5).
Setting the partial derivative (7.9) to zero gives us the following:
2(cid:2)u −2λ u =0.
. 1 1 1
That is,
(cid:2)u =λ u . (7.10)
. 1 1 1
As can be seen, Eq.(7.10) coincides with the definition of eigendecomposition
(see Definition 4.1 of Chap.4) since (cid:2) is a square matrix. This tells us that the
.
solution for the first principal component u , an eigenvector satisfying Eq.(7.10),
1.
points to the direction of maximum variance. Hence, the direction of the first
principal component of the data is the direction with the largest variance, as claimed.
So how big is this variance?


================================================================================
PAGE 205
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 193
Sincevar(Xu )= uT(cid:2)u , if we substitute(cid:2)u =λ u to the variance ofXu
1 1 1. 1 1 1. 1.
and consider the constraint condition ofuTu =1, we can obtain
1 1 .
var(Xu )=uT(cid:2)u =uTλ u =λ uTu =λ .
. 1 1 1 1 1 1 1 1 1 1
This says that the variance of data projections along the first principal component
equals the eigenvalue λ of the first principal component. Since the direction of the
1.
first principal component u captures the largest variation in the data projections,
1.
which is proved to be λ , we can say that the first principal component has the
1.
largest variance among all principal components.
7.4.2 The Second Principal Component
We now consider the second principal component and calculate the partial derivative
∂F2 from Eq.(7.8). By applying Eqs.(7.4), (7.5) and (7.6), we have
∂u2 .
∂F
2 =2(cid:2)u −2λ u −ρu . (7.11)
. 2 2 2 1
∂u
2
Setting the partial derivative (7.11) to zero gives us the following:
2(cid:2)u −2λ u −ρu =0. (7.12)
. 2 2 2 1
Multiplying uT from the left side on both sides of Eq.(7.12), we have
1.
uT2(cid:2)u −uT2λ u −uTρu =0.
. 1 2 1 2 2 1 1
If we take scalars, including Lagrange multipliers in front of vectors, we obtain
2uT(cid:2)u −2λ uTu −ρuTu =0. (7.13)
. 1 2 2 1 2 1 1
SinceuTu =0anduTu =1, from the equation above, we have
1 2 . 1 1 .
ρ =2uT(cid:2)u . (7.14)
. 1 2
We will now show that, in fact,ρ =0. First, multiplying uT from the left side on
. 2.
both sides of Eq.(7.10), it gives us the following:
uT(cid:2)u =uTλ u =λ uTu =0. (7.15)
. 2 1 2 1 1 1 2 1
However, from the matrix transpose rule (see Sect.3.3.11.1 of Chap.3), we have
(uT(cid:2)u )T = uT(cid:2)u . Note that the covariance matrix (cid:2) is symmetrical, and the
2 1 1 2. .


================================================================================
PAGE 206
================================================================================

194 7 Algorithms1:PrincipalComponentAnalysis
transpose of a symmetrical matrix is itself. Therefore, we also now haveuT(cid:2)u =0
1 2 .
since from Eq.(7.15)uT(cid:2)u =0. Together with Eq.(7.14), we obtainρ =0.
2 1 . .
Further, substitutingρ =0into Eq.(7.12), gives us
.
2(cid:2)u −2λ u =0 →(cid:2)u =λ u . (7.16)
. 2 2 2 2 2 2
Hence, we have shown that the solution for the second principal component u ,
2.
given by Eq.(7.16), gives us the direction of the second most maximum variance.
Furthermore, since var(Xu ) = uT(cid:2)u , if we substitute (cid:2)u = λ u to the
2 2 2. 2 2 2.
variance ofXu and consider the constraint condition ofuTu =1, we can obtain
2. 2 2 .
var(Xu )=uT(cid:2)u =uTλ u =λ uTu =λ .
. 2 2 2 2 2 2 2 2 2 2
It says that the variance of data projections along the second principal component
equals the eigenvalue of the second principal component (see Eq.(7.16)).
7.4.3 Data Normalisation
Let us complete Examples 7.1 and 7.2 given earlier in Sect.7.2 of this chapter. We
will find the principal components three times in Example 7.1 to illustrate some
important facts about the process.
Example 7.6 Example 7.1 continued—part 1
First, we take the data as given and found in Example 7.1.W eh av e
⎡ ⎤
32
⎢ ⎥
⎢43⎥
⎢ ⎥
.
X =⎢
⎢
21⎥
⎥
,
⎣22⎦
42
and the covariance matrix is
(cid:8) (cid:9) (cid:8) (cid:9)
1 42 1 1
(cid:2) =cov(X)= = 2 ,
. 4 22 1 1
2 2
where the total variance in the two features (columns) is 1.5, 1 for the first
.
column and 1 for the second column.
2.
(continued)


================================================================================
PAGE 207
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 195
Example 7.6 (continued)
To find the direction of maximum variance, we have proved that it is
in the direction u of the first eigenvector when using principal component
1.
analysis oncov(X)and has the value given by the largest eigenvalue, namely,
.
the eigenvalue λ . Similarly, the second most maximum variance is in the
1.
direction u of the second eigenvector and has the value given by the second
2.
largest eigenvalue, λ , found using principal component analysis oncov(X).
2. .
So, we need to carry out a principal component analysis.
The characteristic polynomial of (cid:2)is found via the following:
.
(cid:8) (cid:9)
1−λ 1
(cid:2)−λI= 2 ,
. 1 1 −λ
2 2
1 1 1
|(cid:2)−λI|=(1−λ)( −λ)− = (4λ2−6λ+1).
.
2 4 4
The eigenvalues are obtained by solving
4λ2−6λ+1=0,
.
√ √
which givesλ = 1(3+ 5)andλ = 1(3− 5)as the eigenvalues of (cid:2).
1 4 . √2 4 . .
So, the largest eigenvalue is 1(3+ 5)=1.31, capturing about87.3%of the
4 . √ .
total variation (which is 1.5), and the second largest is 1(3− 5) = 0.19,
. 4 .
capturing about12.7%
.
of the tota√l variation.
Find u by usingλ = 1(3+ 5)in(cid:2)−λIand solving((cid:2)−λI)u=0:
1. 1 4 . . .
(cid:8) √ (cid:9)(cid:8) (cid:9) (cid:8) (cid:9)
1 1− 5 2 √ u 1 = 0 .
. 4 2 −1− 5 u 0
2
That is, :
(cid:15) √
(1− 5)u +2u =0
1√ 2
.
2u −(1+ 5)u =0.
1 2
√
The solution to the above simultaneous equations isu =1+ 5andu =2.
1 . 2 .
(cid:8) So,√ th(cid:9)e direction of maximum vari(cid:8)ance (cid:9)is the first eigenvector u 1 =
1+ 5 0.85
, which has unit vectoruˆ = , at approximately 32 degrees
. 1 .
2 0.53
to the x-axis.
(continued)


================================================================================
PAGE 208
================================================================================

196 7 Algorithms1:PrincipalComponentAnalysis
Example 7.6 (continued)
√ √
A similar calculation forλ = 1(3− 5)givesu =1− 5andu =2.
2 4 . 1 . 2 .
So,(cid:8) the d√irec(cid:9)tion with the second largest va(cid:8)riance is(cid:9) the second eigenvector
1− 5 −0.53
u = , which has unit vectoruˆ = .
2 . 2 .
2 0.85
The original data with the two principal component directions u and u are
1. 2.
shown in Fig.7.1a, and the data as projected onto the two principal component
directions are shown in Fig.7.1b.
Example 7.7 Example 7.1 continued—part 2
Next, we will take the dataset Xand make it zero mean. That is, each column
.
has a zero mean. This is done by subtracting the mean of the column from
each item, giving
⎡ ⎤
0 0
⎢ ⎥
⎢ 1 1 ⎥
⎢ ⎥
.
X =⎢
⎢
−1−1⎥
⎥
.
⎣−1 0 ⎦
1 0
Now, if we calculate the covariance matrix, we get
(cid:8) (cid:9)
1 42
(cid:2) =cov(X)= .
.
4 22
This is the same covariance matrix as shown in Example 7.1, which is not
surprising since covariance is calculated by taking each item and subtracting
the mean.
Hence, the solution to this is identical to the previous calculation. It is
usual to subtract the mean because it gives smaller numbers and has axes at
the centre of the picture. It is conventional to do this and will be expected, so
it is always done.
The original data that has been made zero mean with the two principal
component directions u and u are shown in Fig.7.2a, and the data as
1. 2.
projected onto the two principal component directions are shown in Fig.7.2b.


================================================================================
PAGE 209
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 197
Fig. 7.1 The left panel illustrates the original data along with the two principal component
directions, u1. and u2.; the right displays the data projected onto these two principal component
directions
Fig. 7.2 The left panel illustrates the original, zero-mean, data along with the two principal
component directions, u1. and u2.; the right panel displays the data projected onto these two
principal component directions
Example 7.8 Example 7.1 continued—part 3
Now, we also normalise each column by dividing by the standard deviation
of each column. This means that the total variance of each column will be 1.
(continued)


================================================================================
PAGE 210
================================================================================

198 7 Algorithms1:PrincipalComponentAnalysis
Example 7.8 (continued)
This gives the following form to the data:
⎡ ⎤
0 √0
⎢ ⎥
⎢
⎢
1 √2 ⎥
⎥
.
X =⎢
⎢
−1− 2⎥
⎥
.
⎣− 1 0⎦
1 0
Calculating the covariance matrix, we get
(cid:13) (cid:14)
1 √1
(cid:2) =cov(X)= 2 ,
. √1 1
2
where the total variance in the two features is 2.
The characteristic polynomial of (cid:2)is obtained via the following:
.
(cid:13) (cid:14)
1−λ √1
(cid:2)−λI= 2 ,
. √1 1−λ
2
1 1
|(cid:2)−λI|=(1−λ)(1−λ)− = (2λ2−4λ+1).
.
2 2
The eigenvalues are obtained by solving
2λ2−4λ+1=0,
.
√
which gives the largest eigenvalue as λ = 2+ 2 = 1.71, capturing about
1 2 . √
85.4%of the total variation, and second largest eigenvalue as λ = 2− 2 =
. 2 2
0.29, capturing about14.6%of the total variation.
. .
Finally, we use λ to find the direction of the maximum variance, u , by
1. 1.
solving
(cid:13) (cid:14)(cid:8) (cid:9) (cid:8) (cid:9)
−√1 √1
u 0
2 2 1 = .
. √1 −√1 u 0
2
2 2
That is,
⎧
⎨−√1 u + √1 u =0
1 2
2 2
. ⎩ √1 u − √1 u =0.
1 2
2 2
(continued)


================================================================================
PAGE 211
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 199
Example 7.8 (continued)
The solution to the above simultaneous equations is u = u . Therefore,
1 2.
any non-zero vector satisfying the condition u = u is a solution to the
1 2.
eigenvector. For instance,u
1
= 1
.
and
(cid:8)
u
2(cid:9)
= 1
.
. So, the direction of max(cid:13)imu(cid:14)m
√1
1
variance is the first eigenvectoru = , which has unit vectoruˆ = 2 ,
1 1 . 1 √1 .
2
which is at 45 degrees to the x-axis.
A similar calculation for λ givesu = −u . Again, any non-zero vector
2. 1 2.
satisfying the condition u1 = −u2 is a solution to the eigenvector. For
.
instance, u
1
= −1
.
and u
2
= 1
.
. So, the(cid:8) dire(cid:9)ction with the second largest
−1
variance is the second eigenvector u = , which has unit vector uˆ =
2 . 2
1
(cid:13) (cid:14)
−√1
2 .
√1 .
2
These values and directions are different from the original ones and show
that normalisation does have an effect. In this case, it has increased the
importance of the second feature of X so that the first eigenvector is rotated
.
round to 45 degrees from the x-axis from 32 degrees as before. The original
data that has been made zero mean and normalised with the two principal
component directions u and u are shown in Fig.7.3a, and the data as
1. 2.
projected onto the two principal component directions are shown in Fig.7.3b.
Fig. 7.3 The left panel illustrates the original, zero-mean, normalised, data along with the two
principal component directions, u1. and u2.; the right panel displays the data projected onto these
two principal component directions


================================================================================
PAGE 212
================================================================================

200 7 Algorithms1:PrincipalComponentAnalysis
Example 7.9 Example 7.2 continued
Again starting with the raw data, we have found that for our data,
⎡ ⎤
234
⎢ ⎥
⎢101⎥
⎢ ⎥
⎢432⎥
X=⎢ ⎥,
. ⎢122⎥
⎢ ⎥
⎣ ⎦
221
222
the covariance matrix is
⎡ ⎤
641
. (cid:2) =cov(X)= 1⎣ 464 ⎦ .
5
146
Now, to find the eigenvalues and eigenvectors for this matrix involves solving
a cubic equation and solving three simultaneous equations. As in all realistic
exercises, this is done with the aid of suitable programs on a computer. So, for
the sake of completeness, this has been done and gives the three eigenvalues
in descending order:
λ =2.44,λ =1,λ =0.16.
. 1 2 3
And it gives the three unit eigenvectors corresponding to these eigenvalues as
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
0.52 −0.71 0.48
. u 1
=⎣
0.67
⎦
u 2
=⎣
0
⎦
u 3
=⎣−0.74 ⎦
.
0.52 0.71 0.48
So, u is the direction of most variance, u is the direction of the second most
1. 2.
variance, and u is the direction of least variance.
3.
If we were to plot the data projected onto the first and second principal
components, then we would capture2.44+1=3.44out of the total of 2.44+
.
1+0.16=3.6variance, that is,95.6%of the total.
. .
Now, we convert each column to have a zero mean and a unit variance.
After making each column zero mean, each column has a variance of 6 (cid:4)= 1.
√ 5 .
So, we divide each element by the standard deviation, which is √6, and the
.
5
(continued)


================================================================================
PAGE 213
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 201
Example 7.9 (continued)
data becomes
⎡ √ √ ⎤
0 √5 2√5
⎢ √ √6 √6 ⎥
⎢
⎢
−√5 −2√5 −√5⎥
⎥
⎢ √6 √ 6 6⎥
. X = ⎢ ⎢ ⎢ 2√ √6 5 √5 6 0 ⎥ ⎥ ⎥,
⎢ ⎢ −√5 0 0 ⎥ ⎥
⎢ 6 √ ⎥
⎣ 0 −0 √5⎦
6
0 0 0
and the covariance matrix is
⎡ ⎤
1 2 1
. (cid:2) =cov(X)=⎣ 3 2 1 3 2 6 3 ⎦ .
1 2 1
6 3
So again, solving using a computer, we get
λ =2.03,λ =0.83,λ =0.14.
. 1 2 3
And it gives the three unit eigenvectors corresponding to these eigenvalues as
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
0.52 −0.71 −0.48
. u 1
=⎣
0.67
⎦
u 2
=⎣
0
⎦
u 3
=⎣
0.74
⎦
.
0.52 0.71 −0.48
Figure 7.4 shows the normalised data and the eigenvectors in the data space.
Figure 7.5 presents the directions of the principal components (eigenvectors)
in the PCA space. Both figures show u in red, u in green, and u in blue.
1. 2. 3.
Notice that the three eigenvectors, the three principal components, are the
same as previously. This is because the original data was chosen so that each
column had the same variance. Therefore, dividing by the standard deviation
meant dividing all the values by the same amount. Not surprisingly, this had
noeffectontheprincipalcomponent directions.Theeigenvalues, orvariances,
are different but are all, in fact, the original ones divided by the variance
(which was 6 = 1.2). For real problems, having the same variance to start
5 .
with will not be the case! So, dividing by the standard deviation is important.
(continued)


================================================================================
PAGE 214
================================================================================

202 7 Algorithms1:PrincipalComponentAnalysis
Example 7.9 (continued)
Again, if we were to plot the data projected onto the first and second
principal components, then we would capture2.03+0.83 = 2.86out of the
.
total of2.03+0.83+0.14=3variance, that is,95.3%of the total.
. .
Figure 7.6 shows projections of the normalised data in the PCA space, from
left to right, displaying PC1 against PC2, PC2 against PC3, and PC1 against
PC3, respectively. We can see that the largest range among projections along
each principal component axis decreases from PC1 to PC3.
Fig. 7.4 The normalised data
and the three eigenvector
directions ( u1.in red, u2.in
green, and u3.in blue) in the
data space
Fig. 7.5 The PCA space
represented by the
eigenvectors ( u1.in red, u2.in
green, and u3.in blue)


================================================================================
PAGE 215
================================================================================

7.4 TheFormulationofPrincipalComponentAnalysis 203
Fig. 7.6 Projections of the normalised data in the PCA space: (from left to right) PC1 versus PC2,
PC2 versus PC3, and PC1 versus PC3
Remark 7.1 As mentioned in Example 7.8, one can find more than one non-zero
eigenvector for a corresponding eigenvalue. That is, there may be more than one
solution for the required unit vector or principal component as long as the condition
solved from the simultaneous equations is satisfied(cid:13). Fo(cid:14)r example, f(cid:13)or a co(cid:14)ndition
√1 −√1
u = u , the principal component may be uˆ = 2 , or uˆ = 2 . Both
1 2. 1 √1 . 1 −√1 .
2 2
vectors lie on the same line but point in opposite directions. It does not affect
visualising the structure in the data when projecting the same data in these two
directions, though one visualisation plot may seem to be a flip of the otherone. (cid:2)
.
Remark 7.2 As mentioned in Example 7.7, it is expected that you should centre
the data by making each feature have a zero mean. It centres the picture and which
makes it easier to interpret. It is also expected that you should normalise the data
by making each feature (column) have a standard deviation, or variance, of 1,
as mentioned in Example 7.8. This is so that one feature does not dominate the
calculation just because it has much larger values. As we have demonstrated here
for really simple data, these two tasks are often not really needed, but do not get
mislead—for real data, they are important tasks to perform. (cid:2)
.
Exercise
7.4 Find the principal components for the following data with and without
normalisation (having both zero means and unit standard deviations):
(1)
⎡ ⎤
34
⎢ ⎥
⎢42⎥
⎢ ⎥
.
X =⎢
⎢
23⎥
⎥
.
⎣22⎦
44
(continued)


================================================================================
PAGE 216
================================================================================

204 7 Algorithms1:PrincipalComponentAnalysis
(2)
⎡ ⎤
22
⎢ ⎥
⎢12⎥
⎢ ⎥
⎢43⎥
X=⎢ ⎥.
. ⎢10⎥
⎢ ⎥
⎣ ⎦
23
22
(3) If you are feeling brave, try this larger one. In fact, it is not too difficult
since f our of the nine values in the covariance matrix are zero, and the
ones on the main diagonal are all the same. This makes getting the first
eigenvalue easy, and the other two are found by factorising a quadratic
equation. Hence, it is possible to do i tbyhand.
⎡ ⎤
254
⎢ ⎥
⎢423⎥
⎢ ⎥
⎢222⎥
X=⎢ ⎥.
. ⎢325⎥
⎢ ⎥
⎣ ⎦
542
232
7.5 Case Study 2 from Chap.1: Continued
We are now ready to answer those five questions asked in Sect.1.3.2 of Chap.1.
1. What are those principal components (PC) axes?
Principal component axes are the eigenvectors computed via eigendecomposition
on the data covariance matrix.
2. What is the relationship between those PCs and the original four features in the
dataset?
Recall in Sect.4.2.3 of Chap.4 that we can obtain positions of data projections
along the first PC axis using the following equation:
. p rojected_data=X n×d u d×1 =u 11 x ,1 +u 21 x ,2 +···+u d1 x ,d ,
where x ,i. is the ith column ofX n×d . . A more general expression to each principal
component is
pc=Xu =u x +u x +···+u x , wherei =1, ··· ,d.
. i 1i ,1 2i ,2 di ,d


================================================================================
PAGE 217
================================================================================

7.5 CaseStudy2fromChap.1: Continued 205
It shows that each PC is a linear combination of all d features, weighted by
the element in the corresponding eigenvector. Note that the number of elements
of each eigenvector (d) is determined by the number of features included in the
data covariance matrix, that is, the number of columnsinX n×d. , which is also d.
3. Why is it necessary to report the variance percentage value?
Now, we know that the principle behind the PCA analysis is to find the direction
that can capture the most significant variance among the data projections in the
PCA space. When doing feature extraction using PCA, reporting how much
percentage of the total variance has been captured by each PC will help us to
decide on how many features to use. Note that each feature extracted via PCA is
a linear combination of all the original features. When visualising the data using
PCA, reporting how much percentage of the total variance has been captured,
especially by the first two PCs, will give us a sense of whether this linear data
visualisation method is a suitable way to visualise the data.
4. How is the variance percentage value calculated?
This has been shown in Sect.4.2.3 of Chap.4: the amount of information
contained in the ith principal component is calculated as (cid:10)λi . However, it
λj .
should be clear now why the eigenvalue is used when calculating the variance
percentage.
5. How is the position of each data in the coordinate plane determined?
This is similar to point (2). In practice, first, we remove the mean value from
each feature in the dataset. Then, we substitute the corresponding data values
and eigenvector elements into the following equation to obtain the coordinate
value in the principal component space:
projected_data=u x +u x +···+u x .
. 1 1 2 2 d d


================================================================================
PAGE 218
================================================================================

Chapter 8
Algorithms 2: Linear Regression
This is the second of three chapters that aim to show how we can apply the
knowledge introduced in previous chapters to formulate three widely used algo-
rithms in the Data Science field. This chapter applies the least-squares technique
for formulating a simple linear algorithm. This algorithm aims to find a linear
relationship between variables that will enable us to estimate the new value of
one variable, called the dependent variable, given new values for one or more
independent variables.
8.1 Simple Linear Regression Algorithm
Linear regression is a technique that statisticians use to describe the relationship
between a dependent variable, also called a regressor, and one or more independent
variables, also called predictors. Figure 8.1 shows the first three chemicals (rep-
resented by crosses) displayed in Table 1.2 of Chap.1. In this example, we want
to estimate enhancement ratios in terms of the molecular weights of chemicals.
The molecular weight is an independent variable, and the enhancement ratio is the
dependent variable. The linear regression algorithm aims to fit a linear line among
the data. However, many straight lines can be fit, for example, the three dashed lines
shown in Fig.8.1. Which one shall we use? Since we want to use the linear line to
estimate values for the dependent variable, we need to find the one that can provide
the estimations as accurately as possible.
Let us start with a simple form with only one independent variable, illustrated in
Fig.8.2. Since we are in two dimensions, then this is a simple straight line, and it
can be mathematically expressed as follows:
f (x)=a +a x, (8.1)
. a 0 1
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 207
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_8


================================================================================
PAGE 219
================================================================================

208 8 Algorithms2:LinearRegression
Fig. 8.1 Three possible
linear regression lines
illustrating the relationship
between molecular weights
and enhancement ratios. The
three cross markers represent
the available chemical
compounds
Fig. 8.2 An illustration of a
linear regression line
where a is the intercept, a is the slope of the line, and x is the variable for which
0. 1.
we have observations, the independent variable. It is similar to the one shown in
Sect.2.3.2.1 of Chap.2. However, here we use a subscript letter a to indicate that
.
a ={a ,a }is the parameter set we need to estimate.
0 1 .
8.2 Least-Squares Estimation
How can we find suitable values for a and a in Eq.(8.1) from the given data?
0. 1.
Ideally, we want to select values for a and a , such as the line can pass through
0. 1.
all the given data points. However, it is impossible in most real-world applications
since data points do not exist on the same line. Therefore, all we can do is to choose
values for a and a suchthatthedifferencesbetween estimationsfromthefittedline
0. 1.
f(x)and actual measurements are as small as possible. This is another optimisation
.


================================================================================
PAGE 220
================================================================================

8.2 Least-SquaresEstimation 209
problem. That is, we want to minimise the sum of differences over all the given data
with respect to a and a .
0. 1.
Now, the question is how we construct the objective function involving the
sum of differences. The differences will be expressed in terms of how far the
observation of the dependent variable is from what the straight line calculates for
that corresponding value of the independent variable. Suppose there are N data
points (x ,y ),i = 1,...,N, where x is the independent variable and y is the
i i .
dependent variable. For any data point (x ,y ), the actual value of the dependent
i i .
variable is y , and the value given by the line, the estimated value of y , is given by
i. i.
a +a x .Thepossibleobjective functionswillbeintermsofsumsofthedifferences
0 1 i.
between these two values, that is, the sums ofy −(a +a x )in some form. Three
i 0 1 i .
possible objective functions are
(cid:2)N
1
Q1(a ,a )= (y −(a +a x )), (8.2)
. 0 1 i 0 1 i
N
i=1
(cid:2)N
1
Q2(a ,a )= |y −(a +a x )|, (8.3)
. 0 1 i 0 1 i
N
i=1
and
(cid:2)N
1
Q(a ,a )= (y −(a +a x ))2. (8.4)
. 0 1 i 0 1 i
N
i=1
The problem with Q1 is that the differences or errors are signed values, and positive
values can cancel with negative values when adding up all errors. The problem
with Q2 is that the absolute value function is not differentiable (recall Example 5.7
in Chap.5). Thus, it is not convenient for further analysis. The problem with Q
is that differences are not weighted equally. That is, large differences are given
more weight than smaller differences. There are problems with all the above three
objective functions. However, Q is not so bad compared with the other two as
differences cannot be cancelled and it is differentiable. Therefore, we minimise Q
with respect to a and a to find the line that best fits the data. This is called the
0. 1.
least-squares method. By convention, Q is divided by 2 as shown as follow s:
(cid:2)N
1
Q(a ,a )= (y −(a +a x ))2. (8.5)
. 0 1 i 0 1 i
2N
i=1


================================================================================
PAGE 221
================================================================================

210 8 Algorithms2:LinearRegression
8.2.1 Deriving the Estimates Using the Least-Squares
Objective Function
Finding the estimates of the parameters of the regression models means we need
to minimise Eq.(8.5). We set the partial derivatives of Q with respecttoa and a
0. 1.
equal to zero:
(cid:2)N
∂Q
= (y −(a +a x ))(−x )=0, (8.6)
. i 0 1 i i
∂a
1 i=1
(cid:2)N
∂Q
= (y −(a +a x ))(−1)=0. (8.7)
. i 0 1 i
∂a
0 i=1
Note that to calculate the above partial derivatives, we have applied the addition and
chain rules for differentiation s(cid:3)hown in Sect.5.2.2 of Chap.5. This does not look
very easy due to the use of the sign. However, it just means many terms like the
.
first term,(y −(a +a x ))2. Each can be differentiated by using the chain rule for
1 0 1 1 .
the squared part, giving two parts u2andu=y −(a +a x ). After differentiating,
. 1 0 1 1 .
all the parts would then be just summed up again. In fact, in the next part of the text,
remember that it is just lots of terms conveniently summed together.
To obtain their mathematical expressions, we can rewrite Eqs.(8.6) and (8.7),
respectively, as follows:
(cid:4) (cid:5) (cid:4) (cid:5)
(cid:2)N (cid:2)N (cid:2)N
x2 a + x a = x y ,
. i 1 i 0 i i
i=1 i=1 i=1
(cid:4) (cid:5) (cid:4) (cid:5)
(cid:2)N (cid:2)N (cid:2)N
x a + 1 a = y .
. i 1 0 i
i=1 i=1 i=1
Further, we can write the above two equations in a matrix equation as was implied
in Sect.3.3.5 of Chap.3:
(cid:6)(cid:3) (cid:3) (cid:7)(cid:6) (cid:7) (cid:6)(cid:3) (cid:7)
N x2 N x a N x y
(cid:3)i=1 i (cid:3)i=1 i 1 = (cid:3)i=1 i i . (8.8)
. N x N 1 a N y
i=1 i i=1 0 i=1 i
Now, to solve simultaneous equations of the form Aa = x, where A is a matrix,
. .
and aand xare vectors, we need to multiply the left-hand side of both sides of the
. .
equation by the inverse matrix to A, namely,A −1,g ivinga=A −1x.
. . .


================================================================================
PAGE 222
================================================================================

8.2 Least-SquaresEstimation 211
So, to obtain estimates for a and a , we need to compute the inverse of
0. 1.
(cid:6)(cid:3) (cid:3) (cid:7)
N x2 N x
(cid:3)i=1 i (cid:3)i=1 i .
. N x N 1
i=1 i i=1
To do so, we need to determine whether its inv(cid:3)erse exists. Let us denote this matrix
as A, and the mean of x as x¯, which isx¯ = 1 N x .
. . N i=1 i.
To show that the inverse exists, we must show that the determinant of Ais non-
.
zero. A standard way to do this is to show that the determinant is the square of
something or the sum of lots of squares where none, or not all, of the square terms
could be zero. This works because all the non-zero square terms are positive, so they
cannot cancel with any negative terms to give an overall total of zero. This is what
we will do in the following calculation.
Note that in this calculation, we use another mathematical trick: adding and
subtracting the same thing in an expression so that we can reorganise the expression
into a convenient form. We will do this in the fourth line of the following (Eq.8.9)
by adding and subtracting x¯2.
.
The determinant of Ais computed as follows:
.
(cid:2)N (cid:2)N (cid:2)N (cid:2)N
detA= x2· 1− x · x
i i i
i=1 i=1 i=1 i=1
(cid:2)N
=N x2−(Nx¯)2
i
i=1
(cid:4) (cid:5)
(cid:2)N
1
=N2 x2−x¯2
N i
i=1
(8.9)
. (cid:4) (cid:5)
(cid:2)N
1
=N2 x2−2x¯2+x¯2
N i
i=1
(cid:4) (cid:5)
(cid:2)N (cid:2)N (cid:2)N
1 1 1
=N2 x2−2x¯ x + x¯2
N i N i N
i=1 i=1 i=1
(cid:2)N
=N (x −x¯) 2.
i
i=1
(cid:3) (cid:3)
We have appliedx¯ = 1 N x andx¯2 = 1Nx¯2 = 1 N x¯2 in the second last
N i=1 i. N N i=1 .
linei nE q. (8.9).
Equation (8.9) shows that as long as all x are not equal, which would make
i.
x =x¯ for all i, the determinant of Awill not be zero and the inverse of Aexists.
i . . .


================================================================================
PAGE 223
================================================================================

212 8 Algorithms2:LinearRegression
Let us(cid:3) do that again withN =2
.
and expand so that we do not have the awkward
looking . signs. (cid:6) (cid:7)
x2+x2 x +x
IfN =2, thenA= 1 2 1 2 andx¯ = 1(x +x ).
. x +x 2 . 2 1 2 .
1 2
detA=2(x2+x2)−(x +x )2
. 1 2 1 2
=2(x2+x2)−(2x¯)2
1 2
=2[(x2+x2)−2(x¯)2]
1 2
=2[(x2+x2)−4(x¯)2+2(x¯)2]
1 2
1
=2[(x2+x2)−4x¯ (x +x )+((x¯)2+(x¯)2)]
1 2 2 1 2
=2[[x2−2x x¯ +(x¯)2]+[x2−2x x¯ +(x¯)2]]
1 1 2 2
=2[(x −x¯) 2+(x −x¯) 2]
1 2
as required.
Since the inverse exists, we can calculate a and a from Eq.(8.8). That is, we
0. 1.
can multiply the inverse of the matrix from the left side on both sides of Eq.(8.8).
We have
(cid:6) (cid:7) (cid:6)(cid:3) (cid:3) (cid:7) (cid:6)(cid:3) (cid:7)
a N x2 N x
−1
N x y
1 = (cid:3)i=1 i (cid:3)i=1 i (cid:3)i=1 i i .
. a N x N 1 N y
0 i=1 i i=1 i=1 i
After calculating the inverse of A, we have the following:
.
(cid:6) (cid:7) (cid:6) (cid:3) (cid:3) (cid:7)(cid:6)(cid:3) (cid:7)
a 1 N 1 − N x N x y
1 = (cid:3) (cid:3)i=1 (cid:3) i=1 i (cid:3)i=1 i i .
. a 0 N N i=1 (x i −x¯) 2 − N i=1 x i N i=1 x i 2 N i=1 y i
Therefore,
(cid:6) (cid:7)
(cid:2)N (cid:2)N (cid:2)N
1
a = (cid:3) N x y − x y , (8.10)
. 1 N N (x −x¯) 2 i i i i
i=1 i i=1 i=1 i=1
and
(cid:6) (cid:7)
(cid:2)N (cid:2)N (cid:2)N (cid:2)N
1
a = (cid:3) x2 y − x x y . (8.11)
. 0 N N (x −x¯) 2 i i i i i
i=1 i i=1 i=1 i=1 i=1


================================================================================
PAGE 224
================================================================================

8.2 Least-SquaresEstimation 213
Remark 8.1 Alternatively, a and a can be rewritten in a more concise way as
1. 0.
follows:
(cid:3)
N (x −x¯)(y −y¯)
a = i= (cid:3)1 i i , (8.12)
. 1 N (x −x¯) 2
i=1 i
and
a =y¯−a ¯x , (8.13)
. 0 1
where x¯ and y¯ are the mean value of x and y, respectively.
. .
We have mainly ignored proofs in this book. However, readers are encouraged to
do the two proofs that the valuesofa in Eqs.(8.10) and (8.12) are the same and that
1.
the values of a
0.
in Eqs.(8.11) and (8.13) are the same as an(cid:3) exercise by th(cid:3)emselves.
T(cid:3)he trick(cid:3)s that may be used in the proof include x¯ =
N
1 x
i.
, Nx¯ = x
i.
, and
x¯ = x . You will find it easier to do the proof for just theN =2case. (cid:2)
i. . .
Toillustratehowtheseformulaeworkandgive yousomeexamples totry,wenow
do some examples and exercises for really small values of N. It should be noted that
for any realistic values of N, this would be calculated using a computer program, as
was the case for real principal component analysis problems in the previous chapter.
The values that satisfy the two simultaneous equations, (8.10) and (8.11), or (8.12)
and (8.13), are the least-squares estimates for a and a and are denoted as aˆ and
1. 0. 1.
aˆ , respectively.
0.
Example 8.1 Find the regression line when we have just two points, so N =
2, where the two points are(2,2)and(4,3).
. . .
Solution The average of the independent variable is x¯ = 2+4 = 3, and the
2 .
average of the dependent variable isy¯ = 2+3 =2.5.
2 .
Using Eq.(8.10), we get
1 1
aˆ = (2×(4+12)−6×5)= .
. 1 2(1+1) 2
Using Eq.(8.11), we get
1
aˆ = (20×5−6×(4+12))=1.
. 0 2(1+1)
(continued)


================================================================================
PAGE 225
================================================================================

214 8 Algorithms2:LinearRegression
Example 8.1 (continued)
Alternately,
using Eq.(8.12), we get
(−1)(−1)+(1)(1) 1
aˆ = 2 2 = .
. 1 (−1)2+(1)2 2
Using Eq.(8.13), we get
5 1
aˆ = − ×3=1.
. 0
2 2
So either way, we get the same answers, that is,aˆ = 1 andaˆ =1.
1 2. 0 .
Of course, with just two different points, you get a unique line that goes
through both points. This is illustrated in Fig.8.3.
Example 8.2 Find the regression line with three points, so thatN = 3. The
.
pointsare(1,2),(2,4), and(3,3).
. . .
Solution The average of the independent variable is x¯ = 6 = 2, and the
3 .
average of the dependent variable isy¯ = 9 =3.
3 .
Again, do it using both sets of equations to show that you get the same
answer:
Using Eq.(8.10), we get
1 3 1
aˆ = (3×(2+8+9)−6×9)= = .
. 1 3(1+0+1) 6 2
Using Eq.(8.11), we get
1 12
aˆ = (14×9−6×(2+8+9))= =2.
. 0 3(1+0+1) 6
Alternately,
using Eq.(8.12), we get
(−1)(−1)+(0)(1)+(1)(0) 1
aˆ = = .
. 1 (−1)2+0+(1)2 2
(continued)


================================================================================
PAGE 226
================================================================================

8.2 Least-SquaresEstimation 215
Example 8.2 (continued)
Using Eq.(8.13), we get
1
aˆ =3− ×2=2.
. 0
2
So either way, we get the same answers, that is,aˆ = 1 andaˆ =2.
1 2. 0 .
This is illustrated in Fig.8.4.
Fig. 8.3 The regression line
for Example 8.1
Fig. 8.4 The regression line
for Example 8.2.


================================================================================
PAGE 227
================================================================================

216 8 Algorithms2:LinearRegression
Exercise
8.1 Find the regression line for the following:
(1) N = 2. Points(2,3)and(3,5).
. . .
(2) N = 3. Points(1,3),(2,2), and(3,4).
. . . .
(3) N = 3. Points(1,4),(2,2), and(3,1.5).
. . . .
(4) N = 3. Points(1,1),(3,4), and(5,4).
. . . .
8.3 Linear Regression with Multiple Variables
So far, we have considered using just one independent variable to estimate the
dependent variable relationship. We now consider having d independent variables.
With one independent variable and one dependent variable, we are working in two
dimensions, and the linear regression line is just a straight line in two dimensions,
namely,f(x)=a +a x. With two independent variables, our regression “line” is
0 1 .
a plane in three dimensions, namely,f(x)=a +a x +a x . This generalises for
0 1 1 2 2.
d independent variables to
(cid:2)d
f (x)=a +a x +a x +···+a x = a x .
. a 0 1 1 2 2 d d j j
j=0
Note that we define x as 1 to give a convenient summation.
0.
So, for each of the N points x = (x ,x , ··· ,x ), this gives the linear
i i1 i2 id .
regression model as follows:
(cid:2)d
f (x )=a +a x +a x +···+a x = a x . (8.14)
. a i 0 1 i1 2 i2 d id j ij
j=0
The objective function is then given by the following:
(cid:2)N (cid:2)d
Q= (y − a x )2. (8.15)
. i j ij
i=1 j=0
So, ford =1, we hav e
.
(cid:2)N
Q= (y −(a +a x ))2,
. i 0 1 i1
i=1


================================================================================
PAGE 228
================================================================================

8.3 LinearRegressionwithMultipleVariables 217
which is the same as previously, apart from the 1 factor, which is a constant, and
2N.
so does not affect things.
And ford =2, we hav e
.
(cid:2)N
Q= (y −(a +a x +a x ))2.
. i 0 1 i1 2 i2
i=1
This is just the sum of the squares of the differences between the real value y and
i.
the corresponding point on the plane, as expected.
If we write the data as a matri⎡x X ⎤. of size ofN ×(⎡d + ⎤1) . , where x 0. is a column
1 a
0
⎢ ⎥ ⎢ ⎥
⎢1⎥ ⎢a 1⎥
vector including N ones, x 0 = ⎢ ⎣ . . ⎥ ⎦ . and a = ⎢ ⎣ . . ⎥ ⎦. , then Eq.(8.14) can be
. .
1 a
N×1 d
rewritten asf (x )=Xa, and Eq.(8.15) can be rewritten as
a i .
Q=(y−Xa)T(y−Xa).
.
Example 8.3 To illustrate the above, consider d = 1 and N = 2. This has
. .
points(x ,y )and(x ,y ). Then, X, a, and yare
11 1 . 21 2 . . . .
(cid:6) (cid:7) (cid:6) (cid:7) (cid:6) (cid:7)
1x a y
X= 11 , a= 0 , y= 1 .
.
1x a y
21 1 2
(cid:6) (cid:7)
a +a x
So, Xais 0 1 11 , where each row isf (x ).
. a +a x . a i .
0 1 21
Hence, we have
(cid:6) (cid:7)
y −(a +a x )
(y−Xa)= 1 0 1 11 ,
. y −(a +a x )
2 0 1 21
and
(cid:14) (cid:15)
. ( y−Xa)T = y 1 −(a 0 +a 1 x 11 ),y 2 −(a 0 +a 1 x 21 ) ,
respectively.
(continued)


================================================================================
PAGE 229
================================================================================

218 8 Algorithms2:LinearRegression
Example 8.3 (continued)
Therefore,(y−Xa)T(y−Xa)is
.
(cid:2)2
(y −(a +a x ))2+(y −(a +a x ))2 = (y −(a +a x ))2.
. 1 0 1 11 2 0 1 21 i 0 1 i1
i=1
(cid:3)
So,(y−Xa)T(y−Xa)= 2 (y −(a +a x ))2 =Qas required.
i=1 i 0 1 i1 .
To obtain a formula for a, we need to find the partial derivative of Q with respect
.
toa. That is,
.
∂Q ∂(y−Xa)T(y−Xa)
=
∂a ∂a
∂(yT −aTXT)(y−Xa)
=
∂a
∂(yTy−aTXTy−yTXa+aTXTXa)
=
(8.16)
. ∂a
∂(yTy−2aTXTy+aTXTXa)
=
∂a
=−2XTy+2XTXa
=−2XT(y−Xa).
From the third line to the fourth line in Eq.(8.16), we have used the property that
the transpose of a scalar is still the scalar itself, that is, the product of aTXTy
.
is a scalar (by checking the size of each factor (see Sect.3.3.3 of Chap.3), and
aTXTy=(aTXTy)T =yTXa. Going from the fourth line to the fifth line, we have
.
differentiated using the two results, Eqs.(7.6) and (7.4), from Chap.7.
Setting the partial derivative to zero, that is,XT(y−Xa)=0, we obtain
.
XTXa=XTy.
.
If the inverse ofXTXexists, then multiplying the inverse from the left side of both
.
sides of the above equation gives
a=(XTX) −1XTy. (8.17)
.
Equation (8.17) is called the normal equation. Applying the normal equation is the
method to solve for aanalytically.
.


================================================================================
PAGE 230
================================================================================

8.3 LinearRegressionwithMultipleVariables 219
Example 8.4 Let us illustrate the partial differentiation result by returning to
our Example 8.3. We have already shown that
Q=(y−Xa)T(y−Xa)=(y −(a +a x ))2+(y −(a +a x ))2,
. 1 0 1 11 2 0 1 21
which is a scalar. We can use the formula for gradient shown in Sect.6.1.4 of
Chap.6. So,
(cid:16) (cid:17) (cid:6) (cid:7)
∂Q ∂Q −2(y −(a +a x ))−2(y −(a +a x ))
. ∂a = ∂ ∂ a Q 0 = −2(y − 1 (a + 0 a x 1 )) 1 x 1 −2(y 2 −(a 0 +a 1 x 21 ))x .
∂a1 1 0 1 11 11 2 0 1 21 21
This has again used the chain rule to substitute for the squared bits in brackets.
If we now substitute X, a, and yinto Eq.(8.16) for computing ∂Q, we can see
. . . ∂a.
that
(cid:6) (cid:7)(cid:6) (cid:7)
1 1 y −(a +a x )
−2XT(y−Xa) =−2 1 0 1 11 .
. x x y −(a +a x )
11 21 2 0 1 21
Multiplying out the matrices, we get
(cid:6) (cid:7)
−2(y −(a +a x ))−2(y −(a +a x ))
1 0 1 11 2 0 1 21 ,
. −2(y −(a +a x ))x −2(y −(a +a x ))x
1 0 1 11 11 2 0 1 21 21
which is the result we got for ∂Q before as required.
∂a.
Before we look at the real way to find these solutions using a gradient descent
algorithm and a computer program, let us look at a couple of simple examples that
can be done by hand to illustrate this result. We will start by re-doing Example 8.2.
Example 8.5 Example 8.2 revisited
Remember, this example hadN =3and was ford =1since we just had one
. .
independent variable. The points were(1,2),(2,4), and(3,3).
. . .
We are going to use the new formula for finding a, namely,
.
a=(XTX) −1XTy.
.
(continued)


================================================================================
PAGE 231
================================================================================

220 8 Algorithms2:LinearRegression
Example 8.5 (continued)
⎡ ⎤ ⎡ ⎤
(cid:6) (cid:7)
11 2
Solution First,X=⎣ 12 ⎦ . andy=⎣ 4 ⎦ . , and we wish to finda= a 0 . .
a
1
13 3
⎡ ⎤
(cid:6) (cid:7) (cid:6) (cid:7) (cid:6) (cid:7)
11
So,XTX= 1 1 1 2 1 3 ⎣ 12 ⎦= 3 61 6 4 . and(XTX) −1 = 6 1 − 14 6 − 3 6 . .
13
⎡ ⎤
(cid:6) (cid:7) (cid:6) (cid:7)
2
AlsoXTy= 111 ⎣ 4 ⎦= 9 . .
123 19
3
(cid:6) (cid:7) (cid:6) (cid:7)(cid:6) (cid:7) (cid:6) (cid:7)
a 14 −6 9 2
So,a= 0 = 1 = .
a 6 −6 3 19 1 .
1 2
Hence,a =2anda = 1 as before.
0 . 1 2.
Example 8.6 We will now do a d = 2 example, which is an example with
.
two independent variables, x and x , and one dependent variable y. We will
1. 2.
do one withN =3. The points are listed in Table 8.1. Apply Eq.(8.17) to find
.
a.
.
⎡ ⎤ ⎡ ⎤
112 1
Solution First, X = ⎣ 121 ⎦ . and y = ⎣ 2 ⎦ . , and we wish to find a =
122 −2
⎡ ⎤
a
0
⎣ ⎦
a 1 . .
a
2
So,
⎡ ⎤⎡ ⎤ ⎡ ⎤
111 112 355
. X
TX=⎣
122
⎦⎣
121
⎦=⎣
598
⎦
,
212 122 589
and
⎡ ⎤
17 −5−5
. ( XTX) −1 =⎣−5 2 1 ⎦ .
−5 1 2
(You can check that last part by showing thatXTX(XTX) −1 =I.)
.
(continued)


================================================================================
PAGE 232
================================================================================

8.4 NumericalComputation:CaseStudy1fromChap.1—Continued 221
Example 8.6 (continued)
⎡ ⎤⎡ ⎤ ⎡ ⎤
111 1 1
AlsoXTy=⎣
122
⎦⎣
2
⎦=⎣
1
⎦
. .
212 −2 0
⎡ ⎤ ⎡ ⎤⎡ ⎤ ⎡ ⎤
a 17 −5−5 1 12
0
So,a=⎣
a 1
⎦=⎣−
5 2 1
⎦⎣
1
⎦=⎣−3 ⎦
. .
a − 5 1 2 0 −4
2
Hence,a =12,a =−3, anda =−4.
0 . 1 . 2 .
Exercise
8.2 These are the same as the last two examples in Exercise 8.1. You should
now calculate the answer using the new method of this section and check you
get the same answer:
(1) d = 1,N =3. Points(1,4),(2,2), and(3,1.5).
. . . . .
(2) d = 1,N =3. Points(1,1),(3,4), and(5,4).
. . . . .
8.4 Numerical Computation: Case Study 1 from
Chap.1—Continued
The normal equation provides a nice way to find the parameters of linear regression
models. RecallXTXis a(d+1)×(d+1)matrix. When d is large, computing the
. .
inverseofXTXcan be very slow. IfXTXis non-invertible, we cannot use the normal
. .
equation. Alternatively, we can apply the gradient descent algorithm described in
Sect.6.2.3 of Chap.6 to obtain estimates.
Table 8.1 Three data points
x1. x2. y
with two independent
1 2 1
variables, x1.and x2., and one
dependent variable, y 2 1 2
2 2 -2


================================================================================
PAGE 233
================================================================================

222 8 Algorithms2:LinearRegression
Table 8.2 The original data and the scaled data
MW:raw_X. Enhancement ratio: y scaledX= ra m w a _ x X − − m m in in .
295 10 0
305 30 1
300 20 0.5
Table 8.3 The initial values
a0. a1. scaledX. y ypred =a0 +a1 ×scaledX. error= 1 2 (y−ypred)2 .
5 1 0 10 5 12.5
1 30 6 288
0.5 20 5.5 105.125
Totalerror:405.625.
In Sect.1.3.1 of Chap.1,w eh av e
⎡ ⎤ ⎡ ⎤
295 10
. r aw_X
=⎣
305
⎦ andy=⎣
30
⎦
.
300 20
We will use this example, where d = 1 and N = 3, to show how the gradient
. .
descent algorithm can be used to estimate a and a of the linear regression model.
0. 1.
We keep four decimal places when it is not divisible.
• Step 1: Data normalisation/scaling. We scale raw_X using
raw_X−min
, where
. max−min .
min and max denote the minimum and maximum values of raw_X. We keep
.
the values of the target variable y unchanged. Note that there are many different
normalisation methods. In this example, we simply rescale data between 0 and 1
(Table8.2).
• Step 2: To fit a liney =a +a ×scaledX, we initialise random values for
pred 0 1 .
a and a and calculate the error given by 1(y −y )2 for each data point.
0. 1. 2 pred .
This error is the same as Q given in Eq. (8.15) when d = 1. For example, we
.
start with the random initial values ofa =5anda =1(Table 8.3).
0 . 1 .
• Step 3: Calculate the partial derivative with respect to a and a , respectively.
1. 0.
The error for each data point is given by
1 1
error= (y−y )2 = (y−(a +a ×scaledX))2.
. pred 0 1
2 2
From Eqs.(8.6) and (8.7), we have the following:
∂error
=−(y−(a +a ×scaledX))×scaledX =−(y−y )×scaledX,
. 0 1 pred
∂a
1


================================================================================
PAGE 234
================================================================================

8.4 NumericalComputation:CaseStudy1fromChap.1—Continued 223
T
pa
a
r
b
ti
l
a
e
l
8
d
.
e
4
r iv
R
at
e
i
s
v
u
e
l t
w
s
i
o
th
f t
r
h
e
e
s pect a0. a1. scaledX. y ypred. error
∂e
∂
r
a
r
0
or
.
∂e
∂
r
a
r
1
or
.
to a1.and a0. 5 1 0 10 5 12.5 − .5 0
1 30 6 288 − .24 − .24
0.5 20 5.5 105.125 − .14.5 − .7.25
Total:405.625. − .43.5 − .31.25
Table 8.5 Results after the first iteration
a0. a1. scaledX. y ypred. error
∂e
∂
r
a
r
0
or
.
∂e
∂
r
a
r
1
or
.
5.435 1.3125 0 10 5.435 10.4196 − .4.565 0
1 30 6.7475 270.3394 − .23.2525 − .23.2525
0.5 20 6.0913 96.7267 − .13.9088 − .6.9544
Total:377.486. − .41.7263 − .30.2069
and
∂error
=−(y−(a +a ×scaledX)) =−(y−y ).
. 0 1 pred
∂a
0
Results are shown in the last two columns of Table 8.4.
• Step 4: Set the learning rate, for example, (cid:3) = 0.01. Update the estimates by
.
applying Eq.(6.7) in Chap.6 with the corresponding total partial derivatives as
follows:
anew =5−0.01×(−43.5)=5.435,
. 0
and
anew =1−0.01×(−31.25)=1.3125.
. 1
• Step 5: Use the updated value for a and a for computing predictions. Then,
0. 1.
calculate new total errors and total partial derivatives.
We can see that the total error has decreased from 405.625 in Table 8.4 to
.
377.485in Table 8.5 after the first iteration using the gradient descent algorithm.
.
As we know, the gradient descent algorithm is an iterative procedure. We have
shown the first iteration in this example. In practice, more iterations are needed
so that the total errors can be minimised and converge to a value as small as
possible.
Of course, this example is here to illustrate how the gradient descent algorithm
works. But because it is a really simple example with d = 1 and N = 3, we can
. .
find a and a directly using Eqs.(8.13) and (8.12) ons caledX and y toget a =10
0. 1. 0 .
and a = 20. The gradient descent values can be seen to be heading in the right
1 .
direction!


================================================================================
PAGE 235
================================================================================

224 8 Algorithms2:LinearRegression
Remark 8.2 Data normalisation is an important step in data pre-processing. In
this example, we have scaled original molecular weights using the minimum and
maximum values among the available molecular weight values. We have not scaled
y, the target values. If we scale the target values, the estimated values from the
fitted regression line need to be transformed back to the original target value space.
For example, if the scaling is done simply by removing the mean value of targets,
then the mean value must be added to the estimated value to obtain the final
prediction. (cid:2)
.
Exercise
8.3 Do the first iteration of the gradient descent algorithm for the following
example:
• d = 1,N = 3. Points(1,1),(3,4), and(5,4). Start with estimates for a
. . . . . 0.
and a usinga =1anda =1. Suppose the learning rate is0.01.
1. 0 . 1 . .
Since the numbers are small, do not bother to scale the values of x (it makes
the calculation easier too). Note that it is the same example as the last exercise
in Exercise8.2—so you know the real answer!
8.5 Some Useful Results
These are properties and formulae that apply once you have found the “regression
line” of best fit and indicate how good your results are.
8.5.1 Residuals
A residual is defined as e = y −y˜ , where y˜ is the estimate of the ith of the N
i i i. i.
points. We have the following useful properties:
• The sum of the residuals is zero.
• The sum of observed target values equals the sum of the estimated values.
If the regression line is found by calculation, then these properties are correct. We
may not find the exact solution for iterative procedures, but one that is close enough
that these two properties are very close to being correct. They give an indication of
how close you have come to the exact solution.


================================================================================
PAGE 236
================================================================================

8.5 SomeUsefulResults 225
Example 8.7 Example 8.1 revisited.
Here, we just had two points and found the unique answer, giving a line that
goes through both points. So, the residuals are both zero, and the observed
target and the estimated values are the same. Thus, both properties are correct.
Example 8.8 Example 8.2 revisited.
Here, we had three points,(1,2),(2,4), and(3,3), and found thata = 1 and
. . . 0 2.
a =2.
1 .
The estimated points on the liney =2+ 1x are the following:
2 .
Forx =1,y˜ = 5;f orx =2,y˜ =3; and forx =3,y˜ = 7.
. 1 2. . 2 . . 3 2.
The sum of the residuals is therefore(2− 5)+(4−3)+(3− 7)=0.
2 2 .
The sumof targets is2+4+3=9, and thesumof estimates is 5+3+7 =9
. 2 2 .
as required.
8.5.2 The Coefficient of Determination
The coefficient of determination, denoted as R2, is defined as follows:
.
(cid:3)
N(y −y˜ )2
R2 =1− (cid:3)i i i ,
. N(y −y¯) 2
i i
where y˜ is the estimate of the ith of the N points and y¯ is the mean value of the
i. .
dependent variable.
The closer the value of R2 is to 1, the better the fit. We have ignored the
.
proof; however, the coefficient of determination can be interpreted as the square of
Pearson’s correlation coefficient (see Sect.4.2.1 in Chap.4) between the observed
target values y and the estimated values y˜ .
i. i.
Example 8.9 Let us do Examples 8.1 and 8.2 again.
For Example 8.1:a llo fy =y˜,s oR2 =1.
i . .
For Example 8.2:y¯ =3and
.
(−1)2+(1)2+(−1)2 1
R2 =1− 2 2 = .
. (1)2+(1)2+0 4


================================================================================
PAGE 237
================================================================================

226 8 Algorithms2:LinearRegression
Exercise
8.4 These data points are the same as Exercise 8.2. In each case, find
(a) the sum of the residuals, (b) the sum of the targets, (c) the sum of the
estimates, and (d) R2:
.
(1) d = 1,N =3. Points(1,4),(2,2), and(3,1.5).
. . . . .
(2) d = 1,N =3. Points(1,1),(3,4), and(5,4).
. . . . .


================================================================================
PAGE 238
================================================================================

Chapter 9
Algorithms 3: Neural Networks
This is the third of three chapters that aim to show how we can apply the knowledge
introduced in previous chapters to formulate three widely used algorithms in
the Data Science field. This chapter considers neural networks. Neural networks
are a huge topic, and there are many textbooks dedicated to describing all the
different types and giving the details of how they work. There are unsupervised
and supervised neural networks dedicated to different tasks. We are only going to
consider one type of supervised neural network: the single-layered and multilayered
perceptrons trained using back-propagation of errors. Knowledge of this type of
network is a good entry point to lots of other networks. This chapter introduces the
basic idea of input data being passed through the network in a forward direction
and the error being propagated backwards through the network with the network
weights being updated using a gradient descent algorithm of the type described in
Sect. 6.2.3 of Chap. 6.
9.1 Training a Neural Network by Gradient Descent
We are leading up to describing the training of a two-layer neural network (NN)
using a gradient descent algorithm. This algorithm uses the gradient of the error
between the outputs of the neural network and the desired target values. It then
adjusts the weights in a neural network by considering the error relative to each
weight by looking backwards through the neural network in a method known
as back-propagation. This is Case Study 3 in Chap. 1. We will do this by first
illustrating the principles on a simple one-layer network.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 227
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_9


================================================================================
PAGE 239
================================================================================

228 9 Algorithms3:NeuralNetworks
9.2 A Simple One-Layer Neural Network
This section explains the training principle behind artificial neural networks using a
simple example of a one-layer neural network with just two inputs and two outputs.
It is a very limited network but will illustrate how the inputs are fed forwards and
the errors fed backwards.
Figure 9.1 shows the architecture of the one-layer neural network used in this
example, where we consider only one input (training) example x, which has two
.
attributes or features x and x . Squares in Fig. 9.1 represent the two input features,
1. 2.
forming the neural network’s input layer. Suppose each input example has two
targets, denoted as t and t . y and y are the outputs or predictions of the neural
1. 2. 1. 2.
network for the given x. We follow the notations used in [5] for weights. That is, we
.
denote each weight asw , where j is the jth output unit and i is the ith node of the
ji.
input layer. Forexample,w denotes the weight going from the first input feature
21.
x to output unit 2 in the output layer. The training of this neural network aims to
1.
adjust weight values to reduce the error, that is, the difference between the targets
and predictions.
In general, the input values are fed to the output nodes as a weighted sum formed
as a dot product of the input vector xand the weight vector w, that is,x·w. Expressed
. . .
in full for our two-node example, we get
a =w x +w x ,
. 1 11 1 12 2
and
a =w x +w x .
. 2 21 1 22 2
The output node can transform this to give an output by using an activation function,
denoted as g, which for these two units can be written as follows:
y = g(a ),
. 1 1
and
y = g(a ).
. 2 2
Fig. 9.1 A simple one-layer
neural network


================================================================================
PAGE 240
================================================================================

9.2 ASimpleOne-LayerNeuralNetwork 229
We are going to consider a linear activation function and then a logistic sigmoid
activation function for g.
Remark 9.1 We can represent the complete operation of finding the inputs to the
first layer of the simple neural network by using the multiplication operation of an
input vector a(cid:2)nd a (cid:3)matrix of(cid:2) weig(cid:3)hts. (cid:2) (cid:3)
w w wT
Letw = 11 ,w = 21 , andW= 1 .Then, we have
1 w . 2 w . wT .
12 22 2
(cid:2) (cid:3) (cid:2) (cid:3)(cid:2) (cid:3)
a wT x
1 = 1 1 .
. a wT x
2 2 2
(cid:2)
.
9.2.1 Linear Activation Function
Figure 9.2 illustrates a linear activation function withy = g(x)= x. Note that this
.
activation function is differentiable andg (cid:2) (x)=1.
.
Applying this linear activation function to our simple one-layer neural network
example, we havey = g(a )=a andy = g(a )=a . With targets t and t , we
1 1 1. 2 2 2. 1. 2.
get the error (E)a s
(cid:5) (cid:5) (cid:6)(cid:6)
(cid:4)2 (cid:4)2 (cid:4)2 (cid:4)2 2
1 1 1
E = (t −y )2 = (t −a )2 = t − w x . (9.1)
. j j j j j ji i
2 2 2
j=1 j=1 j=1 i=1
This error is associated with just one training example, the simplest training method
to explain. So, we are going to update the weights for each input training example.
Fig. 9.2 Linear activation
function


================================================================================
PAGE 241
================================================================================

230 9 Algorithms3:NeuralNetworks
More will be said about this in Sect. 9.5.2. Expanding the equation for E out, we
have
(cid:5)(cid:5) (cid:6) (cid:5) (cid:6) (cid:6)
1 2 2
E = t −(w x +w x ) + t −(w x +w x ) .
. 1 11 1 12 2 2 21 1 22 2
2
We will update the weights by back-propagation of the error. The basic idea is to
apply the gradient descent algorithm (see Sect. 6.2.3 of Chap. 6). That is,
∂E
w ←w −(cid:2) . (9.2)
. ji ji
∂w
ji
So, we need to differentiate Eq. (9.1). Looking at the formula for E, we can see
it contains two terms, so E = 1(E + E ). Each term needs the use of the
2 1 2 .
differentiation of composite functions. Looking at the first term, we haveE = u2,
1 .
whereu=t −(w x +w x ). Let us differentiate with respect tow :
1 11 1 12 2 . 11.
∂E ∂E ∂u
1 = 1 =2u(−x ) =−2(t −(w x +w x ))x =−2(t −y )x .
. 1 1 11 1 12 2 1 1 1 1
∂w ∂u ∂w
11 11
When we work out ∂E2 , we get 0 since E has now in it, and all other variables
∂w11 . 2. 11.
are treated as constants when differentiating with respect tow .
11.
So, adding the two results together and dividing by two, we get
∂E
=−(t −y )x .
. 1 1 1
∂w
11
Repeating this for the other weights, we get
∂E
=−(t −y )x ,
. 1 1 2
∂w
12
∂E
=−(t −y )x ,
. 2 2 1
∂w
21
and
∂E
=−(t −y )x .
. 2 2 2
∂w
22
Or in general:
∂E
=−(t −y )x . (9.3)
. j j i
∂w
ji


================================================================================
PAGE 242
================================================================================

9.2 ASimpleOne-LayerNeuralNetwork 231
Armed with this result, we can then update the weights using Eq. (9.2) as the
following Exampleillustrates.
Example 9.1 We will show how the gradient descent algorithm updates
weights for a really simple example and just one iteration. Assume the initial
input vector isx =x =1, and the target vector ist =0.5andt =0.
1 2 . 1 . 2 .
Also assume thatw =w =0.5,w =w =0.25, and finally(cid:2) =0.1.
11 12 . 21 22 . .
Then,
y =a =w x +w x =1,
. 1 1 11 1 12 2
and
y =a =w x +w x =0.5.
. 2 2 21 1 22 2
So,
1 1
E = ((t −y )2+(t −y )2)= =0.25.
. 1 1 2 2
2 4
Also,
∂E
=−(t −y )x =0.5,
. 1 1 1
∂w
11
∂E
=−(t −y )x =0.5,
. 1 1 2
∂w
12
∂E
=−(t −y )x =0.5,
. 2 2 1
∂w
21
and
∂E
=−(t −y )x =0.5.
. 2 2 2
∂w
22
Using Eq. (9.2), we get the new values for the weights as
w =0.5−(0.1)(0.5)=0.45,
. 11
w =0.5−(0.1)(0.5)=0.45,
. 12
w =0.25−(0.1)(0.5)=0.2,
. 21
(continued)


================================================================================
PAGE 243
================================================================================

232 9 Algorithms3:NeuralNetworks
Example 9.1 (continued)
and
w =0.25−(0.1)(0.5)=0.2.
. 22
So after one iteration, the weights have changed. The new y = 0.9 and
1 .
the newy =0.4. The new error is
2 .
1
E = ((0.5−0.9)2+(0−0.4)2)=0.16.
.
2
Hence, the error was reduced after one iteration.
Remark 9.2 Of course, realistically, applying the gradient descent algorithm
requires many iterations to reduce the error to zero and would be done using
an appropriate computer program.
(cid:2)
.
Exercise
9.1 Here is one for you to try. You will probably need a calculator! Do one
iteration for this one-layer neural network with a linear activation function.
The initial input vector is x = 1 and x = 0; the target is t = 0.25 and
1 . 2 . 1 .
t =0.5.
2 .
Initially,w =w =0.5,w =w =0.25and finally(cid:2) =0.1.
11 12 . 21 22 . .
9.2.2 Logistic Sigmoid Activation Function
Figure 9.3 illustrates a logistic sigmoid activation function with y = g(x)= σ(x)=
1 . Note that this activation function is differentiable.
1+e−x.
Applying the sigmoid activation function to our simple one-layer neural network
example, we havey = g(a )andy = g(a ), whereg(a )= σ(a )= 1 and
1 1 . 2 2 . j j 1+e −aj .
j =1,2. With targets t and t , we get the error as
. 1. 2.
(cid:5) (cid:5) (cid:6)(cid:6)
(cid:4)2 (cid:4)2 (cid:4)2 (cid:4)2 2
1 1 1
E = (t −y )2 = (t −g(a ))2 = t −g w x .
. j j j j j ji i
2 2 2
j=1 j=1 j=1 i=1
(9.4)


================================================================================
PAGE 244
================================================================================

9.2 ASimpleOne-LayerNeuralNetwork 233
Fig. 9.3 Logistic sigmoid
activation function
Again, we are updating the weights for each input training example. Expanding the
equation for E out, we have
(cid:5)(cid:5) (cid:6) (cid:5) (cid:6) (cid:6)
1 2 2
E = t −g(w x +w x ) + t −g(w x +w x ) .
. 1 11 1 12 2 2 21 1 22 2
2
We need to differentiate Eq. (9.4) to update the weights using Eq. (9.2). The only
difference between Eqs. (9.4) and (9.1) is the addition of the function g(a ). So,
j .
when we come to differentiate E after dealing with the square term using u , as
before, we need to find dg = g (cid:2) (a ) before we can get inside the composite
daj j .
function g and differentiate the expression for a in terms of weights and input
j.
values.
Hence, if we considerE = 1(E +E ), then in terms of composite functions,
2 1 2 .
we haveE =u2, whereu=t −g(a )anda =w x +w x .
1 . 1 1 . 1 11 1 12 2.
Now to differentiate with respect tow , we can use
12.
∂E ∂E ∂u ∂a
1 = 1 1 ,
.
∂w ∂u ∂a ∂w
12 1 12
where ∂u = −∂g(a1) = −g (cid:2) (a ). Again differentiating E gives 0 since E does
∂a1 ∂a1 1 . 2. 2.
not containw . So,
12.
∂E
=u(−g (cid:2) (a ))x =−(t −g(w x +w x ))g (cid:2) (a )x =−(t −y )g (cid:2) (a )x .
. 1 2 1 11 1 12 2 1 2 1 1 1 2
∂w
12
In general, we get the following:
∂E
=u(−g (cid:2) (a ))x
j i
∂w
ji
. =−(t −g(w x +w x ))g (cid:2) (a )x (9.5)
j j1 1 j2 2 j i
=−(t −y )g (cid:2) (a )x .
j j j i


================================================================================
PAGE 245
================================================================================

234 9 Algorithms3:NeuralNetworks
Luckily, we already have found the derivative of the sigmoid function from
Example 5.13 of Sect. 5.2.2 of Chap. 5, and we know thatg (cid:2) (x)= σ(x)(1− σ(x)).
.
So,g (cid:2) (a )= σ(a )(1− σ(a )). In particular, we have
j j j .
g (cid:2) (a )= σ(a)(1− σ(a)),
. 1 1 1
and
g (cid:2) (a )= σ(a)(1− σ(a)).
. 2 2 2
So finally,
∂E
=−(t −y )g (cid:2) (a )x =−(t −y )σ(a )(1− σ(a))x ,
. 1 1 1 2 1 1 1 1 2
∂w
12
and in general,
∂E
=−(t −y )g (cid:2) (a )x =−(t −y )σ(a )(1− σ(a ))x .
. j j j i j j j j i
∂w
ji
If we collect all the parts relating to j together and define
δ =(t −y )σ(a )(1− σ(a )), (9.6)
. j j j j j
then we can express the final result as
∂E
=−δ x . (9.7)
. j i
∂w
ji
Readers will see why it is useful to define δ in this way in Sect. 9.4.
j.
Remark 9.3 After defining Eq. (9.5), we have put in the logistic sigmoid forg(x).
.
However, other functions can be used to give different algorithms. The hyperbolic
tangent activation function could be slotted in where nowg(x)= ex−e−x . In fact, if
ex+e−x.
weputing(x)= x, that is, the linear activation function, then sinceg (cid:2) (x) = 1,w e
. .
get all the same results as in the previous section on the linear activation function,
namely, Eq. (9.3).
(cid:2)
.


================================================================================
PAGE 246
================================================================================

9.2 ASimpleOne-LayerNeuralNetwork 235
Example 9.2 To illustrate the logistic sigmoid activation function, g(x) =
σ(x)= 1 , we will do one iteration as we did in Example 9.1. In fact, if
1+e−x.
we take the same start values and targetsasExample9.1, we hav e
x =x =1,t =0.5, andt =0.
1 2 . 1 . 2 .
w =w =0.5,w =w =0.25, and(cid:2) =0.1.
11 12 . 21 22 . .
We keep three decimal places in the following calculation.
First, since
a =w x +w x =1,
. 1 11 1 12 2
and
a =w x +w x =0.5,
. 2 21 1 22 2
we have
y = σ(1)=0.731,
. 1
and
y = σ(0.5)=0.622.
. 2
So,
1
E = ((t −y )2+(t −y )2)=0.220,
. 1 1 2 2
2
Then, using Eq. (9.6),
δ =(t −y )σ(a )(1− σ(a))=(−0.231)×0.731×0.269 =−0.045,
. 1 1 1 1 1
and
δ =(t −y )σ(a )(1− σ(a))=(−0.622)×0.622×0.378 =−0.146.
. 2 2 2 2 2
Also, using Eq. (9.7),
∂E
=−δ x =0.045×1=0.045,
. 1 1
∂w
11
∂E
=−δ x =0.045×1=0.045,
. 1 2
∂w
12
(continued)


================================================================================
PAGE 247
================================================================================

236 9 Algorithms3:NeuralNetworks
Example 9.2 (continued)
∂E
=−δ x =0.146×1=0.146,
. 2 1
∂w
21
and
∂E
=−δ x =0.146×1=0.146.
. 2 2
∂w
22
Finally, we update the weights using Eq. (9.2) to obtain
w =0.5−(0.1×0.045)=0.496,
. 11
w =0.5−(0.1×0.045)=0.496,
. 12
w =0.25−(0.1×0.146)=0.235,
. 21
and
w =0.25−(0.1×0.146)=0.235.
. 22
So after one iteration, we have new weights and will get a new value for an
error ofE =0.215.
.
Exercise
9.2 Here is a harder one for you to try. You will definitely need a calculator!
Do one iteration for this one-layer neural network with a logistic sigmoid
activation function:
The initial input vector is x = 1 and x = 0; the target is t = 0.25 and
1 . 2 . 1 .
t =0.5.
2 .
Initially,w =w =0.5andw =w =0.25. Finally,(cid:2) =0.1.
11 12 . 21 22 . .
These are the same values as in Exercise 9.1, but now you have the
complication of a logistic sigmoid activation function.


================================================================================
PAGE 248
================================================================================

9.3 ASimpleTwo-LayerNeuralNetwork:CaseStudy3fromChap.1 237
9.3 A Simple Two-Layer Neural Network: Case Study 3 from
Chap. 1
We have now set the scene for dealing with Case Study 3 from Chap. 1, a s imple
example of a two-layer neural network with two hidden units only in each layer. This
is a simple two-layer neural network but represents the classic back-propagation
model. The mathematics behind the two-layer neural network follows the same
pattern as the one-layer neural network. However, it is complicated by having two
layers and needing to back-propagate the error to the first-level weights to update
them.
Figure 9.4 shows the architecture of a two-layer neural network used in this
example,whereweconsideroneinput(training)example x,whichhastwoattributes
.
or features x and x only. This figure is identical to Fig. 1.9 in Chap. 1. For the
1. 2.
reader’s convenience, it is repeated here. Notations inFig.9.4 are the same as those
in Fig. 9.1. The two layers mean there are two layers of adaptive weights. The nodes
in between two weight layers are called hidden units. We follow notations used
in [5] for weights, where now we have to distinguish between weights in the two
(l)
layers. That is, we denote each weight asw , where (l)denotes the lth layer, j the
ji. .
jth hidden unit in the corresponding layer or the jth output, and i the ith node of
(1)
the immediate layer to the left. For example, w denotes the weight going from
21.
(2)
the first input feature, x , to hidden unit 2 in the first layer; and w denotes the
1. 12.
weight going from the second hidden unit to the output unit 1 in the second layer.
(1)
z denotes the output of the jth node in the hidden layer. The training of this neural
j .
network again aims to adjust weight values to reduce the error, that is, the difference
between the targets and predictions.
9.3.1 The Feed-Forward Propagation
Each input in the input layer is connected to hidden units via weights of the first
layer. Each hidden unit is a linear combination of the input attributes that are
Fig. 9.4 An illustration of the feed-forward of a simple two-layer neural network


================================================================================
PAGE 249
================================================================================

238 9 Algorithms3:NeuralNetworks
transformed by an activation function. The linear combinations of the two input
attributes can be written as follows:
a (1) =w (1) x +w (1) x ,
. 1 11 1 12 2
and
a (1) =w (1) x +w (1) x .
. 2 21 1 22 2
The transformations by a non-linear activation function, denoted as g , for these two
1.
hidden units can be written as follows:
z (1) =g (a (1) ),
. 1 1 1
and
z (1) =g (a (1) ).
. 2 1 2
(1)
As can be seen, z is the output of a composite function, where the output of a
j .
linear function,a (1) , is the input of a non-linear activation functiong (·). Here, we
j . 1 .
use g as the first activation function in case we want to use different activation
1.
functions in the different layers.
Similarly, for nodes in the output layer, we have
a (2) =w (2) z (1)+w (2) z (1) ,
. 1 11 1 12 2
so
y =g (a (2) )=g (w (2) z (1)+w (2) z (1) ),
. 1 2 1 2 11 1 12 2
and
a (2) =w (2) z (1)+w (2) z (1) ,
. 2 21 1 22 2
so
y =g (a (2) )=g (w (2) z (1)+w (2) z (1) ).
. 2 2 2 2 21 1 22 2


================================================================================
PAGE 250
================================================================================

9.3 ASimpleTwo-LayerNeuralNetwork:CaseStudy3fromChap.1 239
So, in general, the outputs can be written as
(cid:5) (cid:6)
(cid:4)2
y =g w (2) z (1)
k 2 kj j
j=1
(cid:5) (cid:5) (cid:6)(cid:6)
(cid:4)2
. =g 2 w k ( j 2) g 1 a j (1) (9.8)
j=1
(cid:5) (cid:5) (cid:6)(cid:6)
(cid:4)2 (cid:4)2
=g w (2) g w (1) x ,
2 kj 1 ji i
j=1 i=1
where k is the index of the nodes in the second, output, layer, j is the index of the
hidden unit of the first layer, and i is the index of theinputs.
In the feed-forward process, the input information is passed as a forward flow
through the network. Note that the activation function used in different layers for
different hidden units can be the same or different.
9.3.2 The Error Back-Propagation
The weights in any feed-forward network are updated by applying the back-
propagation algorithm. The basic idea is to apply the gradient descent algorithm
(see Sect. 6.2.3 of Chap. 6). In general, this is written as
∂E
w(l) ←w(l) −(cid:2) ,
. nm nm (l)
∂w
nm
where E denotes the error, that is, the difference between the target values and the
neural network outputs, l is the index of the layer, and m and n represent the layers
to the left and right, r espectively.
We update weights layer by layer in the direction from the output layer to the
input layer, which is opposite to the feed-forward propagation. So, in particular, we
use
∂E
w (2) ←w (2)−(cid:2) , (9.9)
. kj kj (2)
∂w
kj
to update the weights from the hidden layer to the output layer. And we use
∂E
w (1) ←w (1)−(cid:2) , (9.10)
. ji ji (1)
∂w
ji
to update the weights from the input to the hidden layer.


================================================================================
PAGE 251
================================================================================

240 9 Algorithms3:NeuralNetworks
9.3.2.1 Updating Weights of the Second Layer
ConsideringFig.9.4,letusagainsupposetheerrormeasureisgivenagainby 1((t −
2 1
y )2+(t −y )2). We update the weights of the second layer, that is, for the weights
1 2 2 .
connecting the hidden nodes to the outputs first.
We compute the error as follows:
(cid:5) (cid:6)
1 (cid:4)2 1 (cid:4)2 2
E = (t −y )2 = t −g (a (2) )
2 k k 2 k 2 k
k=1 k=1
. (cid:5) (cid:5) (cid:6)(cid:6) (9.11)
(cid:4)2 (cid:4)2 2
1
= t −g w (2) z (1) ,
2 k 2 kj j
k=1 j=1
where k in the first summation indicates the index of targets and j in the last
summation is the index of hidden units. Again, we will update the weights after
each input training example.
If you look at Eq. (9.11) and compare it to Eq. (9.4) used in Sect. 9.2.2, you will
see that they are virtually identical. The differences are the naming of indices in the
summations, having superscripts that represent the layer, having g instead of g, and
2.
(1)
having az here instead of the x previously. Then, when we compute the partial
j . i.
derivative of E with respect to the second-layer weights using the chain rule, we get
a result virtually identical to Eq. (9.5) in Sect. 9.2.2. That is, we get
∂E
=−(t −y )g (cid:2) (a (2) )z (1) . (9.12)
. (2) k k 2 k j
∂w
kj
Note that the sum sign has disappeared in Eq. (9.12), as before, because we are
specifying particular values for k and j when calculating the partial derivative. In
this case, we treat other weights and other hidden layer units as constants when
differentiating withrespecttow (2) . For instance, ifk =1andj =2, we get
kj . . .
∂E
=−(t −y )g (cid:2) (a (2) )z (1) .
. (2) 1 1 2 1 2
∂w
12
(1)
The above equation containing justz is because there is only one occurrence of
2 .
(2) (1)
the particular weight,w , in Eq. (9.11) and that is multiplied byz . This result is
12. 2 .
again similar to the result we obtained in Sect. 9.2.2.
We can now collect all the terms containing k together in Eq. (9.12) and define
δ (2) =(t −y )g (cid:2) (a (2) ), (9.13)
. k k k 2 k


================================================================================
PAGE 252
================================================================================

9.3 ASimpleTwo-LayerNeuralNetwork:CaseStudy3fromChap.1 241
and then Eq. (9.12) can be rewritten as follows:
∂E
=−δ (2) z (1) . (9.14)
. (2) k j
∂w
kj
(l)
Readers will see why it is useful to define δ in Sect. 9.4.
k .
Suppose the output units have linear activation functions, that is,g (a)=a, then
2 .
the derivative of these activation functions is 1. This gives us δ (2) = t −y and
k k k.
∂E =−(t −y )z (1) .
∂w(2) k k j .
kj
(2)
Therefore, we can use Eq. (9.9) and updatew as follows:
11.
w (2) ←w (2)+(cid:2)(t −y )z (1) .
. 11 11 1 1 1
Similarly, we have
w (2) ←w (2)+(cid:2)(t −y )z (1) ,
. 12 12 1 1 2
w (2) ←w (2)+(cid:2)(t −y )z (1) ,
. 21 21 2 2 1
and
w (2) ←w (2)+(cid:2)(t −y )z (1) .
. 22 22 2 2 2
This completes the updating of the weights in the second layer.
9.3.2.2 Updating Weights of the First Layer
Now let us consider updating the weights of the first layer, that is, the weights
connecting the inputs to the hidden units. To do it, we rewrite Eq. (9.11) by first
(1) (1) (1)
replacing z with g (a ) and then replacing each a with its summation of
j . 1 j . j .
products of first layer weights and the input values x as follows:
i.
(cid:5) (cid:5) (cid:6)(cid:6)
(cid:4)2 (cid:4)2 2
1
E = t −g w (2) g (a (1) )
2 k 2 kj 1 j
k=1 j=1
(9.15)
. (cid:5) (cid:5) (cid:5) (cid:6)(cid:6)(cid:6)
(cid:4)2 (cid:4)2 (cid:4)2 2
1
= t −g w (2) g w (1) x .
2 k 2 kj 1 ji i
k=1 j=1 i=1


================================================================================
PAGE 253
================================================================================

242 9 Algorithms3:NeuralNetworks
(1)
Updating weights of the first layer is to updatew shown in Eq. (9.15). To compute
ji .
(1)
the partial derivative of E with respect to w , we apply the chain rule through
ji .
several composite functions until we get deep inside the functions to the actual
(1)
weight we are differentiating with respect to, for example, w . We obtain the
21.
following:
(cid:4)2
∂E
=− (t −y )g (cid:2) (a (2) )w (2) g (cid:2) (a (1) )x
(1) k k 2 k kj 1 j i
∂w
ji k=1
(9.16)
.
(cid:4)2
=− δ (2) w (2) g (cid:2) (a (1) )x .
k kj 1 j i
k=1
(2)
Here, we have used the value of δ previously given in Eq. (9.13). Note that the
k .
sum signs over j and i have disappeared since we are specifying particular values
for them when computing the partial derivatives, as before, and all other weights
are treated as constants. There is, however, still a sum of two terms as shown by
(1)
the summation over k. This isbecauseeachw appears twice in the error function
ji .
(1)
E. We can see this by looking at Fig. 9.4 and considering, for instance,w .T his
21.
(1) (1)
weight contributes part of the value of z . B ut z is propagated to both the two
2 . 2 .
output nodes and so contributes to both y and y , that is, to both y ’s. Hence, we
1. 2. k.
(1)
getw appearing twice in the final value of E, once for each value of k.
21.
If we collect all the parts of the expression for ∂E in Eq. (9.16) that don’t
∂w(1).
ji
contain i and define
(cid:4)2
δ (1) =g (cid:2) (a (1) ) w (2) δ (2) , (9.17)
. j 1 j kj k
k=1
(2)
as we did before when defining δ , t hen Eq. (9.16) can be simply rewritten as
k .
follows:
∂E
=−δ (1) x . (9.18)
. (1) j i
∂w
ji
(1) (1)
We computeδ andδ as follows:
1 . 2 .
δ (1) =g (cid:2) (a (1) )(w (2) δ (2)+w (2) δ (2) ), (9.19)
. 1 1 1 11 1 21 2
δ (1) =g (cid:2) (a (1) )(w (2) δ (2)+w (2) δ (2) ), (9.20)
. 2 1 2 12 1 22 2


================================================================================
PAGE 254
================================================================================

9.3 ASimpleTwo-LayerNeuralNetwork:CaseStudy3fromChap.1 243
respectively. Using Eqs. (9.10) and (9.18), noting that we are subtracting the
negative differential, then we can update the first layer weights using
w (1) ←w (1)+(cid:2)δ (1) x . (9.21)
. ji ji j i
As an example, suppose the activation function g of hidden units is the sigmoid
1.
activation function, and g of output units is the linear function as before. We can
2.
(1)
then derive the update method forw by substituting
11.
g (cid:2) (a (1) )= σ(a (1) )(1− σ(a (1) )),
. 1 1 1 1
δ (2) =t −y ,
. 1 1 1
and
δ (2) =t −y
. 2 2 2
(1) (1)
intoδ giving the update forw as follows:
1 . 11.
w (1)←w (1)+(cid:2)δ (1) x =w (1)+(cid:2)σ(a (1) )(1−σ(a (1) ))(w (2) (t −y )+w (2) (t −y ))x .
. 11 11 1 1 11 1 1 11 1 1 21 2 2 1
The other weights are dealt with similarly.
Example 9.3 Again, we will do one iteration of a very simple example.
Assume the initial input vector isx =x =1and thatw (1) =w (1) =0.5and
1 2 . 11 12 .
w (1) = w (1) = 0.25. Also assumew (2) = w (2) = 0.25,w (2) = w (2) = 0.5,
21 22 . 11 12 . 21 22 .
and the target ist = t = 0.5. As in the text, we will have g as a sigmoid
1 2 . 1.
functiong (x) = σ(x)= 1 and g as a linear functiong (x) = x. We
1 1+e−x. 2. 2 .
alsouse(cid:2) =0.1again.
.
Feeding the input values forwards through the network, we have
a (1) =w (1) x +w (1) x =1,
. 1 11 1 12 2
and
a (1) =w (1) x +w (1) x =0.5.
. 2 21 1 22 2
So,
z (1) = σ(a (1) )=0.731,
. 1 1
(continued)


================================================================================
PAGE 255
================================================================================

244 9 Algorithms3:NeuralNetworks
Example 9.3 (continued)
and
z (1) = σ(a (1) )=0.622.
. 2 2
Continuing through to the next layer, we get
a (2) =w (2) z (1)+w (2) z (1) =0.338,
. 1 11 1 12 2
y =a (2) =0.338,
. 1 1
a (2) =w (2) z (1)+w (2) z (1) =0.677,
. 2 21 1 22 2
and
y =a (2) =0.677.
. 2 2
This gives
(cid:4)2
1
E = (t −y )2 =0.0288.
. k k
2
k=1
Now to feed back on the error gradient, we update weights in the second
(2)
layer first. We use Eq. (9.13) to calculate the values of δ . This is quite
k .
straightforward since the activation function here is linear, and so
g (cid:2) (a (2) )=1.
. 2 k
So,
δ (2) =t −y =0.162,
. 1 1 1
and
δ (2) =t −y =−0.177.
. 2 2 2
Then, using Eqs. (9.14) and (9.9), we can calculate new values for the weights.
Note that Eq. (9.9) says wesubtract the (cid:2)term, but (9.14) says that thegradient
.
(continued)


================================================================================
PAGE 256
================================================================================

9.3 ASimpleTwo-LayerNeuralNetwork:CaseStudy3fromChap.1 245
Example 9.3 (continued)
is negative, so we end up adding the update term to the old weight:
w (2) =0.25+(0.1×0.162×0.731)=0.262,
. 11
w (2) =0.25+(0.1×0.162×0.622)=0.260,
. 12
w (2) =0.5+(0.1 ×−0.177×0.731)=0.487,
. 21
w (2) =0.5+(0.1 ×−0.177×0.622)=0.489.
. 22
Now going back to the first layer, we have a sigmoid activation function. To
(1) (cid:2) (1)
calculateδ , we needg (a ):
j . 1 j .
g (cid:2) (a (1) )= σ(a (1) )(1− σ(a (1) ))= σ(1)(1− σ(1))=0.197,
. 1 1 1 1
and
g (cid:2) (a (1) )= σ(a (1) )(1− σ(a (1) ))= σ(0.5)(1− σ(0.5))=0.235.
. 1 2 2 2
Hence, using Eqs. (9.19) and (9.20), we obtain
δ (1) =(0.197)((0.25)(0.162)+(0.5)(−0.177)) =−0.00946,
. 1
and
δ (1) =(0.235)((0.25)(0.162)+(0.5)(−0.177)) =−0.0113.
. 2
Finally, if we take Eq. (9.21), we can calculate new values for the weights:
w (1) =0.5+(0.1 ×−0.00946×1)=0.499,
. 11
w (1) =0.5+(0.1 ×−0.00946×1)=0.499,
. 12
w (1) =0.25+(0.1 ×−0.0113×1)=0.249,
. 21
and
w (1) =0.25+(0.1 ×−0.0113×1)=0.249.
. 22
(continued)


================================================================================
PAGE 257
================================================================================

246 9 Algorithms3:NeuralNetworks
Example 9.3 (continued)
So that has updated all the weights using back-propagation of errors. We will
finally compute the new error. Now, we have
a (1) =0.998anda (1) =0.498.
. 1 2
So,
z (1) = σ(a (1) )=0.731andz (1) = σ(a (1) )=0.622.
. 1 1 2 2
Continuing through to the next layer, we get
a (2) =0.353anda (2) =0.660.
. 1 2
So,
y =a (2) =0.353andy =a (2) =0.660.
. 1 1 2 2
This gives
(cid:4)2
1
E = (t −y )2 =0.0236
. k k
2
k=1
Hence, after one iteration, y is closer to t and y is closer to t , and the error
1. 1. 2. 2.
E has reduced.
Remark 9.4 This sort of calculation would definitely be done using an appropriate
computer program. It is dreadfully boring to do by hand! It was only done here so
that you can check the use of the methods. It should also be noted that (cid:2) would
.
normally be a smaller number than used in all these examples.
(cid:2)
.
Exercise
9.3 Here is a two-layer neural network for you to try. This is easier than
Example 9.3 since both layers use a linear activation function. This means that
bothg (x)=xandg (x)=x. Also, we haveg (cid:2) (a (1) )=1andg (cid:2) (a (2) )=1.
1 . 2 . 1 j . 2 k .
However, you will still definitely need a calculator!
(continued)


================================================================================
PAGE 258
================================================================================

9.4 TheDeltaRule 247
Do one iteration for this two-layer neural network with both layers having
a linear activation function:
The initial input vector is x = 1 and x = 0; the target is t = 0.25 and
1 . 2 . 1 .
t =0.5.
2 .
Initially,w (1) = w (1) = 0.5 and w (1) = w (1) = 0.25. A lso w (2) = w (2) =
11 12 . 21 22 . 11 12
0.25,w (2) =w (2) =0.5and finally(cid:2) =0.1.
. 21 22 . .
This exercise, using linear activation functions, is partly relevant as you will
see when you meet the rectified linear activation function in Sect. 9.6.
9.4 The Delta Rule
(l)
We may consider δ as a quantity measuring the error passing through each node in
k .
different layers. Equation (9.17) says that the error of each node is a weighted linear
(1)
combination of errors from the layer on the immediate right. That is,δ is a linear
j .
(2)
sum ofδ . This is the principle of the back-propagation algorithm. In general, we
k .
have the Delta rule as follows:
(cid:4)
δ (l−1) =g (cid:2) (a (l−1) ) w (l) δ (l) , (9.22)
. j j kj k
k
(cid:2) (l−1)
whereg(a )is the derivative of the relevant activation function.
j .
Figure 9.5 shows that the errors are propagated left from the output layer. That
is, the weighted errors oft −y are saved inδ (2) using Eq. (9.13), which are further
k k. k .
(1)
propagated left to the hidden units and saved as δ using Eq. (9.17). Therefore,
j .
the training of the neural network is iterative. In each iteration, weights are updated
from right to left. Then, new errors are calculated using these updated weight values
as the input is propagated from left to right.
Fig. 9.5 An illustration of the back-propagation algorithm of a simple two-layer neural network


================================================================================
PAGE 259
================================================================================

248 9 Algorithms3:NeuralNetworks
9.5 Implementation Details
9.5.1 Bias
Usually, an extra node with a value of 1 is considered in both the input layer and
the hidden layers. The weight connecting from this extra node is called the bias (or
threshold), which is denoted as w . We have not considered the bias in these simple
0.
examples.
To illustrate its use, consider the input to the first unit in a network with an n unit
input vector. This would now be
w +x w +x w +···+x w ,
. 10 1 11 2 12 n 1n
wherew is the bias, instead of
10.
x w +x w +···+x w .
. 1 11 2 12 n 1n
without bias. So, the output of an activation function would now be
g(w +x w +x w +···+x w ),
. 10 1 11 2 12 n 1n
instead of
g(x w +x w +···+x w ).
. 1 11 2 12 n 1n
The net effect of the bias is to move the activation function sideways. As an
illustration, letx =x w +x w +···+x w andw =1orw =−1, and
1 11 2 12 n 1n. 10 . 10 .
also supposeg(x)is the logistic activation function. Figure 9.6 illustrates this in two
.
dimensions and present the three corresponding function curves withbiasw equal
10.
to −1, 0 and 1 . It shows that involving a constant (1 or −1in the figure) allows us
. .
to shift the function horizontally, left or right, along the x-axis. This means that with
a bias, the most variable part of the curve (i.e., the part with the most slope) can be
Fig. 9.6 An illustration of
shifting a sigmoid function
left or right along the x-axis


================================================================================
PAGE 260
================================================================================

9.6 DeepNeuralNetworks 249
anywhere on the x-axis rather than always atx = 0. Also, with different biases on
.
each unit, this allows each unit to exploit different parts of the activation function
curve.
9.5.2 Stochastic Gradient Descent and Batch
9.5.2.1 Stochastic Gradient Descent
The stochastic gradient descent (SGD) calculates the error between the target and
the prediction for each training example and then updates the weights immediately.
This is the method we have illustrated in our discussion and all the examples. If
we have N training examples, usually the SGD method updates weights N times at
least.
9.5.2.2 Batch
Here, each weight is updated only once after calculating errors over all the training
examples. So, we need to sum all the errors before updating any of the weights. This
requires storing the sum of the errors as each training example is used, but it runs
more quickly.
9.6 Deep Neural Networks
We have only dealt with one- and two-layer networks, but deeper networks are
more useful, and recently, much deeper neural networks have become extremely
fashionable. Here, we briefly mention a couple of factors that have made such really
deep networks possible.
Improvements in computer technology, especially that of the graphics processing
unit (GPU), mean that potentially deeper neural networks could be used without
them taking forever to train, especially when they are used to analyse the vast
amounts of data that are standard now. Deeper networks increase the ability of the
network to learn more complicated relationships between inputs and outputs, so they
have become increasingly more desirable. The use of such networks is often called
deep learning.
Secondly, a purely linear activation function effectively can only deal with linear
relationships, so it is desirable to have a non-linear activation function. However,
the common non-linear functions of the logistic sigmoid or the hyperbolic tangent
saturate at larger input values. So, they do not discriminate well or train well in
these regions. They are most adaptive in the middle where the curve is sloping since
at either end, they are effectively flat, as can be seen at either end of the curves in


================================================================================
PAGE 261
================================================================================

250 9 Algorithms3:NeuralNetworks
Fig. 9.7 ReLU activation
function
Fig. 9.6. So, these non-linear functions have negligible gradient once you get a little
way away from the centre, and this means they have a limited ability to discriminate
at these points. Together with the fact that in a multilayer network, these small errors
are back-propagated a long way means the network does not train well in lots of
circumstances. This problem is known as the vanishing gradient problem.
A different approach is needed, and recently the use of a relatively linear
function, or piecewise linear function, has been tried and found to be exceptionally
good at training and, as a bonus, is really quick to calculate. The main function used
is illustrated in Fig. 9.7. It is in two linear parts; for x ≤ 0, it is zero; for x >0 ,
. .
it is the linear function y = x. It can be written as y = max{0,x}. This function
. .
is non-linear since it acts differently with positive and negative values of x . Also,
using biases, as described inSect.9.5.1, means that different units can use different
parts of the activation function.
This relatively linear function, as shown in Fig. 9.7, is clearly not differentiable
atx =0, but, conveniently, it is continuous.
.
So, if the gradient at the pointx =0is defined to be zero, then this function has
.
a gradient at all points and has been found to work extremely well despite its native
non-differentiability. This function is called a rectified linear activation function,
and a unit with it as its activation function is called a rectified linear activation unit
(ReLU). This sort of unit has become increasingly prevalent recently.
There are lots of other techniques that aid such deep neural networks to work
effectively, especially in image processing, but these are outside the scope of this
book.


================================================================================
PAGE 262
================================================================================

Chapter 10
Probability
This chapter introduces the concept of probability, a way to deduce what is likely
to happen when an experiment is performed. Probability is a value between zero
and one. People also use other terms for probability: chance, percentage, likelihood,
odds, or proportion. Usually, there are four ways to calculate the probability of an
event, which are the following:
• The classical approach. This is a mathematical approach using counting rules. It
is used on random processes with certain assumptions.
(cid:129) The relative frequency approach. This is based on collecting data and finding the
percentage of time that an event (E) occurred on that data.
(cid:129) The subjective approach.
(cid:129) The logical approach.
This book uses the first two approaches.
This chapter and the following two chapters develop enhancements to the basic
algorithms developed so far, especially that of Chap. 8 on simple linear regression.
Chapter 13 will complete this task by introducing the method of maximum
likelihood.
The techniques in these three chapters also deal with statistical analysis and
probabilistic measures of confidence associated with any scientific discipline that
involves vast amounts of noisy real data.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 251
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_10


================================================================================
PAGE 263
================================================================================

252 10 Probability
10.1 Preliminary Knowledge: Combinatorial Analysis
10.1.1 Factorial Notation
The notation n!, read “n factorial”, denotes the product of the positive integers from
.
1ton (without repetition), inclusive :
n!=n×(n−1) ×···×2×1. (10.1)
.
Note that for completion, 1!and 0!are defined as
. .
1!=1,
.
and
0!=1.
.
So,
n!=n×(n−1)!.
.
10.1.2 Binomial Coefficients
(cid:2) (cid:3)
The symbol n , read “en-see-are”, is defined by
r .
(cid:4) (cid:5)
n n!
= , (10.2)
. r r!(n− r)!
where r and n are positive integerswithr ≤n.
.
Since
n! n(n−1) ···(n−(r −1))(n−r)(n−(r +1)) ···3·2·1
=
. (n− r)! (n−r)(n−(r +1)) ···3·2·1
=n(n−1) ···(n−(r −1)),
we have
n! n(n−1) ···(n−(r −1))
= .
. r!(n− r)! r(r−1) ···3·2·1


================================================================================
PAGE 264
================================================================================

10.1 PreliminaryKnowledge:CombinatorialAnalysis 253
Table 10.1 Pascal’s Triangle n=0. 1
n=1. 1 1
n=2. 1 2 1
n=3. 1 3 3 1
n=4. 1 4 6 4 1
n=5. 1 5 10 10 5 1
(cid:2) (cid:3)
The numbers n are called the binomial coefficients since they appear as the
r .
coefficients in the expansion of(a+b)n. That is,
.
(cid:4) (cid:5)
(cid:6)n
n
(a+b)n = an−rbr.
.
r
r=0
Remark 10.1 Expanding (a + b)2 . , we get (cid:2)1a(cid:3) 2 (cid:2) + (cid:3)2ab + (cid:2) (cid:3)1b2 . , so the binomial
coefficients are 1, 2, and 1. These are equal to 2 , 2 , and 2 , respectively.
0 . 1 . 2 .
Simila(cid:2)rl(cid:3)y, (cid:2)w(cid:3)he(cid:2)n (cid:3)expand(cid:2)in(cid:3)g(a+b)3
.
, we get the binomial coefficients 1, 3, 3, and
1, being 3 , 3 , 3 , and 3 , respectively.
0 . 1 . 2 . 3 .
This pattern of binomial coefficients can be extended, and we get a structure
known as Pascal’s triangle for the coefficients. This gives a quick way to calculate
the coefficients. All rows start with the number 1. We can add each consecutive pair
of elements of each row and write their sum in the gap between them, but on the line
below, to get the elements in the next row. See Table 10.1 for a diagram of Pascal’s
triangle.
(cid:2)
.
10.1.3 Permutation and Combination
10.1.3.1 Permutations of n Items Without Repetitions
There are n ways of picking the first item; then there are n−1 ways of picking
.
the second item since we cannot have repetitions, and so on. Hence, there are n!
.
permutations of n objects.
Example 10.1 There are3 ! =3·2·1 = 6permutations of the three letters
.
s,t and u, namely,stu,sut,tsu,tus,ust, and uts.
. .
In permutations, the order of the items matters. So, in Example 10.1 stu is
different from sut, and so on. Think of a password or key to unlock a phone,


================================================================================
PAGE 265
================================================================================

254 10 Probability
the order of the letters or digits is significant, and each permutation is a different
password or key.
10.1.3.2 Permutations of k Items Out of n Wi thoutRepetition
The number of permutations of n objects taken k at a time where the order matters
isdenotedasP(n,k)and is computed as
.
n!
P(n,k)= =n(n−1) ···(n−(k−1)).
. (n−k)!
Example 10.2 There are five numbers, 3, 4, 5, 6, and 7. The number of two
digit numbers we can form by taking any two numbers from these five is
5!
P(5,2)= =5×(5−1)=20,
. (5−2)!
which are 34, 35, 36, 37, 43, 45, 46, 47, 53, 54, 56, 57, 63, 64, 65, 67, 73, 74,
75, and 76.
Again, the order matters, so 34 is different from 43. Again, for a password,
picking 8 out of 26 letters without repetition would give different passwords for each
order. These are permutations. In this case, there are
26!
different passwords: a
(26−8)!.
large number.
10.1.3.3 Combinations of k Items Out of n Wi thoutRepetitions
The number of combinations of n objects taken k at a time is denoted as C(n,k)
.
and is computed as
n!
C(n,k)= . (10.3)
. k!(n−k)!
Here, the order does not count. Think of mixing colour lights—red and green give
yellow, as do green and red. Combinations are just collections of items, and there are
many more permutations than combinations since items in a different order would
be a new permutation but not a new combination. Hence, when the order does not
matter, or where the items are not picked in order, these are combinations. When
doing an exercise, the first question to ask is whether changing the order gives


================================================================================
PAGE 266
================================================================================

10.1 PreliminaryKnowledge:CombinatorialAnalysis 255
a different answer. If the answer is yes, you want permutations; if no, then it is
combinations.
Remark 10.2 Looking at Eqs. (10.3) and (10.2), we can see that using binomial
coefficients is another notation for combinations. Thus,
(cid:4) (cid:5)
n n!
C(n,k)= = .
. k k!(n−k)!
(cid:2)
.
Example 10.3 Continue Example 10.2. Compute the number of combina-
tions of two numbers if we take any two from those five numbers.
Solution The number of combinations is calculated as
5! 5×4!×3!
C(5,2)= = =10.
. 2!(5−2)! 2!3!
When counting the combinations, the order does not matter. For example, 43
and 34 are the same combinations.
Example 10.4 Compute the number of combinations of the letterss,t,u, and
.
v taken three at a time.
Solution The number of combinations is calculated as
4! 4×3!
C(4,3)= = =4.
. 3!(4−3)! 3!1!
Four combinations are stu,stv,suv, and tuv. The following combinations
.
are equal:stu,sut,tsu,tus,ust, and uts, since they are the combinations of
.
three letters,s,t,u, and the order does not matter.
.
Exercises
10.1 (a) Given the letters in the word Wales, how many different five letter
strings of letters, without any repetitions, can you make? (b) Given the letters
in the word Scotland, how many different eight letter strings of letters, without
any repetitions, can you make?
(continued)


================================================================================
PAGE 267
================================================================================

256 10 Probability
10.2 Mary wants to take a photo with her four friends, and they all stand in
one row. If Mary must stand in the middle, how many different photos can
they take?
10.3 How many different passwords can you make by using fo urd igits from
the ten digits on your mobile phone screen, where you are not allowed to
repeat anydigits?
10.4 There are six numbers, 0, 3, 6, 7, 8, and 9. How many different five-digit
numbers can be made where 0 cannot be the first digit?
10.5 How many different fruit salads can you make using four different fruits
from the list: apple, orange, pear, banana, grapefruit, pineapple, and grapes?
10.6 There are 12 dots in a plane. No three dots are in the same line. How
many triangles can be made?
10.2 Probability
10.2.1 Axiomatic Probability Theory
The probability of some event E occurring is the likelihood of that event happening.
Initial illustrations usually consider flipping a coin (or coins) or rolling a die (or
several dice). This is because there are an easily calculated set of possible outcomes.
For a coin, there are just two possible faces, and for a die, we can only get one of six
faces (ignoring landing on an edge or corner or disappearing under the sideboard—
these are unstable or silly outcomes!). This makes it easy to calculate the likelihood
of a particular number being thrown on a die or a particular side of a coin coming
up. This can be codified into an axiomatic theory as follow s.
Let((cid:2),(cid:3),P)denote a probability space, where
.
(cid:129) (cid:2)is the set of all possible outcomes, known as the sample space.
.
Example 10.5 A fair six-sided die is rolled. The number of dots on each side
is from one to six. We use(cid:2) ={1,2,3,4,5,6}to denote all possible numbers
.
of dots on the top of the side after a roll.


================================================================================
PAGE 268
================================================================================

10.2 Probability 257
(cid:129) (cid:3)is a collection of subsets of (cid:2), and each subset is called an event.
. .
Example 10.6 An event (E) could show an odd number when the fair six-
sided die lands. That is,E ={1,3,5},andE ⊂ (cid:2).
. .
(cid:129) P is a probability measure defined as a real-valued function of the elementsof(cid:3)
.
satisfying the following axioms of probability:
– Axiom 1:0≤ P(A)≤1for allA∈ (cid:3).
. .
– Axiom 2:P((cid:2))=1.
.
– Axiom 3: If two events A and B are mutually exclusive, that is, no elements
in common, then the probability of either A or B occurring is the probability
of A occurring plus theprobabilityofB occurring:
P(A∪B)= P(A)+ P(B).
.
Note: set union is defined in Sect. 2.1.3 of Chap. 2
If E is an event,P(E)is the probability of the occurrence of the event, that is,
.
ThenumberofelementsineventE
P(E)= . (10.4)
.
Thesizeofthesamplespace
Note that the maximum probability of any event is one.
Example 10.7 Let us roll a fair six-sided die. The number of dots on each
side is from one to six. Let A be the event showing an even number when it
lands, B showing either three or five dots, and C showing a prime number.
Calculate thefollowingprobabilities:
(cid:129) P(A∪B).
.
(cid:129) P(A∩C).
.
Set union and set intersection are defined in Sect. 2.1.3 of Chap. 2.
Solution Since
(cid:2) ={1,2,3,4,5,6},A ={2,4,6},B ={3,5},andC ={2,3,5},
.
(continued)


================================================================================
PAGE 269
================================================================================

258 10 Probability
Example 10.7 (continued)
we have
A∪B ={2,3,4,5,6},
.
and
A∩C ={2}.
.
Therefore,
#(A∪B) 5
P(A∪B)= = ,
.
#(cid:2) 6
and
#(A∩C) 1
P(A∩C)= = .
.
#(cid:2) 6
Recall that #denotes the cardinality of a finite set (see Sect. 2.1.1 of Chap. 2).
.
Example 10.8 Suppose we roll two fair six-sided dice. What is the probabil-
ity of getting two even numbers? Now, roll three fair six-sided dice. What is
the probability of getting three even numbers?
Solution All possible outcomes are listed in Table 10.2, where elements in
the event of getting two even numbers are shown in red. The sample space
size is 36 since there are 36 possibilities, as shown in Table 10.2, and the
number of elements in the event of getting two even numbers is 9. Therefore,
P(gettingtwoevennumbers)= 9 =0.25,or 25%.
36 . .
It gets harder to count the events once we get to three dice. So let us look
at the method again for two dice and then extend it to three. With two dice,
there are six ways the first dice could fall and six for the second. So, there are
6×6 = 36different possible dice rolls. Similarly, to get two even numbers,
.
there are just three possible dice rolls (one of {2,4,6}) for the first dice and
.
three for the second dice. So, there are 3 × 3 = 9 ways to get two even
.
numbers. So, as before, we get 9 =0.25as the probability.
36 .
For three dice, we can see that there are 6×6×6 = 216 possible dice
.
rolls. To get three even numbers again, there are3×3×3 = 27ways to do
.
that. So, the probability is 27 = 1 =0.125,or1 2.5%.
216 8 . .


================================================================================
PAGE 270
================================================================================

10.2 Probability 259
Table 10.2 Results of rolling
1 2 3 4 5 6
two fair six-sided dice:
1 (1,1) (1,2) (1,3) (1,4) (1,5) (1,6)
elements getting two even
numbers are shown in red 2 (2,1) (2,2) (2,3) (2,4) (2,5) (2,6)
3 (3,1) (3,2) (3,3) (3,4) (3,5) (3,6)
4 (4,1) (4,2) (4,3) (4, 4) (4,5) (4,6)
5 (5,1) (5,2) (5,3) (5,4) (5,5) (5,6)
6 (6,1) (6,2) (6,3) (6,4) (6,5) (6,6)
Example 10.9 Suppose we roll a fair six-sided die twice. What is the
probability of getting two different numbers? And then, if you roll it again,
what is the probability of getting three different numbers?
Solution We are using the principle that we need to find the number of
elements in the event and divide by the total number of all the possibilities (see
Eq. (10.4)). When rolling twice, you can get6×6 = 36different outcomes
.
(as shown in Table 10.2).
To get two different numbers, the first number can be any digit, but the
second can be only one of fi ved ifferentnumbers.So,thereare 6×5 = 30
.
different possibilities. In Table 10.2, this is all the outcomes apart from those
on the main diagonal. Therefore, P (getting two different numbers) = 30 =
36
5,or 83.3%.
6. .
Now, doing the same for three rolls, we have6×6×6=216different rolls.
.
But getting three different numbers requires six choices for the first roll, fi ve
for the second, and f ourf orthethirdroll.Thisgives 6×5×4=120elements
.
in this event. Therefore, P (getting three differentnumbers) = 120 = 5,or
216 9.
55.5%.
.
Note that getting three different numbers is, in fact, the number of
permutations of picking three out of six items without repetition. That is,
it is P(n,k) = 6! = 6! = 720 = 120. It is permutations rather than
(6−3)! 3! 6 .
combinations because you are rolling the dice in order—so a 1 followed by a
2 is different from a 2 followed by a 1.
Exercises
10.7 A coin is flipped three times in succession. What is the probability of
getting exactly two heads?
(continued)


================================================================================
PAGE 271
================================================================================

260 10 Probability
10.8 Suppose we roll a fair six-sided die four times. What is the probability
of getting four different numbers? (You might like to try rolling five times
with five different numbers and rolling six times with six different numbers)
10.9 Telephone numbers include six digits from 0, 1, 2, ···, 8, and 9. What is
.
the probability that all six digits are different if we randomly select a telephone
number?
10.3 Discrete Random Variables
This section and the next define random variables. There are two types of random
variables: discrete, dealt with in this section, and continuous, dealt with in the next
section. A random variable is discrete if it can only take one of a countable set
of values, for example, an integer value, such as the number of aces in a standard
pack of cards or the number of people at a football match. A random variable is
continuous if it can take an infinite number of different values, for example, a real
number value. Most continuous random variables are measurements, for instance, a
person’s weight or height.
Any random variable is a map from the outcome space ( (cid:2)) to the real numbers.
.
It is the result of some outcome of a random experiment.
For example, let us consider throwing two fair dice. The sample space (cid:2)includes
.
all those pairs ( ω’s) of numbers listed in Table 10.2. Suppose we are interested in
.
the total value obtained. Then, we can writeX((1,1)) = 2,X((2,3)) = 5, and so
. .
on, for all ω’s, whereω ∈ (cid:2)and X is the random variable, mappingeachωto the
. . .
sum of two dice values.
If X is a random variable and x,x , and x are fixed real numbers, we may have
1. 2.
the following events:
(X = x), (X≤ x), (X >x)or(x <X≤x ).
. 1 2
These events have probabilities that are denoted by
P(X= x), P(X≤ x), P(X >x)or P(x <X≤x ).
. 1 2
Definition 10.1 (Discrete Random Variables) A random variable X is called
discrete if it only takes values in the integers or (possibly) some other countable
set of realnumbers.


================================================================================
PAGE 272
================================================================================

10.3 DiscreteRandomVariables 261
Example 10.10 The idea of so-called intelligence tests is to ask each individ-
ual to try to solve a certain number of questions. Each question can be solved
either correctly or incorrectly. Usually, the number of correct answers is used
as an empirical measure of individual intelligence. Such a number or score is
a discrete random variable.
The probability mass function,f (x ), is defined as the probability that X takes
X k .
on a certain valuex :
k.
f (x )= P(X=x ). (10.5)
. X k k
The cumulative distribution function, F (x), is defined as the probability that the
X .
random variable, X, will take on a value that is lesser than or equal to a particular
value, x. This is defined as follows:
(cid:6)
F (x)= P(X≤ x)= f (x ). (10.6)
. X X k
xk ≤x
F (x)is a staircase function.
X .
Example 10.11 Suppose we flip a fair coin three times. Let X be the number
of heads in three tosses of the coin. Find the probability mass function and
cumulative distribution function.
Solution The sample space is
(cid:2) ={TTT,HTT,THT,TTH,HHT,HTH,THH,HHH}.
.
x may be any fixed real numbers, though X may take values of 0, 1, 2,
.
and 3 only. So, X is a discrete random variable.From(cid:2), we can see that the
.
probability of getting 0 heads is 1 since it occurs once in the eight possible
8.
outcomes. Similarly, by counting the elements in (cid:2), we can see that the
.
probability of getting one headis 3, of getting two headsis 3, and of getting
8. 8.
three headsis 1. So, the probability mass function is a histogram as illustrated
8.
in Fig. 10.1.
(continued)


================================================================================
PAGE 273
================================================================================

262 10 Probability
Example 10.11 (continued)
Considering the cumulative distribution function as given in Eq. (10.6), we
can set up Table 10.3 to present the cumulative distribution, F (x), and we
X .
can illustrate the cumulative distribution as shown in Fig. 10.2. Please note
that the x-axis and y-axis scales are not consistent across Figs. 10.1 and 10.2.
This was done to better illustrate specific features in each figure.
Table 10.3 is built up as follows. F (−1) = 0, since the number of
X .
elements where the number of heads observed is less than 0 is zero. F (0)=
X
1, since there is only one way of getting zero heads out of the eight possible
8.
flips. F (1) = 4 since there is one way of getting zero heads together with
X 8.
three ways of getting one head out of the eight possible flips. The table
shows the rest of the values, and once we get to x being greater than 3, then
the number of elements with more than threeheadsiszero,soF (4)remains
X .
at 1.
Fig. 10.1 Probability mass
function of X,whereX
represents the number of
heads in three tosses of a fair
coin
Table 10.3 The cumulative distribution function of X,whereX represents the number of heads
in three tosses of a fair coin
x Event(X≤x). FX(x).
−1 ∅ . 0
0 {TTT} 8 1 .
1 {TTT, HTT, THT, TTH} 8 4 .
2 {TTT, HTT, THT, TTH, HHT, HTH, THH} 8 7 .
3 {TTT, HTT, THT, TTH, HHT, HTH, THH, HHH} 1
4 {TTT, HTT, THT, TTH, HHT, HTH, THH, HHH} 1


================================================================================
PAGE 274
================================================================================

10.4 ContinuousRandomVariables 263
Fig. 10.2 The cumulative distribution function of X,whereX represents the number of heads in
three tosses of a fair coin
Exercises
10.10 If you throw two fair dice, we can define a discrete random number
X as the total value obtained by the two dice. So, X takes va lues from
X((1,1)) = 2 up toX((6,6)) = 12. With the help of Table 10.2,drawupa
. .
table of the probabilities of getting a total value from 2 to 12 after throwing
two fair dice. Sketch the probability mass function and the cumulative
distribution function.
10.11 Suppose we flip a fair coin four times. Let X be the number of heads
in four tosses of the coin. Find the probability mass function and cumulative
distribution f unction.
10.4 Continuous Random Variables
Definition 10.2 (Continuous Random Variables) A random variable X is called
continuous if it takes values from a real-valued interval, either open or closed. That
is, it can take one of an infinite number of val ues.
Example 10.12 The average incandescent bulb light span is approximately
1000 hours. The light span of incandescent bulbs can be considered as a
continuous variable. This is because it has a lifespan that is not a whole
number of hours, minutes, or seconds—the bulb could go at any point in time.


================================================================================
PAGE 275
================================================================================

264 10 Probability
Similar to a discrete random variable, we can define a probability density
function and a cumulative distribution function for a continuous random variable.
However, the sum of the possible values of such functions is now calculated using
an integral since it is a continuous (or piecewise continuous) curve.
So, if X is a continuous random variable, then there is a real-valued function,
f , called the probability density function of X, which is a curve, and it satisfies the
X.
following:
(cid:129) f is piecewise continuous. That is, the function is continuous except at finitely
X.
many points.
(cid:129) f(cid:7)X (x) ≥ 0
.
. That is, it does not go below the horizontal axis.
(cid:129) ∞ f (x)dx =1. That is, the total area under the curve is 1.
−∞ X .
Figure 10.3 shows a possible probability density function for the lifespan of a
certain type of incandesce(cid:7)nt bulb produced from the same manufacturing plant.
∞
The area under the curve f (x)dx is the total probability of all lifespans and
−∞ X .
equals 1.
The cumulative distribution function is again defined as the probability that the
random variable, X, will take on a value that is less than or equal to a particular
value, x. So, the cumulative distribution function (cdf),F (x), is a nondecreasing
X .
and continuous function and is defined as
(cid:8)
x
F (x)= P(X≤ x)= f (t)dt. (10.7)
. X X
−∞
Referring again to Fig. 10.3,F (a)is the cumulative probability of a lifespan less
X .
than or equal to a. This is represented as the shaded region in Fig. 10.3.
Fig. 10.3 Probability density function representing the lifespan of a certain type of incandescent
bulb produced by a specific manufacturing plant


================================================================================
PAGE 276
================================================================================

10.4 ContinuousRandomVariables 265
Based on Eq. (10.7), we have the following:
(cid:129) P(a < X ≤b) =F (b) −F (a).
X X .
Recall that the second fundamental theorem of calculus (see Sect. 5.4.3 of
Chap. 5) states that if f is continuouson[a,b], and F is any antiderivative of f
(cid:7) .
with respect to x,then b f (x)dx = F (b)−F (a).For example, this could
a X X X .
be the area under the curve illustrated in Fig. 10.3 between lifespan values of a
and b.
(cid:129) P(X > a)= 1 −F (a).
X .
Proof Since P(X > a)= 1− P(X≤ a), from Eq. (10.7), we have P(X≤
.
a)=F (a). Therefore,P(X >a)=1−F (a). (cid:10)(cid:11)
X . X .
In Fig. 10.3,P(X >a)is the non-shaded region under the curve above the point
.
a on the horizontal axis.
Example 10.13 Suppose the probability density functionf(x)of X is given
.
by the follow ing:
(cid:9)
cos(x), if|x|< π;
f(x)= W 2
.
0, otherwise.
1. Find the value of W.
2. Find the cumulative distribution functionF (x).
X .
3. (cid:129) FindP(X≤0).
.
(cid:129) FindP(X≤ π).
4 .
(cid:129) FindP(X > π).
4 .
(cid:129) FindP(X > π).
2 .
Solution
(cid:7)
1. We use the fact that ∞ f (x)dx =1to find W.
−∞ X . (cid:10)
(cid:7) (cid:7) (cid:10)π
− ∞ ∞ f X (x)dx = − π 2 π co W s(x)dx= sin W (x)(cid:10) (cid:10) 2 = W 2 =1,⇒W =2 . .
2 −π
2
Figure 10.4 illustrates the probability density function curve.
2. Applying Eq. (10.7),
(cid:8)
x
F (x)= f (t)dt,
. X X
−∞
we have the following:
(cid:7)
(cid:129) Ifx <− π,F (x)= x 0dx=0.
2. X −∞ .
(continued)


================================================================================
PAGE 277
================================================================================

266 10 Probability
Example 10.13 (continued)
(cid:7)
(cid:129) If − π ≤ x <π,F (x)= x cos(x)dx= 1 + 1sin(x).
2 2. X −π 2 2 2 .
2
(cid:129) Ifx ≥ π,F (x)=1.
2. X .
3. (cid:129) P(X ≤ 0 ): Using the result for F (x) given above with x = 0 gives
. X . .
P(X≤0)=F (0)= 1 +0= 1.
X 2 2.
This makes sense since from the symmetry of the curve for the
probability density function, as shown in Fig. 10.4, the cumulative
probability up tox =0is exactly half the area under the curve.
.
(cid:129) P(X ≤ π ):P(X≤ π)=F (π)= 1 + 1√ =0.8536.
4 . 4 X 4 2 2× 2 .
(cid:129) P(X > π ) = 1− P(X ≤ π ) =1−0.8536=0.1464.
4 4 .
(cid:129) P(X > π ) = 1− P(X ≤ π)=1−1=0, as expected.
2 2 .
Remark 10.3 If X is a continuous random variable, then the probability of X
taking a specific value C iszero,thatis,P(X= C)=0.
.
Proof Suppose(cid:6)x is a tiny increase of C .Wehave
.
(cid:8)
C+(cid:6)x
P(C <x≤C+(cid:6)x)=F (C+(cid:6)x)−F (C)= f(x)dx,
. X X
C
and
0≤ P(X= C)≤ P(C <X≤C+(cid:6)x).
.
Fig. 10.4 The probability density function of X inExample10.13


================================================================================
PAGE 278
================================================================================

10.4 ContinuousRandomVariables 267
SinceF (x)is continuous from the right, when(cid:6)x approaches zero, we have
X . .
(cid:8)
C+(cid:6)x
lim f(x)dx=0.
.
(cid:6)x→0+
C
That gives us the following:
0≤ P(X= C)≤0.
.
Therefore, we haveP(X= C)=0. (cid:10)(cid:11)
.
An eventE ={X =C}may happen even thoughP(X= C)=0. For example,
. .
the lifespan of incandescent bulbs is approximately from 800 hours to 1200 hours.
The exact event E = X{ = 900 hours} may occur, but P(X= 900 hours) = 0.
. .
That is, the probability of getting an exact value from an infinity of possible answers
must be zero or else the sum of all the probabilities of all the exact values would
add up to infinity and not 1 as is required. So, the probability of getting exactly 900
hours is zero, though we might get 900 hours to the best of our ability to measure
the time.
(cid:2)
.
Exercises
10.12 The probability density function of the random variable X is
(cid:9)
a(2x−x2), 0 <x <2,
f(x)= .
.
0, otherwise.
Compute the following:
1. The value of a
2. The cumulative distribution functionF (x).
X .
3. P(X ≤ 1)
.
4. P(X ≤ 1 )
2 .
5. P(X > 1 )
2 .
10.13 The probability density function of the random variable X is
(cid:9)
8x7, 0 <x <1,
f(x)= .
.
0, otherwise.
(continued)


================================================================================
PAGE 279
================================================================================

268 10 Probability
Compute the following:
1. m sothatP(X >m)= P(X <m)
.
2. n sothatP(X >n)=0.05
.
10.5 Mean and Variance of Probability Distributions
If we know the probability distribution of a random variable, we will know and
be able to describe the properties of the random variable. However, obtaining an
accurate probability distribution in real-world applications is hard. Moreover, we
often only need to know some properties of a random variable but not all, such as the
centre value, the value the random variable is most likely to take, and the correlation
between two random variables. These properties can be parameters of probability
distributions. This section introduces the two most important parameters: mean (or
expected value) and variance.
10.5.1 Mean
Definition 10.3 (Mean) The mean (or expected value) of a random variable X,
denoted byμ ,or E(X), is defined by
X. .
(cid:9)(cid:11)
x f (x ), X :discrete;
μ = E(X)= (cid:7) k k X k (10.8)
. X ∞ xf (x)dx, X :continuous.
−∞ X
The expected value should be regarded as the average (mean) value. If you compare
this with the definition of the sample mean given in Sect. 4.2.1 of Chap. 4, you can
see that this is a weighted mean, weighted by the probability, as will be illustrated
in Example 10.14. Note that we may also denote the expected value asμ =E[X]
X .
in this book.
Example 10.14 Three products are selected randomly from nine products, of
which two are defective. The sample space consists of the distinct, equally
likely, samples of size 3. Let X be the random variable that counts the number
of defective items in a sample. The possible values of X are 0, 1, and 2. What
is the expected value of defective products in a sample of size 3?
(continued)


================================================================================
PAGE 280
================================================================================

10.5 MeanandVarianceofProbabilityDistributions 269
Example 10.14 (continued)
Solution
(cid:129) Letx =0,1,2be the possible values of X.
i .
(cid:129) The number of ways of choosing x
i.
defectives from two defecti(cid:2)ve(cid:3)s and
choosing 3 − x non-defectives from seven non-defectives is 2 and
(cid:2) (cid:3) i. (cid:2) (cid:3) xi .
7 , respectively. (Remember that n is a notation for the number of
3−xi . r .
combinations of n object taken r atatime.)
(cid:129) The total number of possibl(cid:2)e (cid:3)outcomes (i.e., the number of combinations
of three products out of ) is 9 =84.
3 .
(cid:129) The probability of the value x of X is
i.
(cid:2) (cid:3)(cid:2) (cid:3)
2 7
p = xi (cid:2)3(cid:3) −xi , (x =0,1,2).
. i 9 i
3
(cid:129) Applying Eq. (10.8) for the discrete variable, we have
(cid:2) (cid:3)(cid:2) (cid:3) (cid:2) (cid:3)(cid:2) (cid:3) (cid:2) (cid:3)(cid:2) (cid:3)
2 7 2 7 2 7
E(X)=0× 0 3 +1× 1 2 +2× 2 1
.
84 84 84
1×35 2×21 1×7 2
=0× +1× +2× = .
.
84 84 84 3
Note that the three values above 84 in the last line above add up to the total
of 84 combinations as expected. When each is divided by 84, these represent
the probabilities of getting the three different outcomes. This means we have
0 defective products occurring in 35 combinations, 1 defective product in 42
combinations, and 2 defective products in 7 combinations. So, we have a list
of numbers consisting of 35 0s, 42 1s, and 7 2s. Now, just think of these as
a list of 84 numbers. The usual mean of such a list is to add up the numbers,
that is,35×0+42×1+7×2=56, and divide by the total numbers in the
.
list, that is, 84. This division gives 2.
3.
This shows that this definition of mean corresponds with our previous
definition given in Sect. 4.2.1 of Chap. 4.


================================================================================
PAGE 281
================================================================================

270 10 Probability
Example 10.15 Let X be a random variable. Consider its distribution func-
tion on the interval[0,1]has the probability density function:
.
(cid:9)
0, if x <0orx >1;
f (x)=
. X
1, if0≤x ≤1.
ComputeE(X).
.
Solution Applying Eq. (10.8) for the continuous variable, we have
(cid:8)
∞
E(X)= xf (x)dx
. X
−∞
(cid:8) (cid:8) (cid:8)
0 1 ∞
= x×0dx+ x×1dx+ x×0dx
−∞ 0 1
(cid:10)
=0+ 1 x2 (cid:10) (cid:10)
(cid:10)
1 +0
2 x=0
1
= .
2
If you look at the probability density function, you can see it is a square of
height one and width one, as shown in Fig. 10.5. Not surprisingly, the mean
value is in the middle of the x-values atx = 1.
2.
Fig. 10.5 The probability
density function shown in
Example 10.15


================================================================================
PAGE 282
================================================================================

10.5 MeanandVarianceofProbabilityDistributions 271
Table 10.4 Probability distributions of the random variables X1.and X2.
X1. 1 2 X2. 1 2
P 1 3. 2 3. P 1 2. 1 2.
10.5.1.1 Properties of Mean
To motivate the statement of these properties, we will do a simple example that
illustrates them.
Example 10.16 Table 10.4 shows the probability distributions of two inde-
pendent random variables X and X ,a both of which only take the values 1
1. 2.
and 2. Compute the following:
1. E(X )
1 .
2. E(X )
2 .
3. E(aX ), where a is a constant
1 .
4. E(a + X ), where a is a constant
1 .
5. E(X + X )
1 2 .
6. E(X X )
1 2 .
Solution
1. Applying Eq. (10.8) for the discrete variable, we have
1 2 5
E(X )=1× +2× = .
. 1
3 3 3
2. Applying Eq. (10.8) for the discrete variable, we have
1 1 3
E(X )=1× +2× = .
. 2
2 2 2
3. aX takes the two values a and 2a with the probabilities as given in
1.
Table10.4. So again applying Eq. (10.8) for the discrete variable, we have
1 2 5
E(aX )=1a× +2a× =a = aE(X ).
. 1 1
4. a + X takes the two valuesa+3 1anda+3 2with 3 the probabilities as given
1. . .
in Table 10.4. So again applying Eq. (10.8) for the discrete variable, we
have
1 2 5
E(a+X )=(1+a)× +(2+a)× = +a = E(X )+a.
. 1 1
3 3 3
(continued)


================================================================================
PAGE 283
================================================================================

272 10 Probability
Example 10.16 (continued)
5. X + X takes just the values that are the sum of the ones in X and X ,
1 2. 1. 2.
namely, 2, 3, and 4. To get a 2, you must have a 1 in both X and X , so that
1. 2.
has probability 1×1 = 1. To get a 4, both have to be 2, so the probability
3 2 6.
is 2×1 = 1. To get a 3, we can haveX =1andX =2with probability
3 2 3. 1 . 2 .
1 × 1 = 1 or haveX = 2andX = 1with probability 2 × 1 = 1. So,
3 2 6. 1 . 2 . 3 2 3.
the total probability of a 3 is 1 + 1 = 1.
6 3 2.
We now have our probability distribution (2 with probability 1, 3 with
6.
probability 1, and 4 with probability 1) and note that the probabilities add
2. 3.
up to 1 as required.
Finally, we apply Eq. (10.8) for the discrete variable:
1 1 1 19
E(X +X )=2× +3× +4× = .
. 1 2
6 2 3 6
Note that this is the same asE(X )+E(X ).
1 2 .
6. This is similar to the last one. Now, X X can take values of 1, 2, and 4.
1 2.
We get a probability distribution by the same method as before to get a
probability of 1 is 1, a probability of 2 is 1, and a probability of 4 is 1.
6. 2. 3.
So, we apply Eq. (10.8) for the discrete variable:
1 1 1 5
E(X X )=1× +2× +4× = .
. 1 2
6 2 3 2
Note that this is the same asE(X )E(X ).
1 2 .
a Intuitively, two random variables X1. and X2. are independent if knowing the value of one
of them does not change the probabilities for the other one. We will define independent
random variables formally in Chap. 11.
We can now generalise these results and state the properties that apply to both
discrete and continuous random variables:
(1) E(a) = a, where a is a constant.
.
(2) E(aX) = aE(X ), where a is a constant.
.
(3) E(a(cid:11) + X) = E(X(cid:11))+a
.
, where a is a constant.
(4) E( n X ) = n E(X ),ifE(X ),fori =1, ··· ,nexists.
i=1 i i=1 i . i . .
(5) E(XY) = E(X)E(Y), where X and Y are two independent random variables.
Now that we have these properties, we can use them to do examples without going
through all the working used in Example 10.16.


================================================================================
PAGE 284
================================================================================

10.5 MeanandVarianceofProbabilityDistributions 273
Table 10.5 Probability distributions of the random variables X3.and X4.
X3. 1 2 3 X4. 2 3 4
P 1 3. 1 2. 1 6. P 3 1 . 3 1 . 1 3.
Remark 10.4 Note that E(X2) = E(XX) is not, in general, the same as
.
E(X)E(X) since X and X are not independent variables (indeed, they are the
.
same). For example,usingX from Example 10.16, we see that the random variable
1.
X2 =X X can only take the values 1 and 4, since X is either 1 or 2, and it has the
1 1 1. 1.
same probability distribution as X . That is, X2takes the value 1 with probability 1
1. 1. 3.
and the value 4 with probability 2.
3.
Hence, we have thatE(X )was1×1+2×2 = 5, as we saw in Example 10.16.
1 . 3 3 3.
And thereforeE(X2)is1× 1 +4× 2 =3.
1 . 3 3 .
So, in general, for a discrete random variable, we have
(cid:6)
E(X2)= x2f (x ).
. k X k
k
By analogy, for a continuous random variable, we have
(cid:8)
∞
E(X2)= x2f (x)dx.
. X
−∞
We will find that these last two results are important in Sect. 10.5.2. In fact, if X is
a continuous (or discrete) random variable with a probability density function (or
probability massfunction)f(x), the expected value of any functiong(X), denoted
. .
asE(g(X)), can be computed. Readers can refer to the details provided in [10].
.
(cid:2)
.
Example 10.17 Tables 10.4 and 10.5 show the probability distributions of the
independent random variables X , X , X , and X . Compute the following:
1. 2. 3. 4.
1. E(X + X +X + X ).
1 2 3 4 .
2. E(2X +4)
3 .
3. E(X X ).
3 4 .
4. E(X X ).
1 3 .
5. E(X2 ).
3 .
(continued)


================================================================================
PAGE 285
================================================================================

274 10 Probability
Example 10.17 (continued)
Solution
1. We already haveE(X )= 5 andE(X )= 3. Applying Eq. (10.8)forthe
1 3. 2 2.
discrete variables X and X ,wehave
3. 4.
1 1 1 11
E(X )=1× +2× +3× = .
. 3
3 2 6 6
1 1 1
E(X )=2× +3× +4× =3.
. 4
3 3 3
Finally, using the fourth property of means, we have
5 3 11
E(X +X +X +X )= + + +3=8.
. 1 2 3 4
3 2 6
2. Using the second and third properties of means, we have
11 23
E(2X +4)=2× +4= .
. 3
6 3
3. Using the fifth property of means, we have
11 11
E(X X )= ×3= .
. 3 4
6 2
4. Using the fifth property of means, we have
5 11 55
E(X X )= × = .
. 1 3
3 6 18
5. X2 has a probability distribution of 1, with probability 1, 4 with probability
3. 3.
1, and 9 with probability 1. So,
2. 6.
1 1 1 23
E(X2)=1× +4× +9× = .
. 3 3 2 6 6


================================================================================
PAGE 286
================================================================================

10.5 MeanandVarianceofProbabilityDistributions 275
Table 10.6 The probability distribution of the random variable Z
Z −1. 0 1 2. 1 2
p 1 6. 1 5 2. 1 1 2. 1 6. 1 6.
Exercises
10.14 Table 10.6 shows the probability distribution of the random variable
Z. Compute the following:
(1) E(Z).
.
(2) E(−Z + 2) .
.
(3) E(Z2).
.
10.15 Using Tables 10.4, 10.5, and 10.6 and assuming the variables are
independent, compute the following:
(1) E(Z + X + X ).
1 3 .
(2) E(ZX ).
4 .
(3) E(3Z − 5) .
.
(4) E(X Z).
1 .
(5) E(X +X + Z).
2 4 .
10.16 Take any two numbers from 1, 2, 3, and 4. What is the mean value of
the absolute difference between the two numbers?
10.17 The probability density function of the random variable X is
(cid:9)
8x7, 0 <x <1,
f(x)= .
.
0, otherwise.
Compute the following:
(1) E(X).
.
(2) E(X2).
.
10.5.2 Variance
Definition 10.4 (Variance) The variance of a random variable X, denoted by σ2,
X.
orVar(X), is defined by
.
(cid:9)(cid:11)
(x −μ )2f (x ), X :discrete;
σ2 = Var(X)= (cid:7) k k X X k (10.9)
. X ∞ (x−μ )2f (x)dx, X :continuous.
−∞ X X


================================================================================
PAGE 287
================================================================================

276 10 Probability
This definition is basically the same as that given in Sect. 4.2.1 of Chap. 4, but
weighted by the probabilities, as was the case for the mean given in the previous
section.
Comparing the definition of variance with the definition of mean,E(X),givenin
.
Eq. (10.8), we can see that an alternate definition is
σ2 = Var(X)=E((X−E(X))2). (10.10)
. X
Using the properties of mean, we have the following:
Var(X)=E((X−E(X))2)
.
= E(X2−2XE(X)+(E(X))2)
= E(X2)−2E(X)E(X)+(E(X))2 (10.11)
= E(X2)−(E(X))2.
The variance measures the average difference of the actual values from the
average. For example, the average light span of 500 incandescent bulbs is 1000
hours. All these bulbs’ light spans may be between 950 and 1050 hours. It is also
possible that half of them have a light span of about 1400 hours, and the other half
of them have about 600 hours only. To assess the quality of these 500 incandescent
bulbs, we need to measure not only the mean value of the light span but also its
variance. If the variance value is small, the quality is stable.
Example 10.18 If X is again the discrete random variable with a probability
1.
distribution given by Table 10.4, findVar(X ).
1 .
Solution E(X ) = 5 from before. Also, from before, X2just takes the values
1 3. 1.
1 and 4 with probability 1 and 2 respectively. So,E(X2)=1×1+4×2 =3.
3. 3. 1 3 3 .
Applying Eq. (10.11), we have
(cid:4) (cid:5)
5 2 2
Var(X )=3− = .
. 1
3 9
Alternatively, we could go back to the definition, namely, Eq. (10.9) and
use that. In this case,
k =2,x =1,x =2,μ = 5,f (x )= 1, andf (x )= 2,giving
. 1 . 2 . X1 3. X1 1 3. X1 2 3.
(cid:4) (cid:5) (cid:4) (cid:5)
(cid:6) 5 2 1 5 2 2 2
(x −μ )2f (x )= 1− × + 2− × =
. k X1 X1 k
3 3 3 3 9
k
(continued)


================================================================================
PAGE 288
================================================================================

10.5 MeanandVarianceofProbabilityDistributions 277
Example 10.18 (continued)
again. Obviously, the first method is the quickest. The long method was just
used to illustrate that the two methods are equivalent.
Example 10.19 What is Var(X)if X is the outcome of a fair six-sided die
.
with numbers from one to six?
Solution Since
1 1 1 1 1 1 7
E(X)=1× +2× +3× +4× +5× +6× = ,
.
6 6 6 6 6 6 2
and
1 1 1 1 1 1 91
E(X2)=12× +22× +32× +42× +52× +62× = ,
.
6 6 6 6 6 6 6
applying Eq. (10.11), we have
(cid:4) (cid:5)
91 7 2 35
Var(X)= − = .
.
6 2 12
Example 10.20 Let X be a random variable. It has the probability density
function
(cid:9)
1 , ≤a x ≤b;
f (x)= b−a
. X
0, otherwise.
Compute the following:
1. E(X)
.
2. V ar(X )
.
(cid:10)
(cid:7) (cid:10)b
Solution E(X) = − + ∞ ∞ x b− 1 a dx = 2(b x − 2 a) (cid:10) (cid:10) = b+ 2 a. .
(cid:7) a
AlsoE(X2)= +∞ x2 1 dx= 1(b2+ab+a2).
−∞ b−a 3 .
(continued)


================================================================================
PAGE 289
================================================================================

278 10 Probability
Example 10.20 (continued)
Applying Eq. (10.11), we have
Var(X)= E(X2)−(E(X))2
.
(cid:4) (cid:5)
1 a+b 2
= (b2+ab+a2)−
3 2
(b−a)2
= .
12
Again, the long way round is to go back to the definition of variance for a
continuous random variable (Eq. 10.9) using the value ofμ = E(X)= b+a
X 2 .
we have just found:
(cid:8)
∞
Var(X)= (x−μ )2f (x)dx
. X X
−∞
(cid:8) (cid:4) (cid:5)
∞ b+a 2 1
= x− dx
−∞ 2 b−a
(cid:4) (cid:5) (cid:10)
= 1 1 x− b+a 3(cid:10) (cid:10)
(cid:10)
b
b−a3 2
a
(cid:4)(cid:4) (cid:5) (cid:4) (cid:5) (cid:5)
1 b−a 3 a−b 3
= −
3(b−a) 2 2
(b−a)2
= ,
12
as before. But the quicker method is the best. The long method was just used
to illustrate that the two methods are equivalent.
10.5.2.1 Properties of Variance
SinceVar(X)= E(X2)−(E(X))2(Eq. 10.11), the following properties of variance
.
are just consequences of the properties of the mean (see Sect. 10.5.1.1).
(1) V ar(X)=0 ,ifX takes a constant va lue.
.
(2) V ar(aX+ b) = a 2Var(X), where a and b are constants.
.
(3) V ar(X+ Y)= V ar(X)+ Var(Y) ,ifX and Y are two independentvariables.
.


================================================================================
PAGE 290
================================================================================

10.6 SpecialUnivariateDistributions 279
Exercise
10.18 Suppose the probability density function of X is given by
(cid:9)
2, 0 <x <1,
f (x)= 2
. X
0, others.
Compute the following:
(1) E(X2)andE(X4)
. .
(2) V ar(2X 2)
.
(3) V ar(2X 2 + 5)
.
10.6 Special Univariate Distributions
In this section, we will look at some important and famous single-variable probabil-
ity distributions, first discrete ones and then continuous ones.
10.6.1 Discrete Random Variables
10.6.1.1 Discrete Uniform Distribution
This distribution gives the same probability for each value of the random variable.
So if the random variable X takes integer values from a to b, inclusive, then the
probability mass function of the discrete uniform distribution is defined as follows:
(cid:9)
1 a ≤x ≤b;
f (x)= n (10.12)
. X
0, otherwise;
wheren=b−a+1. For example, ifa =3andb=9, then there aren=7values,
. . . .
each of 1.
7.
Figure 10.6 illustrates an example of a discrete uniform distribution in the range
of[a,b].
.
When n = b−a +1 is satisfied, the mean and variance of a discrete uniform
.
distribution are given as follows:
a+b
μ = E(X)= ,
. X
2


================================================================================
PAGE 291
================================================================================

280 10 Probability
Fig. 10.6 An illustration of a
discrete uniform distribution
and
n2−1
σ2 = Var(X)= .
. X 12
For our example where a = 3 and b = 9, we get μ = 12 = 6, the middle
. . X 2 .
number. Also, σ2 = 49−1 = 4. You can check these results by using the original
X (cid:11)12 . (cid:11)
definitions inμ = x f (x )from Eq. (10.8), andσ2 = (x −μ )2f (x )
X k k X k . X k k X X k .
from Eq. (10.9).
However, if n = b − a + 1 is not valid for a discrete uniform distribution,
.
the original definitions for computing the mean and variance should be used. For
example, four numbers,3,5,7,9, betweena =3andb=9. The mean value is still
. . .
equal to 3+5+7+9 =6, while (3−6)2 + (5−6)2 + (7−6)2 + (9−6)2 =5.
4 . 4 4 4 4 .
Exercise
10.19 There are ten balls labelled from zero to nine, respectively, in a bag.
Randomly take a ball from the pack, write down the number on it, and then
put it back in the bag. After many runs, what is the approximate mean value
of those numbers?
10.6.1.2 Bernoulli Distribution
If X is a random variable taking two values,x =1andx =0only, with a probability
. .
of p and1−p, respectively, the probability mass function f of this distribution is
.


================================================================================
PAGE 292
================================================================================

10.6 SpecialUnivariateDistributions 281
defined as follows:
(cid:9)
p ifx =1
f (x;p)= (10.13)
. X
q =1−p ifx =0,
then X has a Bernoulli distribu tion.
The key here is that the random variable can only take one of two values. Some
event happens or does not happen. These are called Bernoulli trials and a set of
Bernoulli trials gives the distribution.
Equation (10.13) can also be expressed as follows:
f (x;p)=px(1−p)1−x,wherex = 0or1. (10.14)
. X
The mean and variance of a Bernoulli distribution are given as follows:
μ = E(X)= p,
. X
and
σ2 = Var(X)= p(1−p).
. X
Again, you can check these results by using the original definitions from
Eqs. (10.8) and (10.9) for the discrete random variable.
Example 10.21 Figure 10.7 shows a simulation result of generating 5000
random numbers from a Bernoulli distribution with a success probability of
P(X=1)=0.2. As can be seen from the figure, about 1000 numbers have a
.
value of 1, and about 4000 numbers have a value of 0. The only two outcomes
are a 1 with probability0.2and a 0 with probability1−0.2=0.8.
. .
10.6.1.3 Binomial Distribution
Let us consider the example of tossing coins again. Suppose we have n independent
coins with a probability p of heads-up. For fair coins, then p = 1, but generally,
2.
the probabilities could be p for heads-up and, therefore, 1 − p for tails. We flip
.
them all simultaneously and check the number of heads-up coins. Alternatively, we
can flip one coin n times and check the number of heads-ups in total. These two
scenarios are equivalent because we assume these coins are independent. Both can
be considered as n independent and identical Bernoulli trials or distributions (two
outcomes: heads with probability p and tails with probability 1 − p). The total
.


================================================================================
PAGE 293
================================================================================

282 10 Probability
Fig. 10.7 The simulation
result of a Bernoulli
distribution
number of heads-ups can be modelled from a binomial distribution. For instance, if
we tossed 100 fair coins, we might expect 50 to be heads-up. But we might want to
know the probability of getting exactly 50 heads-ups (this is actually ≈0.080). Or
.
perhaps exactly 49 heads-ups (≈0.078), 48 heads-ups (≈0.074), and so on. This
. .
sort of question is given by considering a binomial distribution, where we want to
know the probability of getting x events in n trials.
A random variable X is called a binomial random variable withparameters(n,p)
.
if its probability mass function is given as follows:
n!
p (x)= P(X= x)= px(1−p)n−x. (10.15)
. X x!(n−x)!
Here, n is the number of independent trials, p is the probability of success on each
trial, and x is the number of successe(cid:2)s (cid:3)in t hosetrials.
Note that the factors n! are n , which are the binomial coefficients, which
x!(n−x)!. x .
is why this is called a binomial distribution. A random variable having a binomial
distribution can be denoted asX ∼ B(n,p).
.
Using this formula, if we want to get the probability of getting exactly 50 heads
up out of 100 fair trials, we setn=100,x =50, andp = 1. So,
. . 2.
100! 1 1
p (50)= P(X=50)= ( )50( )50 ≈0.080.
. X 50!50! 2 2
as we stated previously.
The mean and variance of a binomial distribution can be obtained as follows:
μ = E(X)= E(X +···+X )=p +···+p =np,
. X 1 n


================================================================================
PAGE 294
================================================================================

10.6 SpecialUnivariateDistributions 283
and
σ2 = Var(X)= Var(X +···+X )= p(1−p)+···+p(1−p)=np(1−p),
. X 1 n
whereX ,...,X are the outcome of each of the n independent Bernoulli trials.
1 n.
Example 10.22 We want to know the probability of having two heads-ups
when flipping a fair coin three times using the Bernoulli distribution and the
binomial distribution separately. Suppose p(heads-up) = 1. Let X be the
2 .
random variable of seeing heads-up.
1. Solution—using Bernoulli distribution three times
We know that the sample space is
(cid:2) ={TTT,HTT,THT,TTH,HHT,HTH,THH,HHH}
.
and there are three elements in the event that have two heads-ups, namely,
HHT,HTH,and T HH :
.
1 1 1
P(X(HHT))= × × ,
.
2 2 2
1 1 1
P(X(HTH))= × × ,
.
2 2 2
and
1 1 1
P(X(THH))= × × .
.
2 2 2
Therefore, we have
1 1 1 3
P(X=2heads-ups)= + + = .
.
8 8 8 8
Note that there is one element in the sample space that has three heads-
ups, three with two heads-ups, three with one heads-up, and one with zero
heads-up. The numbers 1, 3, 3, and 1 are where the binomial coefficients
come from in the next solution.
2. Solution—using binomial distribution
Substitutingn=3andx =2to Eq. (10.15), we have
. .
(cid:4) (cid:5) (cid:4) (cid:5)
3! 1 2 1 3−2 3
p (2)= P(X=2)= 1− = ,
. X 2!(3−2)! 2 2 8
(continued)


================================================================================
PAGE 295
================================================================================

284 10 Probability
Example 10.22 (continued)
where the binomial coefficient returns the number of combinations of two
heads-ups among three tosses and(1)2(1−1)3−2 = 1 gives the probability
2 2 8.
of one desired combination.
Example 10.23 If we flip 20 biased coins simultaneously, what is the
probability that we see 12 heads-ups? Note that the probability of heads-up
of each coin is0.6.
.
Solution Applying Eq. (10.15), wheren=20,x =12andp =0.6,wehave
. . .
20!
P(X=12)= 0.612(1−0.6)20−12 ≈0.1797.
. 12!(20−12)!
Exercise
10.20 A module’s passing rate is85%. Find the probability that:
.
(1) Exactly seven students out of ten pass the module.
(2) Exactly eight students out of ten pass the module.
(3) Exactly nine students out of ten pass the module.
(4) Exactly ten students out of ten pass the module.
10.6.1.4 Poisson Distribution
The Poisson distribution is characterised by the number of events that happen within
some interval. A random variable X is called a Poisson random variable with
parameterλif its probability mass function is given as follows:
.
λx
p (x)= P(X=x eventsinaninterval)=e −λ , x=0,1, ···, (10.16)
. X x!
whereλ >0 is the average number of events per interval and e is Euler’s number
.
2.71828 ....
.
The mean and variance of a Poisson distribution are given by
μ = E(X)= λ,
. X


================================================================================
PAGE 296
================================================================================

10.6 SpecialUnivariateDistributions 285
and
σ2 = Var(X)= λ.
. X
The Poisson distribution is actually a limiting case of the binomial distribution as
n → ∞. Usually, with a large n and a small p, the Poisson distribution is a useful
.
and very good approximation to the binomial distribution. So, it is used when n is
large or unknown and p is small or unknown. If themeanvalueλ =np is known,
.
often in the form of a known average number of events in some interval, then we
can use the Poisson distribution. λis the only parameter in the Poisson distribution,
.
whereas the binomial distribution has two parameters (n and p ).
Example 10.24 Let us use a Poisson distribution to model the number of
patients a doctor can see in one hour. Suppose a doctor was able to see three
patients an hour on average. Find the probability that this doctor can see five
patients in the next hour.
Solution Applying Eq. (10.16), whereλ=3andx =5,wehave
. .
35
P(X=5)=e −3 =0.1008.
. 5!
Exercises
10.21 A doctor can see six patients an hour on average. Use a Poisson
distribution to find:
(1) The probability that the doctor can see five patients in the next hour.
(2) The probability that the doctor can see six patients in the next hour.
(3) The probability that the doctor can see seven patients in the next hour.
(4) The probability that the doctor can see eight patients in the next hour.
10.22 The number of calls a helpdesk receives per minute follows a Poisson
distribution withλ=4.
.
(1) What is the probability that the number of calls is exactly eight in one
minute?
(2) What is the probability that the number of calls is more than five per
minute?


================================================================================
PAGE 297
================================================================================

286 10 Probability
10.6.2 Continuous Random Variables
10.6.2.1 Continuous Uniform Distribution
The discrete uniform distribution can be easily extended to a continuous case. A
random variable X is called a continuous uniform variable in the rangeof[a,b]if
.
its probability density function is given by
(cid:9)
1 a ≤x ≤b;
f (x)= b−a (10.17)
. X
0 otherwise.
The value 1 makes sure that the total area under the curve is 1 so that the total
b−a.
probability is 1.
To find the cumulative distribution, we need to find the area under the curve up
to some random point x. So, we compute the following:
(cid:8) (cid:10)
. F X (x)= x b− 1 a dz= b− z a (cid:10) (cid:10) (cid:10) x = x b− − a a .
a a
Therefore, the cumulative distribution functionF (x)is given by
X .
⎧
⎪⎪⎨0 , x;<a
.
F
X
(x)=
⎪⎪⎩
x
b−
−
a
a, a≤x ≤b; (10.18)
1 , x >b.
Figure 10.8 shows the sketch of the probability density function and the cumulative
distribution function for the continuous uniform distributions. The mean and
Fig. 10.8 A sketch of the probability density function and the cumulative distribution function for
the continuous uniform distribution


================================================================================
PAGE 298
================================================================================

10.6 SpecialUnivariateDistributions 287
variance of a continuous uniform distribution are given as follows (see worked
Example 10.20):
a+b
μ = E(X)= ,
. X
2
and
(b−a)2
σ2 = Var(X)= .
.
12
Example 10.25 Buses arrive at five-minute intervals from 5 pm to 6 pm. A
student arrives at the bus stop at a random time X, which follows a uniform
distribution, between 5 pm and 5:20 pm. What is the probability that the
student waits less than two minutes for a bus?
Solution X is a random variable having a continuous uniform distribution,
wherea =0andb=20(minutes).
. .
Set E as the event the student waits less than two minutes to get on a bus.
We hav e
E ={3 <X <5}+{8 <X <10}+{13 <X <15}+{18 <X <20}.
.
So,
(cid:8) (cid:8) (cid:8) (cid:8)
5 dz 10 dz 15 dz 20 dz
P(E)= + + +
. 20−0 20−0 20−0 20−0
3 8 13 18
1
= (2+2+2+2)
20−0
=0.4.
.
Exercise
10.23 Buses arrive at nine-minute intervals from 5 pm to 6 pm. A student
arrives at the bus stop at a random time X, which follows a uniform
distribution, between 5:20 pm and 5:45 pm. What is the probability that the
student waits less than three minutes for a bus?


================================================================================
PAGE 299
================================================================================

288 10 Probability
Fig. 10.9 Examples of
Gaussian distributions
10.6.2.2 Gaussian or Normal Distribution
A random variable X is distributed normally with mean μ and variance σ2 if its
. .
probability density function is given by
1 −(x−μ)2
. f(x|μ, σ2)= √ e 2σ2 . (10.19)
2πσ2
The normal distribution is also called Gaussian distribution, denoted as X ∼
N(μ,σ2). Figure 10.9 shows four normal distributions, each with a different mean
.
value and standard deviation. As we can see, normal distributions have a bell shape
and are symmetrical in their mean values. The standard deviation controls the shape
of each distribution: the smaller the standard deviation, the narrower the bell curve
and the higher the probability density value at the mean point; on the other hand,
the larger the standard deviation, the wider the curve and the lower the probability
density value at the mean point.
It is convenient to define a “standard” normal distribution with zero mean and a
standard deviation of 1. As we will see later, this enables us to have just a single
table of values to look up information, rather than one for each mean and standard
deviation. All normal distributions can be standardised as shown below.
Definition 10.5 (Standard Normal Distribution) The random variable Z with
zero mean and unit standard deviation, thatis,Z ∼ N(0,1), is called the standard
.
normal distribution. That is,
. f(z|0, 1)= √ 1 e −z 2 2 . (10.20)
2π


================================================================================
PAGE 300
================================================================================

10.6 SpecialUnivariateDistributions 289
Fig. 10.10 The standard normal distribution: (a)68%.of values are within 1 standard deviation of
the mean; (b)95%. of values are within about 2 (or more accurately 1.96) standard deviations of
the mean
A general normal distributionX ∼ N(μ,σ2)can be standardised to a standard
.
normal distribution by computing the z-score for x as follow s:
x−μ
z= . (10.21)
.
σ
A standard normal distribution has the following properties:
(cid:129) About two-thirds (68%) of the total observations lie within one standard devia-
.
tion on either side of the mean: one-third on one side and one-third on the other
(see Panel (a) of Fig. 10.10).
(cid:129) 95% of the total observations are located within about two (or more accurately
.
1.96) standard deviations on either side of the mean (see Panel (b) of Fig. 10.10).
These properties are valid for all normal distributions.
Exercises
10.24 About what percentage of the observations in a normal distribution
will have values greater than one standard deviation above the mean?
10.25 The distribution of scores collected from a module in the past five
years, including 1000 students, is approximately normal. The mean score is
54, and the standard deviation is 4. If the passing score of the module is 50,
approximately how many students failed the module?
Again, to find the cumulative distribution, we need to find the area under the
curve up to some random point z. The cumulative distribution function of Z


================================================================================
PAGE 301
================================================================================

290 10 Probability
satisfying Eq. (10.20) is therefore
(cid:8)
. F Z (z)= P(Z≤z)= √
1 z
e
−
2
ξ2
dξ.
2π −∞
This is the total cumulative probability of the random variable Z being less than z.
(It is quite common to useξ as the integration variable in this work.)
.
By convention, the cumulative distribution function of Z is denoted by (cid:10). So,
.
we have
(cid:8)
. (cid:10)(z)= √
1 z
e
−
2
ξ2
dξ. (10.22)
2π −∞
The cumulative distribution function of a random variable X, where X ∼
N(μ,σ2) and where X can be converted to Z via Z = X−μ, can be obtained as
. σ .
follows:
F (x)=P(X≤ x)
. X
=P(σZ+μ≤ x)
(cid:4) (cid:5)
x−μ
=P Z ≤ .
σ
That is,
(cid:4) (cid:5)
x−μ
F (x)=(cid:10) = (cid:10)(z). (10.23)
. X
σ
(cid:4) (cid:5)
(cid:10)
x−μ
has the standard normal distribution. The term
x−μ
is commonly
σ . σ .
referred to as the z-score. Figure 10.11 shows the standard normal probability
density distribution and its corresponding cumulative distribution. The cumulative
distribution gives us the area under that probability density function for the interval
of negative infinity to a specific z-score. Obviously, ifμ = 0andσ = 1, then we
. .
already have a standard normal distribution and the conversion is unnecessary.
The following lists some properties of (cid:10):
.
(cid:129) lim x→−∞(cid:10)(x) = 0
.
and lim x→∞(cid:10)(x)=1
.
.
(cid:129) (cid:10)(0) = 1, since half the probability is to the left of the mean.
2.
(cid:129) (cid:10)(−z) = 1−(cid:10)(z ), since it is symmetric about the mean.
.
In addition, since P(a < X ≤ b) = F (b) − F (a) (see Sect. 10.4 of this
X X .
chapter), we have
(cid:4) (cid:5) (cid:4) (cid:5)
x −μ x −μ
P(x <X≤x )=(cid:10) 2 −(cid:10) 1 . (10.24)
. 1 2
σ σ


================================================================================
PAGE 302
================================================================================

10.6 SpecialUnivariateDistributions 291
Fig. 10.11 The standard normal distribution. The left panel shows the probability density
distribution; the right panel presents its cumulative distribution
Note that the integration in Eq. (10.22) cannot be evaluated analytically in a
closed form. It requires numerical analysis approximation methods. However, since
the normal distribution is one of the most widely used distributions, people have
constructed a mathematical table for the standard normal distribution.1
Reading a Normal Distribution Table A normal distribution table is usually
composed of three parts:
(cid:129) The heading of rows, which is the left column, contains the integer part and the
first decimal place of the z-score.
(cid:129) The heading of columns contains the second decimal place of the z-score.
(cid:129) The values within the table are the probabilities, which are the area under the
normal curve from the starting point to z. The starting point could be zero
for cumulative from the mean, negative infinity for full cumulative, or positive
infinity for complementary cumulative. (Different books have different tables
with different starting points.)
Example 10.26 Find the cumulative probability of z = 0.56. That is, find
.
(cid:10)(0.56).
.
Solution We find the row starting with 0.5and then across the column to0.06
. .
in a standard normal distribution table. It gives us a probability of0.2123for
.
a cumulative from the mean table (i.e.,0.2123above halfway) or0.7123from
. .
a full cumulative table.
1Theexampletablewehaveusedisathttps://en.wikipedia.org/wiki/Standard_normal_table.


================================================================================
PAGE 303
================================================================================

292 10 Probability
Example 10.27 SupposeX ∼N(3,42). ComputeP(X <10.84).
. .
Solution Using Eq. (10.23),
(cid:4) (cid:5)
10.84−3
F (10.84)=(cid:10) = (cid:10)(1.96).
. X
4
This givesz=1.96. Looking this up in a full cumulative table, we get0.9750.
. .
In fact, the value1.96on either side of the mean represents the distance from
.
the mean that gives95%of the total (see Fig. 10.10). Of the other5%half is
. .
on the left, so the cumulative probability up to1.96is95%+2.5%=97.5%
. .
or0.9750, which is the value we obtained.
.
Exercises
10.26 Suppose X ∼ N(2,32). Compute, using an appropriate table, the
.
following:
(1) P(X <8 ) ,P(X <0.5), and henceP(0.5 <X <8)
. . .
(2) P(−1 < X <5 )
.
(3) The value of C sothatP(X >C)= P(X≤ C)
.
10.27 The lifespan (in years) of refrigerators designed by manufacturer A
follows a normal distribution with a mean value μ of 16 years. If P(12 <
.
X ≤20)=0.8, what is the largest standard deviation value σ?
. .
The normal distribution is extremely widely used in both real-world applications
and theoretical work in research. For example, the heights of adult men or adult
women may be approximately normally distributed. Residual errors from a regres-
sion model may have a normal distribution, and this normality is an assumption
when formulating the linear regression method within the probability framework
(see Chap. 13). There are many more examples of its use.


================================================================================
PAGE 304
================================================================================

Chapter 11
Further Probability
The previous chapter introduced probability, probability distributions for both
discrete and continuous random variables, and properties of probability distributions
like mean and variance. We then illustrated some common and important probability
distributions, like the binomial distribution, the Poisson distribution, and the normal
distribution. All of the introductory probability concerned material related to single
random variables.
In this chapter, we continue with some more advanced topics centering around
multiple random variables and conditional probability, starting with an important
theorem in probability called the central limit theorem.
11.1 The Law of Large Numbers and the Central Limit
Theorem
In practice, people have observed that when the number of experiments approaches
a large number, the probability of the occurrence of a specific event(E)will become
.
stable. For instance, the number of head-ups obtained when flipping a fair coin will
get nearer to half of the total as more flips are performed. In addition, people have
also found that many observed random variables are the sum of other independent
random variables, some of which may not be measurable. Research shows that when
the number of independent random variables approaches infinity, the distribution of
the random variable of the sum of these independent random variables approximates
the normal distribution.
This topic is usually associated with taking samples from a population and how
the mean of the sample relates to the mean of the population. This section introduces
the law of large numbers and the central limit theorem without giving any proof.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 293
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_11


================================================================================
PAGE 305
================================================================================

294 11 FurtherProbability
11.1.1 The Law of Large Numbers
Intuitively, the law of large numbers is obvious! It basically says that if you wish to
know the value of a variable, like the average height of an adult, then you do not
just look at one person; you need to look at lots of people, and the more you look,
the better.
Formally, we have the following. Let X , ..., X be a sequence of independent,
1. . n.
identically distributed random variables withE(X )=μ, i =1,...,n, and define
i .
the sample mean as follows:
(cid:2)n
X ¯ = 1 (X +···+X ).
. n 1 n
n
i=1
Then, for any(cid:2) >0 ,
.
• The weak law of large numbers states that
lim P(|X ¯ −μ| <(cid:2))→1. (11.1)
. n→∞ n
It says that asn →∞, the probability of the difference between the sample mean
.
and the expected mean being less than any small value (cid:2) approaches 1.
.
• The strong law of large numbers states that
P(lim X ¯ =μ)=1. (11.2)
. n→∞ n
It says that as n → ∞, we know almost for sure (P = 1) that the sequence X ¯
. . n.
converges to the expected mean.
Remark 11.1 The weak law tells us how a sequence of probabilities converges. In
other words, for any small value (cid:2) >0 , the probability of the difference between
.
¯
the sample mean X and the population mean μ being less than (cid:2) tends to 1. The
n. . .
strong law states that X ¯ approach μasn →∞with probability 1.
n. . .
The differences between the weak law of large numbers and the strong law of
large numbers are subtle and not important for this book.
In practice, we can apply the principle of the law of large numbers to find the
approximated expected value of a distribution by repeating a procedure over and
over and then computing the average result, which will be close to the expected
value.
For instance, if X is the attendance at a football match, then X is a discrete
i. i.
random number taking integer values from zero to the stadium size. If n is the
number of matches in a month, then we have a sample size of, say, four (one match
each week), and then the average attendance may be reasonably close to the real
average attendance. But ifwe take n as the number of matches inayear, we willhave
a larger sample size, and the mean attendance will be closer to the realmean. (cid:2)
.


================================================================================
PAGE 306
================================================================================

11.1 TheLawofLargeNumbersandtheCentralLimitTheorem 295
Fig. 11.1 The result of a
simulation to estimate the
probability of obtaining 12
heads in 20 coin flips, where
the event occurred 1826 times
out of 10,000 trials
Example 11.1 Continue Example 10.23 from the previous chapter, Chap.10.
We can do a simulation via a computer program to estimate the probability
of seeing 12 heads if we flip 20 coins simultaneously. Let E be the event
where we observe the number of heads-up, X = 12. We have used Python
.
programming to implement the simulation in this example. In one trial, 20
independent experiments can be run, each having a Bernoulli distribution with
the probability of coming-up heads of each coin being 0.6. The number of
.
heads-up can be obtained in these 20 experiments; this is noted. We repeat this
procedure with a large number of trials. Each time the trial of 20 experiments
is performed and the number of heads-up is noted. Figure 11.1 shows the
simulated result with10,000trials. That is, we have done the 20 experiments
.
10,000 times, each time noting the number of heads obtained. Specifically,
.
the number of our events, E, that is, 12 heads observed, in10,000repetitions,
.
is 1826. Therefore, the probability that we see 12 heads if we flip 20 coins
simultaneously is approximated as0.1826based on the law of large numbers.
.
It is larger than0.1797, the theoretical value computed in the example shown
.
in Example 10.23. To make the estimation more accurate, we may increase
the number of trials in the simulation.
11.1.2 Central Limit Theorem
So far, we have that if we take a sample of size n from a population with an unknown
mean, then the mean of the sample is a reasonable estimate of the mean of the
population, and this estimate gets better as n gets larger. The central limit theorem
takes the analysis further and says that if we repeatedly find the mean of an n-
.
sized sample, we expect to get as many below the population (real) mean as there


================================================================================
PAGE 307
================================================================================

296 11 FurtherProbability
are above. In fact, the distribution of sample means approaches a normal distribution
around the population mean, irrespective of the sort of distribution we had in the first
place. This can be seen by looking at Fig. 11.1. Asn gets larger, the distribution gets
closer to a normal distribution, and also, the standard deviation of the distribution
gets smaller. That is, the values get tighter around the populationmean.
Formally, we have the following. Let X ,...,X be n independent random
1 n.
variables, each of which has mean μ
.
and standard deviation σ
.
. Let Y
n
= √(X
1
+
···+X )/nbe the average; thus, Y has mean μand standard deviationσ/ n.I fn
n . n. . .
is large, then the cumulative distribution of Y is very nearly equal to√ the cumulative
distribution of theGaussianwithmeanμand standard deviationσ/ n. That is,
. .
Y −μ
lim n √ ∼N(0,1), (11.3)
.
n→∞ σ/ n
or,
(cid:3) (cid:4)
Y −μ
lim P n √ ≤z = (cid:4)(z). (11.4)
.
n→∞ σ/ n
Example 11.2 In a doctor’s surgery over a long period, it is noted that the
average length of a patient’s appointment is 10 minutes with a standard
deviation of 5 minutes. So, patients are scheduled every 10 minutes for the
21 hours of each morning’s surgery time. Unfortunately, one morning there
2.
is an emergency appointment, so there are 16 patients to see rather than the
normal 15. What is the chance that the doctor can finish on time?
Solution Each patient is assumed to be independent and drawn from the
distribution with a mean time of 10 minutes and a standard deviation of 5
minutes.Thismeansthevariance is52 =25.Leteachofthese16appointment
.
lengths be denoted X for1≤i ≤16. For the total time taken, we have
i. .
Y =X +···+X .
. 16 1 16
The mean for Y is 10 × 16 = 160, using property (4) of means (see
16. . .
Sect. 10.5.1.1 of Chap. 10), and the variance of Y is16×25using property
16. .
(3)of variance (see Sect. 10.5.2.1 of Chap. 10), and so the standard deviation
.
for Y is4×5=20.
16. .
From the central limit theorem, this means that the distribution of Y is
16.
approximately a Gaussian with a mean of 160 and a standard deviation of
20.
(continued)


================================================================================
PAGE 308
================================================================================

11.1 TheLawofLargeNumbersandtheCentralLimitTheorem 297
Example 11.2 (continued)
We want P(y ≤ 150) for the doctor to finish on time. So that we can
.
use the tables of values for a normal distribution, we have to standardise the
normal distribution using z-scores as in Eq. (10.21). This gives
(cid:3) (cid:4)
y−160 150−160 1
P(y≤150)=P ≤ = P(z ≤− ),
.
20 20 2
which using the central limit theorem, Eq. (10.23), and the table (refer to the
information provided in the footnote of Sect. 10.6.2.2) gives us
1 1
P(z ≤− )= (cid:4)(− )=0.3085.
.
2 2
A special case of the central limit theorem is the De Moivre-Laplace theorem. Let
η be a binomial random variable with parameters(n,p). Then, De Moivre-Laplace
n. .
theorem states:
(cid:3) (cid:4) (cid:3) (cid:4)
b−np a−np
P(a≤η ≤b)≈(cid:4) √ −(cid:4) √ , (11.5)
. n np(1−p) np(1−p)
where np and np(1 − p)are the mean and variance of a binomial distribution
.
(see Sect. 10.6.1.3 of Chap. 10). These values are needed to make the normal
distribution into a standard normal distribution so we can use a table of values.
See also Eq. (10.24) in Chap. 10, which is where the above equation comes from.
Example 11.3 There are 100 computers running independently in a PC lab.
The probability of the actual working time of each PC is80%of the total lab
.
opening time each day. Compute the probability that there are between 70 and
86 computers working at any lab opening time.
Solution Suppose each computer has two statuses: working or not working.
Since computers work independently, we consider 100 computers as 100
Bernoulli trials. Repeated Bernoulli trials give us a binomial distribution—
see Sect. 10.6.1.3 of Chap. 10. Suppose the number of working computers is
η
n
∼ B(100,0.8)
.
. Since the mean of this binomial distrib√ution is np = 80
.
and the standard deviation of this binomial distribution is np(1−p) = 4,
.
(continued)


================================================================================
PAGE 309
================================================================================

298 11 FurtherProbability
Example 11.3 (continued)
we can standardise the normal distribution using z-scores as in Eq. (10.21)
and have
(cid:3) (cid:4)
70−80 η −80 86−80
P(70≤η ≤86)=P ≤ n ≤
. n
4 4 4
(cid:3) (cid:4)
η −80
=P −2.5≤ n ≤1.5 .
4
Applying Eq. (11.5), we have
(cid:3) (cid:4)
η −80
P −2.5≤ n ≤1.5 = (cid:4)(1.5)−(cid:4)(−2.5)=0.9270.
.
4
Exercises
11.1 For the doctor’s surgery introduced in Example 11.2:
(1) Confirm that if the expected number of 15 patients turn up for a morning
appointment, then the probability of the doctor finishing on time is 0.5.
.
(2) One of the doctors deals with only older patients, and it is found that
these require an appointment with an average length of 15 minutes and
a standard deviation of 10 minutes. Again, if an emergency patient turns
up one morning so that there are 11 instead of the 10 expected patients to
see, find the chance that the doctor can finish on time.
(3) Now assume that the older patients still take an average of 15 minutes, but
now the standard deviation is 5 minutes. Again, 11 instead of 10 patients
arrive, finding the chance that the doctor can finish on time.
11.2 Compute the probability of coming-up heads is greater than 60 times
when flipping a fair coin 100 times.
Remark 11.2 The law of large numbers (LLN) and the central limit theorem (CLT)
are used for different purposes. LLN addresses the convergence of sample means to
the population mean, while CLT concerns the convergence of sample means to a
normal distribution.
(cid:2)
.


================================================================================
PAGE 310
================================================================================

11.2 MultipleRandomVariables 299
11.2 Multiple Random Variables
In many real-world applications, it is not enough to compute the mean and variance
of one random variable. Studying two or more random variables defined on the same
sample space is important. For example, consider tossing one fair coin five times
and repeat this experiment many times. We use X to denote the random variable of
observing the number of coming-up heads in the first two tosses, and Y denotes the
number of heads-up in the last three. Then, we can compute the joint pr obability
of(X,Y). That is, we are interested in the probability distribution of both random
.
variables, X and Y .
11.2.1 Joint Probability Distributions: Discrete Random
Variables
Let us start with two examples to bear in mind while reading this.
First: We could roll a fair six-sided die and define X = 1 if an even number
.
is thrown and X = 0 otherwise. We could also define Y = 1 if a square number
. .
is thrown (i.e., a one or a four) and Y = 0 otherwise. Here, we can look at joint
.
probabilities such as the chance of throwing an even number and a square number,
that is, bothX =1andY =1.
. .
Second: Consider an ordinary pack of cards and turn over a card to look at it.
DefineX =1if it is a red card andX =0if it is a black card. Also, defineY =1if
. . .
it is a “high” card (defined to be an Ace, a King, a Queen, or a Jack) so thatY = 0
.
for all other cards (2, 3, 4, 5, 6, 7, 8, 9, 10). Here, we can consider the probability of
it being black and a high card (X =0andY =1), for instance.
. .
11.2.1.1 Joint Probability Mass Functions and Cumulative Distribution
Functions
Consider two discrete random variables, X and Y. The joint probability mass
function,p (x,y), can be given by
XY .
p (x ,y )= P(X=x ,Y=y ), (11.6)
. XY i j i j
where (x ,y ) denotes pairs of values X and Y can take. In the case of the die
i j.
roll, both X and Y can only take the values 0 and 1. So, p (x ,y ) is a table of
XY i j .
probabilities with the four combinations, which are
P(X=0,Y=0), P(X =1,Y=0), P(X =0,Y=1), and P(X=1,Y=1).
.


================================================================================
PAGE 311
================================================================================

300 11 FurtherProbability
Properties ofp (x ,y )
XY i j .
1. Each joint probability is between 0 and 1:
0≤p (x ,y )≤1.
. XY i j
2. The sum of all the joint probabilities adds up to 1:
(cid:2)(cid:2)
p (x ,y )=1,
. XY i j
xi yj
where the summation is taken over all possible pairs of(x ,y ).
i j .
The joint cumulative distribution function of X and Y, denotedbyF (x,y), is
XY .
again the sum of all the probabilities, in this case forx up to x andfory up to y.
i. j.
The function is defined by
(cid:2) (cid:2)
F (x,y)= P(X≤ x,Y ≤ y)= p (x ,y ). (11.7)
. XY XY i j
xi ≤x yj ≤y
11.2.1.2 Marginal Probability Distributions
Consider two discrete random variables X and Y .I f
(cid:2)
P(X=x )=p (x )= P(X=x ,Y=y ), (11.8)
. i X i i j
yj
that is, the summation is taken over all possible values(x ,y )with x fixed. In this
i j . i.
case, Eq. (11.8) is called the marginal probability mass function of X. So, you could
find the sum of the probabilities for Y when X is fixed, for instance, at X = 0.
.
Similarly, the marginal probability mass function of Y is given by
(cid:2)
P(Y=y )=p (y )= P(X=x ,Y=y ). (11.9)
. j Y j i j
xi
Remark 11.3 Note that X and Y are independent random variables, if
p (x ,y )=p (x )p (y ). (11.10)
. XY i j X i Y j
(cid:2)
.
We can visualise the joint and marginal probability distributions through a table.


================================================================================
PAGE 312
================================================================================

11.2 MultipleRandomVariables 301
Example 11.4 Flipping two fair coins. Let
(cid:5)
0, Thefirstcointails-up,
X =
.
1, Thefirstcoinheads-up,
and
(cid:5)
0, Thesecondcointails-up,
Y =
.
1, Thesecondcoinheads-up.
Compute the probability distribution of(X,Y).
.
Solution Possible values of (X,Y) are (0,0), (0,1), (1,0) , and (1,1).
. . . . .
Assume two flips are independent.
Applying Eqs. (11.6) and (11.10), we can obtain p ,p ,p , and p .
11 12 21 . 22.
For example,
1 1 1
p = P(X=0,Y=0)= P(X=0)P(Y =0)= × = .
. 11
2 2 4
Therefore, the joint probability mass distribution of (X,Y) is shown in
.
Table 11.1.
Table 11.2 shows the joint cumulative distribution function of(X,Y). The
.
value in each cell is obtained by summing over values of the joint probability
mass function (i.e., to apply Eq. (11.7)). For example, consider F (X =
XY
0,Y=1):
.
1 1 1
F (X =0,Y=1)= P(X=0,Y=0)+P(X=0,Y=1)= + = .
. XY
4 4 2
The marginal probability distribution of (X,Y) is computed by applying
.
Eq. (11.8)o rE q. (11.9). For example, P(X= 0) = p (X = 0) = (P(X =
X
0,Y=0)+ P(X=0,Y=1))= 1+1 = 1. Table 11.3 shows the marginal
4 4 2.
probability distribution of(X,Y).
.
Table 11.1 The joint
X=0 X=1
probability mass function of
X and Y as given i n Y=0 4 1 . 1 4.
Example11.4 Y=1 4 1 . 1 4.


================================================================================
PAGE 313
================================================================================

302 11 FurtherProbability
Table 11.2 The joint
X=0 X=1
cumulative distribution
function of X and Y as given Y=0 4 1 . 1 2.
inExample11.4 Y=1 2 1 . 1
Table 11.3 The marginal X=0 X=1 P(Y=yi).
probability of X and Y (as
giveninExample11.4) are Y=0 4 1 . 1 4. 2 1 .
shown in the last row and the Y=1 4 1 . 1 4. 2 1 .
last column of the table P(X=xi). 2 1 . 1 2. –
Example 11.5 Now, let us do the dice-throwing example where
(cid:5)
0, Anoddnumberisthrown,
X =
.
1, Anevennumberisthrown,
and
(cid:5)
0, Anon-squarenumberisthrown,
Y =
.
1, Asquarenumberisthrown.
Compute the joint probability distribution and the marginal probability distri-
bution for(X,Y).
.
Solution Possible values of (X,Y) are (0,0), (0,1), (1,0), and (1,1).
. . . . .
Assume two throws are independent.
Applying Eqs. (11.6) and (11.10), we can obtain p ,p ,p , and p .
11 12 21 . 22.
For example,
1 1 1
p = P(X=1,Y=1)= P(X=1)P(Y =1)= × = .
. 22
2 3 6
Therefore, the joint probability mass distribution of (X,Y) is shown in
.
Table 11.4.
The marginal probability distribution of (X,Y) is computed by applying
.
Eq. (11.8). For example, P(X = 0) = p (X = 0) = P(X = 0,Y =
X
0)+ P(X= 0,Y = 1) = 1 + 1 = 1. Table 11.4 also shows the marginal
3 6 2.
probability distribution of(X,Y).
.


================================================================================
PAGE 314
================================================================================

11.2 MultipleRandomVariables 303
Table 11.4 The joint probability mass function of X and Y (as giveninExample11.5)w itht he
marginal probability of X and Y shown in the last row and the last column ofthetable
X=0 X=1 P(Y=yi).
Y=0 1 3. 3 1 . 3 2 .
Y=1 1 6. 6 1 . 3 1 .
P(X=xi). 1 2. 2 1 . –
Table 11.5 An example of a X \ .Y Sunny Cloudy Rainy
joint probability mass
function of X (games) and Y Badminton 0 0 9 1 .
(weather) as given i n Swimming 9 3 . 9 1 . 9 1 .
Example11.6 Football 9 2 . 9 1 . 0
Table 11.6 An example of a X \ .Y Sunny Cloudy Rainy P(X=xi).
joint probability mass
function of X (games) and Y Badminton 0 0 9 1 . 9 1 .
(weather) as given i n Swimming 3 9. 9 1 . 9 1 . 9 5 .
Example11.6 Football 2 9. 9 1 . 0 9 3 .
P(Y=yi). 5 9. 9 2 . 9 2 . –
Example 11.6 Suppose we have two random variables. X denotes the game
Jack wants to play; Y denotes the weather condition. Table 11.5 shows the
joint probability mass distribution of (X,Y), where the total probability of
.
this table is equal to 1. The probability of Jack playing football when it is
sunny isP(X=football,Y = sunny)= 2 as shown in the table.
9.
We need to apply Eq. (11.7) to obtain the cumulative distribution. However,
X and Y are two categorical variables, and we cannot simply say Y =
Sunny < Y= Rainy. Therefore, we can only set up the cumulative
.
distribution table if given more information.
Table 11.6 shows the marginal probability distribution. If we sum over a
row in Table 11.5, we are looking at all pairs(x ,y ), where Y can take on all
i j .
three values with the value of X fixed. For example, the probability of Jack
swimmingisP(X= Swimming) = 3 + 1 + 1 = 5. Similarly, if we sum
9 9 9 9.
over a column, we consider all possible values of X, and then we obtain the
marginal probability mass value of Y.


================================================================================
PAGE 315
================================================================================

304 11 FurtherProbability
Exercises
11.3 Given the pack of cards example described earlier, that is,
(cid:5)
0, Thecardisblack,
X =
.
1, Thecardisred,
and
(cid:5)
0, Thecardisa“low”card,
Y =
.
1, Thecardisa“high”card,
find the joint probability distribution of (X, Y) and the marginal probability
distribution of (X, Y).
11.4 There are five balls, three red and two green, in a bag. Take two balls one
by one from the bag without putting them back. Define X and Y as follows:
(cid:5)
0, Thefirstoneisgreen,
X =
.
1, Thefirstoneisred,
and
(cid:5)
0, Thesecondoneisgreen,
Y =
.
1, Thesecondoneisred.
Compute the joint probability distribution of (X, Y) and the marginal
probability of(X,Y).
.
11.5 There are ten balls, five red, three green, and two yellow, in a bag. Take
a ball from the bag and put it back, and then take a second ball. Define X and
Y as follows:
⎧
⎪⎪⎨0,
Thefirstoneisyellow,
. X = ⎪⎪⎩ 1, Thefirstoneisgreen,
2, Thefirstoneisred,
(continued)


================================================================================
PAGE 316
================================================================================

11.2 MultipleRandomVariables 305
and
⎧
⎪⎪⎨0,
Thesecondoneisyellow,
. Y = ⎪⎪⎩ 1, Thesecondoneisgreen,
2, Thesecondoneisred.
Compute the joint probability distribution of (X, Y) and the marginal
probability of(X,Y).
.
11.2.2 Joint Probability Distributions: Continuous Random
Variables
This is similar to the previous section, except, since the random variables are
continuous, we need to integrate rather than add up probabilities. Also, since we
have two random variables, the probability density function is a surface “above”
the two variables plotted horizontally. To find probabilities, we need to use double
integration to find the volume “under” the surface as in Sect. 6.3 of Chap. 6.
11.2.2.1 Joint Probability Mass Functions and Cumulative Distribution
Functions
Consider two continuous random variables X and Y. The joint probability density
function,f (x,y), can be given by
XY .
∂2F (x,y)
f (x,y)= XY , (11.11)
. XY
∂x∂y
whereF (x,y)is the cumulative distribution function given by
XY .
(cid:10) (cid:10)
x y
F (x,y)= f (η,ξ)dηdξ. (11.12)
. XY XY
−∞ −∞
Properties off (x,y)
XY .
1. f(cid:11)XY (x(cid:11), y)≥0
.
, probability is always positive.
2. ∞ ∞ f (x, y)dxdy =1, the total probability is always 1.
−∞ −∞ XY .
3. f
XY
(x, y )
.
is cont(cid:11)in(cid:11)uous for all values of(x,y)
.
, or except for a finite set.
4. P(X, Y ∈ D) = f (x,y)dxdy.
D .


================================================================================
PAGE 317
================================================================================

306 11 FurtherProbability
Example 11.7 Let us start with a really simple example: The joint probability
density function of(X,Y)is given by
.
(cid:5)
1, 0 <x <1, 0 <y <1
f (x,y)=
. XY
0, otherwise.
This is a uniform distribution. The two random variables, x and y, go from
0 to 1, and the probability density function is just a flat surface “above” at a
constant “height” of 1. (See the equivalent distribution with one variable given
inSect.10.6.2.1 and illustrated on the left in Fig. 10.8 in Chap. 10.). Here, it
wou(cid:11)ld be(cid:11) illustrated by a cube of size 1. Show that
∞ ∞ f (x,y)dxdy =1.
−∞ −∞ XY .
Find:
1. The cumulative distribution functionF (x,y)
XY .
2. P(0≤ X < 1 , 0≤ Y <1)
2 2 .
3. P(X+ Y< 1)
.
(cid:12)
(cid:11) (cid:11) (cid:11) (cid:11) (cid:11) (cid:12)1
Solution − ∞ ∞ − ∞ ∞ f XY (x, y)dxdy = 0 1 0 1 1dxdy = 0 1 x (cid:12) (cid:12) dy =
(cid:12) 0
(cid:11) (cid:12)1
0 1 1dy =y (cid:12) (cid:12) =1 . .
0
1. Applying Eq. (11.12)(cid:12), when 0 < x <1(cid:12). and 0 < y <1
.
, we hav e
(cid:11) (cid:11) (cid:11) (cid:12)x (cid:11) (cid:12)y
0 x 0 y 1dηdξ = 0 y η (cid:12) (cid:12) dξ = 0 y xdξ = xη (cid:12) (cid:12) =xy. .
0 0
Hence, we have
(cid:5)
xy, 0 <x <1, 0 <y <1
F (x,y)=
. XY
0, otherwise.
2. P(0≤ X < 1 , 0≤ Y< 1)=
2 (cid:12) 2 . (cid:12)
(cid:11) (cid:11) (cid:11) (cid:12)1 (cid:11) (cid:12)1
0 2 1 0 1 2 1dxdy= 0 2 1 x (cid:12) (cid:12) 2 dy = 0 1 2 2 1dy = 1 2 y (cid:12) (cid:12) 2 = 1 4 . .
0 0
This answer makes sense if you think about it. With 0 < x <1 and
2.
0 <y <1, looking “down” on the cube, we are taking just a quarter of the
2.
horizontal area. So, since the probability density function is uniform, this
gives a quarter of the volume, that is, a quarter of the total probability.
3. P(X+ Y< 1). This is more complicated since the limits of the integration
.
involve the linex +y = 1. Looking back at the examples in Sect. 6.3 of
.
(continued)


================================================================================
PAGE 318
================================================================================

11.2 MultipleRandomVariables 307
Example 11.7 (continued)
Chap. 6, we can use the y limits of 0 to 1−x and the x limits of 0 to 1.
.
This give s
P(X (cid:3) + Y <1) (cid:4) = . (cid:12) (cid:12)
(cid:11) (cid:11) (cid:11) (cid:12)1−x (cid:11) (cid:12)1
0 1 0 1−x 1dy dx= 0 1 y (cid:12) (cid:12) dx= 0 1 1−xdx=x− 1 2 x2(cid:12) (cid:12) = 2 1. .
0 0
This again makes sense, since the linex+y =1cuts the horizontal area in
.
half, and so, since the probability density function is uniform, the volume
and probability are also a half.
Example 11.8 The joint probability density function of(X,Y)is given by
.
(cid:5)
Ce
−(2x+3y),
x >0, y >0
f (x,y)=
. XY
0, otherwise
where C is a constant. Find the follow ing:
1. The value of C.
2. The cumulative distribution functionF (x,y).
XY .
3. P(0≤ X <1 , 0≤ Y <2) .
.
Solution
(cid:11) (cid:11)
1. Since ∞ ∞ f (x,y)dxdy =1, we hav e
−∞ −∞ XY .
(cid:10) (cid:10) (cid:10) (cid:10)
∞ ∞ ∞ ∞
Ce
−(2x+3y)dxdy=C
e
−2xdx
e
−3ydy
.
0 0 0 0
1
=C×(− )
2
(cid:10) (cid:10)
∞ ∞
1
×(− ) e −2xd(−2x) e −3yd(−3y)
3
0 0
(cid:12) (cid:12)
(cid:12)∞ (cid:12)∞
= C e −2x(cid:12) (cid:12) e −3y(cid:12) (cid:12)
6
0 0
C
= =1.
6
Therefore,C =6.
.
(continued)


================================================================================
PAGE 319
================================================================================

308 11 FurtherProbability
Example 11.8 (continued)
2. Applying Eq. (11.12), whenx >0 andy >0 , we hav e
. .
(cid:10) (cid:10) (cid:12) (cid:12)
x y (cid:12)x (cid:12)y
. 6e −(2η+3ξ)dηdξ =e −2η(cid:12) (cid:12) e −3ξ(cid:12) (cid:12) =(e −2x −1)(e −3y −1).
0 0 0 0
Hence, we have
(cid:5)
(e −2x −1)(e −3y −1), x >0, y >0
F (x,y)=
. XY
0, otherwise.
(cid:12) (cid:12)
(cid:11) (cid:11) (cid:12)1 (cid:12)2
3. P(0 ≤ X< 1 , 0 ≤ Y< 2) = 6 1 e −2x dx 2 e −3y dy = e −2x(cid:12) (cid:12) e −3y(cid:12) (cid:12) =
0 0
0 0
(e
−2−1)(e −6−1)≈0.8625.
.
Exercise
11.6 The joint probability density function of(X,Y)is given by
.
(cid:5)
A(x+y), 0 <x <4, 0 <y <4
f (x,y)=
. XY
0, otherwise
where A is a constant. Find the follow ing:
(1) The value of A.
(2) The cumulative distribution functionF (x,y).
XY .
(3) P(0≤ X <2 , 0≤ Y <2) .
.
(4) P(X+ Y< 4) .
.
11.2.2.2 Marginal Probability Distributions
Consider two continuous random variables X and Y. The marginal probability
density function of X or Y is giv enby
(cid:10)
∞
f (x)= f (x,y)dy, (11.13)
. X XY
−∞


================================================================================
PAGE 320
================================================================================

11.2 MultipleRandomVariables 309
or
(cid:10)
∞
f (y)= f (x,y)dx, (11.14)
. Y XY
−∞
respectively. This is likejust adding up the values ina row or a column inthe discrete
case.
Example 11.9 Thejointprobabilitydistributionfunctionofrandomvariables
X and Y is give nby
(cid:5)
2e
−(2x+y),
x >0, y >0
f (x,y)=
. XY
0, otherwise.
Compute the marginal probability distribution of X.
Solution Applying Eq. (11.13), the marginal probability distribution of X is
(cid:10)
∞
f (x)=2e −2x × e −ydy
. X
0
(cid:12)
(cid:12)∞
=−2e −2xe −y(cid:12)
(cid:12)
0
=2e −2x.
Exercises
11.7 The joint probability distribution function of random variables X and Y
is give nby
(cid:5)
f (x,y)= x 1e − x y e −x, x >0, y >0
. XY
0, otherwise.
Compute the marginal probability distribution of X.
11.8 The joint probability density function of(X,Y)is given by
.
(cid:5)
Acos(x−y), 0 <x <π, 0 <y <π
f (x,y)= 2 2
. XY
0, otherwise
(continued)


================================================================================
PAGE 321
================================================================================

310 11 FurtherProbability
where A is a constant. Find the follow ing:
(1) The value of A.
(2) The cumulative distribution functionF (x,y).
XY .
(3) P(0≤ X < π , 0≤ Y <π).
4 4 .
(4) The marginal probability distribution of X.
(5) The marginal probability distribution of Y.
Remark 11.4 Note that X and Y are independent random variables, if
F (x,y)=F (x)F (y),
. XY X Y
or
f (x,y)=f (x)f (y).
. XY X Y
(cid:2)
.
11.2.2.3 Covariance
Consider two jointly distributed continuous random variables X and Y. We can
define the covariance via the expected value as follow s:
(cid:13) (cid:14)
cov(X,Y)=E (X−E(X))(Y −E(Y)) . (11.15)
.
Remark 11.5 We can see that this definition fits the shape of the original sample
covariance definition, which was in Sect. 4.2.1 of Chap. 4, namely,
(cid:15)
n (x −x¯ )(x −x¯ )
cov(x ,x )= i=1 i,h h i,k k .
. h k n−1
As can be seen, it basically multiplies each value of one variable, with its mean
subtracted, and with each value of the other variable, with its mean subtracted.
It then sums all of these together and divides the result by a constant. The final
summing is basically finding a mean again. Hence, the given definition is of the
correct form.
(cid:2)
.


================================================================================
PAGE 322
================================================================================

11.2 MultipleRandomVariables 311
Equation (11.15) can be further expanded using properties of the expected value
given in Sect. 10.5.1.1 of Chap. 10 as follows:
(cid:13) (cid:14)
cov(X,Y)=E (X−E(X))(Y −E(Y))
.
(cid:13) (cid:14)
=E XY −E(X)Y −XE(Y)+E(X)E(Y) (11.16)
= E(XY)−E(X)E(Y)−E(X)E(Y)+E(X)E(Y)
= E(XY)−E(X)E(Y).
Hence, if cov(X,Y) = 0, that is, if X and Y are uncorrelated, then we hav e
.
E(XY)= E(X)E(Y).
.
We have the two following properties of uncorrelated variables:
• If X and Y are independent random variables, they are also uncorrelated. This
can be proved as f ollows:
(cid:10) (cid:10)
E(XY)= xyf (x,y)dxdy
. XY
(cid:10) (cid:10)
= xyf (x)f (y)dxdy
X Y
(cid:10) (cid:3)(cid:10) (cid:4)
= xf (x) yf (y)dy dx (11.17)
X Y
(cid:3)(cid:10) (cid:4)(cid:3)(cid:10) (cid:4)
= xf (x)dx yf (y)dy
X Y
= E(X)E(Y).
• However, if X and Y are uncorrelated, they can stillbe dependent asthefollowing
ex ampleillustrates.
Example 11.10 This relationship is usually illustrated by using variables that
haveclearlygotzeromean,sothatcov(X,Y)isalsozero.Acommonexample
.
is the following:
Let X be uniformly distributed in the interval[−1,1]. It clearly has a mean
.
value at the centre, that is, E(X) = 0. Now, define Y so that y = x2 in the
. .
interval [−1,1] and zero elsewhere. This also obviously has a mean value
.
at the centre, since it is evenly distributed either side of zero. So, E(Y) = 0.
Now, E(XY) = E(X3), which again has its mean value at the centre. So
.
E(XY) = 0. W e n ow have cov(X,Y) = E(XY)− E(X)E(Y) = 0. So, X
. .
and Y areuncorrelated.
(continued)


================================================================================
PAGE 323
================================================================================

312 11 FurtherProbability
Example 11.10 (continued)
However, from the way that Y was defined, it is clearly dependent on X.
So, X and Y are uncorrelated andalsodependent.
We can now do an example of finding the covariance between two continuous
random variables. We will start with the easy example described in Example 11.7.
Example 11.11 Start with the example started in Example 11.7, namely, the
joint probability distribution function of random variables X and Y give nby
(cid:5)
1, 0 <x <1, 0 <y <1
f (x,y)=
. XY
0, otherwise.
Find the covariance of X and Y .
Solution To find the covariance, we will use Eq. (11.16). This requires
finding E(X) and E(Y), which means we need to know f (x) and f (y)
. . X . Y .
as is seen from the definition of mean given in Eq. (10.8) in Sect. 10.5 of
Chap. 10. Butf (x)andf (y)are just the marginal probability distributions
X . Y .
of X and Y. Tofindf (x),w eu seE q .(11.13):
X (cid:12).
(cid:11) (cid:12)1
f X (x)= 0 ∞ 1dy =y (cid:12) (cid:12) =1. .
0
To findf (y),w eu seE q .(11.14):
Y . (cid:12)
(cid:11) (cid:12)1
f Y (y)= 0 ∞ 1dx=x (cid:12) (cid:12) =1. .
0
We can now findE(X)andE(Y)using Eq. (10.8):
. .
(cid:10) (cid:10) (cid:12)
. E (X)= ∞ xf X (x)dx = 1 xdx= x2(cid:12) (cid:12) (cid:12) 1 = 1
−∞ 0 2 0 2
By a similar calculation, we find that
(cid:10) (cid:10) (cid:12)
. E (Y)= ∞ yf Y (y)dy = 1 ydy= y2(cid:12) (cid:12) (cid:12) 1 = 1 .
−∞ 0 2 0 2
(cid:11) (cid:11)
Then, we findE(XY):E(XY)= ∞ ∞ xyf (x,y)dxdy =
. (cid:12) −∞ −∞ XY(cid:12) .
(cid:11) (cid:11) (cid:11) (cid:12)1 (cid:11) (cid:12)1
0 1 0 1 xydxdy = 0 1 yx 2 2(cid:12) (cid:12) dy = 0 1 2 1ydy= y 4 2(cid:12) (cid:12) = 4 1. .
0 0
Finally,cov(X,Y)= E(XY)−E(X)E(Y)= 1 − 1 × 1 =0.
4 2 2 .


================================================================================
PAGE 324
================================================================================

11.2 MultipleRandomVariables 313
Example 11.12 We will continue with the example started in Example 11.9,
namely, the joint probability distribution function of random variables X and
Y give nby
(cid:5)
2e
−(2x+y),
x >0, y >0
f (x,y)=
. XY
0, otherwise.
Find the covariance of X and Y .
Solution To find the covariance, we will use Eq. (11.16). Again, this requires
findingE(X)andE(Y), which means we need to knowf (x)andf (y)as is
. . X . Y .
seen from the definition of mean given in Eq. (10.8) in Sect. 10.5 of Chap. 10.
f (x) and f (y) are the marginal probability distributions of X and Y . In
X . Y .
fact,Example11.9 already foundf (x).I tw asf (x)=2e −2x.
X . X .
To findf Y (y) . , we useE q .(11.14): (cid:12)
(cid:11) (cid:12)∞
f Y (y)=2e −y × 0 ∞ e −2xdx =−e −ye −2x(cid:12) (cid:12) =e −y. .
0
We can now findE(X)andE(Y)using Eq. (10.8):
. .
(cid:10) (cid:10)
∞ ∞
E(X)= xf (x)dx =2 xe −2xdx
. X
−∞ 0
This integral is calculated using integration by parts (Sect. 5.5.2 of Chap. 5)
withu=x and dv =e −2x,g iving
. dx .
(cid:12) (cid:10) (cid:12)
. 2 x e − −2 2 x(cid:12) (cid:12) (cid:12) ∞ −2 ∞ e − −2 2 x ×1dx=0+ e − −2 2 x(cid:12) (cid:12) (cid:12) ∞ = 1 2 .
0 0 0
By a similar calculation, we find that
(cid:10) (cid:10)
∞ ∞
E(Y)= yf (y)dy = ye −ydy =1.
. Y
−∞ 0
Finally, we findE(XY):
.
(cid:10) (cid:10)
∞ ∞
E(XY)= xyf (x,y)dxdy
. XY
−∞ −∞
(cid:10) (cid:10)
∞ ∞
= xy2e −(2x+y)dxdy
0 0
(cid:10) (cid:3)(cid:10) (cid:4)
∞ ∞
=2 ye −y xe −2xdx dy
0 0
(continued)


================================================================================
PAGE 325
================================================================================

314 11 FurtherProbability
Example 11.12 (continued)
(cid:10)
1
∞
= ye −y dy
2
0
1
= .
2
Both integrals use integration by parts again (and in fact are the same as the
integrals calculated to findE(X)andE(Y)).
. .
Finally,
1 1
cov(X,Y)= E(XY)−E(X)E(Y)= − ×1=0.
.
2 2
Exercises
11.9 The joint probability distribution function of random variables X and Y
is given by
(cid:5)
6e
−(2x+3y),
x >0 , y >0
f (x,y)=
. XY
0, otherwise.
Findcov(X,Y).
.
11.10 The joint probability distribution function of random variables X and
Y is given by
⎧
⎨(x+y)
, 0 <x <4, 0 <y <4
. f XY (x,y)= ⎩ 64
0, otherwise
Findcov(X,Y).
.
11.2.3 Multinomial Distribution
A binomial distribution (see Sect. 10.6.1.3 of Chap. 10) i s u sed t o s tudy t he
probability distribution of multiple independent trials, each with two possible
outcomes. The multinomial distribution gives the probability of an event over


================================================================================
PAGE 326
================================================================================

11.2 MultipleRandomVariables 315
multiple trials when we have more than two possible outcomes for each trial. So,
this is a generalisation of a binomial distribution.
Consider n repeated and independent trials. Suppose each trial has k possible
outcomes. Let X be the discrete random variable taking values of x , which
i. i.
is the number of occurrences of outcome i, and i = 1,2, ··· ,k. Then, we
.
have x 1 ,x 2 , ··· ,x k ∈ 0[,1, · (cid:15) ·· ,n] . , such that x 1 + x 2 + ··· +x k = n . . Let
p ,p , ··· ,p ∈[0,1] and k p =1.
1 2 k . i=1 i .
For example, in a bag containing three types of fruit (apples, oranges, and pears),
if you pick one out of the bag, there are just three outcomes (an apple, an orange,
or a pear). If a trial consists of taking one fruit out without looking, noting its type,
and replacing it, then the probabilities remain the same for each trial. Suppose you
do ten trials. Then, x is the number of times an apple is picked, x is the number of
1. 2.
times an orange is picked, and x is the number of times a pear is picked. Obviously,
3.
each x is a number between 1 and 10 (you could pick an apple each time, in which
i.
case x is ten and x and x are zeros). Whatever the types of fruit picked, the total
1. 2. 3.
must add up to ten, that is, x +x +x = 10. The probabilities of picking each
1 2 3 .
fruit,p ,p ,p , depend on the numbers of each in the bag (this does not count any
1 2 3.
differences in feel between the three fruits!).
The probability mass function for the multinomial distribution is given by
n!
. p X1X2 ···Xk (x 1 ,x 2 , ··· ,x k )= x !x !···x ! p 1 x1p 2 x2···p k xk. (11.18)
1 2 k
This is just a generalisation of the equation for the binomial mass function given
in Eq. (10.15) of Chap. 10. In that equation, there were just two outcomes with
probability p and 1−p, where the first outcome happened x times, so the other
.
happenedn−x times. Now, we have k different occurrenceshappeningx , ··· ,x
. 1 k.
times, respectively, to replace the x andn−x.
.
The mean and variance of X are given by
i.
E(X )=np , (11.19)
. i i
and
Var(X )=np (1−p ), (11.20)
. i i i
respectively.


================================================================================
PAGE 327
================================================================================

316 11 FurtherProbability
Example 11.13 Suppose that a fair die is rolled eight times. Find the
probability that one, two, three, and four dots appear once each and five dots
and six dots twice each.
Solution Applying Eq. (11.18), we have
P(X =1,X =1,X =1,X =1,X =2,X =2)
. 1 2 3 4 5 6
(cid:3) (cid:4) (cid:3) (cid:4) (cid:3) (cid:4) (cid:3) (cid:4) (cid:3) (cid:4) (cid:3) (cid:4)
8! 1 1 1 1 1 1 1 1 1 2 1 2
=
1!1!1!1!2!2! 6 6 6 6 6 6
≈0.006.
Exercise
11.11 There are 100 marbles of the same size but four different colours
in a bag. The ratio of red, black, white, and yellow is 2:3:4:1. Jack takes
six marbles from the bag without looking, replacing the marble each time.
Compute the probability that he takes two red, one black, and three yellow.
11.2.4 Multivariate Normal Distribution
Let X ,...,X be independent normally distributed with means μ ,...,μ and
1 d. 1 d.
varianceσ2,...,σ2. Then, the joint density ofX ,...,X is
1 d. 1 d.
(cid:3) (cid:4) (cid:3) (cid:4)
1 (cid:16)d 1 1 (cid:2)d (x −μ )2
f (x ,...,x )= exp − i i , (11.21)
. X 1 d (2π) d 2 i=1 σ i 2 i σ i 2
where Xis a d-dimensional random vector[X ,...,X ].
. 1 d .
Let us look at this ford =2:
.
(cid:3) (cid:3) (cid:4) (cid:3) (cid:4)(cid:4)
1 1 1 (x −μ )2 1 (x −μ )2
f (x ,x )= exp − 1 1 − 2 2
. X 1 2 (2π)σ 1 σ 2 2 σ 1 2 2 σ 2 2
1 1
−(x1 −μ1)2 −(x2 −μ2)2
= e 2σ1 2 ×e 2σ2 2
(2π)σ σ
1 2
(cid:3) (cid:4) (cid:3) (cid:4)
1 1
−(x1 −μ1)2
1 1
−(x2 −μ2)2
= √ e 2σ1 2 × √ e 2σ2 2 .
2π σ 1 2π σ 2


================================================================================
PAGE 328
================================================================================

11.2 MultipleRandomVariables 317
If you look back at Eq. (10.19) in Chap. 10, you can see that this is just two normal
distribution equations in two different dimensions multiplied together.
Since Eq. (11.21) is an equation in vectors, it can also be expressed as follows:
f (x)= 1 exp[− 1 (x−μ)T(cid:2) −1(x−μ)], (11.22)
. X (2π) d 2 |(cid:2) 0 |1 2 2 0
where μis the mean vector and (cid:2) is a diagonal matrix:
. 0.
⎡ ⎤
σ2 0 ... 0
1
⎢ ⎢0 σ2 ... 0 ⎥ ⎥
. (cid:9) 0 =⎢ ⎣ . .
.
. .
.
2 ... . .
.
⎥ ⎦ .
0 0... σ 2
d
Again you can check this by expanding out thed =2case, where
.
(cid:23) (cid:24)
σ2 0
(cid:9) = 1 ,
. 0 0 σ2
2
and
(cid:23) (cid:24)
(cid:9) −1 = 1 × σ 2 2 0 .
. 0 σ2σ2 0 σ2
1 2 1
In fact, (cid:2) in Eq. (11.22) i s a d -dimensional covariance matrix with no cross-
0.
correlation between any of X ,...,X . The multivariate normal distribution can
1 d.
be denoted asX∼N(μ,(cid:2) ).
0 .
A more general formula for the joint density of a random vector X =
[X ,...,X ] of size d, which is distributed according to a multivariate normal
1 d .
distribution and does have a cross-correlation between theX ,...,X , is given as
1 d.
follows:
1 1
f (x)= exp[− (x−μ)T(cid:2) −1(x−μ)], (11.23)
. X (2π) d 2 |(cid:2)|1 2 2
where μis the mean vector and (cid:2) is a general covariance matrix. It can be denoted
. .
asX∼N(μ,(cid:2)).
.
Figure 11.2 shows two contour plots of bivariate normal distributions with
100 samples, each displayed as the plus signs. The left panel shows a bivariate
distribution as
(cid:3)(cid:23) (cid:24) (cid:23) (cid:24)(cid:4)
2 10
N , ,
.
4 01


================================================================================
PAGE 329
================================================================================

318 11 FurtherProbability
Fig. 11.2 Examples of bivariate normal distribution. In the left panel, the two normal random
variables are uncorrelated; in the right panel, the two normal random variables are positively
correlated
where the covariance between X and X is 0, that is, X and X are uncorrelated.
1. 2. 1. 2.
The right panel shows a bivariate distribution as
(cid:3)(cid:23) (cid:24) (cid:23) (cid:24)(cid:4)
2 1 .15
N , ,
.
4 1. 5 4
where the covariance between X and X is 1.5. As can be seen in the plot, it shows
1. 2. .
that as values of X increase, values of X also increase. That is, the two variables
1. 2.
are positively correlated.
Figure 11.3 shows another example of a two-dimensional multivariate normal
distribution:
(cid:3)(cid:23) (cid:24) (cid:23) (cid:24)(cid:4)
0 0.49 0.4
N , .
.
0 0. 4 1
The left panel presents the probability density distribution. The right displays its
corresponding cumulative distribution.
Variables with a multivariate normal distribution with a mean vector μ and a
.
covariance matrix (cid:2)have the following properties:
.
• Every single variable has a univariate normal distribution.
• Any subset of the variables also has a multivariate normal distribution.
• Any linear combination of the variables has a univariate normal distribution.
• Any conditional distribution for a subset of the variables dependent on known
values for another subset of variables is a multivariate distribution.
We will discuss conditional probability in the following section.


================================================================================
PAGE 330
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 319
Fig. 11.3 An example of a two-dimensional multivariate normal distribution. The left panel
presents the probability density distribution. The right displays its corresponding cumulative
distribution
11.3 Conditional Probability and Corresponding Rules
11.3.1 Conditional Probability
So far, we have discussed the probability of an event (E) occurring without any
conditions. Sometimes, a specific event may happen under certain conditions. Let
us consider the following example first.
Example 11.14 100 people showed up for a new test for bowel cancer. 30 of
them have bowel cancer. 40 people had a positive test result, of which 25 had
bowel cancer. Calculate the following:
• The probability of people having bowel cancer and testing positive.
• The likelihood of people having bowel cancer or being tested positive.
Solution Let us define event A, people have bowel cancer, and event B ,t he
test result of people was positive.Figure11.4 shows the Venn diagram of this
example. In this figure, A has 35 people, B has 40 people, and 25 people are
in both A and B,denotedasA∩B in the diagram. So, this means there are
.
ten people in A but notinA∩B, and 15 people in B but notinA∩B.
. .
So, from the diagram, among the total 100 people, 25 people have bowel
cancer and tested positive, that is,A∩B. Thus, the probability of people who
.
(continued)


================================================================================
PAGE 331
================================================================================

320 11 FurtherProbability
Example 11.14 (continued)
actually have bowel cancer and who tested positive is
25
P(A∩B)= =25%.
.
100
To calculate the probability of people having bowel cancer or being tested
positive, we need to know the number of people in set A and B, that is,P(A∪
B). But if we add A and B together, we get the overlapping area of A∩B
. .
twice, so we need to subtract the overlapping area. We, therefore, have 35+
40 − 25 = 50. (Alternatively, we add the number in A but not in A ∩ B,
. .
that is, A\(A∩ B) = 10, plus the number in B but not in A∩B, that is,
. .
B\(A∩ B)= 15, and the number inA∩B = 25. This gives 50, as before).
. .
Thus, the probability of people either having bowel cancer or being tested
positive is
50
P(A∪B)= =50%.
.
100
Alternatively, we have, as a formula (see Sect. 11.3.3),
35 40 25
P(A∪B)= P(A)+ P(B)− P(A∩B)= + − =50%.
.
100 100 100
11.3.1.1 Conditional Probability for Two Discrete Random Variables
Conditional probability is the probability of some event happening given that some
other event has already occurred. So, the probability is conditional on the other event
having occurred. The probability thatX = x given thatY = y is usually written
i. j.
asp X|Y (x i |y j ) . or asp(X=x i |Y =y j ) . .
Fig. 11.4 The Venn diagram
of Example 11.14. A
represents having cancer, and
B represents testing positive


================================================================================
PAGE 332
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 321
Definition 11.1 Suppose (X,Y) are two discrete random variables with joint
.
probability mass function p (x ,y ). The conditional probability mass function
XY i i .
of X given thatY =y is defined as
j.
p (x ∩y )
. p X|Y (x i |y j )= XY i j , wherep Y (y j )>0. (11.24)
p (y )
Y j
(cid:15)
Note that0≤p X|Y (x i |y j )≤1 . and xi p X|Y (x i |y j )=1 . .
This definition makes sense since, whereas normally we are dividing by the whole
population to get a probability, now we are restricted to just those events that occur
whenY =y . So, we divide by just the relevant ones, that is, all theY =y ’s.
j. j.
Example 11.15 Continue Example 11.14. What percent of those who tested
positive have cancer?
Solution Here, we have the condition that we are only interested in those who
tested positive. That is, we want the number who have cancer given that they
have already tested positive. LetX = Adenote people having bowel cancer,
.
and Y = B, the test result of people being positive. So, in Fig. 11.4, w e a re
.
interested in those people with cancer inside circle B,thatis,p(A∩ B)as a
.
proportion of all of B.
Applying Eq. (11.24), we have
p(A∩B) 25%
p(A|B)= = =62.5%.
.
p(B) 40%
In Example 11.14, we were given all the numbers, but often we have to work
them out first, as in the next example.
Example 11.16 A bag contains six red balls and four green balls. A ball is
taken out but not put back, and then a second ball is taken out. Calculate the
probability of picking a red ball as the second ball, given that the first ball was
green.
Solution Let A be the probability that the second ball is red and B be the
probability that the first ball is green. We are interested in p(A|B). So, we
.
need the probability of the first ball being green, P(B), which is 4 = 2.
. 10 5.
(continued)


================================================================================
PAGE 333
================================================================================

322 11 FurtherProbability
Example 11.16 (continued)
We now need P(A∩ B). This is the probability that it is both A and B. So,
.
we need the probability that the first ball is green and then the second is red,
whichis 4 × 6 = 4 . Hence, using Eq. (11.24), we have
10 9 15.
p(A∩B) 4 2
p(A|B)= = 15 = .
. p(B) 2 3
5
Note that this is not equal to the probability of getting a red ball second. Doing
this includes the case of getting a red first and then a red second, which is
6 × 5 = 1 as well as the case of getting a green followed by a red, which is
10 9 3.
4 from above. So, the probability of getting a red second is 1 + 4 = 3.
15. 3 15 5.
Remark 11.6 Note that
p(A
¯|B)=1−p(A|B),
(11.25)
.
¯
where Ais the complement of A.
.
This can be seen by considering the following illustration. If you are a man, then
the probability you have a beard could be 1, so that the probability that you do not
5.
have a beard is 4. If you define A as the probability of having a beard and B as the
5.
probability of being a man, then p(A|B) is the probability that you have a beard
.
given that you are a man, and this is 1. So,p(A ¯|B)is the probability of not having
5. .
a beard given that you are a man, which is1−p(A|B)=1− 1 = 4.
5 5.
(cid:2)
.
Exercises
11.12 The percentage of all adults in America who are women and will have
an episode of depression by the age of 65 is 16.667%. The percentage of
.
all adults in America who are men and will have an episode of depression
by the age of 65 is 10%. What is the probability that a given woman will
.
have an episode of depression by the age of 65 in America? What is the
probability that a given man will have an episode of depression by the age
of 65 in America? (You can assume that the probabilities of an adult being
male or female are both50%.)
.
11.13 In a certain university, there are 1000 students, of which 540 are
female. Of the female students, 300 take humanities subjects, and the rest take
(continued)


================================================================================
PAGE 334
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 323
science subjects. Of the male students, 180 take humanities subjects, and the
rest take science subjects. For a given female student, what is the probability
that they do science? For a given science student, what is the probability of
them being female? For a given male student, what is the probability that they
do science?
11.14 Suppose we flip a fair coin three times. What is the probability of
coming up precisely two heads, given the first flip is a heads-up?
11.15 Ann has two children. You learn that she has a daughter, Sarah. What
is the probability that Sarah’s sibling is a brother? (You can assume that the
probabilities of a child being male or female are both50%.)
.
11.3.1.2 Conditional Probability for Two Continuous Random Variables
This again follows the discrete variable case.
Definition 11.2 Suppose (X,Y) are two continuous random variables with joint
.
probability density functionf (x,y). The conditional probability density function
XY .
of X given thatY =y is defined as
.
f (x,y)
. f X|Y (x|y)= XY , wheref Y (y)>0. (11.26)
f (y)
Y
(cid:11)
Note thatf X|Y (x|y)≥0 . and − ∞ ∞ f X|Y (x|y)dx=1 . .
Example 11.17 Suppose X and Y are two continuous random variables, and
their joint probability density function is given as follows:
(cid:5) √
2xy, 0 <x <1, 0 <y < 2
f (x,y)=
. XY
0, otherwise.
Find the conditional probability density functionf X|Y (x|y) . .
Solution To apply Eq. (11.26), we need to computef (y)first:
Y .
(cid:10)
1
f (y)= 2xydx = y.
. Y
0
(continued)


================================================================================
PAGE 335
================================================================================

324 11 FurtherProbability
Example 11.17 (continued)
Substitutingf (y)=y andf (x,y)=2xy into Eq. (11.26), we have
Y . XY .
2xy
. f X|Y (x|y)= =2x
y
√
for0 <x <1and0 <y < 2.
. .
Exercises
11.16 Suppose X and Y are two continuous random variables and their joint
probability density function is given as follows:
(cid:5)
1, 0 <y≤ x <2
f (x,y)= 2
. XY
0, otherwise.
Find the conditional probability density functionf Y|X (y|x) . .
11.17 Suppose X and Y are two continuous random variables and their joint
probability density function is given as follows:
(cid:5)
Asinxcosy, 0 <x,y <π
f (x,y)= 2
. XY
0, otherwise.
(1) Find the value of A.
(2) Find the conditional probability density functionf X|Y (x|y) . .
(3) Find the conditional probability density functionf Y|X (y|x) . .
11.3.2 Conditional Means and Conditional Variances
11.3.2.1 Conditional Means and Conditional Variances for Two Discrete
Random Variables
If X and Y are two discrete random variables with joint probability massfunction
p (x ,y ), then the conditional mean of Y givenX =x is defined by
XY i i . i.
(cid:2)
. E(Y|x i )= y i p Y|X (y i |x i ), (11.27)
yi


================================================================================
PAGE 336
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 325
and the conditional variance of Y givenX =x is defined by
i.
(cid:2)(cid:13) (cid:14)
2
. Var(Y|x i )= y i −E(Y|x i ) p Y|X (y i |x i ). (11.28)
yi
Note that these follow the same construction as those given in Eq. (10.8) for mean
and Eq. (10.9) for variance in Chap. 10, except that in these, we are summing over
y rather than x since the x ’s are given and so effectively constant.
i. k. i.
11.3.2.2 Conditional Means and Conditional Variances for Two
Continuous Random Variables
If X and Y are two continuous random variables with joint probability density
functionf (x,y), then the conditional mean of Y givenX =x is defined by
XY . .
(cid:10)
∞
. E(Y|x)= yf Y|X (y|x)dy, (11.29)
−∞
and the conditional variance of Y givenX =x is defined by
.
(cid:10) ∞ (cid:13) (cid:14)
2
. Var(Y|x)= y−E(Y|x) f Y|X (y|x)dy. (11.30)
−∞
The variance can also be written as
Var(Y|x)= E(Y2|x) −[E(Y|x)]2. (11.31)
.
Again see Eq. (10.8) for mean and Eq. (10.9) for variance in Chap. 10.
Example 11.18 Suppose X and Y are two continuous variables. The joint
density function is give nby
(cid:5)
24x2y, 0 <x <1, 0 <y <1
f (x,y)= 2
. XY
0, otherwise.
FindE(Y|x).
.
(continued)


================================================================================
PAGE 337
================================================================================

326 11 FurtherProbability
Example 11.18 (continued)
Solution Sincef (x)is independent of y ,E q.( 11.29) can be further written
X .
as follows:
E(Y|x)= (cid:10) ∞ y f XY (x,y)dy = (cid:11) − ∞ ∞ yf XY (x,y)dy .
.
−∞ f X (x) f X (x)
First, findf (x):
X .
(cid:10) (cid:10) (cid:12)
∞ 1 (cid:12)1
. f X (x)= f XY (x,y)dy= 2 24x2ydy=12x2y2(cid:12) (cid:12) 2 =3x2 for0 <x <1.
−∞ 0 0
Therefore, we have
(cid:10) (cid:12)
1 (cid:12)1
. E (Y|x)= 3 1 x2 2 24x2y2dy = 8 3 y3(cid:12) (cid:12) 2 = 1 3 .
0 0
Exercise
11.18 Suppose X and Y are two continuous variables. The joint density
function is give nby
(cid:5)
12x3y2, 0 <x <1, 0 <y <1
f (x,y)=
. XY
0, otherwise.
FindVar(Y|x) (Hint: apply Eq. (11.31)).
.
11.3.3 Mutual Exclusivity
Suppose A and B are two events. Then, they are mutually exclusive if they cannot
co-occur,thatis,A∩B =∅, andP(A∩B)=0. Then, we have
. .
P(A∪B)= P(A)+ P(B)− P(A∩B)= P(A)+ P(B).
.
For example, the event of a student who passes or fails the same module is
mutually exclusive.


================================================================================
PAGE 338
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 327
11.3.4 The Multiplication Rule
The multiplication rule is defined by
P(A∩B)= P(A)P(B|A). (11.32)
.
We can obtain this rule by rearranging the definition of conditional probability. For
two events A and B, if they can occur at the same time, that is, the overlapping
area in a Venn diagram, then the probability of this is given by the product of the
probability of event A occurring and the probability of B occurring given that A
happens. The general multiplication rule is a handy way to find the probability that
two events, A and B, occur if we can easily calculate the conditional probability
P(B|A) and the probability of A.
.
11.3.5 Independence
Event B is considered independent of event A if P(B|A) = P(B). It says that
.
learning that event A happened provides us with no additional information about
event B .U singE q .(11.32), this relationship is more usually written equivalently as
P(A∩B)= P(A)P(B). (11.33)
.
This is a nice result because we can find the probability of two events occurring
without dealing with conditional probability calculations. The definition of inde-
pendence of two events can be extended to describe three or more events.
Example 11.19 Flipping a fair coin three times. Let T , T , and T be the
1. 2. 3.
events that the first, second, and third flips have tails-up. The probability that
weseethreeflipscominguptailsiscomputed as P(T ∩T ∩T )= 1×1×1 =
1 2 3 2 2 2
1,assuming three flips are independent.
8 .
Similarly, if X and Y are two continuous independent random variables, then
f X|Y (x|y)=f X (x) . .


================================================================================
PAGE 339
================================================================================

328 11 FurtherProbability
Exercises
11.19 Consider throwing two fair six-sided dice. Let A be the event that the
first die is odd, B the event that the second die is odd, and C the event that
the product of numbers on these two throws is odd. Determine whether these
events are pairwise independent, that is, if (i) A and B are independent, (ii) A
and C are independent, and(iii)B andC areindependent.
11.20 A fair four-sided die has numbers 1, 2, 3, or 4 on its faces, respectively.
Let A be the event that the face with an even number landed, that is,
A = 2{,4}. List all possible solutions for event B such that A and B are
.
independent. (Hint: start by listing all therealsubsetsof{1,2,3,4}, including
.
the full set {1,2,3,4}, as possible solutions for event B and check each in
.
turn.)
11.3.6 The Law of Total Probability
Suppose B ,B , ··· ,B is a partition of the sample space S, such that any two
1 2 n.
partitions are disjoint, that is,B ∩B =∅, and the union of all theB (cid:13) s is the entire
i j . i .
sample space, that is, ∪ B =S. Then, for any event A, we hav e
i i .
(cid:2)n (cid:2)n
P(A)= P(A∩B )= P(A|B )P(B ). (11.34)
. i i i
i i
This rule is used to find the probability of an event A when we do not know
enough about A’s probability to calculate it directly. Instead, we take related eve nts
B and use them to calculate the probability for A.
i.
Example 11.20 40% of people watched the film The Lord of the Rings.
.
Among them,35%have read the book before seeing the movie. Out of60%
. .
of people who do not watch the movie, only 3%have read the book. What is
.
the probability a random person has not read the book?
Solution Consider event A being a random person who has not read the book,
B the person who watched the movie, and B not watched the movie. Since
1. 2.
B and B together are mutually disjoint, and their union is all of the sample
1. 2.
space (all the people), then we can use the law of total probability. The known
probabilities are shown in Fig. 11.5.
(continued)


================================================================================
PAGE 340
================================================================================

11.3 ConditionalProbabilityandCorrespondingRules 329
Example 11.20 (continued)
Applying the law of total probability (Eq. (11.34)), we have
(1−0.35)×0.4+(1−0.03)×0.6=0.842.
.
Example 11.21 A phone-making company has five pipelines of production.
Each pipeline has a different rate of working and each has errors associated
with it: the first produces 10% of the phones with an error rate of 1%, the
. .
second produces 15% of the phones with an error rate of 1.2%, the third
. .
produces 20% of the phones with an error rate of 1.4%, the fourth produces
. .
25%of the phones with an error rate of1.6%, and the final one produces30%
. . .
of the phones with an error rate of 2%. What is the probability that a random
.
phone is faulty?
Solution LetB , ··· ,B be the working rate in the five pipelines and A the
1 5.
probability of a phone being faulty. Since B , ··· ,B together are mutually
1 5.
disjoint and their union is all of the sample space (all of the production), then
we can use the law of total probability.
Consider the first pipeline. We need P(A|B ), and this says, “Given that
1 .
we have the first pipeline, what is the probability of error?” and that is given
as1%=0.01. Also, we needP(B ), which we also know as10%=0.1. So,
. 1 . .
the first term in the sum to findP(A)is0.01×0.1=0.001. We can work out
. .
the other four terms similarly.
Hence, applying the law of total probability (Eq. (11.34)), we haveP(A)
.
as
0.01×0.1+0.012×0.15+0.014×0.20+0.016×0.25+0.02×0.30=0.0156,
.
which gives a probability forP(A)of1.56%.
. .
Fig. 11.5 Tree diagram
illustrating the partitions of
the sample space and their
associated probabilities in
Example 11.20


================================================================================
PAGE 341
================================================================================

330 11 FurtherProbability
Exercises
11.21 We have four bags containing red and black balls. The first contains
six red and four black balls; the second contains seven red and eight black
balls; the third contains six red and six black balls; and the fourth contains 16
red and four black balls. A bag is selected randomly, and then a ball within
it is selected at random. What is the probability that the ball is red? Compare
this to the chances of picking a red ball if all the balls were just in one bag.
11.22 There are five balls in a bag: two white and three red. Take the first ball
out of the bag without replacing it, and then take out the second one. What is
the probability that the second ball is white?
11.4 Bayes’ Theorem
Recall that the multiplication rule says the probability that events A and B both
occur is the probability that A occurs multiplied by the probability that B happened,
given that A alreadyoccurred.Thatis,
P(A,B)= P(A∩B)= P(A)P(B|A).
.
Alternatively, we have
P(B,A)= P(B∩A)= P(B)P(A|B).
.
Since
P(A,B)= P(B,A),
.
we have
P(A)P(B|A)= P(B)P(A|B).
.
Bayes’ theorem is given by
P(A)P(B|A)
P(A|B)= , (11.35)
.
P(B)
whereP(B)(cid:14)=0.
.
P(B) in Eq. (11.35) may need to be computed by applying the law of total
.
probability, for example, P(B) = P(A)P(B|A) + P(A ¯ )P(B|A ¯ ) and A ¯ is the
. .


================================================================================
PAGE 342
================================================================================

11.4 Bayes’Theorem 331
complement of A. This gives an alternate form for Bayes’ theorem as
P(A)P(B|A)
P(A|B)= . (11.36)
. P(A)P(B|A)+ P(A ¯ )P(B|A ¯ )
In even more generality, if we denote the data we have as D and the hypothesis
about the given dataasH , Bayes’ theorem can be further written in a more general
i.
way as follows:
P(H )P(D|H )
P(H |D)= i i , (11.37)
. i
P(D)
where:
(cid:15) (cid:15)
• P(D)= P(D∩ H ) = P(H )P(D|H )
i i i i i .
• P(H ) defines the prior probability, that is, the probability of the hypothesis
i .
before the new data is observed
• P(D|H )defines the likelihood, that is, the probability of the data under the given
i .
hypothesis
• P(H |D)is called the posterior, that is, the probability of the hypothesis after the
i .
data is observed
Example 11.22 Mrs. Wright runs a small company providing domestic
cleaning services with two employees—Sarah and Ann. Each cleaner has
a very similar workload. From past performances, 85% of customers rank
.
Sarah’s work with five stars (the highest rating) and only50%for Ann’s work.
.
A new review with five stars comes to Mrs. Wright without mentioning the
cleaner’s name. What is the probability that it is a review of Ann’s work?
Solution Let us use D to denote a review with five stars, H to denote
1.
the cleaning done by Ann and H by Sarah. Since cleaners have a similar
2.
workload, we have P(H ) = P(H) = 0.5. To compute P(H |D), we
1 2 . 1 .
apply Eq. (11.37) and the first bullet point below the equation to obtain the
following:
0.5×0.5 0.25
P(H |D)= = ≈37%.
. 1 0.5×0.5+0.85×0.5 0.675
Example 11.23 In the UK, men have a one in eight chance of having prostate
cancer at some point in their life. There is a simple first test that can be done
to determine if a man needs further testing; this is the PSA test. However, this
(continued)


================================================================================
PAGE 343
================================================================================

332 11 FurtherProbability
Example 11.23 (continued)
test is not very accurate. In fact, 3 of the results are false positives; that is, it
4.
gives a positive result when the man does not have the condition. Also, there
is a 1 chance of a false negative, which gives a negative result when the man
7.
has the condition. What are the chances that a man has the condition given
that he gets a positive test result?
Solution Let A be the probability that you have the condition and B be the
probability that you test positive. WerequireP(A|B), namely, the probability
.
you have the condition given that you test positive.
We know that P(A)= 1; P(B|A ¯ ) = 3, namely, that the probability of
8. 4.
testing positive given that you do not have the condition; and P(B ¯|A) = 1,
7.
namely, the probability that the test is negative given that you have the
condition.
To find P(A|B), we will use Bayes’ theorem in the form given in
.
Eq. (11.36) since we do not knowP(B)directly, namely,
.
P(A)P(B|A)
P(A|B)= .
. P(A)P(B|A)+ P(A ¯ )P(B|A ¯ )
NowP(A ¯ )= 7, and using Eq. (11.25), we haveP(B|A)= 6. Plugging these
8. 7.
into the equation, we get
1 × 6
P(A|B)= 8 7 ≈0.14.
. (1 × 6)+(7 × 3)
8 7 8 4
We can show this result using a diagram; see Fig. 11.6. Assuming we have
1120 men (the number is chosen so that it divides nicely), then in the first
split, we have 1120 × 1 = 140 men that have the condition and the rest,
8 .
1120×7 =980, do not. In the top half, it splits again with140×1 =20who
8 . 7 .
have the condition and test negative, and the rest, 140−20 = 120, testing
.
positive. Similarly, in the bottom half,980× 3 =735test positive but do not
4 .
have the condition, and the rest,980× 1 =245, test negative.
4 .
We want the number who test positive and have the condition compared to
the total that tests positive. Namely, 120 ≈0.14.
120+735 .


================================================================================
PAGE 344
================================================================================

11.4 Bayes’Theorem 333
Fig. 11.6 Illustration of the
data presented in
Example 11.23
Exercises
11.23 In Example 11.23, change the numbers to the following. UK men have
a 1 in 12 chance of having prostate cancer at some point in their life. An
improved PSA test has a false positive rate of 1 and a false negative rate of
3.
1 . Again, calculate the chances that a man has the condition given that he
20.
gets a positive test result.
11.24 A type of product sold in a local shop is produced by factories A, B,
and C. Among them, 5/10, 3/10, and 2/10 are from A, B, and C, respectively.
The defective rate of these products is 1/10, 1/15, and 1/20, respectively. One
of these products is drawn randomly from the shop. If it is non-defective, what
is the probability that it is produced by factory A?
11.25 Let A be the event that a positive test result shows up and B the event
that the person has breast cancer. Based on previous clinical records, we hav e
P(A|B)=0.95 andP(A ¯|B ¯ )=0.98. A census is taking place, and we know
. .
the probability of suffering from breast cancer among this population is0.003.
.
ComputeP(B|A).
.
11.26 You have four bags of multicoloured balls. Bag X has 50 balls, Bag Y
has 60 balls, Bag Z has 60 balls, and Bag W has 30 balls. The probability of
picking a red ballfrombagXis 1 , from bag Y is 1 , from bag Z is 1 , and
10. 20. 30.
from bag W is 1. A bag is picked with the probability given by the relative
5.
number of balls in it and a ball is picked at random from this bag. If the ball
is not red, what is the probability of it coming from Bag Z?


================================================================================
PAGE 345
================================================================================

Chapter 12
Elements of Statistics
AswehaveseeninChap.10,probabilitydeduceswhatislikelytohappenwhenan
experimentisperformed.Theentirepoolofsubjectsinaninvestigationiscalleda
population.Itmaybechallengingtoinvolvethewholepopulationintheexperiment.
Instead,randomsamplesmaybechosensothateverymemberofapopulationhasan
equalchanceofbeingselectedasanyothermember.Peopleusestatisticsobtained
fromthesesamplestodescribetheentirepopulation.
We introduce statistics in this chapter. First, we present descriptive statistics:
methods used to describe or summarise observations. Then we briefly bring in
elementary sampling theory. Especially, we introduce two more sampling distri-
butions: Student’s t-distribution and the Chi-square distribution. Finally, we focus
.
on inferential statistics, that is, to infer by what mechanism the observation, the
outcomeofanexperiment,hasbeengenerated.
Because probability and statistics are related, there is quite a lot of overlap
between this chapter and both the two previous chapters. Some topics, such as
averageandstandarddeviation,havebeenintroducedevenearlierinthebook.Inthis
chapter,wecollectalltherelevantresultstogethereveniftheyhavebeenintroduced
before,andwemakebackreferenceswhereappropriate.
12.1 DescriptiveStatistics
Statistics is the most basic and important concept and tool to estimate characteri-
sations of the probability distribution of a population. There are lots of statistics,
like mean, variance, and interquartile range, all of which will be introduced here.
Some of these have been introduced before, but here we bring them all together.
Formally, suppose x ,x ,...,x is a sample of a random variable X. Then the
1 2 n.
functiong(x ,x ,...,x )iscalledastatisticofthesampleiftherearenounknown
1 2 n .
parametersing.
©TheAuthor(s),underexclusivelicensetoSpringerNatureSingaporePteLtd.2025 335
Y.Sun,R.Adams,AMathematicalIntroductiontoDataScience,
https://doi.org/10.1007/978-981-96-5639-4_12


================================================================================
PAGE 346
================================================================================

336 12 ElementsofStatistics
Example12.1 Suppose X is a continuous random variable and X ∼
N(μ,σ2).RememberthismeansthatX isanormalorGaussiandistribution
.
as defined in Sect. 10.6.2.2 of Chap. 10. If μ is known but not σ2, then
(cid:2) (cid:2) . .
n (x −μ)2isastatistic,but n i xi isnot.
i=1 i . σ .
Soastatisticisanumericalquantitycalculatedfromasetofobservations,1 and
statisticsisthecollectionandanalysisofsuchdata.
12.1.1 MeasuresofCentre
Thesamplemean,median,andmodearethreemeasuresofcentraltendency.
12.1.1.1 TheArithmeticMean
Sumupallthevaluesanddividethembythenumberofdatapoints.
Example12.2 Thesetoffivenumbers3, 5, 7, 8,and10hasameanof
.
3+5+7+8+10
=6.6.
.
5
ThegeneraldefinitionofexpectedvalueormeancanbeviewedinSect.10.5.1
ofChap.10.
12.1.1.2 Median
Thatisthenumberfoundatthedataset’smiddlewhenthedatasetissortedintoorder.
Whenthenumberofdataiseven,weusetheaverageofthetwomiddlenumbersas
themedian.
1Strictlyspeaking,astatisticisafunctionofrandomvariables,whileanumericalquantityisits
realisationbasedonaspecificsample.


================================================================================
PAGE 347
================================================================================

12.1 DescriptiveStatistics 337
Example12.3
• Thesetofnumbers3, 5, 7, 8,and10hasamedianof7.
.
• Thesetofnumbers3, 5, 6, 7, 8,and10hasamedianof 6+7 =6.5.
. 2 .
12.1.1.3 Mode
Thatisthevaluethatoccursmostofteninthedataset.
Example12.4
• Thesetofnumbers2, 3, 3, 5, 7, 8, 8, 8, 8,and11hasamodeof8.
.
• Thesetofnumbers3, 5, 6, 7, 8,and10hasnomode.
.
• Thesetofnumbers1, 3, 3, 3, 5, 7, 8, 8, 8,and11hastwomodes,3
.
and8,andiscalledbimodal.
Themodeismostlikelytobeusedwhendataconcernscategories.
Example12.5 Supposewehaveasurveyresultofthesportsstudentslikethe
mostinYear7ofalocalschoolintheStAlbansarea,showninTable12.1.
Whichformofaverageshouldweuseforthistypeofdata?
Inthisexample,Footballwouldbeconsideredaverage,themodalaverage.
Therewouldbenopointinfindingameannumberofstudentsforeachlisted
sportorlookingforamedianvalue.
Remark12.1 Whenwetakemanysamplesfromthesamepopulation,theirmeans
arelikelytodifferlessthantheirmediansormodes.Thatis,themeanisrelatively
stable. The median is preferable if outliers (extremely high or low values) are
observed.
Table12.1 Asurveyresult
Sportsthatstudentslikemost Numberofstudents
ofthesportastudentlikesthe
Swimming 60
mostinYear7ofalocal
schoolintheStAlbansarea Tennis 30
Football 70
Basketball 40


================================================================================
PAGE 348
================================================================================

338 12 ElementsofStatistics
Fig.12.1 Anillustrationof
themean,mode,andmedian
forasymmetricaldistribution
Fig.12.2 Anillustrationoftherelationshipsbetweenthemean,mode,andmedianforasymmet-
ricaldistributions
Themean,median,andmodearethesameinaperfectlysymmetricaldistribution
(seeFig.12.1).Ontheotherhand,theeffectofextremevaluescandistortthemean
andpullitfarfromthecentreofthedistribution.TheleftpanelofFig.12.2shows
a left-skewed (or negatively skewed) distribution. More values occur around the
distribution’s left tail, and its mean value is less than its median value, which is
less than the mode value. In contrast, the right panel presents a right-skewed (or
positively skewed) distribution, where more values occur around the distribution’s
righttail.Itsmeanvalueisgreaterthanitsmedian,whichisgreaterthanthemode
value.
(cid:2)
.


================================================================================
PAGE 349
================================================================================

12.1 DescriptiveStatistics 339
Exercise
12.1 Findthemean,median,andmodeofthefollowingsets:
1. 1, 5, 8, 7, 6, 6, 6,and11.
.
2. 0, 0, 3, 5, 5,and10.
.
3. 1.5, 7.4, 3.7, 5.5, 5.5, 8.3, 2.1,and6.8.
. .
4. 5.8, 6.2, 5.7, 6.1, 6.0, 5.6, 6.0, 5.9,and5.8.
. .
5. 3, 9, 10, 7, 4, 12, 10, 11,and6.
.
12.1.2 MeasuresofVariation
Variation measures how spread out the data we collect is. It is a helpful way to
identifyifourdatahasmanyoutliers.
12.1.2.1 StandardDeviationandVariance
Usually, people use standard deviation and variance to measure the variation. We
haveintroducedtheminSect.4.2.1ofChap.4.Fortheconvenienceofreaders,we
showthecorrespondingequationsagainasfollows.
Suppose X is a data matrix including n data observations with d dimensions
.
(variables,features,orattributes).EachelementofXisdenotedasx ,wherei =
. i,j.
1,...,nandj =1,...,d.Thesamplestandarddeviationforeachdimensionx is
. . j.
definedas
(cid:3)
(cid:2)
n (x −x¯ )2
s(x )= i=1 i,j j ,
. j n−1
wherex¯ isthesamplemeanofthejthdimension:
j.
(cid:4)n
1
x¯ = x .
. j i,j
n
i=1
Thesquaredsamplestandarddeviationiscalledsamplevariance:
var(x )=(s(x ))2.
. j j
ReaderscanviewthegeneraldefinitionofvarianceinSect.10.5.2ofChap.10.


================================================================================
PAGE 350
================================================================================

340 12 ElementsofStatistics
Exercise
12.2 Findthestandarddeviationofthefollowing:
(1) 1, 5, 8, 7, 6, 6, 6,and11.
.
(2) 0, 0, 3, 5, 5,and10.
.
(3) 1.5, 7.4, 3.7, 5.5, 5.5, 8.3, 2.1,and6.8.
. .
(Hint:Youhavealreadyfoundthemeansforthesethreeintheprevious
exercise.)
(4) Findthestandarddeviationofeachdimensionforthefollowingdata:
⎡ ⎤
5.8 3 10.7
⎢ ⎥
⎢6.2 9 10.6⎥
⎢ ⎥
⎢5.71010.8⎥
⎢ ⎥
⎢6.1 7 10.9⎥
⎢ ⎥
. D=⎢ ⎢ 6.0 4 10.8⎥ ⎥
⎢ ⎥
⎢5.61210.9⎥
⎢ ⎥
⎢6.01010.1⎥
⎣ ⎦
5.91110.2
5.8 6 10.4
Hint:Youhavealreadyfoundthemeansforthefirsttwocolumnsinthe
previousexercise.
Remark12.2 The standard deviation, divided by n − 1, calculates the sample
.
standard deviation. Readers may see a different version from other resources as
follows:
(cid:3)
(cid:2)
n (x −μ )2
σ = i=1 i,j j ,
. j
n
which computes the population standard deviation. There are two differences
compared with the sample standard deviation equation. First, the mean value μ
j.
isthepopulationmean,notthesamplemeanx¯ .Second,thedenominatorunderthe
j.
squarerootisn,notn−1.Statisticiansfoundthatthesamplemeanaftertakinga
.
large number of samples is smaller than the population mean. To compensate for
this difference, a smaller denominator n−1 is used when computing the sample
.
standarddeviationsothats(x )isasclosetoσ aspossible.
j . j.
(cid:2)
.


================================================================================
PAGE 351
================================================================================

12.1 DescriptiveStatistics 341
12.1.2.2 CovarianceandPearsonCorrelationCoefficient
Inaddition,weintroducedthesamplecovariancemeasureandPearsoncorrelation
coefficientinSect.4.2.1ofChap.4,whichisaquantitativemeasurethatdescribes
the strength of association between two variables. Again for the convenience of
readers,weshowthecorrespondingequations.Thesamplecovarianceisgivenby
(cid:2)
n (x −x¯ )(x −x¯ )
cov(x ,x )= i=1 i,h h i,k k ,
. h k n−1
andPearsoncorrelationcoefficientr isgivenby
cov(x ,x )
r = √ h k .
.
var(x )var(x )
h k
Exercise
12.3 FindthecovarianceandPearsoncorrelationcoefficientbetween:
(1) Column1andcolumn2ofmatrixDfromExercise12.2(4).
.
(2) Column1andcolumn3ofmatrixDfromExercise12.2(4).
.
(3) Column2andcolumn3ofmatrixDfromExercise12.2(4).
.
(4) Now for D from Exercise 12.2 (4) form a new matrix where all the
.
columns are put in order separately, from the lowest value at the top
to the highest value at the bottom of each column. Repeat the previous
calculationsofcovarianceandPearsoncorrelationcoefficientforcolumns
1and2,1and3,and2and3forthenewmatrix.
12.1.2.3 CoefficientofVariation
Anotherusefulwaytomeasurethevariationistousethecoefficientofvariation,a
ratioofthesamplestandarddeviationtothemean,asshownasfollows:
s
coefficientofvariation= . (12.1)
. x¯
Wecannotcomparestandarddeviationswithdifferentmeasurementunits.However,
since the coefficient of variation is independent of measurement units, we can use
it to compare variations in different datasets with varying units of measurement.
A drawback of the coefficient of variation is that it fails to be useful when x¯, the
.
samplemeanvalue,isclosetozero.


================================================================================
PAGE 352
================================================================================

342 12 ElementsofStatistics
Example12.6 Afridgemanufacturerhastwofridgemodels,M andN.The
mean lifetimes of the two models are x¯ = 20 years and x¯ = 15 years,
M . N .
andstandarddeviationsares = 3.5yearsands = 3,respectively.Which
M . N .
modelhasthegreaterrelativedispersion?
Solution ApplyingEq.(12.1)tomodelsM andN,respectively,wehave:
3.5
coefficientofvariation= =17.5%formodelM
.
20
and
3
coefficientofvariation= =20%formodelN.
.
15
Therefore,modelN hasagreaterrelativedispersion.
Exercise
12.4 Two class students attended a maths competition. The mean score of
classAisx¯ =50anditsstandarddeviationiss =10,andthemeanscore
A . A .
ofclassBisx¯ = 60anditsstandarddeviationiss = 12.Whichclasshas
B . B .
thegreaterrelativedispersion?
12.1.3 The RangeandtheInterquartile
Definition12.1(Range) Therangeequalsthehighestvalueminusthelowestvalue
inagivenreal-valueddataset.
Definition12.2(Interquartile Range) The interquartile range (IQR) equals the
upperquartile,denotedasQ ,minusthelowerquartile,denotedasQ .Theupper
3. 1.
quartile(Q )isanumbersuchthattheintegraloftheprobabilitydensityfunction
3.
from −∞tothisnumber(Q )equals0.75,andthelowerquartile(Q )isanother
. 3. 1.
number such that the integral of the probability density function from − ∞ to
.
this number (Q ) equals 0.25. Figure 12.3 illustrates the positions of Q , Q (the
1. 1. 2.
median),andQ inadataset,wherevaluesaresortedinascendingorder.
3.


================================================================================
PAGE 353
================================================================================

12.1 DescriptiveStatistics 343
Fig.12.3 Thepositionsofquantileswithinthedataset
Fig.12.4 Boxplot
representingthedistribution
ofadataset
12.1.3.1 Boxplot
A boxplot shows the quartiles of a dataset as a box, and its “whiskers” (lines
extendingupwardsanddownwardsfromthebox)showtheextentoftherestofthe
distribution, except for points that are determined to be “outliers” using a method
thatisafunctionoftheinterquartilerange.
Figure12.4illustratesaboxplot:Q andQ controlthewidthofthebox,which
1. 3.
equals the interquartile range (IQR); the solid line within the box, highlighted
with an overlaid dashed line, represents the median of the data; the bar on the top
indicatesthemaximumvalue;thebaratthebottomshowstheminimumvalue;two
circle signs display the outliers. Outliers are data items outside 1.5 times the IQR
.
abovetheupperquartileandbelowthelowerquartile,thatis,
outliervalues≤Q −1.5×IQR or outliervalues≥Q +1.5×IQR. (12.2)
. 1 3
Pleasenotethattherearedifferentmethodsthatcanbeusedtocomputequartiles.
Weconsiderthemedian-basedmethodonlyinthisbook.


================================================================================
PAGE 354
================================================================================

344 12 ElementsofStatistics
Exercise
12.5 Findthefollowingmanually:
(1) Mode
(2) Median
(3) IQR
(4) Outliers
forbothofthefollowingtwodatasets:
• 90,86,87,88,50,66,95,87,72,78,77,87,86,62,87,110
.
• 55,72,52,45,58,55,30,52,38,55,42,65,53,55,80,48
.
NotethatFig.12.4isgeneratedfromthefirstdataset.
12.2 ElementarySamplingTheory
12.2.1 Random Samplingwith andWithoutReplacement
Random sampling is the process of collecting a representative sample from a
population.Itmeanseachmemberofthepopulationhasanequalchance ofbeing
selectedandisindependentofothermembersselectedinthesample.
Samplingwhereeachmemberofthepopulationisallowedtoappearonlyonceis
calledrandomsamplingwithoutreplacement.Incontrast,ifeachmemberisallowed
toappearmorethanonce,itiscalledrandomsamplingwithreplacement.
Letusconsidersamplesofthesamesizecollectedfromthesamepopulation.We
cancalculatethemeanforeachsample.Theprobabilitydistributionofmeanvalues
obtained from these samples is called a sampling distribution of means. Similarly,
wecangetasamplingdistributionofstandarddeviationsorsamplingdistributions
ofotherstatistics.
12.2.2 SamplingDistributionsofMeans
ThistopicisrelatedtothematerialweintroducedinChap.11onthecentrallimit
theorem. At that point, we talked about population means and the fact that the
distribution of sample means approximated a normal distribution. So here we are
talkingaboutthesamplingdistributionofsamplemeans,whichagainapproximates
anormaldistribution.
Let us denote the population mean and standard deviation by μ and σ, respec-
. .
tively,andthesamplemeanandstandarddeviationbyx¯ ands,respectively.
.


================================================================================
PAGE 355
================================================================================

12.2 ElementarySamplingTheory 345
Thekeytothisworkisthat:Thesamplingdistributionofmeansisapproximately
a normal distributionwithmean μ x¯. and standard deviation σ x¯. for a large value of
the sample size n, n ≥ 30, irrespective of the population distribution as long as
.
thepopulationsizeisatleasttwicethesamplesizeandthemeanandthestandard
deviationofthepopulationdistributionarefinite.
Based on the central limit theorem (see Sect. 11.1.2 of Chap. 11), the mean
and standard deviation of a sampling distribution of means can be calculated as
follows:
• If all samples of size n are drawn from an infinite population or if sampling is
withreplacement,thenwehave
. μ x¯ =μ, (12.3)
and
σ
. σ x¯ = √ . (12.4)
n
Sothesamplingdistributionofmeans,x¯,hasGaussiandistribution:
.
σ
N(μ,(√ )2).
.
n
• If all samples of size n are drawn from a finite population of size n without
p.
replacement(n >n),thenwehave
p .
. μ x¯ =μ,
which is the same as Eq. (12.3). The formula for the standard deviation of all
samplemeansmustbemodifiedbyincludingafinitepopulationcorrection.That
is,
(cid:3)
σ n −n
. σ x¯ = √ n n p −1 . (12.5)
p
Sothesamplingdistributionofmeans,x¯,hasGaussiandistribution:
.
(cid:11) (cid:12) (cid:3) (cid:13) (cid:14)
N μ, √ σ n p −n 2 .
. n n −1
p


================================================================================
PAGE 356
================================================================================

346 12 ElementsofStatistics
Example12.7 RandomlytakesamplesfromN ∼ (52,6.32).Ifthesizeofa
.
sampleis36,computetheprobabilityofP(50.4≤x¯ ≤53.2).
.
Solution Considerallsamplesaredrawnfromtheinfinitepopulation.Apply-
ingEqs.(12.3)and(12.4)wehaveμ x¯ = μ = 52 . ,σ x¯ = √6.3 . . Sothatx¯ . hasa
36
GaussiandistributionN(52,(√6.3 )2).NowuseEq.(10.24)toget
.
36
53.2−52 50.4−52
P(50.4≤x¯ ≤53.2) = (cid:3)( √ )−(cid:3)( √ )
.
6.3/ 36 6.3/ 36
≈(cid:3)(1.14)−(cid:3)(−1.52)=0.8729−0.0643=0.8086.
Here we have used the values from a standard normal distribution table.a to
lookup(cid:3)(1.14)and(cid:3)(−1.52).
. .
aThe example table we have used is at https://en.wikipedia.org/wiki/Standard_normal_
table.
Example12.8 Suppose that the heights of 2000 female students at a uni-
versity are normally distributed with a mean of 165 centimetres (cm) and a
standarddeviationof8cm.If100samplesof15studentseacharecollected,
what is the expected mean and standard deviation of the resulting sampling
distributionofmeansifthesamplingwasdonewithoutreplacement?
Solution ApplyingEqs.(12.3)and(12.5),wehave
. μ x¯ =165cm,
and
(cid:15)
8 2000−15
. σ x¯ = √
15
2000−1 ≈2.06cm.
Exercises
12.6 RandomlytakeasamplefromN ∼ (60,102).Ifthesizeofthesample
.
is100,computetheprobabilityofP(|μ−x¯|<0.3).
.
(continued)


================================================================================
PAGE 357
================================================================================

12.2 ElementarySamplingTheory 347
12.7 MarksinacertainexamatAlevelaredistributedaccordingtoanormal
distribution with a mean of 57 and a standard deviation of 12. A random
sampleofstudentsofsize64istaken,computetheprobabilitythatthesample
meaniswithin1ofthepopulationmean.
12.8 Themeanweightandstandarddeviationofasetoffourhundredmarble
balls are 4.58 grams and 0.3 grams, respectively. A random sample of 50
marble balls is taken out one at a time, collected together, and weighed.
Compute the probability that this random sample of 50 has a total weight
ofmorethan230grams.
12.2.3 SamplingDistributionsofProportions
Wearenowgoingtolookatthesamplingdistributionofsampleproportions.Here
wearelookingattheproportionofheads-upsofatossofafaircoin,theproportion
of defective items from a product line, or the proportion of people in a university
with a particular brand of phone. Here the event is a Bernoulli event: The coin is
heads-upornot,anitemisdefectiveornon-defective,andapersonatauniversity
has that phone brand or not. So the distribution for multiple Bernoulli events is
binomial, and the formulae for mean and standard deviation are derived for that
distribution(seeSect.10.6.1.3ofChap.10).
The key to this section is that, using the central limit theorem (see Sect. 11.1.2
ofChap.11),thesamplingdistributionofsampleproportionswillfollowanormal
distribution.
Supposethatapopulationisinfiniteandthattheprobabilityofaneventoccurring
ispandnotoccurringis1−p.Thenumber(x)oftheeventoccurringinasample
.
with a size of n can be modelled using a binomial distribution, where the mean is
μ =npandthevarianceisσ2 =np(1−p)(fromSect.10.6.1.3ofChap.10).So
X . X .
thenumberxofdefectiveitemsinasamplecouldbe6,forexample.Theproportion
oftheeventoccurringx timesinthesampleis x.Soifthesamplesizeis100,then
n.
theproportionofdefectiveitemsinthesampleis x = 6 .
n 100.
Consider all possible samples of size n drawn from the same population and
denotethemeanandstandarddeviationofthesamplingdistributionofproportions
byμ andσ ,respectively;thenwehave
p. p.
μ =p, (12.6)
. p
and
(cid:15)
p(1−p)
σ = . (12.7)
. p
n


================================================================================
PAGE 358
================================================================================

348 12 ElementsofStatistics
w√hich can be obtained by dividing the mean, np, and standard deviation,
np(1−p),ofthebinomialdistributionbythesamplesizen.
.
Again, for a large value of n, n ≥ 30, the sampling distribution of sample
.
proportionsfollowscloselytoanormaldistribution.
NotethatEqs.(12.6)and(12.7)arealsovalidforsamplingwithreplacementin
a finite population. For finite populations where sampling is without replacement,
wehave
μ =p,
. p
and
(cid:15) (cid:3)
p(1−p) n −n
σ = p , (12.8)
. p n n −1
p
wheren isthesizeofthepopulationandn >n.
p. p .
Example12.9 Find the probability that there will be 8 or more heads-ups
15.
in150tossesofafaircoin.
Solution Considerthe150tossesofthecointobeasamplefromaninfinite
populationofallpossibletossesofthecoin.
ApplyingEqs.(12.6)(cid:16)and(12.7),wehave
μ =0.5andσ = 0.5(1−0.5) ≈0.0408.
p . p 150 .
Usingthenormalapproximationtothebinomial,weconvert 8 ≈0.5333
15 .
tothestandardz-score = 0.5333−μp = 0.5333−0.5 ≈ 0.82(seeEq.(10.21)in
σp 0.0408 .
Chap.10)sothatwecanusethestandardnormaldistributiontable.
WerequireP(z > 0.82),whichistheprobabilitygivenbytheareaunder
.
thenormalcurvetotherightofz-score = 0.82.Thevalueinthetable(refer
.
totheinformationprovidedinthefootnoteofSect.10.6.2.2)is0.7939,sothe
probabilitytotherightis1−0.7939=0.2061.
.
Exercises
12.9 Ithasbeenfoundthat2%oftheproductsproducedbymanufacturerA
.
aredefective.Whatistheprobabilitythatasampleof400productsfromthe
manufacturer3%orlesswillbedefective?
.
(continued)


================================================================================
PAGE 359
================================================================================

12.2 ElementarySamplingTheory 349
12.10 Itisknownthat25%ofthestudentpopulationhaveaparticularbrand
.
ofphone.Whatistheprobabilitythatinasampleof80students:
(1) Atleast24%ofthemhavethatbrandofphone?
.
(2) Nomorethan30%ofthemhavethatbrandofphone?
.
12.2.4 StandardErrors
The standard deviation of a statistic’s sampling distribution is often called its
standard error. For example, Eq. (12.4) is x¯’s (with replacement) standard error,
.
andEq.(12.7)istheproportion’s(withreplacement)standarderror.
12.2.5 DegreesofFreedom
The value of degrees of freedom is the maximum number of independent values
usedtocalculatetheestimatethatarefreetovary.Supposewewanttouseasample
ofsizentocomputeoneestimate.Thequantityn−1isthenumberofdegreesof
.
freedomofanestimate,sincethelastoneisfixedbytheconstraintofproducingthe
answer.
Example12.10 Consider selecting five students (n = 5) to attend a maths
.
competition.Theaveragescoreofthesefivestudentsinthemostrecentmaths
testmustbe60.Theoretically,fourstudentscanbechosenrandomly,withthe
fifthstudenthavingtohaveaspecificmathsscoresothatthefinalaverageis
60.Therefore,thedegreesoffreedomis5−1=4.
.
Theformulaforcalculatingthenumberofdegreesoffreedomisdifferentifwe
haveadifferentnumberofsamplesorifthenumberofestimatesismorethanone.
Forexample,ifwehavetwosampleswhosesizesaren andn ,wewanttoestimate
1. 2.
the mean. Then the degrees of freedom is n +n −2. Another example can be
1 2 .
viewedinSect.12.3.3.2ofthischapter,wherethedegreesoffreedomofatwo-way
classificationtableis(thenumberofrows−1)×(thenumberofcolumns−1).
.


================================================================================
PAGE 360
================================================================================

350 12 ElementsofStatistics
12.2.6 TwoSpecificSamplingDistributions
WehaveintroducedsomeessentialdistributionsinChap.10.Wecontinuebyintro-
ducing two more important and useful distributions in the following subsections:
Student’s t-distribution and the Chi-square (χ2) distribution, both widely used in
.
modellingsamplingdistributions.
12.2.6.1 Student’st-Distribution
This is usually used when we have a small sample, n < 30, and also when the
.
underlyingpopulation’sstandarddeviationisunknown.
ArandomvariableXisdistributednormallywithmeanμandvarianceσ2,that
. .
is, X ∼ N(μ,σ2). Suppose we take a sample (x ,...,x ) of size n. Let x¯ be
. 1 n . .
the sample mean and s2 be the sample variance. From the central limit theorem,
.
the random variable Z = x¯− √μ has a standard normal distribution. The random
.
σ/ n
variableT,whichhasthesamplestandarddeviation(aknownquantity)ratherthan
thepopulationstandarddeviation,isdefinedasfollows:
x¯ −μ
t = √ . (12.9)
.
s/ n
ThisrandomvariablehasadistributionknownasStudent’st-distributionwithn−1
.
degreesoffreedom.Thisdistributionissimilartoanormaldistributionbuthaswider
tails. As n increases, or equivalently, the number of degrees of freedom increases,
thisdistributionapproachesanormaldistribution.
ThisisillustratedinFig.12.5,whichshowstdistributionswithdifferentdegrees
of freedom (ν). As seen in the left panel, their density distributions all have a bell
.
shapesimilartonormaldistributionsandaresymmetricalaboutthemean.Student’s
t-distributionismainlyusedforasmallsampleoflessthan30,whichismorelikely
.
to generate values far away from its mean (to have wider tails). As shown in the
rightpanel,thecumulativeprobabilityapproachesthevalueonefasterasthevalue
ofdegreesoffreedomincreases.
Similartothenormaldistribution,peoplehaveconstructedamathematicaltable
for the t statistic. The value in the table gives the number of standard deviations
.
from the mean you need to capture a certain proportion of the data. For instance,
we already know that we capture 95% of the data for a normal distribution if we
.
move 1.96 standard deviations on either side of the mean. (See the properties of
a standard normal distribution in Sect. 10.6.2.2 of Chap. 10.) There are different
versionsoft-tables,butacommononeisshowninthefootnote.1Ithasvaluessuch
ast ,t ,t asheadingsforthecolumns.Supposewewantthecolumnwitht
.50 .75 .80. .975.
1Theexampletablewehaveusedisathttps://www.tdistributiontable.com/.


================================================================================
PAGE 361
================================================================================

12.2 ElementarySamplingTheory 351
Fig.12.5 Student’s t.-distributions with varying degrees of freedom. The left panel shows the
densitydistributionsandtherightpanelshowsthecumulativeprobabilities
as the heading. This means that there are 2.5%(= 0.025) of the data to the right
.
of the distribution and, therefore, 5%(= 0.05) of the data not in the centre (see
.
secondandthirdheadingrows).Ifyoulookdownthecolumn,youwillseethatfor
df = 10 (degrees of freedom = 10) row, the value is 2.228. This means that for
.
thet-distribution,youneedtogo2.228standarddeviationsoneithersidetocapture
.
95% of the data. This is because there are more values in the tails (the tails are
.
wider),soyouneedtogofurtheroutfromthemean.Ifyoucontinuedownthet
.975.
column,thevaluesgetcloserto1.96,andinfact,asyougettothe∞valuefordf,
.
youfinallyget1.96.
Example12.11 For Student’s t-distribution with 8 degrees of freedom, find
thevalueoft forwhichtheprobabilityontherightoft is0.05.
.
Solution Iftheprobabilityontherightis0.05,thentheprobabilitytotheleft
.
oftis1−0.05=0.95.Referringtothetstatisticstable,proceedingdownward
.
underthecolumnnotedwithdf (degreesoffreedom)untilreachingentry8,
andthenproceedingrighttothecolumnheadedt ,onecanseetheresultis
.95.
1.860.Thatistherequiredvalueoft.
.


================================================================================
PAGE 362
================================================================================

352 12 ElementsofStatistics
Exercise
12.11 ForStudent’st-distributionwith15degreesoffreedom,findthevalue
oft forwhichtheprobabilityontherightoft is(1)0.05and(2)0.01.
. .
12.2.6.2 TheChi-Square(χ2)Distribution
.
Supposex ,...,x areindependentandaredrawnfromanormaldistributionwith
1 n.
standard deviation σ. The sample mean is x¯ and the standard deviation is s. We
. .
definetheChi-square(χ2)statisticasfollows:
.
(cid:4)n (x −x¯)2
χ2 = i (12.10)
. σ2
i=1
withn−1degreesoffreedom.Eachtermofthesummationisasquaredz-score-like
.
value.Therefore,allχ2valuesareequaltoorgreaterthanzero.Fromthedefinition
.
of standard deviation (Sect. 4.2.1 of Chap. 4), the Chi-square statistic can also be
writtenas
(cid:4)n (x −x¯)2 (n−1)s2
χ2 = i = . (12.11)
. σ2 σ2
i=1
Itcanbeseenthattheχ2 statisticisaratiomeasuringthedeviationofthesample
.
fromthepopulation.
Ifweconsidermanysamplesofsizendrawnfromthesamenormalpopulation
and compute χ2 for each of them, then a sampling distribution for χ2 can be
. .
obtainedandiscalledtheChi-squareddistributionwithn−1degreesoffreedom.
.
Figure12.6showsChi-squaredistributionswithdifferentdegreesoffreedom(ν).
.
The left panel shows that Chi-square probability density functions are positively
skewed for the lower values of the degrees of freedom. When ν = 1 and ν = 2,
. .
thecurvestartshighandthendropsoff.Itshowsahighprobabilitythatχ2 isclose
.
to zero. When ν = 3, 4, or 5, the distribution has a much longer tail on the right
.
hand of the curve. As ν further increases, the distribution looks more similar to a
.
normaldistribution.TherightpanelshowsthatChi-squarecumulativedistributions
approachonefasterwhenthevalueofdegreesoffreedomdecreases.
Intuitively, if ν = 1, then n = 2 and we are taking just two observations from
. .
a normal distribution. Recall that a normal distribution has a bell shape with a
high probability for data being observed around the centre of the bell shape (see
Fig. 10.9). Therefore, when taking only two observations, they are both likely to
be close to the centre. So (x −x¯)2 will be small, and hence χ2 is small. This is
i . .
whytheν = 1curveissoskewedtolowvalues.Asν increases,yougetmoreand
. .


================================================================================
PAGE 363
================================================================================

12.2 ElementarySamplingTheory 353
Fig.12.6 Chi-square distributions with varying degrees of freedom. The left panel shows the
densitydistributions,andtherightpanelshowsthecumulativeprobabilities
morerepresentativeobservationsofthevaluesfromtheoriginalnormaldistribution,
so the distribution of values for χ2 begins to look more and more like a normal
.
distribution,asisseeninFig.12.6.
ThemeanofaChi-squaredistributionisitsdegreesoffreedom,andthevariance
istwotimesitsdegreesoffreedom.
PeoplehaveconstructedamathematicaltablefortheChi-squarestatistic.2 Each
row of the χ2 table represents a particular value of the degrees of freedom and,
.
therefore, the sample size. For instance, if the sample size is eight, the number of
degrees of freedom is 7, so that is the row to use. The columns represent the area
yourequireundertherelevantcurvemeasuredfromtheright,thatis,theprobability
of exceeding a value of χ2, since the χ2 values are on the horizontal axis. So, for
. .
instance,ifyouwantedtheprobabilityofexceedingacertainvaluetobeonly0.05
(so only 5% of the values are above it), then you use the column headed .05. The
.
value in the table is the value of χ2 that you must not exceed if you wish to have
.
95%oftheareatotheleftandonly5%totheright.Inthecaseoftheν =7example,
. . .
the value in the table is 14.07. If you look at the ν = 7 curve in Fig. 12.6, then it
.
looksreasonablethatjust5%oftheareaunderthecurveisabove(totherightof)
.
14.07andthat95%istotheleftof14.07.Ifyoucontinuetolookalongthe7degrees
.
offreedomrowgoingleft,thevaluesofχ2getsmaller(youaregoingtothelefton
.
thegraph).Sothenextvalueinthetableis12.02,andforthisvalueofχ2 thereis
. .
2Theexampletablewehaveusedisathttps://www.statisticshowto.com/tables/chi-squared-table-
right-tail/.


================================================================================
PAGE 364
================================================================================

354 12 ElementsofStatistics
10%oftheareatotheright,asitsaysatthetopofthecolumn.Sothefurtheryou
.
gototheleft,thelowerthevaluesofχ2yougetandthereforethelargerpercentage
.
oftheareaunderthecurvethereistotherightofthatpoint.Hence,thevaluesinthe
χ2 tablearecriticalvaluesofχ2,effectivelytellingyouhowfartotherightofthe
. .
areayouare.
Example12.12 ForaChi-squaredistributionwith4degreesoffreedom,find
thevalueofχ2forwhichtheprobabilityontherightofχ2is0.05.
. . .
Solution If the probability on the right is 0.05, then the probability to
.
the left of χ2 is 1 − 0.05 = 0.95. Referring to the χ2 statistics table,
. . .
proceedingdownwardunderthecolumnnotedwithdf (degreesoffreedom)
until reaching entry 4, and then proceeding right to the column headed .05,
.
onecanseetheresultis9.48773or9.49.Thatistherequiredvalueofχ2.
. . .
Exercise
12.12 For a Chi-square distribution with 12 degrees of freedom, find the
value of χ2 for which the probability on the right of χ2 is (1) 0.05 and (2)
. . .
0.01.
.
12.3 Inference
Peoplemakeinferencesaboutentirepopulationsbasedoncertainsamplesofdata.
Therearetwomaintypesofinference:classicalandBayesianinferences.Wefocus
onclassicalinferenceinthisbook.Typesofclassicalinferenceinclude:
• Pointestimationcomputesasinglenumber,anestimateofsomeparameter.
• Intervalestimationprovidesarangeofvaluesinwhichtheparameteristhought
toliebasedontheobserveddata.
• Testing hypothesis is to test a hypothesis about whether a parameter value is to
beacceptedorrejectedbasedontheobserveddata.
12.3.1 PointEstimation
Pointestimationusessampledatatocalculateanunknownvalueforsomeparame-
ter.Forexample,estimateapopulationmean,variance,orotherstatistics.


================================================================================
PAGE 365
================================================================================

12.3 Inference 355
Example12.13 Estimatetheprobabilityofheads-upforacoin.Wehavethe
followingobservationsfrom10tossesofthecoin:
H,T,T,T,T,H,T,H,T,H,
.
whereH denotesheads-upandT tails-up.
Solution Among10observations,fourareheads-up.Weuseptodenotethe
probabilityofhavingheads-ups.Letp˜ denotetheestimatorofp.Wehave
.
1+1+1+1
p˜ = =0.4.
.
10
To approach the actual probability, more experiments (tosses) need to be
done according to the law of large numbers (see Sect. 11.1.1 of Chap. 11).
p˜ =0.4representsourbestestimatesofar.
.
Exercise
12.13 ThelifespanofatypeofbulbfollowsX ∼ N(μ,σ2).Bothμandσ
. . .
are unknown. Randomly take five of this type of bulb and test their lifespan
(inhours):
1400,1520,1368,1600,1544.
.
Estimateμandσ.
. .
More systematic methods of constructing estimators include the least-squares
techniqueandthemaximumlikelihood(ML)method.Wehaveintroducedtheleast-
squarestechniqueinSect.8.2ofChap.8,whichminimisesthesumofthesquareof
the differences between the observations and their expected values. We will show
howMLworksinChap.13.
12.3.2 IntervalEstimation
A confidence interval is a range of values constructed using a sampling method
based on a point estimate. With this method, many intervals can be constructed
across repeated samples. The proportion of intervals constructed across many
samples containing the true parameter is called the confidence level. Therefore,


================================================================================
PAGE 366
================================================================================

356 12 ElementsofStatistics
a confidence interval is constructed using a method that will capture the true
parameteratthespecifiedconfidencelevel.Supposetheconfidencelevelisat68%.
.
Ifwerepeatthesamplingprocess100timesfromapopulation,weexpect68ofthe
100 confidence intervals around the found sample parameter will contain the true
populationparameter.
Calculating a confidence interval involves finding a point estimate and then
incorporatingamarginoferrortocreatearange:
Confidenceinterval=Pointestimate±Marginoferror. (12.12)
.
The margin of error is a value that represents the variability of the point estimate
andisbasedonourdesiredconfidencelevel,thevarianceofthedata,andhowbig
thesampleis.
12.3.2.1 ConfidenceIntervalsforMeans
Letusconsiderestimatingthepopulationmeanusingsamplemeans.Asmentioned
inSect.12.2.2ofthischapter,thedistributionofsamplemeansforalargenumber
of samples approximates a normal distribution with a mean value around the
population mean (see Eq. (12.3)). For simplicity, we consider a standard normal
distribution, though it can be any normal distribution. Figure 12.7 shows the
Fig.12.7 Anillustrationoftherelationshipbetweenthepopulationmeanandsamplemeans.Four
horizontallinesshowthedistanceof1SD.oneithersideofthespecificsamplemean


================================================================================
PAGE 367
================================================================================

12.3 Inference 357
standard normal distribution of sample means with one standard deviation (SD)3
marked off on either side of the population mean (0). That is 68% of all sample-
.
meansfallwithinthisrange.Wehaveillustratedfoursamplemeans:s ,s ,s and
1. 2. 3.
s . For each sample mean we have shown the range of the distance for 1 standard
4.
deviationeachsideasahorizontalbarbelowthehorizontalaxis.Fors ands ,the
1. 2.
range sample mean ± 1 SD will include the real population mean, since both s
. 1.
and s are within the range of 1 standard deviation of the real mean, as shown in
2.
Fig. 12.7. However, s and s lie outside one standard deviation of the population
3. 4.
mean,sotheirrange samplemean±1SD willnotcontainthepopulationmean.
.
Ifanormaldistributionisagoodfittothesamplingdistribution,thenthemargin
oferrorisusuallyestimatedas
Marginoferror=Z ×SD, (12.13)
. c
whereZ was1intheexampleinthepreviousdiscussion.Sothemarginoferroris
c.
acertainnumberofstandarddeviations.
Remark12.3 Supposewewanta68%confidencelevel,aswedidabove.Wewant
.
theprobability,theareaunderthestandardnormaldistributioncurvebetween −Z
c.
and Z , to be 68%. We will check using a z-score table (the normal distribution
c. .
table),wherethecumulativeprobabilityistheareaunderthestandardnormalcurve
totheleftofZ.Wewantapproximately16%aboveand16%belowthelimitsforan
. .
intervalcontaining68%.Fromthetable,thecumulativeprobabilityfor −Z ≤−1
. c .
is 15.87% (0.1587 in the table, being virtually as near to 0.16 as we can get). For
.
theupperbound,wewanttogetasnearaspossibleto16%+68%=84%,andthe
.
cumulative probability for Z ≤ 1 is 84.13% (0.84134 in the table). So we obtain
c . .
Z =1andthemarginoferror=1×SDasweillustrated.
c . .
Toconstructa95%confidenceintervalforthetruepopulationmean,weusethe
.
standard normal distribution. By checking the same z-score table, the probability
for −Z ≤ −1.96 is 2.5%, and the probability for Z ≤ 1.96 is 97.5%. Since
c . . c . .
97.5%−2.5%=95%,wehaveZ =1.96andtheMarginoferror=1.96×SD.
. c . .
IntheexampleshowninFig.12.7,the95%confidenceintervals ±1.96SDwill
3 .
includethepopulationmean,whichisnotcapturedbythe68%confidenceinterval
s ±1 SD. Thus, we can make a wide estimate with a high confidence level or a
3 .
moreaccurateestimatewithalowconfidencelevel.The95%isprobablythemost
.
commonvaluethatisused.
(cid:2)
.
ConsiderthestandarddeviationofsamplemeansasgivenbyEq.(12.4),wecan
computetheconfidenceintervalatacertainconfidencelevelforthepopulationmean
μ when σ2 is known (if the sampling is either from an infinite population or with
. .
3Moreprecisely,itshouldbethestandarderrorsinceweconsiderthesamplingdistributionofa
statistic(seeSect.12.2.4).


================================================================================
PAGE 368
================================================================================

358 12 ElementsofStatistics
replacementfromafinitepopulation)usingthefollowingequation:
(cid:17) (cid:18)
σ σ
x¯ −Z √ ,x¯ +Z √ , (12.14)
. c c
n n
where n is the sample size, x¯ is the sample mean, σ is the population standard
. .
deviation and √σ is the sample standard deviation. Note that as n gets larger, that
.
n
is,thesamplesizegetslarger,thenthequantityZ √σ getssmaller.Thismakesthe
c n .
bounds tighter to the population mean. So, if Z is chosen as 1.96, giving a 95%
c. .
confidenceinterval,thenalargersamplesizegivesatighter95%interval.
.
Another way to construct a confidence interval is by using the left and right
criticalvalues.Thatis,weconstructtheseintervalsbetweentheleftcriticalvaluec
L.
andtherightcriticalvaluec basedontheobservedstatistic,s .Underrepeated
R. obs.
sampling, approximately (1 − α) × 100% of such intervals will contain the true
.
populationparameter(p ):
true.
c ≤s ≤c , (12.15)
. L obs R
whereboththeleftandrightcriticalvaluescanbeobtainedfromthecorresponding
probability distribution table. To get 95% confidence, (1 − α) must be 0.95, so
. .
α = 0.05.Thismeanswemusthave α = 0.025above(totheright)andbelow(to
. 2 .
theleft)oftheinterval.
Figure 12.8 illustrates that an interval from the left critical value to the right
criticalvaluerepresentstheconfidenceintervalat(1−α)×100%confidencelevel.
.
WecanobtainEq.(12.14)byapplyingEq.(12.15).Ifweconsiderthedistribution
of sample means and use the property that the left critical value is the negative of
the right critical value Z for the standard normal distribution, then by converting
c.
anobservedsamplemeantoitscorrespondingz-score,wehave
−Z ≤
x¯ −μ
x¯
≤Z ,
. c √σ c
n
Fig.12.8 Anillustrationofconfidenceintervals.Theleftpanelshowsaconfidenceintervalfora
symmetricaldistribution,whiletherightpanelshowsoneforanasymmetricaldistribution


================================================================================
PAGE 369
================================================================================

12.3 Inference 359
where μ x¯. equals to the true population mean (see Sect. 12.2.2 of this chapter).
Rearrangingtheseinequalities,weget
σ σ
. x¯ −Z c √ ≤μ x¯ ≤x¯ +Z c √ ,
n n
whichisEq.(12.14)fortheboundsofthepopulationmean.
If the sampling is without replacement from a population of finite size n ,
p.
the standard error can be calculated using Eq. (12.5). However, it is usually
approximatedbyEq.(12.4)whenn >>n.
p .
Example12.14 A sample of size n = 225 produced the sample mean of
.
x¯ = 16.Assumingthepopulationstandarddeviationσ = 3,computea95%
. . .
confidenceintervalforthepopulationmeanμ.
.
Solution Wesubstituten = 225,x¯ = 16,andσ = 3inEq.(12.14).Letus
. . .
gothroughtheworkingtofindthebounds,Z ,again.
c.
Werequire95%tobeinthemiddleoneithersideofthemean.Thismeans
.
weneed2.5%tobeateachendofthedistributionor0.025ofthetotal.Since
. .
thenormaldistributiontablegivesthecumulativetotalsfromtheleft,weneed
tofindthepointswith2.5%,or0.025ofthetotalontheleft,andthen97.5%,
. . .
or0.975ofthetotal,whichisnearlythroughtotherightend.
.
So we look up 0.025 probability in the body of the table and find that it
.
occurswhenwelookupaboundof −1.96.Similarly,welookup0.975in
. .
thetableandgetaboundof +1.96.So,asbefore,theprobabilityisbetween
.
Z = 1.96 and Z = −1.96, and this will give us 95% confidence
97.5% . 2.5% . .
limits.Theintervalis
(cid:17) (cid:18)
3 3
16−1.96× √ ,16+1.96× √ =[15.608,16.392].
.
225 225
Theinterval[15.608, 16.392]istheconfidenceintervalatthe95%confidence
. .
levelforμ.
.
Remark12.4 Ofcourse,ifthestandarddeviationofthepopulationisnotknown,
then the standard deviation of the sample can be used. In this case, we use the
valuesinthet-statistictableratherthanthez-scoretable.Forexample,ifwehave
60 degrees of freedom (a sample size of 61) and want a 95% confidence interval,
.
theninEq.(12.14),wereplaceσ bythesamplestandarddeviation,s,andZ =1.96
. c .
witht =2.00.
.
(cid:2)
.


================================================================================
PAGE 370
================================================================================

360 12 ElementsofStatistics
Remark12.5 Note that when you construct a 95% confidence interval for a
.
population mean from a sample mean, it is not true to say that there is a 95%
.
probabilitythatthetruemeanlieswithinthisinterval.Thisisbecausethetruemean
isafixedvalue,anditiseitherinornotinaconfidenceintervalconstructedforthat
sample.Theconfidencelevelappliestothemethodusedtoconstructtheintervaland
thesamplefromwhichitcomes.Hence,thecorrectinterpretationisthat,ifwewere
torepeatthesamplingprocess100times,thenabout95oftheresultingconstructed
intervalswouldcontainthetruemean[12].
(cid:2)
.
Exercise
12.14 Suppose that the heights of 2000 female students at a university
are normally distributed with a standard deviation of 8 cm. Compute a
95% confidence interval and a 99% confidence interval for estimating the
. .
population mean (μ) height of the university’s female students. The mean
.
heightofasampleof100femalestudentsis163cm.
12.3.2.2 ConfidenceIntervalsforProportions
We are now back to using the binomial distribution since we are considering
multipleexamplesofaneventbeingtrueornot,thatis,Bernoullievents.Consider
sampling from an infinite population or a finite population with replacement. We
can compute the confidence interval for the population proportion p at a certain
confidencelevelusingthefollowingequation:
(cid:17) (cid:15) (cid:15) (cid:18)
p¯(1−p¯) p¯(1−p¯)
p¯−Z ,p¯+Z , (12.16)
. c c
n n
(cid:16)
where
p¯(1−p¯)
isthestandarddeviationofthesamplingdistributionofproportions
n .
(seeEq.12.7)withp¯ beingtheproportionfromasampleandnthesamplesize.
.
For sampling from a finite population without replacement, the standard devi-
ation of the sampling distribution of proportions should be calculated using
Eq.(12.8).


================================================================================
PAGE 371
================================================================================

12.3 Inference 361
Example12.15 1000 randomly selected students were asked whether they
preferred going swimming or doing athletics. The results show that 696
students prefer going swimming, and 304 prefer doing athletics. Compute a
99%confidenceintervalforthepopulationproportionp.
.
Solution Let p be the true fraction of preferring going swimming in the
population; p¯ = 696 = 0.696 be the fraction in the sample. We substitute
1000 .
n = 1000, p¯ = 0.696 in Eq. (12.16). This time we want 99% confidence
. . .
intervals, so we want 0.5% at either end. We therefore look up 0.005 and
. .
1 − 0.005 = 0.995 in the body of the table. This gives the bounds Z as
. c.
−2.58and +2.58.Theintervalis
. .
(cid:17) (cid:15) (cid:15) (cid:18)
0.696×0.304 0.696×0.304
0.696−2.58× ,0.696+2.58×
.
1000 1000
≈[0.658,0.734].
Exercise
12.15 A sample poll of 200 voters chosen randomly from all voters in a
cityshowedthat59%ofthemfavouredaparticularcandidate.Findthe95%
. .
confidenceintervalandthe99%confidenceintervalfortheproportionofthe
.
votersinfavourofthiscandidate.
12.3.2.3 ConfidenceIntervalsforχ2
.
By applying Eq. (12.15), we can define confidence limits for χ2 by using a χ2
. .
distributiontable.
Theχ2distributiontablehasvaluesforχ2withthecolumnheadingrepresenting
. .
thearea,α,totherightofthevalue.So,youusethecolumnheaded0.05ifyouwant
.
5% above (to the right of) the value and, therefore, 95% below (to the left of) it.
. .
Togetanintervalof95%,youneedequalareasoneitherside.Hence,youusethe
.
columnfor0.025,giving2.5%totheright,andthecolumnfor0.975,giving97.5%
. .
to the right and 2.5% to the left. This gives 2.5% on either side of the interval (in
. .
bothtailsofthedistribution).


================================================================================
PAGE 372
================================================================================

362 12 ElementsofStatistics
Considerusingaχ2 distributiontablefortheareasα totherightofthecritical
. .
value.Sinceα ≤1,then α ≤1−α.Thus,χ2 ≥χ2 ,sinceχ2valuesgetlargeras
. 2 2. α 1−α. .
2 2
youprogressacrossthetabletotheright.SubstitutingEq.(12.11)intoEq.(12.15),
wehave
(n−1)s2
χ2 ≤ ≤χ2,
. 1−α
2
σ2 α
2
where χ2 and χ2 are critical values for which α of the area lies in each tail of
α. 1−α. 2.
2 2
theχ2distribution.
.
Therefore, we can estimate the confidence interval of the population standard
deviation σ in terms of an observed sample standard deviation by rearranging the
.
aboveinequalitiesgivingthefollowingequation:
(n−1)s2 (n−1)s2
≤σ2 ≤ . (12.17)
. χ2 χ2
α 1−α
2 2
Example12.16 A sample of size n = 30 produced the sample standard
.
deviation s = 12.5. Compute a 90% confidence interval for the population
. .
standarddeviationσ.
.
Solution First,weneedtofindthecriticalvalues.Thevalueofthedegreesof
freedomisgivenby30−1 = 29.A90%levelofconfidenceleaves10%of
. . .
thetotalareainthetailsoftheχ2 distribution,with5%ineachtail.Wehave
. .
usedaχ2distributiontableshowingtheareatotherightofthecriticalvalue:
.
χ2 ≈42.557
. 0.05
and
χ2 ≈17.708.
. 0.95
SubstitutevaluestoEq.(12.17),wehave
(30−1)×12.52 (30−1)×12.52
≤σ2 ≤ ,
.
42.557 17.708
106.47≤σ2 ≤255.89.
.
Therefore, the confidence interval for the popul√ation stand√ard deviation
at the 90% confidence level is approximately [ 106.47, 255.89] ≈
.
[10.32, 16.00].
.


================================================================================
PAGE 373
================================================================================

12.3 Inference 363
Exercise
12.16 Asampleofsizen = 25producedthesamplestandarddeviations =
.
6.4. Compute a 90% confidence interval and a 95% confidence interval for
. . .
thepopulationstandarddeviationσ.
.
12.3.2.4 SummaryofConfidenceIntervals
• Confidenceintervalsformeans:involvecalculatingtheconfidencelimitsforthe
populationmeanμgiventhesamplemeanx¯.Thesamplesizenisneededand
. .
shouldbe ≥30.Youalsoneedthestandarddeviationσ forthepopulation.The
. .
valuesarelookedupinastandardnormaldistributiontable,orz-scoretable,for
whateverconfidencelevelyourequire;thisgivesyouthevaluesofZ touse.The
c.
limitsarecalculatedusingEq.(12.14).
• Confidence intervals for proportions: involve calculating the confidence limits
for the population proportion p given the sample proportion p¯. The sample
.
sizenisneededandshouldbe ≥ 30again.Thevaluesareagainlookedupina
.
standardnormaldistributiontable,orz-scoretable,forwhateverconfidencelevel
yourequire;thisgivesyouthevaluesofZ touse.Thelimitsarecalculatedusing
c.
Eq.(12.16).
• Confidence interval for χ2: involves finding the confidence limits for the
.
populationstandarddeviationσ giventhesamplestandarddeviations.The
.
samplesizenisneeded.Thisvaluegivesyouthedegreesoffreedomtolookup
inaχ2 tablewhichgivesyouthevaluesofχ2 andχ2 touse.Thelimitsare
. α. 1−α.
2 2
calculatedusingEq.(12.17).
Exercises
12.17 A random sample of size n = 5000 of people that are 55 years old
.
and above in the UK found that 82% had a smartphone. Compute a 95%
. .
confidenceintervalfortheproportionofthewholepopulationoftheUKthat
are55yearsoldandaboveandthatownasmartphone.
12.18 Arandomsampleofsize20ofscrewlengthsthataresupposedtobe
5cmlonghadastandarddeviationof0.07cm.Finda90%confidenceinterval
. .
fortheactualstandarddeviation.
12.19 A random sample of 2000 7-year-old boys in the UK had a mean
weightof22.9kg.Assumingthatthepopulationstandarddeviationis3.2kg,
. .
computea99%confidenceintervalforthepopulationmean.
.


================================================================================
PAGE 374
================================================================================

364 12 ElementsofStatistics
12.3.3 TestingHypothesis
This is an important section and is, in a sense, what we have been leading up to.
Hypothesistestingisusedinlotsofareasinscience,socialscience,andbusiness.A
hypothesistestisastatisticaltestusedtoascertainwhetherwecanassumeaspecific
condition is valid for the entire population, given a data sample. Ahypothesis test
generally looks at two opposing hypotheses about a population: the null and the
alternativehypotheses.
• Anullhypothesisisastatementbeingtestedandisthedefaultcorrectanswer.
• Analternativehypothesisisastatementthatisoppositetothenullhypothesis.
Basedonthesampledatafromapopulation,ahypothesistestdetermineswhether
ornottorejectthenullhypothesis.
If we reject a hypothesis when it should be accepted, the error is called type I
error.Ontheotherhand,ifweacceptahypothesiswhenitshouldberejected,the
erroriscalledtypeIIerror.Theonlywaytoreducebothtypesoferroristoincrease
thesamplesize,whichmayormaynotbepossible.
Supposethatthesamplingdistributionofastatisticwithaspecifichypothesisisa
normaldistributionwithmeanμ x¯. andstandarddeviationσ x¯. .Thedistributionofthe
standardisedvariableisthestandardisednormaldistribution.Ifthenullhypothesis
is true, there is a 95% probability that the z-score of the observed sample statistic
.
will lie within [−1.96,1.96]. If we observe a sample statistic whose z-score lies
.
outsidethe[−1.96,1.96]range,weconcludethatsuchaneventcouldonlyhappen
.
withaprobabilityofonly0.05or5%ifthenullhypothesisweretrue.Therefore,we
. .
wouldbeinclinedtorejectthenullhypothesis.
In hypothesis testing, the test’s significance level is the maximum probability
with which we would be willing to risk a type I error. This significance level
is a predefined threshold chosen by the user, such as 0.05. For example, at a
significancelevelof0.05inatwo-tailedtest,thechanceofgettingaresultoutside
the[−1.96,1.96]rangeisonly5%orless.Supposetheprobabilitycomputedbased
. .
on the observed test statistic is less than the significance level. In that case, it is
reasonabletorejectthenullhypothesissincetheresultisunlikelytohavehappened
bychanceunderthenullhypothesis.
Thisrange, ±1.96,isalmost2standarddeviationsoneithersideofthemeanin
.
astandardnormaldistributionandiscommonlyusedforhypothesistestsatthe5%
.
significancelevelinsocialsciencesorwithsubjectareaswhereonlysmallsample
sizesareavailable.Thiscanbereferredtoasa2σ result.Inparticlephysics,a5σ
. .
result is usually needed. For example, to reject the null hypothesis that the Higgs
boson did not exist and that the results found were just chance, a one-sided 5σ
.
resultwasneeded.Thiscorrespondstoapproximatelya0.00003%chancethatthe
.
observeddatacouldhappenbychance,withasignificancelevelof0.0000003fora
.
one-sidedtest.
Remark12.6 Distributions like the normal distribution and t-distribution are
.
symmetricalandhavetwotails.Ifactionsfollowingtheresultfromastatisticaltest


================================================================================
PAGE 375
================================================================================

12.3 Inference 365
are the same regardless of which tail is considered, then we can use two-tailed or
two-sidedtests.Weusuallyconsiderhalfofthesignificancelevelforeachsidewhen
givenasignificancelevelforsuchtests.One-tailed orone-sided testsmaybeused
whenwemaybeinterestedinonlyextremevaluestoonesideofthedistribution,or
actions following the result from a statistical test are different if different tails are
considered.Thatis,thechoiceofatwo-sidedorone-sidedtestisdeterminedbythe
actualproblem.Researchersshoulddecidewhethertheywanttouseaone-tailedor
two-tailed test before collecting the data. If in doubt it is usual to do a two-tailed
test.
(cid:2)
.
Thefollowingshowsthegeneralprocedureforconductingahypothesistest:
• Specifythehypotheses:
H :representsthenullhypothesis.H :representsthealternativehypothesis.
0. 1.
• Determine (1) which test to use, one-tailed or two-tailed with the significance
level,and(2)thesamplesizeforthetestsample.
• Collectthedata.
• Decidewhethertorejectorfailtorejectthenullhypothesis.
– Compute the sample statistic. This step varies based on the test used, for
example,t-statisticorχ2statistic.
.
– Checkthecorrespondingdistributiontable:
· If the probability of the computed statistic occurring is less than the
.
significancelevel,thenwerejectthenullhypothesis;
· If the probability of the computed statistic occurring is greater than the
.
significancelevel,wefailtorejectthenullhypothesis.
Inthischapterweconsidertwotypesoftest:thet-testandtheChi-squaredtest.
.
Theyareeachillustratedbyconsideringtwokindsoftest.
Thet-testisusedto:
.
• Investigate the likely mean of a normally distributed population from a single
smallsample
• Investigatewhethertwosmallsamplescomefromthesameunderlyingnormally
distributedpopulation
TheChi-squaredtestisusedto:
• To compare an observed distribution from an expected, or desired, distribution
(one-wayclassification)
• To compare two distributions to test for any significant differences (two-way
classification)


================================================================================
PAGE 376
================================================================================

366 12 ElementsofStatistics
12.3.3.1 Thet-Tests
The t−tests are used for small samples, usually less than 30. We assume that
.
observationsareindependent,thepopulationdistributionshouldbenormal,andthe
populationsizeshouldbeatleasttentimeslargerthanthesamplesize.
• Means
To test the null hypothesis that a normal population has the mean μ , that is,
0.
H : μ = μ , while H : μ (cid:8)= μ , we use thet statistic, that is, to apply
0 0. 1 0.
Eq. (12.9) to check if the observed sample mean is significantly different from
thepopulationmean.
Example12.17 A machine has produced mugs with a bottom thickness of
4millimetres(mm).Todeterminewhetherthemachineisinproperworking
order,asampleof15mugsischosen,forwhichthemeanthicknessis4.2mm
.
andthestandarddeviationis0.2mm.Testthehypothesisthatthemachineis
.
inproperworkingorderusingasignificancelevelof0.05.
.
Solution
– Step1:Writethehypothesesasfollows:
· H : μ=4mmandthemachineisproperlyworking.
. 0 .
· H : μ(cid:8)=4mmandthemachineisnotproperlyworking.
. 1 .
– Step2:Whichtestshouldbeused:atwo-tailedorone-tailedtest?
Sincethethicknesscanbegreaterorlessthanthemeanvalueandbothare
treatedasnotworkingproperly,wedoatwo-tailedtest.Thesamplesizeis
n=15andthesignificancelevelissetto0.05inthequestion.
. .
– Step3:Thesampleinformationincludesthesamplemeanx¯ =4.2andthe
.
samplestandarddeviations =0.2.
.
– Step4:Computethet-statisticusingEq.(12.9)asfollows:
4.2−4
t = √ ≈3.87.
.
0.2/ 15
We want a significance of 0.05 on a two-tailed test, so we need to look
.
in the t-table column with its third row saying 0.05 (the top row says
.
t ). Two critical values for which 2.5% of the area lies in each tail of
.975. .
the t distribution with 15−1 = 14 degrees of freedom are found in the
.
columnspecifiedas −2.145and2.145(thevalueinthetablebeing2.145).
. . .
Since3.87isnotcoveredin[−2.145,2.145],indicatingtheprobabilityof
. .
observing a t-statistic value like this is less than 5%, we reject H at the
. 0.
0.05significancelevel.
.


================================================================================
PAGE 377
================================================================================

12.3 Inference 367
Exercises
12.20 The lifespan of a type of product should be at least 1000 hours.
Randomlytake16productsfromthistypeandtesttheirlifespans.Theaverage
lifespanofthissampleis920hours.Itisknownthatthelifespanofthistypeof
productfollowsanormaldistributionwithastandarddeviationof100hours.
Testthehypothesisthatthisproducttypeisnon-defectiveusingasignificance
levelof0.05.
.
12.21 Apacketofwashingpowderislabelledashaving235gramsinit.In
ordertoadheretoEUpackagingregulations,itmustnotbetoofarlowerthan
this. We do not care if it is higher. We randomly take 5 samples and note
the actual weights; they are 237, 230, 232, 234, and 232 grams. Determine
whether this test shows that the washing powder is likely to conform to the
regulationswitha0.05significancelevel.[Hint:Useaone-tailedt-test.]
. .
12.22 Acertainbrandofphoneisadvertisedashavinga23-hourvideoplay
backcapability.12phonesaretakenatrandomandtestedtogivethefollowing
times:20,22,23,23,28,23,20,22,22,20,20,and21.Useatwo-tailedt-test
.
todetermineifitsatisfiesthisclaimwitha0.1significancelevel.
.
• Differencebetweenmeansoftwosamples
Suppose that two random samples of sizes n and n are drawn from normal
1. 2.
populations whose standard deviations are equal, that is, σ = σ . The sample
1 2.
means are x¯ and x¯ , and the sample standard deviations are s and s ,
1. 2. 1. 2.
respectively. To test the null hypothesis that the samples come from the same
population,thatis,H : μ =μ andσ =σ ,weusethet scoregivenby
0 1 2 1 2.
x¯ −x¯
t = (cid:16)1 2 , (12.18)
.
σ 1 + 1
n1 n2
where
(cid:3)
(n −1)s2+(n −1)s2
σ = 1 1 2 2. (12.19)
. n +n −2
1 2
ThedistributionoftisStudent’sdistributionwithn +n −2degreesoffreedom.
1 2 .


================================================================================
PAGE 378
================================================================================

368 12 ElementsofStatistics
Example12.18 The history GCSE scores of 16 students from Year 11
of a secondary school have a mean of 65 and a standard deviation of 7,
while the history GCSE scores of 15 students from Year 11 of another
secondaryschoolhaveameanof60andastandarddeviationof13.Istherea
significantdifferencebetweenthehistoryGCSEscoresofthetwogroupsata
significancelevelof0.05?
.
Solution
– Step 1: Denote two schools’ mean scores of history GCSE scores as μ
1.
andμ ,separately.Writethehypothesesasfollows:
2.
· H : μ =μ ,
. 0 1 2.
· H : μ (cid:8)=μ .
. 1 1 2.
– Step 2: Which test should be used: a two-tailed or one-tailed test? Since
one school’s mean score can be higher or lower than the other school’s
meanvalueandbothindicatethatthetwoschools’meanscoresdiffer,we
do a two-tailed test. The significance level is set to 0.05, and the sample
.
meansarex¯ =65andx¯ =60inthequestion.
1 . 2 .
– Step3:UnderthenullhypothesisH ,applyingEq.(12.19),wehave
0.
(cid:3)
(16−1)72+(15−1)132
σ = ≈10.3.
. 16+15−2
– Step4:Computethet-statisticbyapplyingEq.(12.18):
65−60
t = (cid:16) ≈1.35.
.
10.3 1 + 1
16 15
Wewantasignificanceof0.05onatwo-tailedtest,soweneedtolookin
.
the t-table column with its third row saying 0.05 (the top row says t ).
. .975.
So,twocriticalvaluesforwhich2.5%ofthearealiesineachtailofthet
.
distributionwith16+15−2 = 29degreesoffreedomare −2.045and
. .
2.045.Since1.35iscoveredin[−2.045,2.045],indicatingtheprobability
. . .
ofobservingat-statisticvaluelikethisisatleast95%,wedonotrejectH
. 0.
atthe0.05significancelevel.
.
Remark12.7 Letuschangethescenarioabit.Oneofthetwoschoolshasadopted
anewpracticeinteachinghistoryandperformsbetterintheGCSEexamthanthe
other.Weareinterestedonlyindifferenceswherethe‘newpractice’students’mean
scoreisgreaterthanthe‘normal’teachingstudents’meanscore.


================================================================================
PAGE 379
================================================================================

12.3 Inference 369
In this case we use a one-tailed test. Instead of taking the 2.5% differences in
each tail, we take the whole 5% from one tail, the tail on the side where the ‘new
practice’ samples would outperform the ‘normal’ samples.Inthis case,ithappens
tobetheright-handtail.Nowwewantasignificanceof0.05onaone-tailedtest,so
.
weneedtolookinthet-tablecolumnwithits secondrowsaying0.05(thetoprow
.
sayst ).
.95.
(cid:2)
.
Exercise
12.23 Theweightof10studentsfromYear1ofaprimaryschoolhasamean
of 18.5kg and a standard deviation of 2kg, while the weight of 20 students
.
fromYear1ofanotherprimaryschoolhasameanof17.5kgandastandard
.
deviationof3.5kg.Isthereasignificantdifferencebetweentheweightofthe
.
twogroupsatasignificancelevelof0.01?
.
12.24 Thelengthof8newbornbabyboysinoneareaoftheUKhasamean
of55cmandastandarddeviationof5cm,whilethelengthof12newbornbaby
boysinanotherareaoftheUKhasameanof50cmandastandarddeviation
of4cm.Isthereasignificantdifferencebetweenthelengthsofthetwosetsof
babiesatasignificancelevelof0.05?
.
12.25 Theweightof10newbornbabygirlsinoneareaoftheUKhasamean
of 3.5kg and a standard deviation of 0.5kg, while the weight of 15 newborn
. .
baby girls in another area of the UK has a mean of 3.2kg and a standard
.
deviationof0.3kg.Isthereasignificantdifferencebetweentheweightofthe
.
twosetsofbabiesatasignificancelevelof0.01?
.
12.3.3.2 Chi-SquareTests
TheChi-squaretestcanbeusedtodeterminehowwellatheoreticaldistribution,for
example,thenormalandbinomialdistributions,fitsanempiricaldistributionwhich
isobtainedfromthesampledata.Thekeyistoestimatetheexpectedfrequency.
Since the Chi-squared test is a ‘goodness of fit’ test, people usually use it as
a one-tailed test. Using it as a two-sided test means we are also concerned about
whether the fit may be far too good, which usually is not a problem in real-world
applications.
• One-wayclassificationtables
TheChi-squaretestcantestwhethertheobservedfrequenciesdiffersignificantly
from the expected frequencies. To do a Chi-square test, we may set up a table
likeTable12.2.Itiscalledaone-wayclassificationtable,inwhichtheobserved
frequencies occupy a single row and n columns. The number of degrees of


================================================================================
PAGE 380
================================================================================

370 12 ElementsofStatistics
Table12.2 Observedand Event E1. E2. ··· . En.
theoreticalfrequencies
Observedfrequency o1. o2. ··· . on.
Expectedfrequency e1. e2. ··· . en.
freedom is n − 1 for a one-way classification table. Based on the theoretical
.
work showing the relationships among the normal, binomial, and Chi-square
distributions[13],ratherthanusingEq.(12.11),peoplecomputetheχ2 statistic
.
asfollows:
(o −e )2 (o −e )2 (o −e )2 (cid:4)n (o −e )2
χ2 = 1 1 + 2 2 +···+ n n = i i , (12.20)
.
e e e e
1 2 n i=1 i
whereo arethenobservedfrequenciesande arethenexpectedfrequencies.
i. i.
Remark12.8 Asaruleofthumb,tousetheChi-squaretest,theexpectednumber
ofcountsineachcellshouldbeatleast5.
(cid:2)
.
Example12.19 We have a die with 10 faces. Suppose there is an equally
likely chance that the die can land on any face. The die has the following
numbersonitsfaces:0,1,2,3,4,5,6,7,8,9,respectively.Wethrowthedie
.
400times.LetXbethenumbershownonthelandedface.Table12.3showsX
valuesandtherespectivefrequencies.UsetheChi-squaretestatasignificance
levelof0.05totestthehypothesisthatthedieisfair.
.
Solution
– Specifythehypotheses:
H :Thedieisfair.H :Thedieisnotfair.
0. 1.
– We consider a one-tailed test since it evaluates the right-hand tail area,
indicatingasignificantdisagreementbetweentwodistributions.Thesam-
plesizeis400andthesignificancelevelissetto0.05inthequestion.
– TheobserveddataisshowninTable12.3.
– Decidewhethertorejectorfailtorejectthenullhypothesis.
· Compute the χ2 statistic. First, we need to calculate the expected
. .
frequencies.UnderH thatthedieisfair,weexpectthefrequencyvalue
0.
tobe 400 =40foreachfacevalue.Therefore,wecansetupTable12.4.
10 .
(continued)


================================================================================
PAGE 381
================================================================================

12.3 Inference 371
Example12.19(continued)
FromTable12.4,wecancomputeχ2usingEq.(12.20)asfollows:
.
χ2 = (37−40)2 +(46−40)2 +(41−40)2 +(40−40)2 +(40−40)2 +(36−40)2 +
40 40 40 40 40 40
(39−40)2 + (37−40)2 + (38−40)2 + (46−40)2 =2.8.
40 40 40 40 .
· Checkthecriticalvaluefromthecorrespondingdistributiontablebased
.
onthesignificancelevelandthedegreesoffreedom.
The degree of freedom of this one-way classification is 10 −1 = 9.
.
Therefore, χ2 = 16.919 obtained from a χ2 table showing the area
0.05 . .
to the right of the critical value. So for the value of χ2 = 16.919 the
.
areatotherightis5%.Foranylargervalueofχ2,theareaislessthan
. .
5% because we are further to the right hand end of the curve. For any
.
smallervalue,theareaislargerthan5%,andwearefurtherleftonthe
.
curve.
· Comparethecomputedstatistictothecriticalvalue.
.
Our value of χ2 is 2.8, and this is less than 16.919 and so further
. . .
left. This indicates that the area to the right of our observed statistic
value(2.8)isgreaterthan5%.Remember,wearetestinghowcloseour
.
observedfrequenciesarefromtheexpectedfrequencies,thatis,weare
testingifthedieappearstobefair.Alargervalueofχ2indicatesthatthe
.
observed values are further from the expected values and, in this case,
the value of χ2 = 2.8 does not go past the significance level of 0.05.
. .
Wethereforeconcludethattheresultisnotsignificantatthe0.05level.
.
Thus,wedonotrejectH atthesignificancelevelof5%,andweeither
0. .
concludethatthedieisfairorpendingfurthertests.
Onlyifwehadgotamuchhighervalueforχ2,oneabove16.919,would
. .
we feel able to reject the null hypothesis. In this case, the chance of
gettingsucharesultfromafairdicewouldhavebeenlessthan5%,and
.
sowecouldconcludethatthediewasunfair.
Table12.3 ThedistributionofXusedinExample12.19
X 0 1 2 3 4 5 6 7 8 9
Frequency 37 46 41 40 40 36 39 37 38 46
Table12.4 TheobserverandexpecteddistributionofXusedinExample12.19
X 0 1 2 3 4 5 6 7 8 9
Observedfrequency 37 46 41 40 40 36 39 37 38 46
Expectedfrequency 40 40 40 40 40 40 40 40 40 40


================================================================================
PAGE 382
================================================================================

372 12 ElementsofStatistics
Table12.5 ThetheoreticaldistributionofZinExercise12.26
zi. 1 2 3 4 5 6
pi. 1 3 1 6. 3 9 6. 3 7 6. 3 5 6. 3 3 6. 3 1 6.
Table12.6 Thedistribution
zi. 1 2 3 4 5 6
ofZof360throwsin
Observedfrequency 90 145 35 50 15 25
Exercise12.26
Expectedfrequency
Table12.7 TheactualdistributionofchocolatesXinExercise12.27
xi. 1 2 3 4 5 6 7 8
Observedfrequency 24 34 25 35 36 26 36 24
Exercises
12.26 A pair of fair dice is thrown. Let Z be the smaller value of the two
numbers. The distribution of Z consists of its values with their respective
probabilities.ThishasbeenworkedoutforyouandisgiveninTable12.5.
Table12.6showstheobservedfrequenciesofZof360throws.Fillintheir
expectedfrequencies.
CalculatetheChi-square(χ2)value.Doestheobserveddistributiondiffer
.
significantlyfromtheexpecteddistribution,usingasignificancelevelof0.05?
.
12.27 A box of chocolates of a certain make has 8 types of chocolates:
hazelnuts, caramel, orange, fudge, almond, raspberry, coffee, and truffle.
These are labelled types 1,2,··· ,8, respectively. Each box contains 24
.
chocolates, and 10 boxes were selected, and the types counted to get a
distributionXasshowninTable12.7.
Itisexpectedthateachtypeofchocolatehasanequalchanceofappearing
inabox.CalculatetheChi-square(χ2)value.Doestheobserveddistribution
.
differ significantly from the expected distribution, using a significance level
of0.05?
.
12.28 A certain university has percentage targets for its overall degree
classification and a percentage distribution Z of actual results from a recent
year.Boththeexpectedfrequencyandtheobservedfrequency,Z,areshown
inTable12.8.
CalculatetheChi-square(χ2)value.Doestheobserveddistributiondiffer
.
significantlyfromtheexpecteddistribution,usingasignificancelevelof0.05?
.


================================================================================
PAGE 383
================================================================================

12.3 Inference 373
Table12.8 Thedistribution
zi. 1st 2i) 2ii) 3 Pass Fail
ofobservedandexpected
Observedfrequency 20 25 29 14 11 1
gradesatauniversityin
Exercise12.28 Expectedfrequency 15 20 25 20 15 5
• Two-wayclassificationtables
A two-way classification table, also called a contingency table, concerns two
variables. The observed frequencies occupy m rows (that is, m specified values
foronevariable)andncolumns(thatis,nspecifiedvaluesforanothervariable).
AsmentionedinSect.12.2.5ofthischapter,thenumberofdegreesoffreedom
ofatwo-wayclassificationtableis(m−1)×(n−1).
.
TheChi-squaretestcantesttheassociationbetweenthetwovariablesinatwo-
way table. The null hypothesis H assumes no association between the two
0.
variables in the rows and columns, while the alternative hypothesis H states
1.
thatsomeassociationexists.
Example12.20 Twogroups,AandB,consistof150people,eachwithtype
2diabetes.Thetwogroupsaretreatedidentically,exceptthatgroupAmustdo
supervisedaerobicactivitiesfortwohourseachweek.Afterhalfayear,itis
foundthatingroupsAandB,120and100people,respectively,putdiabetes
into remission. Use the Chi-square test at a significance level of 0.05 to test
.
thehypothesisthatthesupervisedaerobicactivitieshelpdiabetesremission.
Solution
– Specifythehypotheses:
H : There is no difference between the results of the two groups of
0.
people—that is, doing the supervised aerobic activities has no effect.
In other words that the diabetes remission is independent of, or has no
associationwith,doingthesupervisedaerobicactivities.
H :Representsalternativehypothesis.
1.
– Weconsideraone-tailedtest.Thesamplesizeis150+150=300,andthe
.
significancelevelissetto0.05inthequestion.
– WesetupTable12.9usingthedatainthequestion.
– Decidewhethertorejectorfailtorejectthenullhypothesis.
· Computethestatistic.First,weneedtocalculatetheexpectedfrequen-
.
cies.UnderH thatdoingthesupervisedaerobicactivitieshasnoeffect,
0.
weexpect (120+100) =110peopleineachofthegroupstoputtheminto
2 .
diabetesremissionand 30+50 = 40ineachgroupnottoputtheminto
2 .
remission.WecansetupTable12.10.
Wecancomputeχ2usingEq.(12.20)asfollows:
.
χ2 = (120−110)2 + (100−110)2 + (30−40)2 + (50−40)2 =6.82.
110 110 40 40 .
(continued)


================================================================================
PAGE 384
================================================================================

374 12 ElementsofStatistics
Example12.20(continued)
· Checkthecriticalvaluefromthecorrespondingdistributiontablebased
.
onthesignificancelevelandthedegreesoffreedom.
Thedegreesoffreedomofthistwo-wayclassificationis(2−1)×(2−
1) = 1.Therefore,χ2 = 3.84obtainedfromaχ2 tableshowingthe
. 0.05 . .
areatotherightofthecriticalvalue.
· Comparingourcomputedstatisticstothecriticalvalue.
.
Sinceχ2 = 6.82 > 3.84,showingthatourvalueisfurthertotheright
.
andindicatingtheareatotherightofourobservedstatisticvalue(6.82)
.
is less than 5%. We therefore conclude that the result is significant at
.
the 0.05 level, that is, our value of χ2 is too big. Thus, we reject H ;
. . 0.
that is, there is a difference between the two groups of people, and
wecanconcludethatdoingsupervisedaerobicactivitieshelpsdiabetes
remission.
Exercises
12.29 Twouniversitieshavepercentagedistributionsofactualresultsfroma
recentyear.BothresultsareshowninTable12.11.
The null hypothesis H is that there is no difference between the two
0.
universities in exam performance. A student chosen randomly has the same
probabilityofobtaininganygradefromeitheruniversity.UsetheChi-square
testatasignificancelevelof0.05totestthishypothesis.
.
12.30 Twofactoriesaremakingthesameproductandbothproduceacertain
number of defective items. The percentage of good and defective items is
giveninTable12.12.
The null hypothesis H is that there is no difference between the two
0.
factories. Use the Chi-square test at a significance level of 0.05 to test this
.
hypothesis.
Table12.9 Theobserved
Remission Noremission Total
frequenciesusedin
GroupA 120 30 150
Example12.20testing
diabetesandaerobicexercise GroupB 100 50 150
Total 220 80 300
Table12.10 Theexpected
Remission Noremission Total
frequenciesusedin
GroupA 110 40 150
Example12.20testing
diabetesandaerobicexercise GroupB 110 40 150
Total 220 80 300


================================================================================
PAGE 385
================================================================================

12.3 Inference 375
Table12.11 Thedistribution
1st 2i) 2ii) 3 Pass Fail
ofgradesintwouniversities
University1 20 25 29 14 11 5
inExercise12.29
University2 12 25 27 20 14 6
Table12.12 Thedistribution
Good Defective
ofdefectiveproductsintwo
Factory1 95 5
factoriesinExercise12.30
Factory2 87 13
Table12.13 Theobserved
Remission Noremission Total
frequencies
GroupA 120 40 160
GroupB 120 40 160
Total 240 80 320
Table12.14 Theobserved
Remission Noremission Total
frequencies
GroupA 24 8 32
GroupB 120 40 160
Total 144 48 192
Intheexamplesandexercisesinthissectionsofar,bothrowtotalswerethesame.
Butwhathappensiftherowtotalsaredifferent?
First, consider a slightly silly and unrealistic example with data similar to that
in Example 12.20, as shown in Table 12.13. Here the data in groups A and B are
identical!Hence,doingsupervisedaerobicactivitiesobviouslyhasnoeffect,andso
we know we will not reject the hypothesis H (H was that doing the supervised
0. 0.
aerobicactivitieshasnoeffect).Infact,ifwetriedtocalculatetheχ2 statistic,we
.
wouldimmediatelyfindthattheexpectedvaluesareallthesameastheactualvalues.
Thisisbecausetheexpectedvaluesarefoundineachcasebydividingthecolumn
sum in the observed frequencies table by two. All the terms in the χ2 calculation
.
wouldbezero.Thevaluefromtheχ2tablewouldbethesameasinExample12.20,
.
soourχ2valueof0islessthanthe3.84valueintheχ2table,andwegetthecorrect
. . .
conclusionthatwedonotrejectthehypothesisH .
0.
NowsupposeeachofthevaluesinGroupAwere5timessmallersinceGroupA
wasasmallgroup.ThisisshowninTable12.14.


================================================================================
PAGE 386
================================================================================

376 12 ElementsofStatistics
Table12.15 Theobserved
Distinction Pass Fail Total
frequenciesinExample12.21
F 10 60 10 80
M 5 29 6 40
Total 15 89 16 120
TheproportionsofpeopleingroupAarethesameasinGroupB,soagain,we
donotrejectthehypothesisH ,thatdoingthesupervisedaerobicactivitieshasno
0.
effect. But if we blindly, and incorrectly, tried to calculate the χ2 statistic using
.
the method used before involving column totals, that is, the expected values are
24+120 =72and 8+40 =24,respectively,wewouldgetaverylargenumberforthe
2 . 2 .
χ2statistic.Infact,wewouldget
.
(24−72)2 (120−72)2 (8−24)2 (40−24)2
+ + + ≈91.3.
.
72 72 24 24
This is obviously totally wrong, and the problem is we have not considered the
differentrowsums.
Thecorrectmethodtousetofindtheexpectedvaluesforeachcellistousethe
formula:
expectedvalue=((rowtotal)∗(columntotal))/(grandtotal). (12.21)
.
So,forinstance,GroupAremissioncellwouldbe 32×144 = 24,whichisthesame
192 .
astheobservedvalue.Checktheothercellsallgivethesameexpectedvalueasthe
observed value. Now we would again get all zeros in the χ2 calculation, and all
.
wouldbewell.
Remark12.9 Infact,Eq.(12.21)reducestothepreviousmethodiftherowtotals
are equal. That is because each row total is half the grand total, so rowtotal = 1
grandtotal 2.
andhenceexpectedvalue= columntotal asbefore.
2 .
(cid:2)
.
Wewillnowdoaproperexample.
Example12.21 The results for an MSc exam for 80 female students and
40 male students are shown in Table 12.15. Use the Chi-square test at a
significancelevelof0.05totestthehypothesisthatthethereisnodifference
.
betweenthefemalesandmalesintheirperformance.
(continued)


================================================================================
PAGE 387
================================================================================

12.3 Inference 377
Example12.21(continued)
Solution
– Specifythehypotheses:
H : There is no difference between the females and males in their
0.
performance.
H :Representsalternativehypothesis.
1.
– Weconsideraone-tailedtest.Thesamplesizeis80+40 = 120,andthe
.
significancelevelissetto0.05inthequestion.
– Decidewhethertorejectorfailtorejectthenullhypothesis.
· Computethestatistic.First,weneedtocalculatetheexpectedfrequen-
.
cies.Here,sincetherowsumsaredifferent,weuseEq.(12.21).These
areshowninTable12.16.Forinstance,inthecellforfemalesthatpass,
wehavethattheexpectedvalue= 80×89 =59.33.
120 .
Wecancomputeχ2usingEq.(12.20)asfollows:
.
χ2 = (10−10)2 + (5−5)2 + (60−59.33)2 + (29−29.67)2 + (10−10.67)2 +
10 5 59.33 29.67 10.67
(6−5.33)2 ≈0.149.
5.33 .
· Checkthecriticalvaluefromthecorrespondingdistributiontablebased
.
onthesignificancelevelandthedegreesoffreedom.
Thedegreesoffreedomofthistwo-wayclassificationis(2−1)×(3−
1) = 2.Therefore,χ2 = 5.99obtainedfromaχ2 tableshowingthe
. 0.05 . .
areatotherightofthecriticalvalue.
· Comparingourcomputedstatisticstothecriticalvalue.
.
Since χ2 = 0.149 < 5.99, indicating the area to the right of the
.
observed statistic value (0.149) is greater than 5%, we conclude that
. .
theresultisnotsignificantatthe0.05level.Thus,wedonotrejectH
. 0.
atthe5%level.
.
Table12.16 Theexpected
Distinction Pass Fail Total
frequenciesinExample12.21
F 10 59.33 10.67 80
M 5 29.67 5.33 40
Total 15 89 16 120
Table12.17 Theobserved
Distinction Pass Fail Total
frequenciesinExercise12.31
F 9 64 7 80
M 6 26 8 40
Total 15 90 15 120


================================================================================
PAGE 388
================================================================================

378 12 ElementsofStatistics
Table12.18 Observed
A B C D
frequenciesinExercise12.32
Girls 20 50 60 15
Boys 25 60 50 15
Exercises
12.31 DothesamecalculationasinExample12.21butwiththenewvalues
giveninTable12.17.Thatis,usetheChi-squaretestatasignificancelevelof
0.05totestthehypothesisthatthereisnodifferencebetweenthefemalesand
.
malesintheirperformance.
12.32 ConductastandardChi-squaredtestusingasignificancelevelof0.05.
.
Maths exam grades are compared between 150 boys and 145 girls in the
sameyeargroupinasecondaryschool(seeTable12.18).Thenullhypothesis
H is that there is no difference between girls and boys in the maths exam
0.
performance. A girl and boy chosen randomly have the same probability of
obtaininganygrade.UsetheChi-squaretestatasignificancelevelof0.05to
.
testthishypothesis.
Remark12.10 TheChi-squaretestreliesontheChi-squaredistribution,whichis
a continuous probability distribution. However, data in contingency tables consist
of discretecounts. This mismatch can cause inconsistencies between theobserved
(discrete)teststatisticandthetheoretical(continuous)Chi-squaredistribution.
To address this, a continuity correction is sometimes applied to adjust the test
statistic and better approximate the continuous distribution, especially when the
degreesoffreedomareequaltooneorwhensamplesizesaresmall,suchaswhen
theexpectedfrequenciesinsomecellsarelessthan5.
For example, Yates’ continuity correction can be applied when the number of
degreesoffreedomisone.Yates’correctionadjuststheteststatisticasfollows:
(cid:4)n (|o −e |−0.5)2
χ2(correct)= i i .
.
e
i=1 i
Please note that in this book, we use the original Chi-square test formula without
applyingYates’continuitycorrection,evenwhenthedegreesoffreedomequals1.
(cid:2)
.


================================================================================
PAGE 389
================================================================================

Chapter 13
Algorithms 4: Maximum Likelihood
Estimation and Its Application to
Regression
In this chapter, we first introduce the maximum likelihood estimation method.
Then we show how it can be applied in enhancing the linear regression algorithm
introduced in Chap. 8. Moreover, since the algorithm is now configured in a
proper probability and statistical framework, we can set up confidence intervals for
estimators using the methods presented in Chap. 12. Finally, we use the maximum
likelihood estimation technique to introduce logistic regression, which is actually a
classification algorithm.
13.1 Maximum Likelihood Estimation
Before we start to introduce the maximum likelihood estimation method, let us have
a look at Example 13.1 first.
Example 13.1 Suppose data are generated independently from an identical
Gaussian distribution, but we do not know which Gaussian distribution it
was. That is, we do not know the parameters μ and σ for the Gaussian
. .
distribution. Suppose that two of the data points are x = 0 and x = 1.
1 . 2 .
Different Gaussian distributions will give a different probability (density) for
these two points being generated. That is, each Gaussian distribution will give
a different likelihood for these two points being generated. We can illustrate
this by considering two particular Gaussian distributions. So we want to know
the probability of obtaining x = 0 and x = 1 from the distribution, that
1 . 2 .
is, to compute p(x = 0,x = 1|μ,σ). Let us consider the two Gaussian
1 2 .
distributions:
(continued)
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 379
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_13


================================================================================
PAGE 390
================================================================================

380 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Example 13.1 (continued)
1. X ∼N(2, 22 ).
.
2. X ∼N(1, 12 ).
.
Figure 13.1 shows these two Gaussian distributions using solid and dash-
dotted lines, respectively, and presents the pointx =0with a cross andx =1
. .
with a square.
We can apply Eq. (10.19) from Chap. 10 to compute the probability
(density).
IfX ∼N(2,22), we hav e
.
1 −(0−2)2
.
p (x =0|μ=2,σ=2)= √ e 2×22 ≈0.12,
2π ×22
and
1 −(1−2)2
.
p (x =1|μ=2,σ=2)= √ e 2×22 ≈0.18.
2π ×22
It givesp(x =0,x =1|μ=2,σ=2)=0.12×0.18=0.0216.
1 2 .
IfX ∼N(1,1), we hav e
.
1 −(0−1)2
.
p (x =0|μ=1,σ=1)= √ e 2×12 ≈0.24,
2π ×12
1 −(1−1)2
.
p (x =1|μ=1,σ=1)= √ e 2×12 ≈0.40.
2π ×12
It givesp(x =0,x =1|μ=1,σ=1)=0.24×0.40=0.096.
1 2 .
Intersections of the horizontal dashed lines in Fig. 13.1 indicate the corre-
spondingp(x|μ,σ) values of a specified Gaussian distribution for each data
.
observation.
Therefore, thelikelihood thatweobserve datax =0andx =1generated
1 . 2 .
from the Gaussian distribution with a mean value of 1 and a standard deviation
of 1 is higher than observing them from the Gaussian distribution with a mean
of 2 and a standard deviation of 2.
Theabove example showsthatthelikelihoodofgenerating0and1candifferwith
different Gaussian distributions. The maximum likelihood (ML) method finds the
estimate that gives the observed data the highest likelihood, the highest probability
(density) that an event has occurred. That is, it finds the values of the parameters
μ and σ that maximise the probability (density). As was shown in Chap. 5,
. .


================================================================================
PAGE 391
================================================================================

13.1 MaximumLikelihoodEstimation 381
Fig. 13.1 Illustration for
0.4
Example 13.1
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
-6 -4 -2 0 2 4 6 8
finding maximums involves differentiation and, in particular, since there are usually
multiple parameters, it involves partial differentiation as discussed in Chap. 6.
Definition 13.1 (Maximum Likelihood Estimation) Suppose X is a random vari-
able with a probability density function f(x;θ , ··· ,θ), where θ , ··· ,θ are k
1 k . 1 k.
unknown parameters.Letx , ··· ,x be observations of X. The joint probability of
1 N.
observations is given by
(cid:2)N
L(θ , ··· ,θ)= f(x ;θ , ··· ,θ).
. 1 k i 1 k
i=1
This function is called the likelihood function and is a function of unknown
parameters (θ , ··· ,θ) [14]. We wish to maximise it. So if there existsθ ˆ , ···,θ ˆ
1 k. 1 k.
where the following equation holds:
L(θ ˆ , ···,θ ˆ )= max {L(θ , ··· ,θ)}, (13.1)
. 1 k 1 k
(θ1,···,θk)∈(cid:5)
thenθ ˆ , ···,θ ˆ are the maximum likelihood estimation of θ , wherej =1,...,k.
1 k. j. .
The notation used inf(x;θ , ··· ,θ)just reminds you that the values used after
1 k .
the semicolon are parameters of the particular probability density function being
considered. So, for example, in the Gaussian distribution used in Example 13.1, the
probability density function would be written as f(x;μ,σ). In Example 13.1 we
.
just took the product of two values of x, namely,x =0andx =1.S o
1 . 2 .
(cid:2)2
L(μ,σ)= f(x ;μ,σ).
. i
i=1


================================================================================
PAGE 392
================================================================================

382 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Exampl(cid:3)e 13.2 Suppose X is a(cid:3) random variable X ∼ N(μ,σ2)
.
. Show that
μˆ = 1 Nx and (σˆ )2 = 1 N(x −μ)2 using the maximum likelihood
N i i. N i i .
estimate method.
Solution SinceX ∼N(μ,σ2), its probability density function (Eq. 10.19) is
.
given by
1 −(x−μ)2
. f (x;μ, σ)= √ e 2σ2 .
2πσ2
The likelihood function over n observations is
(cid:2)N
L(μ,σ)= f(x ;μ,σ)
i
i=1
. =
(cid:2)N (cid:4)
√
1
e
−(xi
2
−
σ
μ
2
)2 (cid:5)
2πσ2
i=1
(cid:3)
= √ 1 e − 2σ 1 2 N i=1 (xi −μ)2 .
( 2πσ2)N
It is more convenient to maximise the logarithm of the likelihood function,
that is, max (θ1,···,θk)∈(cid:5) {lnL(θ 1 , ··· ,θ k )} . . When taking logarithms of both
sides, things that are multiplied become added, and ln(eg(x)) = g(x)for any
.
g(x). Hence, we get the following:
.
(cid:6) (cid:7)
(cid:8)N
1 1
lnL(μ,σ)=ln √ − (x −μ)2
( 2πσ2)N 2σ2 i
i=1
.
(cid:9) (cid:8)N
1
=−Nln(σ)−Nln( 2π)− (x −μ)2.
2σ2 i
i=1
To find μˆ and (σˆ )2 so that lnL(μˆ,σˆ )is maximised, we can do the partial
. . .
derivatives with respect to each parameter and then set them to zero:
(cid:10) (cid:3)
∂lnL(μ,σ) = 1 N (x −μ)=0
∂μ σ2 i=1 (cid:3) i
.
∂lnL(μ,σ) =−N + 1 N (x −μ)2 =0.
∂σ σ σ3 i=1 i
Rearranging these, the solution is
(continued)


================================================================================
PAGE 393
================================================================================

13.1 MaximumLikelihoodEstimation 383
Example 13.2 (continued)
(cid:10) (cid:3)
μˆ = 1 Nx ,
N i(cid:3)i
. (σˆ )2 = 1 N (x −μ)2.
N i=1 i
We can now apply the result from Example 13.2 to Example 13.1.
Example 13.3 We now want to maximise the parameters of a Gaussian
distribution (Gaussian probability density function) to get the most likely
Gaussian distribution that contains just the two points x = 0 and x = 1,
1 . 2 .
as in Example 13.1. From Example 13.2 we have
(cid:10) (cid:3)
μˆ = 1 Nx ,
N i(cid:3)i
. (σˆ )2 = 1 N (x −μ)2.
N i=1 i
So forN =2,x =0andx =1, this givesμˆ =0.5and(σˆ )2 =0.25and so
. 1 . 2 . . .
the standard deviation, σˆ,i s0.5.
. .
To show that this gives a higher probability than the previous Gaussian
distributions used in Example 13.1, we will do the same calculations as we
did in Example 13.1:
1 −(0−0.5)2
.
p (x =0|μ=0.5,σ=0.5)= √ e 2×0.52 ≈0.48,
2π ×0.52
and
1 −(1−0.5)2
.
p (x =1|μ=0.5,σ=0.5)= √ e 2×0.52 ≈0.48.
2π ×0.52
This givesp(x =0,x =1|μ=0.5,σ=0.5)=0.48×0.48=0.23 which
1 2 .
is higher than both the values in Example 13.1.
Intuitively, looking at Fig. 13.2, for the values we get for μˆ and σˆ (the dotted
. .
curve), then a value for the mean of halfway between the two pointsx = 0
1 .
and x = 1 and a standard deviation of 0.5 seems right for the best fitting
2 . .
Gaussian.


================================================================================
PAGE 394
================================================================================

384 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Fig. 13.2 Illustration for 0.8
Example 13.3
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-6 -4 -2 0 2 4 6 8
Exercise
13.1 Suppose X is a random variable, and its probability density function i s
(cid:10)
λe
−λx,
x >0
f(x)=
.
0 , x0,<
where λ >0 . If x ,...,x are observations of X, find the maximum
. 1 n.
likelihood estimate of λ.
.
Now suppose we have just two points in our distribution, namely,x =1and
1 .
x =2.
2 .
(1) Supposeλ=1, findp(x =1,x =2|λ=1).
. 1 2 .
(2) Supposeλ=2, findp(x =1,x =2|λ=2).
. 1 2 .
(3) Nowcalculatethevalueyouwouldgetfor λwhenyouhavethetwopoints
.
x = 1andx = 2using the maximum likelihood estimate ofλ = λ
1 . 2 . best.
you found in the first part of this Exercise. Using this value of λcalculate
.
p(x =1,x =2|λ=λ ).
1 2 best .
13.2 Revisiting Linear Regression
13.2.1 Linear Regression with Maximum Likelihood
Estimation
Suppose Y is the dependent variable, and X is the independent variable. We use
y and x to denote the sample values of these two variables separately. We want to
understand the relationship between these two variables from the given data and to
be able to predict y when a new sample valuex isobserved.


================================================================================
PAGE 395
================================================================================

13.2 RevisitingLinearRegression 385
Given a sample value x from X, there could be a set of different values found for
y, and those values of y would form some sort of distribution. We can model this
conditional distribution of y given the sample value of X. This distribution has some
unknown parameters. For example, if we use a normal distribution (seeEq.10.19 in
Chap. 10) to model the given data, we need to estimate the mean μand the standard
.
deviation σ from the data.
.
When introducing the least-squares method in Chap. 8, the regression line of y
on x is written as
f (x)=a +a x,
. a 0 1
where x is the observed input value,andf (x)gives the estimated value of y . This
a .
equation is referred to as noise-free and gives a fixed value for y. However, we can
include a noise or error element and can write the linear regression model as follow s:
y =a +a x+(cid:8), (13.2)
. 0 1
where (cid:8) is an unobserved error term. This model has two parts: the deterministic
.
part,a +a x, and the stochastic part, (cid:8). There are three assumptions made for this
0 1 . .
model:
1. For each observed x value, there is a normal distribution of Y from which the
sample value y is drawn at random.
2. The mean μof the normal distribution of Y to the corresponding x value lies on
.
the straightlinea +a x.
0 1 .
3. The standard deviation σ of each normal distribution of Y is a constant as x
.
varies. That is, the noise is assumed to be the same at allpoints.
Figure 13.3 shows the normal distribution of Y about the regressionlinea +a x
0 1 .
for three selected values of x.
Based on these assumptions, we have the probability density distribution of y
given by
. f(y|x;a 0 ,a 1 ,σ2)= √
1
e
(y−(a0
2σ
+
2
a1x))2
, (13.3)
2πσ2
with the expected y value lying along the solid line shown in Fig. 13.3, that is,
E(y)=a +a x, and the variance of y is σ2. So the deterministic part of y is given
0 1 . .
by its mean lying on theliney = a +a x, and the stochastic part is the normally
0 1 .


================================================================================
PAGE 396
================================================================================

386 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Fig. 13.3 A schematic diagram illustrating the three assumptions of the linear regression model
distributed error (cid:8). In fact, we can see that given Eq. (13.2), Eq. (13.3) can be written
.
as
1 (cid:8)2
.
f((cid:8)|σ)= √ e2σ2, (13.4)
2πσ2
which says the unobserved error, (cid:8), has a Gaussian distribution with a zero mean
.
and standard deviation of σ.
.
Example 13.4 Consider a simple linear regression model given by y = 2+
1.5x +(cid:8), where (cid:8) ∼ N(μ = 0,σ2 = 9). What is the mean of y given t hat
. .
x =2, that is, what isE(y|x =2)?
. .
Solution Substituting x = 2 into y = 2 + 1.5x + (cid:8), we hav e y = 2 +
. .
1.5 × 2 + (cid:8) = 5 + (cid:8) and E(y|x = 2) = E(5 + (cid:8)). Applying the third
. .
property (3) in Sect. 10.5.1.1 of Chap. 10, that is, E(a + X) = E(X)+
a ,where a is a constant , we obtainE(y|x = 2) = 5+E((cid:8))= 5, since (cid:8)
. . .
has zero mean.
This, of course, is the value of y found by substituting x = 2 into y = 2+
.
1.5x, using the second assumption above that says the mean value lies on the
.
straight linea +a x, namely, the deterministic part of the model.
0 1 .


================================================================================
PAGE 397
================================================================================

13.2 RevisitingLinearRegression 387
Exercise
13.2 Consider a simple linear regression model given byy = −4+3x+(cid:8),
.
where(cid:8) ∼N(μ=0,σ2 =0.01).
.
(1) What isE(y|x =1)?
.
(2) What is the standard deviation (std) of y given thatx =1, that is, what is
.
std(y|x =1)?
.
Suppose there are N data points (x ,y ) for i = 1, ··· ,N. If the y (cid:6) s a re
i i . . i.
independently normal distributed, thatis,y ∼N(a +a x ,σ2), then the likelihood
i 0 1 i .
function is again formed by a product of probabilities:
(cid:2)N
L(a ,a ,σ)= p(y |x ;a ,a ,σ)
0 1 i i 0 1
i=1
. = (cid:2)N √ 1 e −(yi −(a0 2σ + 2 a1xi))2 (13.5)
2πσ2
i=1
(cid:3)
= √ 1 e − 2σ 1 2 N i=1 (yi −(a0 +a1xi))2 .
( 2πσ2)N
Equation (13.5) is the likelihood function for data that can be plotted in two
dimensions, one independent variable X and one dependent variable Y. This can
be extended by considering multiple independent variables. Here we consider d
independent variables. This analysis closely follows the exposition giveninSect.8.3
of Chap. 8. The main difference between this and Chap. 8 is that Chap. 8 does
not have any error or noise terms, and so has no reference to (cid:8) or to σ2. In
. .
fact, the result we now obtain for the coefficients of the line of best fit, or, since
we are now in multiple dimensions, the coefficients of hyperplane of best fit ,
a = (a ,a ,...,a )T, will be the same as we got in Chap. 8 since it is the
0 1 d .
deterministic part of the model.
Let the data be a matrix of the size ofN×(d+1), X, where the first column is a
. .
column vector including N ones.Letx bethe ithdataitemofarowvector including
i.
d+1elements, abe ad+1column vector including d coefficientsa ,...,a plus
. . . 1 d.
a , and yand (cid:2) be column vectors including y and (cid:8) ,i = 1,...,N, respectively.
0. . . i. i. .
Then we have
y=Xa+(cid:2). (13.6)
.


================================================================================
PAGE 398
================================================================================

388 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
The assumptions are(cid:8) ∼N(0,σ2), and the error terms are uncorrelated. That is,
.
E[(cid:8) (cid:8) |X]=0. (13.7)
. i j
Therefore, the likelihood function is
(cid:2)N
L(a,σ2)= p(y |x ;a,σ2)
i i
i=1
. = (cid:2)N √ 1 e −(yi −xia 2 )T σ2 (yi −xia) (13.8)
2πσ2
i=1
=(2πσ2) −N/2e − 2σ 1 2 (y−Xa)T(y−Xa) .
The logarithm ofL(a,σ2), denoted asL(a,σ2),i s
. .
N N 1
L(a,σ2) =− ln(2π)− ln(σ2)− (y−Xa)T(y−Xa). (13.9)
. 2 2 2σ2
In order to find the maximum-likelihood estimations, we need to find the first-
order derivative of Lwith respect to aand σ2as follows:
. . .
∂L 1 1
=− 2(−X)T(y−Xa)= XT(y−Xa), (13.10)
. ∂a 2σ2 σ2
and
∂L N 1
=− + (y−Xa)T(y−Xa). (13.11)
. ∂σ σ σ3
The detailed derivative of
∂(y−Xa)T(y−Xa)
can be viewed in Eq. (8.16) in Chap. 8.
∂a .
We set the derivatives of Eqs. (13.10) and (13.11) to zero to obtain a and σ2, and
. .
the solution is the estimator
a˜ =(XTX) −1XTy,
.
the same as Eq. (8.17) in Chap. 8, and
(y−Xa)T(y−Xa)
σ˜2 = .
.
N


================================================================================
PAGE 399
================================================================================

13.2 RevisitingLinearRegression 389
Remember that ais ad+1length vector since it contains a . So, we can obtain an
. . 0.
unbiased estimator for σ˜2, by dividing(y−Xa)T(y−Xa)byN −(d+1)instead
. . .
of N, that is,
(y−Xa)T(y−Xa)
σ˜2 = . (13.12)
. N −(d+1)
We have yet to introduce the concept of unbiased estimator. Readers may refer to
[15].
We use the notation a˜ and σ˜2 to denote they are estimators to distinguish them
. .
from the true parameters aand σ2.
. .
Example 13.5 Suppose there are a set of data as shown in Table 13.1, where
x are the values of an independent variable X, and y are the corresponding
i. i.
values of the dependent variable Y. Note that hered+1=2since there is just
.
one independent variable. This is picked to make the calculation easier. Also
we only have three data points, soN =3. Realistic examples would have both
.
d and N larger.
Fit a linear regression model for the data using the maximum likelihood
estimate method and round results to two decimalplaces.
⎡ ⎤
1
Solution
Lety=⎣
1.8
⎦
. and X . be the inputs including x i. and one column of
4.2
(cid:15) (cid:16)
111
1s corresponding to a . We haveXT = .
0. .
124
Since
⎡ ⎤
(cid:15) (cid:16) (cid:15) (cid:16)
11
. X TX= 111 ⎣ 12 ⎦= 3 7 ,
124 721
14
then
(cid:15) (cid:16)
(XTX) −1 ≈ 1.5 −0.5 .
. −0.5 0.214
Substituting(XTX) −1, XT, and yinto Eq. (8.17), we have
. . .
⎡ ⎤
(cid:15) (cid:16)(cid:15) (cid:16) (cid:15) (cid:16) (cid:15) (cid:16) (cid:15) (cid:16)
1
. a˜ = − 1 0 .5 .5 0 − .2 0 1 . 4 5 1 1 1 2 1 4 ⎣ 1.8 ⎦≈ 1 − .0 0 8 .2 6 ≈ − 1. 0 0 . 9 2 = a a 0 .
1
4.2
(continued)


================================================================================
PAGE 400
================================================================================

390 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Example 13.5 (continued)
Furthermore, substituting y, X, a,N =3, andd =1into Eq. (13.12), we have
. . . . .
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
(cid:6) (cid:15) (cid:16)(cid:7) (cid:6) (cid:15) (cid:16)(cid:7)
σ˜ . 2 = (3− 1 2) ⎣ 1 1 .8 ⎦−⎣ 1 1 1 2 ⎦ 1 − .0 0 8 .2 6 T ⎣ 1 1 .8 ⎦−⎣ 1 1 1 2 ⎦ 1 − .0 0 8 .2 6 ≈0.05.
4.2 14 4.2 14
Therefore, the fitted linear regression is
y˜ =−0.2+1.09x+(cid:8),
.
where(cid:8) ∼N(μ=0,σ˜2 =0.05).
.
The fitted linear regression line is shown in Fig. 13.4.
Suppose now there is new data atx = 3. Estimate its expected y˜ value using
. .
the above fitted linear regression model, that is, to compute E(y˜|x = 3,μ=
0,σ˜2 =0.05).
.
Solution E(y˜|x =3;μ=0,σ2 =0.05) =−0.2+1.086×3≈3.06.
.
Table 13.1 The data for
xi. yi.
Example 13.5
1 1
2 1.8
4 4.2
Fig. 13.4 Visualisation of
the fitted linear regression
line in Example 13.5


================================================================================
PAGE 401
================================================================================

13.2 RevisitingLinearRegression 391
To illustrate this a bit further, we will do another example using the same X
values as inExample13.5 and Table 13.1 but two different sets of Y values. This is
just to save having to redo the calculationofXTX!
.
Example 13.6 The new values for the first set of data are shown in the
left two columns of Table 13.2. Since the x values are the same as in
i.
Example 13.5, we get the same matrix for X and the same matrix for
.
(XTX) −1, namely,
.
(cid:15) (cid:16)
(XTX) −1 ≈ 1.5 −0.5 .
. −0.5 0.214
⎡ ⎤
1
However, y . now
isy=⎣
3
⎦
. .
3
To find a˜ we again use Eq. (8.17):
.
⎡ ⎤
(cid:15) (cid:16)(cid:15) (cid:16) (cid:15) (cid:16) (cid:15) (cid:16)
1
. a˜ = − 1 0 .5 .50 − .2 0 1 . 4 5 1 1 1 2 1 4 ⎣ 3 ⎦≈ 0.5 1 66 ≈ 0. 1 57 .
3
Finally, we again use Eq. (13.12) to find σ˜2:
.
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤
(cid:6) (cid:15) (cid:16)(cid:7) (cid:6) (cid:15) (cid:16)(cid:7)
. σ˜2 = (3− 1 2) ⎣ 1 3 ⎦−⎣ 1 1 1 2 ⎦ 0.5 1 66 T ⎣ 1 3 ⎦−⎣ 1 1 1 2 ⎦ 0.5 1 66 ≈1.14.
3 14 3 14
Therefore, the fitted linear regression is
y˜ =1+0.57x+(cid:8),
.
where(cid:8) ∼N(μ=0,σ˜2 =1.14).
.
This fitted linear regression line is shown in Fig. 13.5. We can see by
comparing Figs. 13.4 and 13.5 that the line is closer to the points in
Fig. 13.4 and hence we need a smaller value for σ˜2 in Example 13.5 than
.
in Example 13.6.
Finally, we will use ⎡ the ⎤ two right-hand columns in Table 13.2. Again X . is the
1
same, but
nowy=⎣
2
⎦
. .
4
(continued)


================================================================================
PAGE 402
================================================================================

392 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Example 13.6 (continued)
(cid:15) (cid:16)
0
Using the same method we geta˜ = .However, when we come to find σ˜2,
. .
1
we need to calculate(y−Xa)which is as follows:
.
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
(cid:6) (cid:15) (cid:16)(cid:7)
1 11 0
.
⎣
2
⎦−⎣
12
⎦ 0 =⎣
0
⎦
.
1
4 14 0
Hence,σ˜2 =0.
.
This is because these values of X and Y actually lie on a s traightline.
Finally, we will do an example with more data points.
Table 13.2 The data for
xi. yi. xi. yi.
Example 13.6
1 1 1 1
2 3 2 2
4 3 4 4
Fig. 13.5 Visualisation of
the fitted linear regression
line in Example 13.6


================================================================================
PAGE 403
================================================================================

13.2 RevisitingLinearRegression 393
Example 13.7 Suppose there are a set of data as shown in Table 13.3, where
x are the values of an independent variable X, and y are the corresponding
i. i.
values of the dependent variable Y. Again this data can be plotted in two
dimensions since it has just one independent variable, sod +1=2, but now
.
we have five data points, that isN =5.
.
Fit a linear regression model for the data using the maximum likelihood
estimate method and round results to two decimal places.
Solution⎡ ⎤
2.4
⎢ ⎥
⎢ 2 ⎥ (cid:15) (cid:16)
⎢ ⎥ 11111
Lety=⎢
⎢
1.6⎥
⎥.
andXT =
12345 .
.
⎣ 1 ⎦
0.4
So
(cid:15) (cid:16)
5 15
XTX= ,
.
1555
and
(cid:15) (cid:16)
(XTX) −1 = 1.1 −0.3 .
. −0.3 .01
Substituting(XTX) −1, XT, and yinto Eq. (8.17), we have
. . .
(cid:15) (cid:16)
2.98
a˜ = .
. −0.5
Furthermore, substituting y, X, a,N =5, andd =1into Eq. (13.12), we have
. . . . .
1
σ˜2 ≈ ×0.028≈0.0093≈0.01.
. (5−2)
Therefore, the fitted linear regression is
y˜ =2.98−0.5x+(cid:8),
.
where(cid:8) ∼N(μ=0,σ˜2 =0.01).
.
Suppose now there is new data atx =3.5. Estimate its expected y˜value using
. .
the above fitted linear regression model, that is, to compute E(y˜|x =3.5,μ=
0,σ˜2 =0.01).
.
Solution
E(y˜|x =3.5;μ=0,σ2 =0.01)=2.98−0.5×3.5=1.23.
.


================================================================================
PAGE 404
================================================================================

394 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Exercise
13.3 Given the two sets of data as shown in Table 13.4, where x are the
i.
values of an independent variable X and y are the corresponding values of
i.
the dependent variable Y, fit a linear regression model for each dataset using
the maximum likelihood estimate method and round results to two decimal
places.
13.4 Given the data as shown in Table 13.5, where x are the values of an
i.
independent variable X and y are the corresponding values of the dependent
i.
variable Y, fit a linear regression model for each set of data using the
maximum likelihood estimate method and round results to two decimal
places, except σ˜2, which should be given to 3 decimal places.
.
13.2.2 Sampling Distribution of the Linear Regression
Estimators
We now know how to estimate values for a˜, the mean value or the hyperplane (line
.
in 2-dimensional space) of best fit, and σ˜2, the variance, and to get an estimated
.
value for a new data point. The question now is: How accurate are those estimates?
Table 13.3 The data for
xi. yi.
Example 13.7
1 2.4
2 2
3 1.6
4 1
5 0.4
Table 13.4 The data for
xi. yi. xi. yi.
Exercise 13.3
1 7 1 7
3 4 3 5
4 1 4 0
Table 13.5 The data for
xi. yi.
Exercise 13.4
1 0.6
2 1
3 1.6
4 2


================================================================================
PAGE 405
================================================================================

13.2 RevisitingLinearRegression 395
That is the topic of this section—finding confidence limits for these estimates with
a specified level of confidence.
Valuesofestimators a˜and σ˜2obtainedfromEqs.(8.17)and(13.12),respectively,
. .
may change when the observations of y and X change. Just like sampling distri-
. .
butions of means or sampling distributions of proportions (covered in Sects. 12.2.2
and12.2.3ofChap.12),the a˜and σ˜2oflinearregressionmodelshavetheirsampling
. .
distributions, which approximate a normal distribution.
To get to the equations for the sampling distributions of a˜ and σ˜2 and derive
. .
confidence limits, we need some fairly detailed preliminary mathematical results.
These mathematical results are separated out and collected in the next subsec-
tion. They are needed to justify the equations on the sampling distributions that
will be covered in the subsections afterwards, namely, Sects. 13.2.2.2, 13.2.2.3,
and 13.2.2.4. All of this is leading up to the key results on confidence limits of
the sampling distributions, namely, Eqs. (13.19), (13.20), and (13.24).
13.2.2.1 Preliminary Knowledge
• Expectation and Variance of Random Vectors
IfX ,X ,...,X are random variables, then the vectorx = (X ,X ,...,X )T
1 2 t. 1 2 t .
is a vector of random variables.
Definition 13.2 The expected value of a vector of random variables is defined
as the vector of the expected values of the component parts. That is,
(cid:19) (cid:20)
T
E[x]= E[X ],E[X ],...,E[X ] .
. 1 2 t
The variance-covariance matrix of xis
.
(cid:21) (cid:22)
Var[x]=E (x−E[x])(x−E[x])T
⎡ ⎤
Var(X ) cov(X ,X )... cov(X ,X )
1 1 2 1 t
⎢ ⎥ (13.13)
. ⎢cov(X
2
,X
1
) Var(X
2
) ... cov(X
2
,X
t
)⎥
=⎢ ⎣ . .
.
. .
.
... . .
.
⎥ ⎦ .
cov(X ,X ) cov(X ,X ) ... Var(X )
t 1 t 2 t
We know some properties of mean and variance in Sects. 10.5.1.1 and 10.5.2.1
of Chap. 10 for random variables. Here are a couple of vector generalisations of
these properties.


================================================================================
PAGE 406
================================================================================

396 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Suppose Ais as×t matrix of constants, bis as×1vector of constants, and
. . . .
xis at ×1vector of random variables.
. .
E[Ax+b]=AE[x]+b, (13.14)
.
Var(Ax+b)=AVar(x)A T. (13.15)
.
The first of these two properties is illustrated in Example 13.8 since the expected
value of a vector of random variables was easily defined above. It also looks like
the properties of mean given in Sect. 10.5.1.1 of Chap. 10 and so an example
is easy to understand. The second is more complicated since the variance of a
vector of random variables is a matrix involving variances on the main diagonal
and covariances in the other places, as shown in Eq. (13.13) above. So, we give
proof of it instead of giving an example so that you can apply it with trust. If you
are happy to accept the result, you can skip the proof. First, we give an example
on the first property, which is the one involving the mean.
⎡ ⎤
12
Example 13.8 Suppose A . is the matrix of constants: A = ⎣ 34 ⎦ . ; b . is a
56
vector of constants, b = (1,2,3)T; and x is a vector of random variables,
. .
x=(X ,X )T.
1 2 .
Now
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
12 (cid:15) (cid:16) 1 X +2X +1
1 2
. A x+b=⎣ 34 ⎦ X 1 +⎣ 2 ⎦=⎣ 3X 1 +4X 2 +2 ⎦ .
X
56 2 3 5X +6X +3
1 2
So
⎡ ⎤
E[X +2X +1]
1 2
.
E [Ax+b]=⎣ E[3X
1
+4X
2
+2]⎦
E[5X +6X +3]
1 2
⎡ ⎤
E[X ]+2E[X ]+1
1 2
=⎣ 3E[X ]+4E[X ]+2 ⎦
1 2
5E[X ]+6E[X ]+3
1 2
=AE[x]+b,
as required. The second line uses the first three properties of mean from
Sect. 10.5.1.1 of Chap. 10.


================================================================================
PAGE 407
================================================================================

13.2 RevisitingLinearRegression 397
Now we prepare for the proof of the second property by developing a useful
variation on the definition ofVar[x].
.
Example 13.9 Prove the following useful variation on the definition of
Var[x].
.
Var[x]=E[xx T]−E[x](E[x])T. (13.16)
.
Solution Before we start, let us recall the second property in Sect. 3.3.11.1
of Chap. 3: (A + B)T = AT + BT. Therefore, we have (x − E[x])T =
.
xT − (E[x])T. Recall the second property in Sect. 10.5.1.1 of Chap. 10:
.
E(aX) = aE(X), where a is a constant. Thus, we have E[E[x]xT] =
.
E[x]E[xT] since each element of E[x] is a constant. In addition, by the
. .
definition (13.2) above, of the expected random vector of random variables,
we haveE[xT]=(E[x])T.
.
Proof Applying Eq. (13.13), we have
(cid:21) (cid:22)
Var[x]=E (x−E[x])(x−E[x])T
.
(cid:21) (cid:22)
=E (x−E[x])(xT −(E[x])T)
(cid:21) (cid:22)
=E xxT −E[x]xT −x(E[x])T +E[x](E[x])T
=E[xxT]−E[x]E[xT]−E[x](E[x])T +E[x](E[x])T
=E[xxT]−E[x](E[x])T.
(cid:7)(cid:8)
Now we can give the proof of the second property as promised:
Proof V ar(Ax+b) = AV ar(x)A T.
.
Applying Eqs. (13.16), (13.14), and the third property in Sect. 3.3.11.1 of
Chap. 3, that is,(AB)T =BTAT, we hav e
.
(cid:21) (cid:22)
Var(Ax+b)=E (Ax+b)(Ax+b)T −E[Ax+b](E[Ax+b])T
.
=E[AxxTAT +bxTAT +AxbT +bbT]
−(AE[x]+b)(AE[x]+b) T


================================================================================
PAGE 408
================================================================================

398 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
= AE[xxT]AT + bE(xT )AT +AE(x)bT + bbT
− AE[x](E[x])TAT − b(E[x])TAT − AE(x)bT − bbT
= AE[xxT]AT − AE[x] (E[x])TAT
=AVar(x)AT.
(cid:7)(cid:8)
The last step of the above has used the result shown in Example 3.26 in Chap. 3.
• Suppose Cand Dare two matrices with a size ofm×nandn×m, respectively;
. . . .
then
tr(CD)= tr(DC),
.
as illustrated in Example 13.10.
⎡ ⎤
(cid:15) (cid:16)
41
Example 13.10 LetU=
2−13
. , andV=⎣ 02 ⎦ . .
0 5 1
21
⎡ ⎤
(cid:15) (cid:16) (cid:15) (cid:16)
41
. U V= 2−13 ⎣ 02 ⎦= 14 3 .
0 5 1 2 11
21
⎡ ⎤ ⎡ ⎤
(cid:15) (cid:16)
41 8 1 13
. V
U=⎣
02
⎦
2−13
=⎣
010 2
⎦
.
0 5 1
21 4 3 7
tr(UV)= tr(VU)=25.
.
• A square matrix Mis defined to be idempotent if and only ifM2 =M.
. .
Example 13.11 Suppose M is idempotent. We use I to denote an identity
. .
matrix whose size is the same as M.
.
(I−M)(I−M)=I2−MI−IM+M2 =I−2M+M=I−M.
.
Therefore,I−Mis also idempotent.
.


================================================================================
PAGE 409
================================================================================

13.2 RevisitingLinearRegression 399
Every idempotent matrix, except the identity matrix, is singular. Its rank is equal
to its trace.
Example 13.12 Let the hat matrix, H = X(XTX) −1XT, where X is a N ×
. .
(d+1)matrix withN ≥d+1. His therefore anN ×N matrix. Since
. . . .
H2 =X(XTX) −1XTX(XTX) −1XT
.
=X[(XTX) −1(XTX)](XTX) −1XT
=XI(XTX) −1XT
=X(XTX) −1XT
=H,
His therefore idempotent, and the rank of it equals
.
tr(H)= tr(X(XTX) −1XT)
.
= tr(XTX(XTX) −1)
= tr(I (d+1)×(d+1) )
=d+1.
Inside the trace calculation, between the first and second line, we reversed two
matrices to put XT at the front as we illustrated in Example 13.10.
.
• The simplified version of Cochran’s theorem
LetY ∼ N(0,σ2I)and His an idempotent matrix of rankd +1. ThenYTHY
. . . .
is a Chi-square distribution, that is,YTHY ∼ σ2χ2 ; andYT(I−H)Yis also
d+1. .
a Chi-square distribution, that is,YT(I−H)Y∼σ2χ2 .
N−(d+1).
That completes the set of preliminary results we need.
13.2.2.2 Sampling Distribution of Estimators a˜
.
What form of distribution should we consider for a˜? Recall the central limit theorem
.
(see Sect. 11.1.2 of Chap. 11), which states that samples of sums or means of a
random variable tend to be normally distributed in large samples. Consider a˜ as a
.
weighted mean value of y. We can use the normal distribution for a˜.
. .


================================================================================
PAGE 410
================================================================================

400 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Consider the data generated from Eq. (13.6):
y=Xa+(cid:2).
.
Based on the three assumptions of the ordinary linear regression model presented
in Sect. 13.2.1, we have E[y ] =Xa and Var(y) = σ 2I , where I is a N ×N
. N. N. .
identity matrix. This last result is because the matrix of Var((cid:2)) is just a diagonal
.
matrix of variances of the components of (cid:2) (all equal to σ2) with the other values
. .
in the matrix equal to zero since the error terms are uncorrelated and therefore all
covariance terms are zero.
Now using Eq. (8.17) for a˜, namely,
.
a˜ =(XTX) −1XTy,
.
the expected value of a˜ is
.
E[a˜ ]=E[(XTX) −1XTy]
. =(XTX) −1XTXa since E[y]=Xa (13.17)
=a,
and the variance of a˜ is
.
Var(a˜)= Var((XTX) −1XTy)
=(XTX) −1XTVar(y)((XTX) −1XT)T
(13.18)
.
=(XTX) −1XTσ2IX(XTX) −1
=σ2(XTX) −1.
The third line of the last result relies on three results from previous work: namely,
that(AT)T =A,(AB)T =BTAT, and(A −1)T =(AT) −1. That is,
. . .
((XTX) −1XT)T =X((XTX) −1)T =X((XTX)T) −1 =X(XTX) −1.
.
Therefore, the distribution of a˜ isN(a,σ2(XTX) −1).
. .
One of the properties of the multivariate normal distribution is that every single
variable has a univariate normal distribution (see Sect. 11.2.4 of Chap. 11). If we let
a˜ be the ith element of a˜, then
i. .
a˜ ∼N(a ,σ2m ),
. i i ii
where a is the ith element of a and m is the ith diagonal element of M =
i. . ii.
(XTX) −1.
.


================================================================================
PAGE 411
================================================================================

13.2 RevisitingLinearRegression 401
If we replace the actual standard deviation with the estimated standard deviation,
then the distribution is Student’s t-distribution (see Sect. 12.2.6.1 of Chap. 12). The
.
confidence interval can be constructed by applying Eq. (12.13) on the confidence
intervals of means that is found in Sect. 12.3.2.1 of Chap. 12. We need to change Z
c.
to the critical value from a t-distribution table, so that marginoferror = t ×SD.
. c .
Since the standard deviation is the square root of the variance, we get the following
confidence limits for α :
i.
(cid:9)
a˜ ±t σ˜2m , (13.19)
. i c ii
where t is the critical value with degrees of freedom of N −(d +1) if there are
c. .
N observationsandd+1elements in a(there are d independent variables) and the
. .
required confidence level is c .
In this subsection and the following two subsections, we will continue with
Example 13.5 to quickly illustrate finding the appropriate confidence limits. At the
end of all three subsections we will do a full example involving all three subsections.
You need to bear in mind that these examples are really simple ones in the sense that
there is only a small amount of data. Hence, all the confidence limits come out very
wide. Realistic examples have much larger amounts of data, but would be done
using an appropriate computer program.
Example 13.13 Continue with Example 13.5—Part 1: constructing the95%
.
confidence interval for a. Since this is a two-tailed test, we needt in the
. 0.975.
t-table. (cid:15) (cid:16)
−0.2
We substitutea˜ = ,σ˜2 = 0.05, the elements of main diagonal of
. .
1.09
(cid:15) (cid:16)
(XTX) −1 = 1.5 , and t = 12.71 with degrees of freedom of N −
. 0.975 .
0.214
(d+1)=1into Eq. (13.19) and obtain
.
(cid:23)
(cid:15) (cid:16) (cid:6)(cid:15) (cid:16)(cid:7) (cid:15) (cid:16) (cid:15) (cid:16)
−0.2 1.5 −0.2 3.48
±12.71× 0.05× = ± .
.
1.09 0.214 1.09 1.31
Therefore, the confidence interval for the intercept, a , i s [−0.2 −
0.
3.48, −0.2+3.48] = [−3.68, 3.28] and similarly the confidence interval
.
for the gradient, a ,i s[−0.22, 2.40].
1. .
As indicated before, these values give a very wide confidence interval since
we only had three data points. With lots more data points, we would be much
further down the appropriate t-table column and would get a much smaller t
c.
value.


================================================================================
PAGE 412
================================================================================

402 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
13.2.2.3 Sampling Distribution of Variance σ˜2
.
The residual vector is given by
e=y−y˜
.
=(I −X(XTX) −1XT)y
N
=(I −X(XTX) −1XT)(Xa+(cid:2))
N
=(I −X(XTX) −1XT)Xa+(I −X(XTX) −1XT)(cid:2)
N N
=0+(I −X(XTX) −1XT)(cid:2)
N
=(I −H)(cid:2),
N
where H is the hat matrix introduced in Example 13.12. Note that we substituted
.
y˜ = Xa˜ = X(XTX) −1XTy, using the equation for a˜, into the second equation line.
. .
Also, the first term of the fourth equation line is equal to
I Xa−X(XTX) −1XTXa
. N
=I Xa−X(XTX) −1(XTX)a
N
=Xa−XI d+1 a
=Xa−Xa
=0.
As shown in Sect. 13.2.2.1, if H is idempotent, then I −H is also idempotent.
. N .
Also since HT = (X(XTX) −1XT)T = H, therefore H is symmetric. This means
. .
thatI −His also symmetric, that is,(I −H)T =(I −H).
N . N N .
So the sum of the squares of the residuals is
eTe=(cid:2)T(I −H)T(I −H)(cid:2)
. N N
=(cid:2)T(I −H)2(cid:2)
N
=(cid:2)T(I −H)(cid:2),
N
where we have used the idempotent property, that is, if matrixI −His idempotent,
N .
(I −H)2 =(I −H).
N N .
Sinceσ˜2 ∝eTe(see Eq. (13.12)), we have
.
σ˜2 ∝(cid:2)T(I −H)(cid:2),
. N


================================================================================
PAGE 413
================================================================================

13.2 RevisitingLinearRegression 403
and by applying Cochran’s theorem, we have
σ˜2 ∝σ2χ2 .
. N−(d+1)
Recall that we constructed the confidence interval for the population standard
deviation in Sect. 12.3.2.3 of Chap. 12 by applying Eq. (12.17). The confidence
interval for σ2 can be constructed in the same way with degrees of freedom of
.
N −(d+1)in this case, that is,
.
(N −(d+1))σ˜2 (N −(d+1))σ˜2
≤σ2 ≤ . (13.20)
. χ2 χ2
α 1−α
2 2
Example 13.14 Continue with Example 13.5, part 2: constructing the 95%
.
confidence interval for σ2.
.
By applying Eq. (13.20), where χ2 = 5.024 and χ2 = 0.001 with
0.025 . 0.975 .
degrees of freedom of 1 (we have used a χ2 distribution table showing the
.
area to the right of critical value), then we have
(cid:15) (cid:16)
(3−2)×0.05 (3−2)×0.05
, =[0.01, 50].
.
5.024 0.001
Again we have very large confidence limits.
13.2.2.4 Prediction
Finally, assume that we use the fitted model to make a prediction y for a new
new.
data pointx . We hav e
new.
E[y ]=E[x a˜ +(cid:8) ]=x a˜, (13.21)
. new new new
and
Var(x a˜)=x Var(a˜)(x )T. (13.22)
. new new new
Substituting Eq. (13.18) into Eq. (13.22), we have
Var(x a˜)=σ2x (XTX) −1(x )T. (13.23)
. new new new
If we replace the true standard deviation with the estimated standard deviation,
then the distribution is Student’s t-distribution. Hence, we can again use Eq. (12.13)
.
on the confidence intervals of means that is found in Sect. 12.3.2.1 of Chap. 12.
Again, we change Z to the critical value from a t-distribution table as we did
c. .


================================================================================
PAGE 414
================================================================================

404 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
in Sect. 13.2.2.2 of this chapter; then the marginoferror = t ×SD. Hence, the
c .
confidence interval can be easily constructed as follows:
(cid:9)
x a˜ ±t σ˜2x (XTX)−1(x )T. (13.24)
. new c new new
Example 13.15 Continue with Example 13.5, part 3: We use the fitted model
to make a mean predictiony forx =3. As before, we have that
new. new .
y =a +a ×x =−0.2+1.086×3=3.06.
. new 0 1 new
Or, in vector form, using Eq. (13.21), we have
(cid:15) (cid:16)
(cid:24) (cid:25) −0.2
. y new =x new a˜ = 13 =3.06.
1.086
By applying Eq. (13.24) with x = 1[,3] , t he 95% confidence interval
new . .
ofy is computed as follows:
new.
(cid:23)
(cid:15) (cid:16) (cid:15) (cid:16)
1.5 −0.5 1
3.06±12.71 0.05 ×[1,3]× ×
. −0.5 0.214 3
≈3.06±1.85
=[1.21, 4.91].
We will now do a full example.
Example 13.16 Continue with Example 13.7. H ere N = 5 and d = 1, s o
. .
the degrees of freedom is N −(d +1) = 3. First, let us construct the 95%
. .
confidence interval for(cid:15) a . . (cid:16)
2.98
We substitute a˜ = , σ˜2 = 0.0093, the elements of main diagonal
−0.5 . .
(cid:15) (cid:16)
of(XTX) −1 = 1.1 , andt = 3.182with degrees of freedom of 3 into
. 0.975 .
0.1
Eq. (13.19) and obtain
(cid:23)
(cid:15) (cid:16) (cid:6)(cid:15) (cid:16)(cid:7) (cid:15) (cid:16) (cid:15) (cid:16)
2.98 1.1 2.98 0.32
±3.182× 0.0093× = ± .
. −0.5 0.1 −0.5 0.097
(continued)


================================================================================
PAGE 415
================================================================================

13.2 RevisitingLinearRegression 405
Example 13.16 (continued)
Therefore, the confidence interval for the intercept, a , is [2.66, 3.30] and
0. .
similarly the confidence interval for the gradient, a ,i s[−0.60, −0.40].
1. .
These confidence limits are tighter than in the previous example since we
now have five data points.
Second, we construct95%confidence interval for σ2.
. .
By applying the Eq. (13.20), where χ2 = 9.348 and χ2 = 0.216
0.025 . 0.975 .
with degrees of freedom of 3 (we have again used a χ2 distribution table
.
showing the area to the right of critical value), we have
(cid:15) (cid:16)
(5−2)×0.0093 (5−2)×0.0093
, =[0.003, 0.13].
.
9.348 0.216
Finally, use the fitted model to make a mean prediction y for x =
new. new
3.5. That is, by applying Eq. (13.21), we have
.
y =a +a ×x =2.98−0.5×3.5=1.23.
. new 0 1 new
By applying Eq. (13.24) withx =[1,3.5],t he95%confidence interval
new . .
ofy is computed as follows:
new.
(cid:23)
(cid:15) (cid:16) (cid:15) (cid:16)
1.1 −0.3 1
1.23±3.182 0.0093 ×[1,3.5] ×
. −0.3 .01 3.5
=1.23±0.15
=[1.08, 1.38].
Table 13.6 The data for
xi. yi.
Exercise 13.5
−0.5. 2.5.
1 0.5.
2 0


================================================================================
PAGE 416
================================================================================

406 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Exercise
13.5 Suppose a set of data is shown in Table 13.6, where x is the value of an
i.
independent variable X, and y is the corresponding value of the dependent
i.
variable Y. Fit a linear regression model for the data and construct 95%
.
confidence intervals for aand σ2. Round results to two decimal places.
. .
A new data point is given at x = 1.5, find the expected y˜ value, and
. .
construct95%confidence limits for this value.
.
13.6 Continuation of Exercise 13.4. The data is given in Table 13.5, where x
i.
is the value of an independent variable X, and y is the corresponding value of
i.
the dependent variable Y. Construct95%confidence intervals for the values
.
of aand σ2that you have already found in Exercise 13.4.
. .
A new data point is given at x = 2.5, find the expected y˜ value, and
. .
construct95%confidence limits for this value.
.
13.3 The Logistic Regression Algorithm
As mentioned in Chap. 1, there are two categories of supervised learning: regression
and classification. In this section, we briefly show how to formulate a two-class
classification algorithm, logistic regression, also called logit regression, using the
maximum likelihood technique. This is a classification method despite its name.
A classification problem could actually be a classification into multiple classes (a
picture is a flower, a dog, a human, or a tank) or a two-class problem (with these
symptoms, you have or have not got a particular medical condition). The most
common type is two classes, so we only consider that here.
Recall a sigmoid function defined asσ(z)= 1 is bounded between 0 and 1
1+e−z.
(see Fig. 5.5 in Chap. 5). Since it only has values between 0 and 1, it is a suitable
functionforconvertingarealnumberintoaprobability.So,logisticregressionusesa
sigmoid function to estimate the probability ofP(y =c |x ), that is, the probability
i j i .
that an instance i belongs to a specificclassc given its features x . Here, each of
j. i.
thei =N data points x is a vector of d features.
. i.
Let z denote a linear combination of features, z = x a, where a includes
i . .
coefficients. That is,z=a x +a x +···+a x , where x is 1. For instance,
0 i0 1 i1 d id. i0.
with only one feature, we have z = a +a x . Also, let Y be a discrete random
0 1 i1.
variable that only takes two values, c = 1 and c = 0, giving a two-class
1 . 2 .
classification. We have
1 1
P(y =1|x )= , P(y =0|x )=1− , where z=x a.
. i i 1+e−z i i 1+e−z i
(13.25)


================================================================================
PAGE 417
================================================================================

13.3 TheLogisticRegressionAlgorithm 407
Remark 13.1 We are finding the estimated value for Y (which is either 0 and 1
using a threshold on the sigmoid function) from some linear combination of the
features. For example, with only one feature, we have for the probability in the
y =1case as
i .
1
.
. 1+e−(a0 +a1xi1)
Note that in the general sigmoid function
1
,
. 1+e−(a0 +a1x)
the a gives the steepness of the curve. Figure 13.6 shows that the curve witha =2
1. 1 .
(black) is steeper than the one witha =1(red), and the curve witha =2(black)
1 . 1 .
is symmetrical with the one witha =−2 (blue) about the line ofx =0.
1 . .
For a given value of a , a is related to where the sigmoid gets to the half-height
1. 0.
value ( 0.5
.
), that is, when e −(a0 +a1x) = 1
.
or when a
0
+a
1
x = 0
.
. So the sigmoid
reaches half height whenx =−a0. For example, as illustrated in Fig. 13.7, when a
is fixed as 1 andy = 0.5is indi a c 1 a . ted using the green dashed line, we can see how 1 .
.
the curve changes as a varies from −1(red), 0 (black), and 1 (blue). Thus, varying
0. .
the value of a is related to how far the curve moves to the left or right.
0.
Hence, adjusting the coefficients in avaries where and how the sigmoid comes.
.
The same sort of thing is true in the multiple features case, but it is not possible to
draw.
(cid:2)
.
We can rewrite Eq. (13.25)u sing Eq. (10.14) from Chap. 10 as follows:
.
P(y
i
|x
i
)= σ(x
i
a)yi(1− σ(x
i
a))1−yi. (13.26)
Fig. 13.6 An illustration 1
showing how varying a1.in
0.9
the sigmoid function
1+e−(a
1 1x+a0).affects the 0.8
steepness of the curve 0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-6 -5.5-5 -4.5-4 -3.5-3 -2.5-2 -1.5-1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6


================================================================================
PAGE 418
================================================================================

408 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Fig. 13.7 An illustration of 1
the effect of varying a0.in the
sigmoid function
1+e−(a
1 1x+a0).: 0.8
varying a0.is related to how 0.6
far the curve moves to the left
or right 0.4
0.2
0
-6 -5.5-5 -4.5-4 -3.5-3 -2.5-2 -1.5-1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6
If we treat y ’s as independent, then the likelihood function is
i.
(cid:2)N
.
L = σ(x
i
a)yi(1− σ(x
i
a))1−yi.
i=1
It is more convenient to minimise the negative logarithm of the likelihood:
(cid:6) (cid:7)
(cid:8)N
L =− y lnσ(x a)+(1−y )ln(1− σ(x a)) . (13.27)
. i i i i
i
Unlike simple linear regression, there is no closed form (that is, no explicit
formula) to calculate a. However, we can find a suitable estimated, a˜, where
. .
a˜ = a[˜,...,a˜ ]T, using an optimisation method. Let us apply the gradient
0 d .
L
descent algorithm. To do so, we need to compute the partial derivatives ∂ , where
∂aj .
j =0,...,d.
.
Let us take each of the summation terms in Lin turn. So let
.
(cid:6) (cid:7)
SL =− y lnσ(x a)+(1−y )ln(1− σ(x a)) .
. i i i i
Because the derivative of a sum is the sum of derivatives (see Addition Rule in
L SL
Sect. 5.2.2 of Chap. 5), the partial derivative ∂ is simply the sum of the ∂ , that
∂aj . ∂aj .
is, the sum of the following for each data item(x ,y )for each i from1 ···N:
i i . .
(cid:6) (cid:7)
∂SL ∂ ∂
=− y lnσ(x a)+ (1−y )ln(1− σ(x a))
i i i i
∂a ∂a ∂a
j j j
(cid:6) (cid:7)
y 1−y ∂
=− i − i σ(x a) derivative of a log function
σ(x a) 1− σ(x a) ∂a i
i i j
(cid:6) (cid:7)
y 1−y
. =− i − i σ(x a)(1− σ(x a))x chainrule
σ(x a) 1− σ(x a) i i ij
i i
(cid:6) (cid:7)
y − σ(x a)
=− i i σ(x a)(1− σ(x a))x
σ(x a)(1− σ(x a)) i i ij
i i
=(σ(x a)−y )x .
i i ij
(13.28)


================================================================================
PAGE 419
================================================================================

13.3 TheLogisticRegressionAlgorithm 409
Note that for the third equation line above, we have used Example 5.13 from
Sect. 5.2.2 in Chap. 5 to differentiate the sigmoid function and have applied the
following:
(cid:8)d
x a= a x =a x +a x +···+a x ,
. i j ij 0 i0 1 i1 d id
j=0
and so
∂x a
i =x .
. ij
∂a
j
Therefore, we have
∂L (cid:8)N
= (σ(x a)−y )x .
. i i ij
∂a
j i=1
To update a over all data items, we apply Eq. (6.7) from Chap. 6 and have
j.
(cid:6) (cid:7)
(cid:8)N
anew =aold −(cid:8) (σ(x )−y )x , (13.29)
. j j ia i ij
i=1
where (cid:8) is the learning rate.
.
Example 13.17 Suppose we have a set of data items[−1.5,−1,0,0.3], so
.
we only have one feature. We apply the sigmoid functionwherez =2+4x
i i.
to the data, that is,a =2anda =4. Figure 13.8 shows the inputs against
0 . 1 .
their sigmoid function values. Since the sigmoid function values of −1.5
.
and −1are less than the threshold 0.5, we set the class labels for these two
. .
data items to 0 (circle signs) and for the other two data items to 1 (square
signs). Therefore, the corresponding estimated class labels are[0,0,1,1].
.
More usually, of course, we already have the class for the data items and
are using this technique as a supervision technique to determine the model
(finding the values of a and a ). Having found the model, we can use it to
0. 1.
determine the class for a new, unknown data item. So now assume that we
do not know a and a , and we want to fit a logistic regression model for the
0. 1.
data using the gradient descent algorithm with one iteration and a learning
rate of 0.1. We already know that the first two data items are in the class
.
labelled 0, and the others are in the class labelled 1. So the class labels are
[0,0,1,1].
.
(continued)


================================================================================
PAGE 420
================================================================================

410 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Example 13.17 (continued)
Solution Let us set initial values for a and a as 0.5 and 0.4, respectively.
0. 1. . .
Substituting each data value toz =0.5+0.4x , we hav e
i i.
z =0.5+0.4×(−1.5) =−0.1,
. 1
z =0.5+0.4×(−1)=0.1,
. 2
z =0.5+0.4×0=0.5,
. 3
z =0.5+0.4×0.3=0.62.
. 4
Substituting z to Eq. (13.25), we have
i.
1
P(y =1|x )= ≈0.475,
. 1 1 1+e0.1
1
P(y =1|x )= ≈0.525,
. 2 2 1+e−0.1
1
P(y =1|x )= ≈0.622,
. 3 3 1+e−0.5
1
P(y =1|x )= ≈0.650.
. 4 4 1+e−0.62
Considering the threshold is 0.5, then the estimated class label y is
. i.
0, 1, 1, 1, respectively. So this setting gives a misclassification to the second
.
data item. Note that the place where the sigmoid gets to the halfway value is
−a0 =−0.5 =−1.25 which is to the left of the second point (see Fig. 13.8).
a1 0.4 .
So not surprisingly it does not classify this one correctly. This sort of analysis
would be impossible visually with multiple points and many dimensions.
We apply Eq. (13.29) to update the a . When updating a ,w eu sex =1
j. 0. i0 .
and have
anew =0.5−0.1×((0.475−0)
. 0
+(0.525−0)+(0.622−1)+(0.650−1))1≈0.473.
anew =0.4−0.1×((0.475−0)×(−1.5)+(0.525−0)
. 1
×(−1)+(0.622−1)0+(0.650−1)×0.3)≈0.534.
(continued)


================================================================================
PAGE 421
================================================================================

13.3 TheLogisticRegressionAlgorithm 411
Example 13.17 (continued)
With the initial avalues, the second data item was misclassified. After the first
.
iteration,
z =0.473+0.534×(−1.5) =−0.328,
. 1
z =0.473+0.534×(−1) =−0.061,
. 2
z =0.473+0.534×0=0.473,
. 3
z =0.473+0.534×0.3=0.633.
. 4
Substituting z to Eq. (13.25), we have
i.
1
P(y =1|x )= ≈0.419≈0.42,
. 1 1 1+e0.328
1
P(y =1|x )= ≈0.485≈0.49,
. 2 2 1+e0.061
1
P(y =1|x )= ≈0.616≈0.62,
. 3 3 1+e−0.473
1
P(y =1|x )= ≈0.653≈0.65.
. 4 4 1+e−0.633
So using the threshold of 0.5, all 4 data points are now classified correctly.
.
Again looking at where the sigmoid gets to the halfway value, that is, −a0 =
a1
−0.473 = −0.88 , which is to the right of the second point, it is reasonable
0.534 .
that it gets this point classified correctly now. However, it is only just below
the 0.5probability, so perhaps more iterations are needed to make it a better
.
predictor of new points. Getting a result in one iteration is not realistic. In this
example, data are classified correctly with one iteration partly due to having
an unrealistically high value for the learning rate (and choosing good values
for the example).


================================================================================
PAGE 422
================================================================================

412 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Fig. 13.8 Scatter plot
showing data and their
sigmoid values in Example
13.17. The dashed line marks
the threshold at 0.5.
Example 13.18 Suppose we have a new set of data items [−1,0,1,4], s o
.
we only have one feature again. Suppose the class labels are [0,0,1,1]. Fit
.
a logistic regression model for the data using the gradient descent algorithm
with two iterations and a learning rate of 0.05.
Solution Let us set initial values for a and a as −0.3and 0.1, respectively.
0. 1. . .
Substituting each data value intoz =−0.3+0.1x gives
i i.
z =−0.3+0.1×(−1) =−0.4,
. 1
and similarly,
z =−0.3, z =−0.2, andz =0.1.
. 2 3 4
Substituting each z into Eq. (13.25), we have
i.
1
P(y =1|x )= ≈0.401,
. 1 1 1+e0.4
and similarly,
P(y =1|x )≈0.426, P(y =1|x )≈0.450, and P(y =1|x )≈0.525.
. 1 2 1 3 1 4
Using the threshold of 0.5, then the estimated class label y is0, 0, 0, 1.
. i. .
So this setting gives a misclassification to the third data item.
To update the a we apply Eq. (13.29) . When updating a , we again us e
j. 0.
x =1and have
i0 .
anew =−0.3−0.05×((0.401−0)+(0.426−0)+(0.450−1))
. 0
(continued)


================================================================================
PAGE 423
================================================================================

13.3 TheLogisticRegressionAlgorithm 413
Example 13.18 (continued)
+(0.525−1)× 1≈−0.290 .
anew =0.1−0.05×((0.401−0)×(−1)+(0.426−0)×0+(0.450−1)
. 1
×1+(0.525−1)×4)≈0.243.
So after the first iteration we can recalculate the z , using the new values of a
i. 0.
and a , and then use Eq. (13.25) to get the probabilities:
1.
z =−0.543, z =−0.290, z =−0.047, andz =0.682.
. 1 2 3 4
So
P(y =1|x )≈0.367, P(y =1|x )≈0.428, P(y =1|x )≈0.488, and
. 1 1 1 2 1 3
P(y =1|x )≈0.664.
1 4 .
With the same threshold of 0.5, we get estimated class labels of[0,0,0,1],
. .
so the third data item is still misclassified.
For the next iteration we again update the a giving
j.
a =−0.287, anda =0.354.
. 0 1
Finally, to see how we are doing, we calculate the z s and the probabilities to
i.
get
z =−0.641, z =−0.287, z =0.067, andz =1.13,
. 1 2 3 4
and
P(y =1|x )≈0.35, P(y =1|x )≈0.43, P(y =1|x )≈0.51, and
. 1 1 1 2 1 3
P(y =1|x )≈0.76.
1 4 .
Now we have the correct classification of[0,0,1,1].
.


================================================================================
PAGE 424
================================================================================

414 13 Algorithms4:MaximumLikelihoodEstimationandItsApplicationtoRegression
Exercise
13.7 Continue Example 13.17. Do the second iteration of the gradient
descent algorithm and see if it gets all the points correctly classified more
definitely, that is, it might make a better predictor.
13.8 Suppose we have a new set of data items[−1,0,1,3]and suppose the
.
class labels are[0,0,1,1]. Fit a logistic regression model for the data using
.
the gradient descent algorithm with one iteration and a learning rate of 0.05,
starting with values of a and a as −0.3and 0.2, respectively.
0. 1. . .


================================================================================
PAGE 425
================================================================================

Chapter 14
Data Modelling in Practice
In previous chapters, we have introduced some fundamental mathematical and
statistical knowledge needed to understand algorithms and create new approaches.
This chapter will deal with some of the important issues surrounding data analysis.
The fields of machine learning and data science have developed rapidly recently
with many new versions of algorithms being presented and evaluated, each suited
for different tasks. There are too many to describe here and specialised literature is
needed to introduce you to the ones in any area that you wish to study.
However, there are some really fundamental issues that need mentioning in
this book, such as data pre-processing, model selection, model evaluation, and
understanding the bias-variance trade-off in model design. In this chapter, all of
these will be discussed and we will use these issues to motivate the detailed
discussion of two particular algorithms that can improve model generalisation,
namely, ridge regression and early stopping.
14.1 Data Pre-Processing
Chapter 1 mentions that data scientists need to explore data to understand the
relationships among the data better after obtaining some new raw data. To do that,
one should spend some time learning some essential knowledge in the problem
domain, for example, understanding the meaning of each feature or attribute and
how they relate to the target of the problem going to be solved.
14.1.1 Questions to Ask When Pre-Processing the Data
As mentioned before, we need to check whether the data is organised. If it is
unorganised, we need to convert it into a table-like structure. Then we need
to understand what each row and column represents and whether each attribute
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 415
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4_14


================================================================================
PAGE 426
================================================================================

416 14 DataModellinginPractice
is quantitative or qualitative. Apart from these, some common issues must be
considered when exploring the data.
• Is the dataset balanced or imbalanced?
This question is vital to a classification problem, though it has also been drawn to
attention when dealing with the regression problem in recent years [16]. Let us
consideratwo-classapplication.Iftheratioofthesizesofthetwoclassesismuch
lower than 1, that is, the number of patterns in one class is much higher than in the
other, then the trained model will tend to predict any unseen data belonging to the
majority class. Therefore, it is helpful to balance the training dataset. Methods for
balancing can be categorised into two groups: undersampling the majority class
and oversampling the minority class. Readers interested in this topic may start by
reading [17] and [18].
• Are there any inconsistent1 data points?
We briefly discuss two types of inconsistency. First, by inconsistent, we mean
data items that have the same feature values but different target values or class
labels. For example, we have a dataset of customer profiles, and we want to
use the data to train a model to predict whether a customer would like to buy a
newly published book about cooking. Consider two attributes we use: the amount
of money each customer spent and the number of books the customer bought
in the last six months. With only two attributes available, there may be many
inconsistent data items. That is, customers who spent the same amount of money
and bought the same number of books in the last six months may or may not buy
the newly published book as shown in Customer 1 and Customer 3 in Table 14.1.
To cope with this problem, we can add more features to the dataset. For example,
the gender and age of each customer and the types of book each customer prefers
to buy as shown in Table 14.2. In this way, we can alleviate many inconsistent
data items.
The second type of inconsistency refers to data that violate general observations
in the training set. Consider two attributes of a dataset: a book title and its
author. The data displayed in Table 14.3 would be a data inconsistency. When
Table 14.1 An example of data inconsistency
ID The amount spent (£) The number of books Preference on the new book
1 3100 1 Yes
2 4060 2 Yes
3 3100 1 No
1 This book focuses on addressing data quality issues such as label inconsistency caused by
duplicates or measurement errors. In contrast, modelling input-dependent noise variance (het-
eroscedasticity), where noise levels vary systematically with the inputs, requires probabilistic
approaches such as heteroscedastic Gaussian Processes or quantile-based regression. These
methods explicitly account for structured noise and are beyond the scope of this book.


================================================================================
PAGE 427
================================================================================

14.1 DataPre-Processing 417
Table 14.2 Adding more features for dealing with inconsistent data
The amount The number Types of Preference on the
ID Gender Age spent (£) of books purchased book new book
1 Female 35 3100 1 Cooking Yes
2 Female 56 4060 2 Fiction Yes
3 Male 41 3100 1 Finance No
Table 14.3 Another example
Book Title Author
of data inconsistency
Harry Potter and the Philosopher’s Stone Chris Columbus
the book is Harry Potter and the Philosopher’s Stone, we expect the author to
be J. K. Rowling. Chris Columbus is the director of the film Harry Potter and
the Philosopher’s Stone. So there is something wrong here. To deal with this
inconsistency, we need to check data accuracy by identifying and removing the
causes of errors. We can compare data from different sources to identify and
resolve any discrepancies.
• Are there any missing data?
It is common to see missing values in the collected dataset. It may be caused by
errors during collection or by the fact that data are not available. For example,
some people are likely to want to avoid answering specific questions in a survey.
Data may be missing completely at random. That is, missing values can be
observed in all features, and all data items have the same probability of having
missing values. Data may be missing randomly for a specific feature or a
set of features, or not all data items have the same chance of being missing.
Alternatively, missing data may not be at random. That is, the probability of
being missing is entirely different for different values of the same feature.
It is crucial to identify missing values and determine why they are generated.
When we have enough data representing the underlying distribution, removing
observations involving missing values from the dataset is the easiest way to deal
with missing values. Many methods have been proposed to deal with missing
values—for example, replacing with the mean value of the corresponding
attribute. However, replacing missing values may introduce bias. Therefore, extra
care should be taken to check whether it still makes sense with the new filled-in
values. Readers may find more details in [19].
• Are there repeated data?
By repeated data, we mean those duplications among observations. How to deal
with replicated data depends on the algorithm being used and the size of the
dataset. It is generally a good idea to remove duplicated data items. Alternatively,
we may consider whether it is necessary to add more features, as discussed in
dealing with inconsistent data.


================================================================================
PAGE 428
================================================================================

418 14 DataModellinginPractice
Example 14.1 illustrates whether involving repeated data may affect results. Here
we consider applying the principal component analysis technique on a small
dataset with repeated data items. This material was covered in Sect. 4.2.3 of
Chap. 4.
Example 14.1 Suppose we have two datasets as follows:
⎡ ⎤
5 4
⎢ ⎥
⎢− 2 2⎥
⎢ ⎥
⎢−4−4⎥
X1=⎢ ⎥,
. ⎢ 4 4 ⎥
⎢ ⎥
⎣ 2 −2 ⎦
0 0
and
⎡ ⎤
5 4
⎢ ⎥
⎢− 2 2⎥
⎢ ⎥
⎢−4−4⎥
⎢ ⎥
⎢ 4 4 ⎥
X2=⎢ ⎥.
. ⎢ 2 −2⎥
⎢ ⎥
⎢ ⎥
⎢ 0 0 ⎥
⎣ ⎦
5 4
5 4
As seen, X1 includes six unique data items, while X2 includes the
same data except that the data point [5,4] is duplicated three times. The
.
following analysis is done with the aid of suitable programs on a computer.
The mean of X 1 i s[0.83 ˙ ,0.6 ˙] and the mean of X 2 i s[1.875,1.5]. A fter
. .
removing the mean values from each data matrix, we obtain the follow ing:
⎡ ⎤
˙ ˙
4.16 . 33
⎢ ⎢−2.83 ˙ .13 ˙ ⎥ ⎥
⎢ ⎥
⎢−4.83 ˙ −4.6 ˙⎥
. n ewX1=⎢ ⎢ 3.16 ˙ . 33 ˙ ⎥ ⎥ ,
⎢ ⎥
⎣ 1.16 ˙ −2.6 ˙⎦
−0.83 ˙ −0.6 ˙
(continued)


================================================================================
PAGE 429
================================================================================

14.1 DataPre-Processing 419
Example 14.1 (continued)
and
⎡ ⎤
3.125 2.5
⎢ ⎥
⎢−3.875 0.5 ⎥
⎢ ⎥
⎢−5.875 −5.5⎥
⎢ ⎥
⎢ 2.125 2.5 ⎥
newX2=⎢ ⎥.
. ⎢ 0.125 −3.5⎥
⎢ ⎥
⎢
⎢
−1.875 −1.5 ⎥
⎥
⎣ ⎦
3.125 2.5
3.125 2.5
The covariance matrix ofnewX1is shown as follows:
.
(cid:8) (cid:9)
˙ ˙
12.16 8.1 3
. c ov_newX1= ˙ ˙ ,
8.13 1.06
and the covariance matrix ofnewX2is given by
.
(cid:8) (cid:9)
12.41 8.79
cov_newX2= ,
.
8.79 10.00
where two decimal places are kept for each value. The results of eigende-
composition on both covariance matrices are displayed in Table14.4. The
eigenvectors are the two columns of the matrices in each case. Again, two
decimal places are kept in all results.
As can be seen, the PCA results of the two datasets are not identical,
though they are close to each other in this ex ample.
Table 14.4 The results of
The covariance matrix Eigenvalues Eigenvectors
eigendecomposition on (cid:10) (cid:11)
cov_newX1.andcov_newX2. cov_newX1. [19.58, 3.25] . 0.74−0.67
.
0.67 0.74
(cid:10) (cid:11)
cov_newX2. [20.07, 2.34] . 0.75−0.66
.
0.66 0.75


================================================================================
PAGE 430
================================================================================

420 14 DataModellinginPractice
Exercise
14.1 If you fancy reminding yourself about performing PCA, or if you want
to try a suitable program on a computer, try the following exercise. Given the
two datasets in transpose form as follows:
(cid:8) (cid:9)
1001
X1T = ,
.
2011
and
(cid:8) (cid:9)
10010000
X2T = .
.
20111111
The second dataset has the same values as the first except that the data point
[0,1] is duplicated five times. Apply principal component analysis to both
.
datasets, having first removed the mean values from the datasets. Show that
the eigenvalues and eigenvectors are different.
Solving the above issues is also called data cleaning. After addressing these
issues, data scientists need to focus on understanding the statistics of each predictor
and the relations among predictors. The aim is to find more information about the
data than was available when we initially saw it. This helps us to identify suitable
models to apply, whether to adapt appropriate approaches or to create a new method
to solve the problem.
Applying descriptive statistics is helpful at this stage. In addition, employing
unsupervised learning methods, for example, principal component analysis, can
help to visualise the data and extract features. The difference between extracted and
selected features is that extracted features differ from the original data attributes.
For example, features extracted from the principal component analysis are linear
combinations of original features. However, selected features are a subset of the
features in the original feature set. In the following subsection, we will follow [20]
to present a simple approach to carrying out feature selection.
14.1.2 A Simple Feature Selection Method
Suppose we have a structured dataset. The procedure used to select features
considers the correlation between attributes as follows:


================================================================================
PAGE 431
================================================================================

14.1 DataPre-Processing 421
Table 14.5 The correlation
f1 f2 f3 f4 f5 f6
coefficient matrix of the
dataset with six features
f1 1 0.80 0.61 0.91 0.20 − .0.45
f2 0.80 1 0.75 0.37 0.85 − .0.30
f3 0.61 0.75 1 0.18 0.39 0.52
f4 0.91 0.37 0.18 1 0.21 − .0.15
f5 0.20 0.85 0.39 0.21 1 0.52
f6 − .0.45 − .0.30 0.52 − .0.15 0.52 1
1. Calculate the correlation matrix of the features.
2. Determine thetwofeatures Aand B,associated withthelargest absolute pairwise
correlation.
3. Determine the average of the absolute correlations between A and the other
features and the average between B and the otherfeatures.
4. Remove the feature whose average correlation is the biggest.
5. Repeat Steps 2–4 until no absolute correlations are above the threshold.
Example 14.2 Consider a structured dataset with six features: from f 1 to
f6. Table 14.5 shows its correlation coefficients matrix. Select five features
from the original six features using the method introduced above.
Solution
1. Determine the two features associated with the largest absolute pairwise
correlation:
As seen in Table 14.5, features f1 and f 4 have the largest correlation:
0.91.
.
2. Determine the average absolute correlation between f1 and the other
features and the average between f4 and the other features.
0.80+0.61+0.20+0.45 = 0.515 is the average of the absolute correlations
4 .
between f1 and the other four features.
0.37+0.18+0.21+0.15 = 0.2275 is the average of the absolute correlations
4 .
between f 4 and the other four features.
3. Remove the feature whose average correlation is the biggest.
We remove f 1 since its average correlation is bigger than f4’s.
Therefore, the five selected features are f 2, f3, f4, f5, andf6.


================================================================================
PAGE 432
================================================================================

422 14 DataModellinginPractice
Exercise
14.2 Continue Example 14.2 and further remove one feature using the
method introduced in Sect. 14.1.2 of this chapter.
Remark 14.1 Why do we remove highly correlated features?
A set of highly correlated features usually provides little or no additional
information but increases the model complexity. The model complexity usually
refers to the number of features, the number of terms included in a given predictive
model, and whether the model is linear or non-linear.
(cid:2)
.
14.2 Model Selection
14.2.1 Data Splitting
When training a model using a supervised learning algorithm, we usually separate
the whole dataset into a training, validation, and test set.
The training set provides examples for the model to learn the mapping from
inputs to the corresponding targets. The validation set helps search for the most
suitable hyperparameters (user pre-set parameters). The test set determines how well
the trained model performs on data it has never seen before, which is crucial: The
model must never see the test data in the training phase.
The goal of model training is not to learn an exact representation of the training
data itself but rather to build a statistical model of the process that generates the
data. That is, to find the model having the best performance on new data, this is
known as the model’s generalisation ability.
14.2.2 Model Evaluation
Performance metrics are usually calculated for the validation and test sets when
assessing how well a model fits data.
14.2.2.1 Regression Models
Let us use n to denote the number of data items in the validation or test set,
whichever we are evaluating at the time, y˜ denotes the estimated value for data
i.


================================================================================
PAGE 433
================================================================================

14.2 ModelSelection 423
item i, y is the actual target value for data value i, and y is the mean of all target
i. .
values in the validation or test set.
• The mean squared and root mean squared errors.
The mean squared error (MSE) is defined as follows:
(cid:12)
n(y˜ −y )2
MSE= i i i . (14.1)
.
n
The root mean squared error (RMSE) is given by
(cid:13)
(cid:12)
n(y˜ −y )2
RMSE= i i i . (14.2)
.
n
The lower the MSE or RMSE, the better a model fits a dataset. In practice, we use
the RMSE more often since it is measured in the same units as the target value of
the dependent variable.
• Methods used for linear regression models
1. The coefficient of determination
The coefficient of determination was defined in Sect. 8.5.2 of Chap. 8, and is
denoted as R2. It is a value between 0 and 1. It is defined based on assumptions
.
underlying the linear regression algorithm. Readers are referred to [15] f or
more details about the linear regression method. Here, we simply repeat its
definition:
(cid:12)
n(y −y˜ )2
R2 =1− (cid:12)i i i . (14.3)
. n(y −y)2
i i
If all estimated values are equal to their target values, then the numerator
in Eq. (14.3) is zero and R2 equals one, indicating the model fits the data
.
perfectly. If the ratio of the numerator and denominator is one, then R2equals
.
zero, suggesting the model cannot fit the data, and all estimated values are
equal to the mean of the actual target values.
2. Scatter plots of residual against predictions
It is common to plot residuals against features to look for extra regression
structures. Residuals were defined in Sect. 8.5.1 of Chap. 8 ase =y −y˜ , or
i i i.
in vector formase=y−y˜, where y˜ is the expected or estimated value.
. .
Recall (using Eq. (8.17) in Chap. 8) the expected prediction from a linear
regression model is given by
Xa˜ =X(XTX) −1XTy.
.


================================================================================
PAGE 434
================================================================================

424 14 DataModellinginPractice
Fig. 14.1 Scatter plots of residuals versus a feature
The residual is calculated as follows:
e=y−X(XTX) −1XTy.
.
Therefore, we have
XTe=XTy−XTX(XTX) −1XTy=0. (14.4)
.
Equation (14.4) shows that if we calculate the dot product of residuals and
any feature in the data matrix X, the result must always be equal to zero.
.
So the residuals and the features are always independent. In Sect. 11.2.2.3 of
Chap. 11, we show if two random variables are independent, they are also
uncorrelated. Thus, we expect to observe a scatter plot similar to Panel (a)
in Fig. 14.1, where the residuals and feature values are uncorrelated. We do
not expect a straight line, a positive relationship as shown in Panel (b), or
a negative relationship between the residuals and feature values. If we see a
quadratic curve, such as in Panel (c), it suggests we need a quadratic term in
the regression.
14.2.2.2 Classification Models
First, let us define the confusion matrix shown in Table 14.6.
Table 14.6 A confusion matrix: where TN is the number of true-negative samples, FP false-
positive samples, FN false-negative samples, and TP true-positive samples
Predicted negative Predicted positive
Actual negative TN FP
Actual positive FN TP


================================================================================
PAGE 435
================================================================================

14.2 ModelSelection 425
The classification accuracy rate is given by
TN+ TP
accuracyrate= . (14.5)
. TN+ FP+ FN+ TP
For a problem domain with an imbalanced dataset, that is, a dataset where one
class is much bigger than the other, the classification accuracy rate is not sufficient
as a standard performance measure. This is because you can get good accuracy by
always predicting the majority class. So, if you use accuracy as your sole training
criterion of success you, are likely to get a model that just predicts the majority class.
Several common performance metrics, such as recall, precision, and F-score, which
are calculated to fairly quantify the performance of the classification algorithm on
the minority class, can be defined as follows:
TP
Recall= , (14.6)
.
(TP+FN)
TP
Precision= , (14.7)
.
(TP+FP)
2·Recall·Precision
F-score= , (14.8)
.
Recall+Precision
FP
FPrate= , (14.9)
.
FP+TN
TN
True-negativerate= . (14.10)
.
TN+FP
Recall, also called sensitivity, measures the true-positive rate, that is, the number
of actual positives you get right. Precision, also called positive predictive value,
measures the accuracy rate of predicted positive values, that is, the number of
predicted positives you have got right. Usually, a trade-off between precision and
recall is integrated into the metrics, such as the F-score. The false-positive rate, or
FP rate, is the number of actual negatives that you get wrong. A high F-score and
lowFPratearegenerally seenasthepreferredcriterionofsuccess.The true-negative
rate is also called specificity, which is equal to1−FPrate.
.


================================================================================
PAGE 436
================================================================================

426 14 DataModellinginPractice
Table 14.7 The confusion
Predicted negative Predicted positive
matrix of the balanced dataset
Actual negative 496 4
used in Example 14.3
Actual positive 0 500
Example 14.3 Consider a perfectly balanced dataset with 1000 data points,
500 in each class. Table 14.7 shows the confusion matrix. Compute accuracy
rate, recall, precision, F-score, FP rate, and true-negative rate.
Solution We have TP = 500, TN = 496, FP = 4, and FN = 0 from
. . . .
Table 14.7.
996
accuracyrate= =0.996.
.
1000
500
Recall= =1,
.
500
500
Precision= ≈0.992,
.
504
2×1×0.992
F-score= ≈0.996,
. 1+0.992
4
FPrate= =0.008.
.
500
496
True-negativerate= =0.992.
. 496+4
Exercises
14.3 Let us consider a dataset with a highly imbalanced class distribution
with 1000 data points in total, but with only 10 data points in the positive
class and the rest in the negative class. Table 14.8 shows a confusion matrix
that could have been produced by training the algorithm on accuracy alone—
it has predicted most of the data as being negative since that was the majority
class. Compute accuracy rate, recall, precision, F-score, FP rate, and true-
negative rate.
(continued)


================================================================================
PAGE 437
================================================================================

14.2 ModelSelection 427
Now suppose it was trained on the F-score, and the possible results are
shown in Table 14.9. Compute accuracy rate, recall, precision, F-score, FP
rate, and true-negative rate for this confusion matrix.
14.4 Consider a reasonably balanced dataset, with 1000 data points in total.
Table 14.10 shows the confusion matrix. Compute accuracy rate, recall,
precision, F-score, FP rate, and true-negative rate.
14.5 Let us consider a dataset with a reasonably imbalanced class distribution
with1000datapointsintotalbutwithonly100datapointsinthepositiveclass
and the rest in the negative class. Table 14.11 shows the confusion matrix. The
algorithm has predicted most of the data as being negative since that was the
majority class. Compute accuracy rate, recall, precision, F-score, FP rate, and
true-negative rate. A perfect predictor for this dataset would get the results
shown in Table 14.12. Compute accuracy rate, recall, precision, F-score, FP
rate, and true-negative rate for this confusion matrix.
Table 14.8 The confusion
Predicted negative Predicted positive
matrix of the imbalanced
Actual negative 982 8
dataset trained on accuracy in
Exercise 14.3 Actual Positive 8 2
Table 14.9 The confusion
Predicted negative Predicted positive
matrix of the imbalanced
Actual negative 982 8
dataset trained on F-score in
Exercise 14.3 Actual positive 0 10
Table 14.10 The confusion
Predicted negative Predicted positive
matrix of the dataset in
Actual negative 475 15
Exercise 14.4
Actual positive 10 500
Table 14.11 The confusion
Predicted negative Predicted positive
matrix of the imbalanced
Actual negative 855 45
dataset in Exercise 14.5
Actual positive 80 20
Table 14.12 The perfect
Predicted negative Predicted positive
confusion matrix of the
Actual negative 900 0
imbalanced dataset in
Exercise 14.5 Actual positive 0 100


================================================================================
PAGE 438
================================================================================

428 14 DataModellinginPractice
14.2.3 Understanding Bias-Variance Trade-Off
Definition 14.1 (Bias) Bias means that an estimator is calculated in a way that is
systematically different from the quantity that it is supposed to estimate.
˜
Letf(x) be a point estimator andf(x) the ground truth. The bias of the estimator
. .
is defined as follows:
B(f ˜ (x))=E{f ˜ (x)}−f(x). (14.11)
.
It says the bias measures how far the average estimate of a model is from the ground
truth. A positive bias means that the model value is overestimated, and a negative
bias means that the model value is underestimated. The bias may be caused by
making wrong assumptions when choosing a model.
Example 14.4 Recall that the distribution of estimated a˜ for a linear regres-
.
sion approximates the normal distribution given by N(a,σ2(XTX) −1) (see
.
Sect. 13.2.2.2 in Chap. 13). That is, E[a˜] =(X TX) −1XTXa = a (see
.
Eq. 13.17). Applying Eq. (14.11), we have B(a˜) = E[a˜] −a = 0 . That
.
is, under the assumptions mentioned in Sect. 13.2.1 of Chap. 13, estimates of
the ordinary linear regression coefficients are unbiased. This is an important
result for ordinary linear regression.
For example, if the true underlying relationship between the independent
variable and the dependent variable isf(x)=a +a x, thenE[a˜ ]=a and
0 1 . 0 0.
E[a˜ ]=a if we estimate a˜ from the ordinary linear regression method.
1 1. .
Definition 14.2 (Variance) Variance is due to the model’s excessive sensitivity to
˜
small variations in the training data. Letf(x) be a point estimator (see Sect. 12.3.1
.
of Chap. 12). The variance of the estimator is defined as follows:
(cid:14)(cid:15) (cid:16) (cid:17)
var{f ˜ (x)}=E f ˜ (x)−E{f ˜ (x)} 2 . (14.12)
.
The variance measures the variability of a model estimate when changing the
training examples.
Remark 14.2 If the model does not change much between samples, the model
would be considered a low-variance model. On the other hand, if the model changes
drastically between samples, then that model would be considered a high-variance
model.
(cid:2)
.


================================================================================
PAGE 439
================================================================================

14.2 ModelSelection 429
Fig. 14.2 Scatter plot of the
data from Example 14.5, with
the solid line showing the
estimated linear regression
line
Example 14.5 Figure 14.2 shows 66 data points with two variables. Values
of the independent variable X are in the interval of [50,460], and values of
.
the dependent variable Y are in the interval of [0,24]. W e h ave u sed s imple
.
linear regression to estimate the relationship between these two variables. The
estimated line is shown as a solid line.
We divide the whole data set randomly into two samples, including 33
data points for each. Then simple linear regression is applied f oreachsample.
Figure14.3 shows the results. Data in sample 1 are denoted as plus signs, and
data in sample 2 are denoted as square signs. The estimated line from sample
1 is shown as the solid line, while the line from sample 2 is depicted as the
dash-dotted line. As observed, the estimated model remains largely consistent
between the two samples. So, the model is considered a low-variance model.
Now, suppose we had employed a polynomial model with a degree of 5
for each of the two samples. The estimated curves for each sample are shown
in Fig. 14.4 as solid and dash-dotted curves. As can be seen, this estimated
model changes between samples, especially with x values less than 100 and
between 300 and 400. So, this model is considered a high-variance model.
The generalisation error, measured on the test set, can be shown to be composed
of the sum of the bias squared, the variance, and the irreducible error. We cannot
do anything about the irreducible error, or noise, but it is important that the full
generalisation error includes both the bias squared and the variance. We will show
that the generalisation error, or mean squared error, is composed of these three
factors by breaking it down, or decomposing it, as shown in the following example.
This process is quite complicated and can be skipped if needed.


================================================================================
PAGE 440
================================================================================

430 14 DataModellinginPractice
Fig. 14.3 The solid line is
fitted to sample 1, while the
dash-dotted line is fitted to
sample 2, using the data in
Example 14.5
Fig. 14.4 The solid curve is
fittedt os ample1 a ndt he
dash-dotted curve to sample
2,usingdatafrom
Example14.5. Both models
are fitted with a polynomial
of degree 5
Example 14.6 Let us decompose the mean squared error in the ordinary
linear regression model.
First, recall the equation for variance, namely, Eq. (10.11) of Chap. 10,
showed the following equality:
E((X−E(X))2)= E(X2)−(E(X))2.
.
If we add(E(X))2to both sides of the above equation, it gives us
.
E(X2)=E((X−E(X))2)+(E(X))2. (14.13)
.
Now suppose X and Y are two variables, and the underlying f unction is
y = f(x)+(cid:3), where(cid:3) ∼N(0,σ2).S oσ2is the variance of the error term (cid:3),
. . . .
which has a Gaussian distribution (see Sect. 13.2.1 of Chap. 13).
(continued)


================================================================================
PAGE 441
================================================================================

14.2 ModelSelection 431
Example 14.6 (continued)
SinceE(y) = E(f(x)+ (cid:3)),E((cid:3)) = 0, andE(f(x)) = f(x)for a fixed
. . .
function f at the point x ,w eh aveE(y)= f(x).
.
We fit a linear regression linef ˜ (x)= ax+a using N training ex amples,
0.
(x ,y ), i =1,...,N, to minimise the square error:
i i .
(cid:18)N
(y −f ˜ (x ))2.
. i i
i
Given any new data point(x
new
,y(cid:14)ne(cid:15)w )
.
from the same
(cid:16)
di(cid:17)stribution, we can
estimate the expected square error,E y −f ˜ (x ) 2 , as follows:
new new .
(cid:14)(cid:15) (cid:16) (cid:17) (cid:14) (cid:15) (cid:16) (cid:17)
E y −f ˜ (x ) 2 =E (y )2−2y f ˜ (x )+ f ˜ (x ) 2
. new new new new new new
(cid:15) (cid:16) (cid:15) (cid:16)
=E (y )2 −2E(y )E f ˜ (x )
new new new
(cid:14)(cid:15) (cid:16) (cid:17)
+E f ˜ (x ) 2
new
(cid:15) (cid:16) (cid:15) (cid:16)
=E (y )2 −2f(x )E f ˜ (x )
new new new
(cid:14)(cid:15) (cid:16) (cid:17)
+E f ˜ (x ) 2 , (14.14)
new
whereE(y )= f(x )for any x value as noted above.
new new .
Consider the first term on the right-hand side of the last equal sign of
Eq. (14.14). We can rewrite it using Eq. (14.13) with X = y and then
new.
can useE(y )= f(x )to obtain
new new .
(cid:15) (cid:16) (cid:14)(cid:14) (cid:15) (cid:16)(cid:17) (cid:17) (cid:14) (cid:15) (cid:16)(cid:17)
2 2
E (y )2 =E y −E y + E y
. new new new new
(cid:14)(cid:15) (cid:16) (cid:17) (cid:15) (cid:16)
2 2
=E y − f(x ) + f(x ) . (14.15)
new new new
(continued)


================================================================================
PAGE 442
================================================================================

432 14 DataModellinginPractice
Example 14.6 (continued)
Now consider the third term on the right-hand side of the last equal sign of
Eq. (14.14). Again we can rewrite it using Eq. (14.13) withX =f ˜ (x ).W e
new .
obtain
(cid:14)(cid:15) (cid:16) (cid:17) (cid:14)(cid:14) (cid:15) (cid:16)(cid:17) (cid:17) (cid:14) (cid:15) (cid:16)(cid:17)
E f ˜ (x ) 2 =E f ˜ (x )−E f ˜ (x ) 2 + E f ˜ (x ) 2 .
. new new new new
(14.16)
Now we can substitute Equations (14.15) and (14.16) for the first and last
terms of the last equal sign of Eq. (14.14) to get a new version of Eq. (14.14):
(cid:14)(cid:15) (cid:16) (cid:17) (cid:14)(cid:15) (cid:16) (cid:17) (cid:15) (cid:16)
E y −f ˜ (x ) 2 =E y − f(x ) 2 + f(x ) 2
. new new new new new
(cid:15) (cid:16) (cid:14)(cid:14)
−2f(x )E f ˜ (x ) +E f ˜ (x )
new new new
(cid:15) (cid:16)(cid:17) (cid:17) (cid:14) (cid:15) (cid:16)(cid:17)
2 2
−E f ˜ (x ) + E f ˜ (x )
new new
(cid:14)(cid:15) (cid:16) (cid:17) (cid:14)
=E y − f(x ) 2 +E f ˜ (x )
new new new
(cid:15) (cid:16) (cid:17) (cid:14) (cid:15) (cid:16)(cid:17)
−E f ˜ (x ) 2 + f(x )−E f ˜ (x ) 2 .
new new new
(14.17)
The last line of Eq. (14.17) includes three terms. From left to right:
(cid:14)(cid:15) (cid:16) (cid:17)
2
• The first term is the noise E y − f(x ) = E((cid:3)2). Now from
new new .
Eq. (10.11) of Chap. 10 we have E((cid:3)2) = Var((cid:3))+ E((cid:3))2 = σ2 using
.
Sect. 13.2.1 of Chap. (cid:14)13 as noted abov
(cid:15)
e. So the
(cid:16)
fir(cid:17)st term is the noise, σ2
.
.
• The second term E f ˜ (x ) − E f ˜ (x ) 2 is the variance (see
new new .
Eq. 14.12).
• The third term is the bias squared (see Eq. 14.11).
Hence, we have seen in the above example (Example 14.6) that when we assess
a trained model on a test set, the error over the test set, also called the generalisation
error, can be decomposed into three parts. To minimise the generalisation error, we
want to reduce both the bias squared and the variance, since we cannot change the
irreducible error (the noise in Example 14.6). However, as illustrated in Fig. 14.5,
bias squared decreases and variance increases as the model complexity increases.


================================================================================
PAGE 443
================================================================================

14.2 ModelSelection 433
Fig. 14.5 Illustration of the
bias-variance trade-off
It indicates a trade-off between the bias squared and variance. Where the bias and
variance are both relatively small, we get the minimum generalisation error. More
details about error decomposition can be read in [21].
In practice, we usually start with several models widely used in many different
real-world applications. These models are not interpretable due to their complexity,
but they may produce better results with high probability. Then we can look into
simpler models that are interpretable. The aim is to consider using a model that is as
simple as possible but provides a similar performance to that of the complex models.
14.2.4 Underfitting and Overfitting
Underfitting occurs when models make little to no attempt to fit the data. Models
that are high bias and low variance are prone to underfitting. A badly underfitted
model is really unable to do the job of either fitting the training data or providing
a useful estimation tool in the case of the test data, or any unseen data. It is really
not a useful model at all. We need to do something about it such as adding more
features and/or using a more complicated model to overcome this underfitting.
Overfitting is the result of the model trying too hard to exactly fit into the
training set, resulting in a lower bias but a much higher variance. Since the model
fits the training set so well it often does not perform well on the test data or on
any new unseen data, that is, its generalisation ability can be poor. To overcome
this overfitting, we may use fewer features and/or use more training data. Another
approach is to use a regularisation technique to stop the model from being only
suitable for the data it has been trained on.
Regularisation is, therefore, a technique to prevent overfitting or to help opti-
misation. Usually, it is done by adding additional terms in the objective, or cost,
function.
In the next two sections, we will introduce two widely used regularisation
methods: ridge regularisation and early stopping.


================================================================================
PAGE 444
================================================================================

434 14 DataModellinginPractice
14.3 Ridge Regression
14.3.1 The Closed-Form Solution
In Chap. 8, we have shown that the coefficients of a linear regression model with
multiple variables can be computed using the normal equation (see Eq. (8.17) i n
Sect.8.3 of Chap. 8), that is,
a=(XTX) −1XTy,
.
(cid:4)
where Xis the input matrix including a column vector of 1s, and yis the dependent
. . .
variable. This result was found by minimising the sum of the squares of the errors
between the data and the linear approximation.
To control the model complexity, the ridge regression method proposed in [22]
involves adding a penalty term, also called regularisation term, to the least-squares
error function. It may, at first sight, seem strange to add another term to the
objective function since minimising the error function must produce the best linear
approximation to the data. However, that is what overfitting is all about. Using just
the sum of the squares of the errors means the training data is fully satisfied, but it
must be remembered that the real aim is to make the linear approxi(cid:12)mation best for
the test set or any unseen data. In the case of ridge regression, 1λ da2 is added
2 j j.
into the objective function shown in Eq. (8.15) in Chap. 8, where the regularisation
parameter is λandλ ∈[0,∞). That is,
. .
(cid:18)N (cid:18)d (cid:18)d
1
Ridge_Q= (y − a x )2+ λ a2. (14.18)
. i j ij 2 j
i=1 j=0 j=1
Remark 14.3 We can compare Eq. (14.18) with the previous least-squares error
formula given in E(cid:12)q. (8.15) of(cid:12) Chap. 8. Looking at Eq. (14.18), if λ = 0
.
, we
haveRidge_Q = N (y − d a x )2,which is the same as the calculation
i=1 i j=0 j ij .
of the least-squares error, namely, Eq. (8.15), exactly as we would have expected.
Whenλ = ∞, we consider two cases: (1) if any of the estimateda (cid:7)= 0, we hav e
. (cid:12) j .
Ridge_Q =∞; (2) if all thea =0,w eh aveRidge_Q= N (y )2.
. j . i=1 i .
(cid:2)
.
To minimiseRidge_Q
.
, both (cid:12)terms in Eq. (14.18) should be as small as possible.
When λ >0 , minimising 1λ d a2 means forcing the a to be as small as
. 2 j=1 j. j.
possible. That is how the ridge regression method controls the model complexity.
Remark 14.4 The complexity of the ridge model is lower than the complexity of
its corresponding ordinary linear regression model.
(cid:2)
.
Note that j starts with 1 in Eq. (14.18). That means ridge regression does not
penalise the intercept.


================================================================================
PAGE 445
================================================================================

14.3 RidgeRegression 435
We can write Eq. (14.18) in its matrix form as follows:
. Ridge_Q=(y−Xa)T(y−Xa)+λaT ∗ a∗. (14.19)
The difference between a
.
and a∗. is that a∗. does not include the intercept a
0.
. T o
(cid:4) (cid:4)
remove the awkward-looking a∗. we can multiply the second term by I
.
, where I
.
is an identity matrix, with a size of (d +1)×(d +1), except with a zero in the
.
top-left cell, corresponding to the a term. Therefore, the objective function of the
0.
ridge regression is given by
Ridge_Q=(y−Xa)T(y−Xa)+λaTI (cid:4) a. (14.20)
.
E ⎡ xamp ⎤ le 14.7 Suppose aT ∗ = a[ 1 ,a 2 ] . and aT = a[ 0 ,a 1 ,a 2 ] . . Let I (cid:4) =
000
⎣ 010 ⎦ . . ComputeaT ∗ a∗. andaTI (cid:4) a . .
001
(cid:8) (cid:9)
a
Solution aT ∗ a∗ =[a 1 , a 2 ] a 1 = a 1 2 +a 2 2. .
⎡ 2⎤⎡ ⎤ ⎡ ⎤
000 a a
0 0
aTI (cid:4) a =[a 0 ,a 1 ,a 2 ]⎣ 010 ⎦⎣ a 1 ⎦=[0,a 1 ,a 2 ]⎣ a 1 ⎦=a 1 2+a 2 2 . .
001 a a
2 2
Therefore,aT ∗ a∗ =aTI (cid:4) a . .
To obtain a formula for a, we need to find the partial derivative ofRidge_Qwith
. .
respect to a. The working of the derivative of the first term in Eq. (14.20) is the
.
same as the one shown in Eq .(8.16) in Chap. 8. The derivative of the second term is
λ∂aTI(cid:4)a.
To calculate it, we can apply Eq. (7.4) of Chap. 7 and obtain2λI
(cid:4)
a. Hence,
∂a . .
we can obtain the derivative ofRidge_Q:
.
∂Ridge_Q ∂[(y−Xa)T(y−Xa)+λaTI (cid:4) a]
=
.
∂a ∂a
=−2XT(y−Xa)+2λI (cid:4) a. (14.21)
Therefore, by setting Eq. (14.21) equal to zero and rearranging the formula, exactly
like we did when we set Eq. (8.16) equal to zero in Chap. 8, we obtain the closed-
form solution of ridge regression as follows:
a=(XTX+λI (cid:4) ) −1XTy.
.


================================================================================
PAGE 446
================================================================================

436 14 DataModellinginPractice
As we can see, XTX + λI (cid:4) replaces the XTX in Eq. (8.17) to give the ridge
. .
regression solution. To distinguish this from the solution a for the ordinary linear
.
regression, we denote the solution for the ridge regression as a . That is,
R.
a =(XTX+λI (cid:4) ) −1XTy. (14.22)
. R
Ifλ=0,E q. (14.22) gives the same solution as we obtained from the least-squares
.
technique in Chap. 8.
14.3.2 Bias and Variance of Ridge Regression Coefficients
14.3.2.1 Bias
Substitutingy=Xa+(cid:2) to Eq. (14.22), we have
.
a =(XTX+λI (cid:4) ) −1XT(Xa+(cid:2))
. R
=(XTX+λI (cid:4) ) −1XTXa+(XTX+λI (cid:4) ) −1XT(cid:2). (14.23)
Therefore, the expected estimator of a is given by
R.
E[a˜ ]=(XTX+λI (cid:4) ) −1XTXa+(XTX+λI (cid:4) ) −1XTXE((cid:2))
. R
=(XTX+λI (cid:4) ) −1XTXa, (14.24)
where we have used the assumption thatE((cid:2)) = 0 (see Sect. 13.2.1 in Chap. 13).
.
Therefore, the ridge estimator is biased, sinceE[a˜ ] (cid:7)= a. Substituting Eq. (14.24)
R .
to Eq. (14.11), where we have a˜ as the point estimator of a,g ives
R. .
B(a˜ )=(XTX+λI (cid:4) ) −1XTXa−a
. R
=(XTX+λI (cid:4) ) −1XTXa−(XTX) −1(XTX)a
(cid:15) (cid:16)
= (XTX+λI (cid:4) ) −1−(XTX) −1 XTXa. (14.25)
The middle line in Eq. (14.25) is obtained by multiplying a by (XTX) −1(XTX)
. .
which is a matrix and its inverse and so is just the identity matrix. We obtain
E[a˜ ] =a only if λ = 0, which is indeed the linear regression without the ridge
R . .
regularisation term.
Equation (14.25) shows the ridge estimator is biased ifλ (cid:7)= 0. The lower the λ
. .
value, the lower the bias.


================================================================================
PAGE 447
================================================================================

14.3 RidgeRegression 437
14.3.2.2 Variance
SinceXTX(XTX) −1 gives an identity matrix, we can multiply it to Eq. (14.22) and
.
obtain the following:
a =(XTX+λI (cid:4) ) −1XTy
. R
=(XTX+λI (cid:4) ) −1XTX(XTX) −1XTy.
Applying Eq. (8.17) to the second line of the above equation, we obtain
a =(XTX+λI (cid:4) ) −1XTXa˜. (14.26)
. R
The variance of the estimated value of a , a˜ , for the ridge regression is
R. R.
Var(a˜ )= Var((XTX+λI (cid:4) ) −1XTXa˜)
. R
(cid:15) (cid:16)
=(XTX+λI (cid:4) ) −1XTXVar(a˜)XTX (XTX+λI (cid:4) ) −1 T
(cid:15) (cid:16)
=(XTX+λI (cid:4) ) −1XTXσ2(XTX) −1XTX (XTX+λI (cid:4) ) −1 T
(cid:15) (cid:16)
=σ2(XTX+λI (cid:4) ) −1XTX (XTX+λI (cid:4) ) −1 T , (14.27)
where we have used the second property in Sect. 13.2.2.1 of Chap. 13, namely,
Eq. (13.15), to simplify the variance, the fourth property in Sect. 3.3.11.1 of Chap. 3
to evaluate the transpose of multiple matrices and Var(a˜) = σ2(XTX) −1 (see
.
Eq. (13.18)) to replaceVar(a˜), where σ2is the variance of the error term (cid:2).
. . .
Example 14.8 Consider an example withd = 1, which is just one indepen-
.
dent variable. Also let us have 5 points, so N = 5. Given the 5 points as
.
(1,2), (2,4) and (3,3), (4,4) and (5,5). Perform ridge regression and find
. . . . .
the estimated value of a , that is, a˜ ,f or (a) λequal to zero (so we actually
R. R. .
get a˜ and not a˜ , since this would be ordinary linear regression), (b)λ = 1,
. R. .
and (c) λ = 10. For each value of λ calculate the value of the bias and the
. .
variance.
⎡ ⎤ ⎡ ⎤
11 2
⎢ ⎥ ⎢ ⎥
⎢12⎥ ⎢4⎥
⎢ ⎥ ⎢ ⎥
Solution We have X=⎢
⎢
13⎥
⎥.
andy=⎢
⎢
3⎥
⎥.
.
⎣14⎦ ⎣4⎦
15 5
(cid:8) (cid:9)
a
(a) Forλ=0, we wish tofinda= 0 .
. .
a
1
(continued)


================================================================================
PAGE 448
================================================================================

438 14 DataModellinginPractice
Example 14.8 (continued)
(cid:8) (cid:9) (cid:8) (cid:9) (cid:8) (cid:9)
SoXTX= 5 15 and(XTX) −1 = 1 55 −15 = 1.1 −0.3 .
1555 . 50 −15 5 −0.3 .01 .
(cid:8) (cid:9)
18
AlsoXTy= . Sinceλ=0we use Eq. (8.17) from Chap. 8.S o
. .
60
(cid:8) (cid:9)
a˜ =(XTX) −1XTy= 1.8 .
.
0.6
Now for(cid:8) th(cid:9)e bias, we use Eq. (14.25) with λ = 0
.
and get that the bias is
0
B(a˜)= , as expected.
.
0
For the variance, we use Eq. (14.27) withλ=0,g iving
.
(cid:8) (cid:9)
Var(a˜)=σ2(XTX) −1 =σ2 1.1 −0.3 .
. −0.3 .01
(b) Forλ=1, we need matrix λI (cid:4) which is
. .
(cid:8) (cid:9)
00
.
.
01
We now need to useXTX+λI (cid:4) a lot. Forλ=1,
. .
(cid:8) (cid:9)
XTX+λI (cid:4) = 5 15 .
.
1556
Using Eq. (14.22), we get
(cid:8) (cid:9)
1.96
a˜ = .
. R
0.55
For bias, we use Eq. (14.25) withλ=1and get that the bias is,
.
(cid:8) (cid:9)
0 .027
B(a˜ )= a.
. R 0−0.09
(continued)


================================================================================
PAGE 449
================================================================================

14.3 RidgeRegression 439
Example 14.8 (continued)
(cid:8) (cid:9)
1.8
If we assume thata=a˜ = , then
.
0.6
(cid:8) (cid:9)
0.16
B(a˜ )= .
. R −0.05
For variance, we use Eq. (14.27) withλ=1,g iving
.
(cid:8) (cid:9)
0.94 −0.25
Var(a˜ )=σ2 .
. R −0.25 0.08
c) Forλ=10, we need matrix λI (cid:4) which is
. .
(cid:8) (cid:9)
0 0
.
010
So
(cid:8) (cid:9)
XTX+λI (cid:4) = 5 15 .
.
1565
Using Eq. (14.22), we get
(cid:8) (cid:9)
2.7
a˜ =
. R
0.3
For bias, we use Eq. (14.25) withλ=10and get that the bias is,
.
(cid:8) (cid:9)
0 .15
B(a˜ )= a.
. R 0−0.5
(cid:8) (cid:9)
1.8
If we again assume thata=a˜ = , then
.
0.6
(cid:8) (cid:9)
0.9
B(a˜ )= .
. R −0.3
(continued)


================================================================================
PAGE 450
================================================================================

440 14 DataModellinginPractice
Example 14.8 (continued)
For variance, we use Eq. (14.27) withλ=10,g iving
.
(cid:8) (cid:9)
0.425 −0.075
Var(a˜ )=σ2 .
. R −0.075 0.025
Notice that we get three different lines for the three different values of λ.
.
These ares hown in Fig .14.6. If you look at the three values of bias we have
calculated, you will see that the absolute value increases as λincreases. Also,
.
if you look at the three matrices of values for the variance, you will see that
the variance gets smaller as λincreases. So, we have demonstrated that as λ
. .
increases, the bias gets larger and the variance gets smaller.
Remark 14.5 As already remarked, the aim of ridge regression is to find a better
result on the test set than an overfitted linear approximation that favours the training
data too much. This is where the validation set comes in. We could use different
values for λand then test each on the validation data. The value of λthat gives the
. .
best result on the validation data would be the one used for the final test on the real
test data.
(cid:2)
.
Fig. 14.6 Regression lines
for Example 14.8 withλ=0.
(solid line),λ=1.(dashed
line), andλ=10.
(dash-dotted line)


================================================================================
PAGE 451
================================================================================

14.4 EarlyStopping 441
Exercise
14.6 Perform ridge regression on the d = 1, N = 4 example with the
. .
four points as (1,3), (2,3) and (3,2) and (4,1). Find the value of a˜ or
. . . . R.
a˜, as appropriate, the bias and the variance for (a) λ = 0 and (b) λ = 10,
. . .
respectively. Forλ=10assume thata=a˜ to get a value for the bias.
. .
14.4 Early Stopping
When training a neural network or linear regression model using the gradient
descent method, people usually need to pre-set the number of iterations before the
training. After the weights are updated at each iteration, the error on the training and
validation sets can be calculated. We can then produce learning curves by plotting
these errors against the number of training iterations, as shown in Fig. 14.7.A sw e
can see, as the number of iterations increases, the validation error decreases first
and then increases while the training error keeps falling to convergence. Since we
want the model to perform well on the validation set and then later on the test set,
the training should stop when the training error reaches a reasonably small value,
and the validation error reaches a minimum before going up. The number of training
iterations that gives the minimum error on the validation set is the optimal number of
iterations, presented as the dashed line showninFig.14.7. Training that is stopped
at the optimal number of iterations is called early stopping, another technique to
prevent overfitting.
Fig. 14.7 Illustration of
learning curves


================================================================================
PAGE 452
================================================================================

442 14 DataModellinginPractice
Algorithm 1 displays the pseudocode for implementing the early-stopping
method. First, we initialise a model and pre-set the number of training iterations and
the minimum validation error as a huge value. As the training continues, we check
whether the validation error is smaller than the previous one. If not, the training
stops.
Algorithm 1 Pseudocode for the Early Stopping Algorithm
Initialising a model
Pre-set a number of iterations: Items
best_iteration = None
best_model = None
minimum_validation_error = infinite
for <iteration in 1:Items> do
<model: update the weights>
<calculate the training error>
<calculate the validation error>
if validation error ≤ minimum validation error then
minimum_validation_error =validation error
best_iteration = iteration
best_model = model
else
iteration = Items
end if
endfor


================================================================================
PAGE 453
================================================================================

Solutions
Problems of Chap. 1
1.1 d.
1.2
(1) Unstructured.
(2) Unstructured.
(3) Unstructured.
(4) Structured.
1.3
(1) Qualitative.
(2) Qualitative.
(3) Quantitative.
(4) Qualitative.
1.4
(1) Nominal.
(2) Nominal.
(3) Ordinal.
(4) Nominal.
(5) Ordinal.
1.5
(1) Ordinal
(2) Nominal.
(3) Interval.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 443
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4


================================================================================
PAGE 454
================================================================================

444 Solutions
(4) Ratio.
(5) Interval.
(6) Ratio.
Problems of Chap. 2
2.1
(1) True.
(2) True.
(3) False.
(4) False.
(5) True.
(6) False.
(7) True.
2.2
(1) 6.
(2) 0.
(3) 1.
2.3
(1) {{},{−1},{1},{−1,1}}. Cardinality is22 =4.
. .
(2) {{},{0},{1}, {2},{3}, {0,1},{0,2},{0,3},{1,2},{1,3},{2,3} ,
.
{0,1,2},{0,1,3},{0,2,3},{1,2,3},{0,1,2,3}}. Cardinality is24 =16.
. .
(3) 28 = 256.
.
2.4
(1) A∪ B ={b, c, e, f, g,h,i,k}.
.
(2) C ∩ B ={g, h, i}.
.
(3) A∪ B ={a, d, j,l,m}.
.
(4) A\(B ∩C) ={b, c,e,f}.
.
(5) A∪ B ∪ C ={b, c, e, f, g, h,i,k,l,m}.
.
(6) A∩ B ∩ C =g{ ,h}.
.
(7) (A∪ B)\C ={b, c,e,k}.
.
(8) (A∪ B)\C ={a, d, f, g, h, i,j,l,m}.
.
2.5
(1) A ={0,1,2, 3, 4, 5,6,7,8,9,10}.
.
(2) B ={6,7,8,9,10,11 }.
.
(3) C ={0,1,2,3,4} .
.
(4) D ={0,1,2,3, 4,5, 6, 7}.
.
(5) E ={4,5,6,7, 8,9,10,11}.
.


================================================================================
PAGE 455
================================================================================

Solutions 445
2.6
(1) False.
(2) True.
(3) False.
(4) True.
2.7
(1) {(2, 0), (2,1), (3,0), (3,1), (5,0), (5,1)}.
.
(2) ∅.
.
2.8
(1) True.
(2) False.
(3) False.
(4) True.
2.9
(1) Odd.
(2) Neither odd nor even.
(3) Neither odd nor even.
(4) Neither odd nor even.
(5) Odd.
(6) Even.
(7) Neither odd nor even.
(8) Even.
(9) Odd.
2.10
√
(1) f −1(x) = 3x−10.
.
(2) f −1(x) = arcsin x.
3.
(3) f −1(x) = e(x−4)−1.
.
(4) f −1(x) = log x .
3 1−x .
2.11
(1) g ◦ f( x)= 25x 2+20x+4.
.
(2) g ◦ f( x)= sin(2x).
.
(3) g ◦ f( x)= e 2x.
.
(4) g ◦ f( x)= x.
.
(5) g ◦ f( x)=cos3x.
.


================================================================================
PAGE 456
================================================================================

446 Solutions
Problems of Chap. 3
3.1
⎡ ⎤ ⎡ ⎤
5 −1
(1)
⎣
7
⎦
, . and
⎣−5 ⎦
. .
6 4
⎡ ⎤ ⎡ ⎤
4 −4
(2)
⎣
2
⎦
, . and
⎣−2 ⎦
. .
10 −10
3.2
(1) − 5.
.
(2) − 11 .
.
(3) 0.
.
(4) 0.
.
3.3
√
(1) d(w,z) =6 2 ,
.
(2) d(a,b) = 10 .
.
3.4
(1) The direction of u is θ ≈ −0.9828 radians (or θ ≈ 2.1588 radians) and the
. . .
direction√ of v
.
isθ ≈0.67√47
.
radians.
(2) (cid:8)u(cid:8)= 1√3
.
and (cid:8)v(cid:8)= 41.
.
(3) d(u,v) = 58 .
.
3.5
(1) u·v= 12.
.
(2) u·w= 0.
.
(3) u·s =−8.
.
(4) u·t= 10.
.
3.6
(1) wˆ = ( √2 , √1 )
.
.
5 5
(2) sˆ = ( √3 , √1 )
.
.
10 10
(3) ˆ t= ( √3 , √1 , √−1 ).
.
11 11 11
(4) vˆ =( √−1 , √2 , √4 , √1 ).
.
22 22 22 22
3.7
⎡ ⎤
− 4 1 2
(1) U + V= ⎣ 10 0.6 ⎦ . .
1 −4


================================================================================
PAGE 457
================================================================================

Solutions 447
⎡ ⎤
− 2 1 2
(2) 2U− 4V = ⎣ 14 1.2 ⎦ . .
2 −14
⎡ ⎤
7 −26
(3) − 3U+ 2V= ⎣− 25 −1.8 ⎦ . .
− 3 17
3.8
(cid:6) (cid:7)
20 10 2
(1) .
.
17 8 14
⎡ ⎤
9 6
(2)
⎣
0 3
⎦
. .
15 −3
(cid:6) (cid:7)
16
(3) .
.
10
(cid:6) (cid:7)
27 54
(4) .
.
14 43
3.9
(1) 8.
(2) 14.
3.10
(1) 30.
(2) − 3.
.
(3) 0.
3.11
(1) 80.6.
.
(2) − 42.3 .
.
3.12
(cid:6) (cid:7)
(1) A −1 =− 1 6 3 .
18 −2 −4 .
(2) The inverse(cid:6) matri(cid:7)x does not exist.
(3) C −1 = 1 2 −1 .
30 4 1 3 .
(4) I −1 = I .
.
3.13
⎡ ⎤
1 4 7
(1) AT = ⎣ 2 5 0 ⎦ . .
10 −1−3


================================================================================
PAGE 458
================================================================================

448 Solutions
⎡ ⎤
− 1 0
(2) BT = ⎣ 4 5 ⎦ . .
−138
(cid:8) (cid:9)
(3) CT = ⎡ 1 0, − ⎤ 2, 23,−1 . .
1
⎢ ⎥
(4) DT = ⎢ ⎣ − 0 0 .7 ⎥ ⎦. .
10
3.14
Yes, these three vectors are orthogonal to each other.
3.15
(cid:12) (cid:13)
√1 − √3
(1) a. QT = 10 10 .
√3 √1 .
(cid:12) 10 10 (cid:13)
√1 − √3
b. Q −1 = 10 10 .
√3 √1 .
10 10
c. Q
.
is an(cid:12) orthogona(cid:13)l matrix sinceQT =Q −1
.
.
√4 − √3
(2) a. QT = 5 5 .
√3 √4 .
(cid:12)5 5 (cid:13)
√4 − √3
b. Q −1 = 1 5 5 .
5 √3 √4 .
5 5
c. Q is not an orthogonal matrix since QT (cid:9)= Q −1. In fact, the columns of Q
. . .
are orthogonal but not of unit norm.
3.16
(1) Yes, S is a subspace of R3.
1. .
(2) Yes, S is a subspace of R3.
2. .
(3) Yes, S is a subspace of R2.
3. .
3.17
(1) Linearly independent.
(2) Linearly independent.
(3) Linearly dependent.
3.18
(1) det(A) = 0.
.
(2) The columns are linearly dependent.
(3) No.
(4) The rank ofA=2.
.


================================================================================
PAGE 459
================================================================================

Solutions 449
Problems of Chap. 4
4.1
Possible solutions are:
(1) λ = 6,λ =2,uT =[√1 ,√3 ], anduT =[√1 ,−√1 ].
1 . 2 . 1 10 10 . 2 2 2 .
(2) λ = 6,λ =1,uT =[√1 ,√4 ], anduT =[√1 ,−√1 ].
1 . 2 . 1 17 17 . 2 2 2 .
(3) λ = 2,λ =−3,uT =[√1 ,−√2 ], anduT =[−√3 ,√1 ].
1 . 2 . 1 5 5 . 2 10 10 .
(4) λ = 4,λ =3,uT =[−√1 ,√1 ]anduT =[−√3 ,√2 ].
1 . 2 . 1 2 2 . 2 13 13 .
(5) λ = −6 ,λ = −3 ,λ = 4,uT = [√2 ,√1 ,√−6 ],uT = [√1 ,0,−√1 ], and
1 . 2 . 3 . 1 41 41 41 . 2 2 2 .
uT =[√4 ,√7 ,√3 ].
3 74 74 74 .
4.2
We have complex eigenvalues. (Complex eigenvalues of a matrix with non-zero
eigenvectors are beyond the scope of this book.)
4.3
(cid:12) (cid:13)
√1 √1
(1) U = 10 2 .So
√3 √−1 .
10 2
(cid:12) √ √ (cid:13)(cid:6) (cid:7)(cid:12) (cid:13) (cid:6) (cid:7)
√5 √5
31
√1 √1
60
D= 2 2 2 2 10 2 = .
. √3 −√1 35 √3 √−1 02
2 2 2 2 10 2
(cid:12) (cid:13)
√1 √−2
(2) U = 5 5 .So
√2 √1 .
5 5
(cid:12) (cid:13)(cid:6) (cid:7)(cid:12) (cid:13) (cid:6) (cid:7)
√1 √2 √1 √−2
32 70
D= 5 5 5 5 = .
. √−2 √1 26 √2 √1 02
5 5 5 5
The columns of Uare orthogonal, sinceu ·u =0, and both are of unit length
. 1 2 .
so Uis an orthogonal matrix.
.
4.4
(1) The mean of each variable is 0. . √
(2) The standard deviation of each variable is 5.
.
(3) The covariance between two variables is 4.
(4)
(cid:6) (cid:7)
54
.
.
45


================================================================================
PAGE 460
================================================================================

450 Solutions
(5) λ = 9,λ =1,uT =[√1 ,√1 ], anduT =[√1 ,√−1].
1 . 2 . 1 2 2 . 2 2 2 .
(6) The first principal component captures 90% of the total variation, and the
.
sec√ond captures10%
.
of the total variation.
(7) [3 2,0].
.
4.5
One possible solution is shown as follows:
⎡ ⎤ ⎡ ⎤
√1 0
. Y = ⎢ ⎢ ⎢ ⎢ ⎢ − 3 0 3− 3 0 3 ⎥ ⎥ ⎥ ⎥ ⎥ = ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ − 0 √ 2 1 2 0 0 ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ (cid:6) 6 0 0 2 (cid:7)(cid:12) √ √1 1 2 − √1 √ 2 1 (cid:13) T .
⎣− 1 1⎦ ⎣ 0 −√1 ⎦ 2 2
2
1 −1 0 √1
2
4.6
16.30.
.
4.7
λ = 62 =9andλ = 22 =1.
1 5−1 . 2 5−1 .
Problems of Chap. 5
5.1
(1) 1.
(2) 3.
(3) 0.
(4) 6.
(5) 1 .
2.
5.2
(1) 0, 2 x.
.
(2) 1, 2x + 1.
.
5.3
(1) 1.
(2) 6x5.
.
(3) 10ex.
.
(4) 5 .
x .
(5) 1 sinx +cos xlnx.
x .
(6) ex cos x+ex sin x .
(cosx)2 .
(7)
10e(10x+1).
.


================================================================================
PAGE 461
================================================================================

Solutions 451
(8) 8e2x.
.
(9) 5e3x cos5x + 3e3xsin5x.
.
(10) 1 − 2x ln (8x) .
xe(x2) e (x2) .
5.4
(1) 6x lnx + 5x.
.
(2) a2y.
.
(3) a2y.
.
5.5
(1) f (x ) has the maximum value of 31 atx =−2and the minimum value of −77
. . .
atx =4.
.
(2) f (x ) has the maximum value of 1 at x = 0 and the minimum value of 3 at
. . 4.
x = 1.
2.
(3) f (x ) has the maximum value of 32 atx = 2and the minimum value of −32
. . .
atx =−2.
.
(4) f (x ) has the minimum value of 3 atx =−1.
. 2.
(5) f (x ) has the maximum value of 11 atx =1and the minimum value of −5at
. . .
x =−1andx =3, respectively.
. .
(6) f (x ) has the maximum value ofeπ/2 ≈4.81atx = π and the minimum value
. . 2.
of −e 3 2 π ≈−111.32 . atx = 3 2 π . .
(7) f (x ) has the maximum value ofe −1atx =1.
. . .
(8) f (x ) has the maximum value of4e −2 ≈0.54atx =2and the minimum value
. . .
of 0 atx =0.
.
5.6
(1) e2x + C.
2 .
(2) − 2x4+ C.
.
(3) 6ln|x|+C .
.
(4) − 3e −x + C.
.
(5) − 5cos x + C.
.
5.7
(1) − 4cos x + ex +C.
√ .
(2) π2 + 3.
18 2 .
(3) ex−e−x + C.
2 .
5.8
3
(1) Substitutingu=2−x2 gives − (2−x2)2 +C.
. 3 .
(2) Substitutingu=x−1gives − 1 +C.
. x−1 .
(3) Substitutingu=4x gives 1.
. 4.
(4) Substitutingu=1+x3 gives − 1 +C.
. 3(1+x3) .


================================================================================
PAGE 462
================================================================================

452 Solutions
(5) Substitutingu=1+x gives 1.
. 8.
(6) Substitutingu=sinx gives 3.
. 2.
(7) Substitutingu=1+x2 gives ln|1+x2| +C.
. 2 .
(8) Substitutingu=x2 gives 1ex2 +C.
. 2 .
5.9
(1) (x − 1)ex +C.
.
(2) − x2 cos x + 2x sinx +2cosx+C.
.
(3) x4 (ln x − 1 )+C.
4 4 .
(4) 2ln2− 1.
.
(5) − x cos4x + sin4x +C.
4 16 .
(6) x2 ln5x − x2 +C.
2 .
(7) x−sin x cos x +C.
2 .
Problems of Chap. 6
6.1
(1) ∂f = 3x2y + 10xy2+2y3,
∂x .
∂f =x3+10x2y+6xy2,
∂y(cid:14) .
(cid:14)
∂f(cid:14) =1,
∂x x=3 .
(cid:14)y=−1
(cid:14)
∂f(cid:14) =−45.
∂y x=3 .
y=−1
(2) ∂f = 2x sin y−3cosy,
∂x .
∂f =x2cosy+3xsiny,
∂y(cid:14) .
(cid:14)
∂ ∂ f x (cid:14) x=1 =2 . ,
(cid:14)y=π
(cid:14) 2
∂
∂
f
y
(cid:14)
x=1
=3.
.
y=π
2
(3) ∂f = 2y3e2x + 3y2e3x +4ye4x,
∂x .
∂f =3y2e2x +2ye3x +e4x,
∂y(cid:14) .
(cid:14)
∂f(cid:14) =36,
∂x x=0 .
(cid:14)y=2
(cid:14)
∂f(cid:14) =17.
∂y x=0 .
y=2
6.2
The maximum error in the area is0.075cm2, and this represents0.3%error.
. .


================================================================================
PAGE 463
================================================================================

Solutions 453
6.3
(1) ∂f = 12x3y + 18x2y2− 8xy3+y4,
∂x .
∂2f =4y(9x2+9xy−2y2),
∂x2 .
∂f =3x4+12x3y−12x2y2+4xy3,
∂y .
∂2f =12x(x2−2xy+y2),
∂y2 .
∂2f =12x3+36x2y−24xy2+4y3.
∂x∂y .
(2) ∂f = 2x sin y +18x2cosy,
∂x .
∂2f = 2siny+36xcosy,
∂x2 .
∂f =x2cosy−6x3siny,
∂y .
∂2f =−x 2(siny+6xcosy),
∂y2 .
∂2f =2xcosy−18x2siny.
∂x∂y .
(3) ∂f = 2xe(x2+y 2),
∂x .
∂2f =2(2x2+1)e(x2+y2),
∂x2 .
∂f =2ye(x2+y2),
∂y .
∂2f =2(2y2+1)e(x2+y2),
∂y2 .
∂2f =4xye(x2+y2).
∂x∂y .
(4) ∂f = 3x2 ln (x3+ y3)+3x2,
∂x .
∂2f =6x(ln(y3+x3)+1)+ 9x4 ,
∂x2 y3+x3.
∂f =3y2ln(x3+y3)+3y2,
∂y .
∂2f =6y(ln(y3+x3)+1)+ 9y4 ,
∂y2 y3+x3.
∂2f = 9x2y2 .
∂x∂y x3+y3.
(5) ∂f = 2x − 3 y,
∂x x2.
∂2f =2+ 6y,
∂x2 x3.
∂f = 3,
∂y x.
∂2f =0,
∂y2 .
∂2f =− 3 .
∂x∂y x2.
6.4
(1) ∂z = esin t ln (cos t)cos t − esin tsint.
∂t cost .
(2) ∂z = 4s(s2+ t2)sin (st2)+ t2((s2+ t2)2+1)cos(st2),
∂s .
∂z =2st((s2+t2)2+1)cos(st2)+4t(s2+t2)sin(st2).
∂t .


================================================================================
PAGE 464
================================================================================

454 Solutions
6.5
(1) The gradient vector at the point(1,1)is[3,3],
. .
The gradient vector at the point(2,1)is[12,24],
. .
The gradient vector at the point(1,2)is[24,12].
. .
(2) The gradient vector at the point(0,1)is[0,2],
. .
The gradient vector at the point(1,0)is[0,1],
. .
The gradient vector at the point(π,π)is[0.674,0],
2 2 . .
The gradient vector at the point(π,π)is [0.675, 1.547].
4 4 .
6.6
(cid:6) (cid:7)
2x sin y x2 cosy
(1) J = .
y3 cos x 3y 2sinx .
(cid:6) (cid:7)
cos θ −r sin θ
(2) J = .
.
sin coθsr θ
⎡ ⎤
sinθ cos φc ro s φ cos θ −r sin φ sin θ
(3) J = ⎣ sinφ sin sθi rn φcos θ srinθcosφ ⎦ . .
cosθ −rsinθ 0
6.7
(cid:6) (cid:7)
y2ex + 2ey 2xey + 2yex
(1) H= .
2xey +2yex x2ey +2ex .
⎡ ⎤
6xy2z 6x2yz −2z3 3x2y2− 6yz2
(2) H= ⎣ 6x2yz − 2z3 2x3z 2x3y − 6xz2 ⎦ .
.
3x2y2− 6yz2 2x3y − 6xz2 −12xyz
6.8
(2).
6.9
(1) The critical points are at (−1,−2), (−1,1), (2,−2), (2,1). The local
.
maximum value at (−1,−2) is 27. The local minimum value at (2,1) is −27.
. . .
(2) The critical points are at(1,−1), (1,−1). The local maximum value at (1, −1)
3 3 . .
is 6.
(3) The critical point is at(−1,−1).The local minimum value at this point is 1.
3 3 . 3.
(4) The critical points are at(0,−3), (0,2), (3,−3), (3,2).The local maximum
.
value at (0,−3) is 85. The local minimum value at (3,2) is −67.
. .
6.10
(1)
⎧
⎪⎪⎨x =−1
2
.⎪⎪⎩ y = 1
2
λ=1.


================================================================================
PAGE 465
================================================================================

Solutions 455
The relative extreme of the function is 1.5 obtained at (−1,1) subject to the
. 2 2 .
given constraint.
(2)
⎧
⎪⎪⎨x = 1
2
. ⎪⎪⎩ y = 1 2
λ =−3.
4
The relative extreme of the function is 0.25 obtained at (1,1) subject to the
. 2 2 .
given constraint.
6.11
(2).
6.12
(1) 1,
(2) 451 ,
3.
(3) 2.
6.13
(1) 1 ,
8.
(2) 5 ,
6.
(3) 74.4,
.
(4) 16 .
315 .
6.14
(1) π(e4− 1) ≈168.384,
.
(2) 10,
(3) π(1− cos1) ≈1. 44.
.
Problems of Chap. 7
7.4 Possible answers:
(1) With and without normalisation:
λ =1.25,λ =0.75.
. 1 2
(cid:12) (cid:13) (cid:12) (cid:13)
√1 −√1
u = 2 ,u = 2 .
. 1 √1 2 √1
2 2


================================================================================
PAGE 466
================================================================================

456 Solutions
(2) Without normalisation:
λ =2,λ =0.4.
. 1 2
(cid:12) (cid:13) (cid:12) (cid:13)
√1 −√1
u = 2 ,u = 2 .
. 1 √1 2 √1
2 2
With normalisation:
5 1
λ = ,λ = .
. 1 2
3 3
(cid:12) (cid:13) (cid:12) (cid:13)
√1 −√1
u = 2 ,u = 2 .
. 1 √1 2 √1
2 2
(3) Without normalisation:
λ =1.8,λ =1.6,λ =1.4.
. 1 2 3
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
√1 √1
0
⎢ 2 ⎥ ⎢ 2⎥
. u 1 =⎣ 0 ⎦,u 2 =⎣ 1 ⎦ ,u 3 =⎣ 0 ⎦.
−√1 0 √1
2 2
With normalisation:
λ =1.125,λ =1,λ =0.875.
. 1 2 3
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
√1 √1
0
⎢ 2 ⎥ ⎢ 2⎥
. u 1 =⎣ 0 ⎦,u 2 =⎣ 1 ⎦ ,u 3 =⎣ 0 ⎦.
−√1 0 √1
2 2
Problems of Chap. 8
8.1
(1) aˆ =−1.0 andaˆ =2.0.
0 . 1 .
(2) aˆ = 2.0 andaˆ =0.5.
0 . 1 .
(3) aˆ = 5.0 andaˆ =−1.25.
0 . 1 .
(4) aˆ = 0.7 5andaˆ =0.75.
0 . 1 .


================================================================================
PAGE 467
================================================================================

Solutions 457
8.3
After the first iteration, a = 0.97 and a = 0.89. The total error is 1.4428,
0 . 1 . .
and the total partial derivatives are ∂error = 1.92and ∂error = 6.88, keeping two
∂a0 . ∂a1 .
decimal places, respectively.
8.4
(1) The sum of the residuals is 0. The sum of the target values is 7.5.T he sumo f
.
the estimatesis7.5.R2 ≈0.89.
. .
(2) The sum of the residuals is 0. The sum of the target values is 9. The sum of the
estimates is 9.R2 =0.75.
.
Problems of Chap. 9
9.1
Initially:y =0.5,y =0.25andE =0.0625.
1 . 2 . .
After the first iteration:w =0.475,w =0.5,w =0.275, andw =0.25.
11 . 12 . 21 . 22 .
Now:y =0.475,y =0.275andE =0.051.
1 . 2 . .
9.2
Initially:y =0.622,y =0.562andE =0.0711.
1 . 2 . .
After the first iteration:w =0.491,w =0.5,w =0.248, andw =0.25.
11 . 12 . 21 . 22 .
Now:y =0.620,y =0.562andE =0.0703.
1 . 2 . .
9.3
Initially:y =0.188,y =0.375andE =0.0097.
1 . 2 . .
After the first iteration:
w (1) =0.508,w (1) =0.50,w (1) =0.258, andw (1) =0.25.
11 . 12 . 21 . 22 .
w (2) =0.253,w (2) =0.252,w (2) =0.506, andw (2) =0.503.
11 . 12 . 21 . 22 .
Now:y =0.194andy =0.387andE =0.0080.
1 . 2 . .
Problems of Chap. 10
10.1
(a) The number of different “words” from Wales is5!=120.
.
(b) The number of different “words” from Scotland is8!=40320.
.
10.2 The number of different photos they can take is4!=24.
.
10.3
The number of passwords that can be made is 10! =5040.
6! .


================================================================================
PAGE 468
================================================================================

458 Solutions
10.4
The number of five digits that can be made isP(6,5)− P(5,4)=600.
.
10.5
The number of fruit salads that can be made is 7! =35.
4!3! .
10.6
The number of triangles that can be made is 12! =220.
3!9! .
10.7
The probability of getting two heads is0.375.
.
10.8
The probability of getting four different numbers is
6×5×4×3
or0.2778.
64 . .
The probability of getting five different numbers is
6×5×4×3×2
or0.0926.
65 . .
The probability of getting six different numbers is
6×5×4×3×2×1
or0.0154.
66 . .
10.9
The probability that all six digits are different is
10×9×8×7×6×5
or0.1512.
106 . .
10.10
See Table S.1.
10.11
See Table S.2.
10.12
a = 3.
4.
F (x)= 3(x2− x3 )for0≤ x <2.
X 4 3 . .
Belowx =0it is zero, abovex =2it is 1.
. .
P(X≤1)= 1;P(X≤ 1)= 5 ;P(X > 1)=1− 5 = 27.
2. 2 32. 2 32 32.
10.13 √ √
m= 80.5andn= 80.95.
. .
Table S.1 Answer to Exercise 10.10: the probability distribution of X, whereX is the total value
after throwing two fair di ce
x 1 2 3 4 5 6 7 8 9 10 11 12 13
fX(xk)= (cid:19) P(X=xk). 0 3 1 6. 3 2 6. 3 3 6. 3 4 6. 3 5 6. 3 6 6. 3 5 6. 3 4 6. 3 3 6. 3 2 6. 3 1 6. 0
FX(x)= xk ≤x fX(xk). 0 3 1 6. 3 3 6. 3 6 6. 1 3 0 6. 1 3 5 6. 2 3 1 6. 2 3 6 6. 3 3 0 6. 3 3 3 6. 3 3 5 6. 1 1
Table S.2 Answer to Exercise 10.11: the probability distribution of X, whereX is the number of
heads when tossing four fa ircoins
x −1. 0 1 2 3 4 5
fX(xk)= (cid:19) P(X=xk). 0 1 1 6. 1 4 6. 1 6 6. 1 4 6. 1 1 6. 0
FX(x)= xk ≤x fX(xk). 0 1 1 6. 1 5 6. 1 1 1 6. 1 1 5 6. 1 1


================================================================================
PAGE 469
================================================================================

Solutions 459
10.14
(1) E(Z) = 3 ,
8 .
(2) E(−Z +2) = 13 = 1.625,
8 .
(3) E(Z2) = 49 .
48 .
10.15
(1) E(Z + X + X ) =3.875,
1 3 .
(2) E(ZX ) =1.125 ,
4 .
(3) E(3Z −5) =−3.875,
.
(4) E(X Z) =0.625 ,
1 .
(5) E(X + X + Z) = 4.875.
2 4 .
10.16
Let X be the random variable that represents the absolute difference between the
twonumbers.E(X)= 5.
3 .
10.17
(1) E(X) = 8.
9.
(2) E(X2) = 4.
5.
10.18
(1) E(X2) = 1 ;E(X4)= 1 ,
12. 80 .
(2) V ar(2X 2) = 1 ,
45 .
(3) V ar(2X 2+ 5) = 1 .
45.
10.19
μ =4.5.
X .
10.20
The probability that:
(1) exactly seven students out of 10 pass the module is about0.13.
.
(2) exactly eight students out of 10 pass the module is about0.28.
.
(3) exactly nine students out of 10 pass the module is about0.35.
.
(4) exactly ten students out of 10 pass the module is about0.20.
.
10.21
(1) The probability that the doctor will see five patients is about0.16.
.
(2) The probability that the doctor will see six patients is about0.16.
.
(3) The probability that the doctor will see seven patients is about0.14.
.
(4) The probability that the doctor will see eight patients is about0.10.
.
10.22
(1) The probability that the number of calls is exactly eight in one minute is about
0.0298.
.


================================================================================
PAGE 470
================================================================================

460 Solutions
(2) The probability that the number of calls is more than five per minute is about
0.215.
.
10.23
The probability that the student waits less than three minutes is0.36.
.
10.24
About16%.
.
10.25
About 160 students.
10.26
(1) P(X <8 ) =0.9773 ,P(X <0.5)=0.3085,
. .
andP(0.5 <X <8)=0.6688;
.
(2) P(−1 < X <5 )=68.27% ;
.
(3) C =μ = 2.
.
10.27
σ =3.125.
.
Problems of Chap. 11
11.1
(2) 0.3264.
(3) 0.1841
11.2 .
0.0228.
11.3
See Table S.3
11.4
See Table S.4.
11.5
See Table S.5.
Table S.3 The answer to X=0 X=1 P(Y=yi).
Exercise 11.3
Y=0 2 9 6. 2 9 6. 1 9 3.
Y=1 1 2 3. 1 2 3. 1 4 3.
P(X=xi). 1 2 3 6. 1 2 3 6. –


================================================================================
PAGE 471
================================================================================

Solutions 461
Table S.4 The answer to X=0 X=1 P(Y=yi).
Exercise 11.4
Y=0 1 1 0. 1 3 0. 2 5.
Y=1 1 3 0. 1 3 0. 3 5.
P(X=xi). 2 5. 3 5. –
Table S.5 The answer to X=0 X=1 X=2 P(Y=yi).
Exercise 11.5
Y=0 5 2 0. 5 3 0. 1 1 0. 1 5.
Y=1 5 3 0. 10 9 0. 2 3 0. 1 3 0.
Y=2 1 1 0. 2 3 0. 1 4. 1 2.
P(X=xi). 1 5. 1 3 0. 1 2. –
11.6
(1) A = 1 ,
64.
(2)
(cid:20)
1 x2y+ 1 xy2, 0 <x <4, 0 <y <4
F (x,y)= 128 128
. XY
0, otherwise.
(3) P(0≤ X <2 , 0≤ Y< 2)= 1,
8.
(4) P(X+ Y< 4) = 1.
3.
11.7
f (x)=e −x.
X .
11.8
(1) A = 1 .
2 .
(2) F (x, y) = 1 sin x sin y + 1 cos x cos y − 1 cos x− 1cosy+ 1 for 0 <x <
XY 2 2 2 2 2.
π, 0 <y <π; otherwise,F (x,y)=0.
2 2. XY √ .
(3) P(0≤ X < π , 0≤ Y< π )=1− 2.
4 4 2 .
(4) f (x) = 1 cos x + 1sinx.
X 2 2 .
(5) f (y) = 1 cos y + 1siny.
Y 2 2 .
11.9
E(X)= 1;E(Y)= 1;E(X,Y)= 1.
2. 3. 6.
Cov(X,Y)=0.
.
11.10
E(X)= 7;E(Y)= 7;E(X,Y)= 16.
3. 3. 3 .
Cov(X,Y) =−1.
9 .


================================================================================
PAGE 472
================================================================================

462 Solutions
11.11 (cid:21) (cid:22) (cid:21) (cid:22) (cid:21) (cid:22) (cid:21) (cid:22)
2 1 0 3
P = 6! 2 3 4 1 =0.00072.
2!1!0!3! 10 10 10 10 .
11.12
The probability that a given woman will have an episode of depression by the
age of 65 in America is33.34%.
.
The probability that a given man will have an episode of depression by the age
of 65 in America is20%.
.
11.13
P(science |female) = 44.4%.
.
P(female| science) = 46.15%.
.
P(science |male) = 60.87%.
.
11.14
P(two heads-up |first comes heads up) = 50%.
.
11.15
P(son |daughter) = 2.
. 3.
11.16
f Y|X (y|x)= x 1, y≤ x <2,0 <x <2. .
11.17
A=1 . ,f X|Y (x|y)=sinx, . f Y|X (y|x)=cosy . .
11.18
E(Y|x)= 3;E(Y2|x)= 3.
4. 5.
Var(Y|x)= 3 .
80 .
11.19
A and B are independent, A and C are not independent, and B and C are not
independent.
11.20
B ={1,2,3,4},B ={1,2},B ={1,4},B ={2,3}, andB ={3,4}.
. . . . .
11.21
P = 71 .
120 .
If all balls in one bag,P = 35.
57 .
11.22
P = 2.
5 .
11.23
57 ≈20.6%.
277 .
11.24
135 ≈48.9%.
276 .


================================================================================
PAGE 473
================================================================================

Solutions 463
11.25
285 ≈12.51%.
2279 .
11.26
29 ≈31.52%.
92 .
Problems of Chap. 12
12.1
See Table S.6.
12.2
(1) 2.82
(2) 3.76
(3) 2.47
(4) 0.19
(5) 3.16
(6) 0.3
12.3
(1) cov(x , x ) =−0. 125 andr(x ,x ) =−0.20.
1 2 . 1 2 .
(2) cov(x , x ) =−0.01125andr(x ,x ) =−0.19.
1 3 . 1 3 .
(3) cov(x , x ) =−0. 225 andr(x ,x ) =−0.24.
2 3 . 2 3 .
(4) cov(x , x ) = 0.6, cov(x ,x ) = 0.055, cov(x ,x ) = 0.9375, and
1 2 . 1 3 . 2 3 .
r(x ,x )=0.98,r(x ,x )=0.95,r(x ,x )=0.99.
1 2 . 1 3 . 2 3 .
12.4
Class A and Class B have the same relative dispersion:20%.
.
12.5
For the first dataset:
1. Mode = 87.
.
2. Median =86.5.
.
3. IQR= 13.
.
4. 50 and 110 are outliers.
Table S.6 Answers to
Question number Mean Median Mode
Exercise 12.1
(1) 6.25 6 6
(2) 2 6 3 . 4 0a nd 5
(3) 5.1 5.5 5.5
(4) 5.9 5.9 5.8 and 6.0
(5) 8 9 10


================================================================================
PAGE 474
================================================================================

464 Solutions
For the second dataset:
1. Mode = 55.
.
2. Median =54.
.
3. IQR= 10.
.
4. 30, 72, and 80 are outliers.
12.6
23.58%.
.
12.7
49.72%.
.
12.8
30.85%.
.
12.9
92.36%.
.
12.10
(1) 58.32%.
.
(2) 85.08%.
.
12.11
(1) t = 1.753.
.
(2) t = 2.602.
.
12.12
(1) χ2 =21.03.
.
(2) χ2 =26.22.
.
12.13
The estimated μis1486.4; the estimated σ is98.53.
. . . .
12.14
The95%confidence interval is[161.432,164.568].
. .
The99%confidence interval is[160.939,165.061].
. .
12.15
The95%confidence interval is[0.522,0.658].
. .
The99%confidence interval is[0.500,0.680].
. .
12.16
The90%confidence interval of σ is[5.196,8.425].
. . .
The95%confidence interval of σ is[4.997,8.903].
. . .
12.17
The95%confidence interval is[0.809,0.831].
. .


================================================================================
PAGE 475
================================================================================

Solutions 465
12.18
The90%confidence interval of σ is[0.056,0.096].
. . .
12.19
The99%confidence interval is[22.7,23.1].
. .
12.20
t =−3.2; reject H at0.05significance level.
. 0. .
12.21
t =−1.69; do not reject H at0.05significance level.
. 0. .
12.22
t =−1.535; do not reject H at 0.1significance level.
. 0. .
12.23
t =0.83; do not reject H at0.01significance level.
. 0. .
12.24
t =2.48; reject H at0.05significance level.
. 0. .
12.25
t =1.88; do not reject H at0.01significance level.
. 0. .
12.26
See Table S.7.
χ2 =84.75; reject H at0.05significance level.
. 0. .
12.27
χ2 =7.53; do not reject H at0.05significance level.
. 0. .
12.28
χ2 =9.62; do not reject H at0.05significance level.
. 0. .
12.29
χ2 =3.58; do not reject H at0.05significance level.
. 0. .
12.30
χ2 =3.91; Reject H at0.05signif
. 0. .
12.31
χ2 =3.8; do not reject H at0.05significance level.
. 0. .
12.32
χ2 =2.29; do not reject H at0.05significance level.
. 0. .
Table S.7 The distribution
zi. 1 2 3 4 5 6
of Z of 360 throw s
Observed frequency 90 145 35 50 15 25
Expected frequency 110 90 70 50 30 10


================================================================================
PAGE 476
================================================================================

466 Solutions
Problems of Chap. 13
13.1
The maximum likelihood estimate of λisλ ˆ = (cid:19) n .
. n i=1 xi .
(1) p(x = 1|λ = 1)≈0.37,p(x =2|λ=1)≈0.14,
. .
p(x =1,x=2|λ=1)=0.0518.
.
(2) p(x = 1|λ = 2)≈0.27,p(x =2|λ=2)≈0.037,
. .
p(x =1,x=2|λ=2)=0.01.
.
(3) λ ˆ = 2 , p(x = 1|λ = 2)≈0.34,p(x =2|λ= 2)≈0.18,
3 3 . 3 .
p(x =1,x=2|λ= 2)=0.0612.
3 .
13.2
(1) E(y|x =1) =−1 .
.
(2) std(y|x =1) = 0. 1.
.
13.3
For the first dataset (the left one in Table 13.4), the fitted linear regression is
y˜ =9.14−1.93x+(cid:9),
.
where(cid:9) ∼N(μ=0,σ˜2 =0.64).
.
For the second dataset (the right one in Table 13.4), the fitted linear regression is
y˜ =9.71−2.14x+(cid:9),
.
where(cid:9) ∼N(μ=0,σ˜2 =4.57).
.
13.4
The fitted linear regression is
y˜ =0.1+0.48x+(cid:9),
.
where(cid:9) ∼N(μ=0,σ˜2 =0.004).
.
13.5
The fitted linear regression is
y˜ =1.86−1.03x+(cid:9),
.
where(cid:9) ∼N(μ=0,σ˜2 =0.16).
.
The95%confidence interval for a is[−1.92,5.64]and for a is[−3.89,1.83].
. 0. . 1. .
95%confidence interval for σ2is[0.032,160].
. . .
y is0.315.I ts95%confidence interval is[−3.18,3.81].
new. . . .


================================================================================
PAGE 477
================================================================================

Solutions 467
13.6
The95%confidence interval for a is[−0.23,0.43]and for a is[0.36,0.60].
. 0. . 1. .
The95%confidence interval for σ2is[0.001,0.16].
. . .
y is 1.3. Its95%confidence interval is[1.16,1.44].
new. . . .
13.7
anew =0.456andanew =0.657.
0 . 1 .
P(y =1|x )≈0.37,
. 1 1
P(y =1|x )≈0.45,
. 2 2
P(y =1|x )≈0.61,
. 3 3
P(y =1|x )≈0.66.
. 4 4
13.8
anew =0.293 andanew =0.309.
0 . 1 .
P(y =1|x )≈0.354,
. 1 1
P(y =1|x )≈0.427,
. 2 2
P(y =1|x )≈0.504,
. 3 3
P(y =1|x )≈0.653.
. 4 4
Problems of Chap. 14
14.1 :
For X1:
.
Eigenvalues are0.873and0.127.
. .
Eigenvectors are[0.526,0.850]and[−0.850,0.526].
. .
For X2:
.
Eigenvalues are0.397and0.103.
. .
Eigenvectors are[0.615,0.788]and[−0.788,0.615].
. .
14.2 :
f2 and f5 have the largest correlation.
Average absolute correlation for f 2 is0.473
.
Average absolute correlation for f 5 is0.373
.
Therefore, remove f2.


================================================================================
PAGE 478
================================================================================

468 Solutions
14.3 :
For the first part:
984
Accuracyrate= =0.984.
.
1000
2
Recall= =0.2,
.
10
2
Precision= =0.2,
.
10
2×0.2×0.2
F-score= =0.2,
. 0.2+0.2
8
FPrate= =0.008.
.
990
982
True-negativerate= =0.992.
. 982+8
For the second part:
992
Accuracyrate= =0.992.
.
1000
10
Recall= =1,
.
10
10
Precision= ≈0.556≈0.56,
.
18
2×1×0.556
F-score= ≈0.71,
. 1+0.556
8
FPrate= =0.008.
.
990
982
True-negativerate= =0.992.
. 982+8
14.4 :
975
Accuracyrate= =0.975.
.
1000


================================================================================
PAGE 479
================================================================================

Solutions 469
500
Recall= ≈0.980≈0.98,
.
510
500
Precision= ≈0.971≈0.97,
.
515
2×0.980×0.971
F-score= ≈0.98,
. 0.980+0.971
15
FPrate= ≈0.031.
.
490
475
True-negativerate= ≈0.969.
.
490
14.5 :
First part:
875
Accuracyrate= =0.875.
.
1000
20
Recall= =0.2,
.
100
20
Precision= ≈0.308≈0.31,
.
65
2×0.2×0.308
F-score= ≈0.24,
. 0.2+0.308
45
FPrate= =0.05.
.
900
855
True-negativerate= =0.95.
.
900
Second part:
1000
Accuracyrate= =1.
.
1000
100
Recall= =1,
.
100
100
Precision= =1,
.
100


================================================================================
PAGE 480
================================================================================

470 Solutions
2×1×1
F-score= =1,
. 1+1
0
FPrate= =0.
.
900
900
True-negativerate= =1.
.
900
14.6 :
(a) λ = 0:
.
(cid:6) (cid:7)
4
a˜ =
. −0.7
(cid:6) (cid:7)
0
B(a˜)= .
.
0
(cid:6) (cid:7)
1.5 −0.5
Var(a˜)=σ2 .
. −0. 5 .02
(b) λ = 10:
.
(cid:6) (cid:7)
2.83
a˜ =
. R −0.23
(cid:6) (cid:7)
0 .167
B(a˜ )= a.
. R 0−0.67
Ifa=a˜, then
.
(cid:6) (cid:7)
−1.2
B(a˜ )= .
. R
0.5
(cid:6) (cid:7)
0.39 −0.055
Var(a˜ )=σ2 .
. R −0.055 0.022


================================================================================
PAGE 481
================================================================================

References
1. Statista Research Department, Amount of data created, consumed, and stored 2010–2023, with
forecasts to 2028, accessed Nov 21, 2024. Available at: https://www.statista.com/statistics/
871513/worldwide-data-created/
2. Ozdemir, S.: Principles of Data Science, Packt, Birmingham-Mumbai (2016).
3. Moss, G.P., Shah, A.J., Adams, R.G., Davey, N., Wilkinson, S.C., Pugh, W.J., Sun, Y.: The
application of discriminant analysis and Machine Learning methods as tools to identify and
classify compounds with potential as transdermal enhancers. European Journal of Pharma-
ceutical Sciences, 45, 116–127 (2012).
4. Fisher, R.A.: The use of multiple measurements in taxonomic problems. Annual Eugenics,
7, Part II, 179–188 (1936); also in Contributions to Mathematical Statistics John Wiley, NY
(1950).
5. Bishop, C.M.: Neural Networks for Pattern Recognition, Clarendon Press, Oxford (1995).
6. Courant, R., John, F.: Introduction to Calculus and Analysis, Volume One, Springer (1998).
7. Kenneth, H.R.,:Discrete Mathematics and its Applications, 7th edition, McGraw-Hill (2012)
8. Gilbert, S.: Linear Algebra and Its Applications, Edition Fourth, Brooks Cole (2006).
9. Manly, B.F.J., Navarro Alberto, J.A.,:Multivariate Statistical Methods: A primer, Edi-
tion Fourth, Chapman and Hall/CRC (2017).
10. Ross, S.: A First Course in PROBABILITY, 8th Edition, Prentice Hall (2010).
11. Miklavcic, S.J.:An Illustrative Guide to Multivariable and Vector Calculus, Springer (2020).
12. Spiegelhalter, D.:The Art of Statistics: Learning from Data, Pelican (2020).
13. Lancaster, H.O.:The Chi-squared Distribution, Wiley (1969).
14. Cox, D.R., Hinkley, D.V.:Theoretical Statistics, Chapman And Hall, London (1979).
15. Wooldridge,J.M.:Introductory Econometrics-A Modern Approach, Cengage (2018).
16. Ribeiro, R.P., Moniz, N.: Imbalanced regression and extreme value prediction. In Machine
Learning. (2020), 109(9): 1803–1835.
17. Haibo H., Garcia, E.A.: Learning from Imbalanced Data. In IEEE Transactions on Knowledge
and Data Engineering. (2009), 21(9): 1263–1284.
18. Chawla, N.V., Herrera, F., Garcia, S., Fernandez, A.: SMOTE for Learning from Imbalanced
Data: Progress and Challenges, Marking the 15-year Anniversary. In Journal of Artificial
Intelligence Research. (2018), 61: 863–905.
19. Tlamelo E., Thabiso M., Dimane M., Thabo S., Banyatsang M., Oteng, T.: A survey on missing
data in machine learning. In Journal of Big Data. (2021), 8:140.
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 471
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4


================================================================================
PAGE 482
================================================================================

472 References
20. Kuhn, M., Johnson, K.: Applied Predictive Modeling, Springer (2013).
21. Domingos, P.: A Unified Bias-Variance Decomposition and its Applications In Proceedings of
the Seventeenth International Conference on Machine Learning, (June 2000), 231–238.
22. Hoerl, A., Kennard, R.: Ridge Regression: Biased Estimation for Nonorthogonal Problems. In
Technometrics, (1970), 12, 55–67.


================================================================================
PAGE 483
================================================================================

Index
A Combination, 254
Activation function, 228 Composite function, 45
Antiderivative, 143 Conditional mean, 324
Arithmetic mean, 336 Conditional probability, 319, 320, 323
Associative property of multiplication, 60 Conditional variance, 324
Confidence intervals for means, 356
Confidence intervals for proportions, 360
B Confidence Intervals for χ2 ., 361
Back-propagation, 230, 239 Confusion matrix, 424
Basis, 87 Continuity, 127
Bayes’ theorem, 330 Continuous random variables, 263
Bernoulli distribution, 280 Continuous uniform distribution, 286
Bias, 428 Covariance, 100, 310, 341
Bias and variance of ridge regression Covariance matrix, 101
coefficients, 436 Cumulative distribution function, 299, 305
Bias in neural network, 248
Binary relation, 33
Binomial coefficients, 252 D
Binomial distribution, 281 Data normalisation, 194, 224
Bounded function, 41 Data pre-processing, 415
Boxplot, 343 Data Science, 1
Data visualisation, 10
Definite integral, 144
C Degrees of freedom, 349
Cardinality, 24 Delta rule, 247
Cartesian Product, 32 Dependent variable, 36, 207
Central limit theorem, 296 Derivative, 129
Chain rule, 132 Determinant, 68
Characteristic polynomial, 92 Diagonalisation, 97
Chi-square (χ 2 .) Distribution, 352 Diagonal matrix, 68
Chi-square test, 369 Diagonal of a matrix, 67
Classification, 3 Differential, 129
Cochran’s theorem, 399 Differentiation of composite Function, 159
Coefficient of determination, 225, 423 Discrete random variable, 260
Coefficient of variation, 341 Discrete uniform distribution, 279
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025 473
Y. Sun, R. Adams, A Mathematical Introduction to Data Science,
https://doi.org/10.1007/978-981-96-5639-4


================================================================================
PAGE 484
================================================================================

474 Index
Distributive properties, 61 Integration of double integrals using Polar
Dot product, 50, 53 coordinates, 181
Double integral, 176 Interquartile range, 342
Interval, 24
Interval estimation, 355
E Interval level, 20
Early stopping, 441 Inverse function, 43
Eigendecomposition, 91 Inverse matrix, 71
Eigenvalue, 92
Eigenvector, 92
Even function, 42 J
Expected random vector of random variables, Jacobian matrix, 163, 182
395 Joint probability distribution, 299, 305
Expected value, 268 Joint probability mass function, 299, 305
Exponential function, 38
L
F Law of large numbers, 294
Factorial, 252 Law of total probability, 328
Feature selection, 420 Least-squares estimation, 208
Feed-forward propagation, 237 Left-singular vectors, 109
The first partial derivative, 155 Likelihood function, 381
FP rate, 425 Limit, 121
F-score, 425 Linear combination, 80
Function, 34, 35 Linear function, 37
Linearly dependent, 83
Linearly independent, 83
G Linear regression, 207, 216
Gaussian distribution, 288 Linear regression with maximum likelihood
Generalisation error, 429 estimation, 384
Gradient, 161 Linear transformation, 62
Gradient descent algorithm, 173 Local maxima, 139, 166
Graph of a Function, 36 Local minima, 139, 166
Logarithmic function, 39
Logic, 31
H Logistic regression, 406
Hat matrix, 399 Logistic sigmoid activation function, 232
Hessian matrix, 164 Lower quartile, 342
Hyperparameter, 422
M
I Marginal probability distribution, 300, 308
Idempotent, 398 Matrix, 55
Identity matrix, 71 Matrix addition, 56
Image compression ratio, 118 Matrix decomposition, 91
Imbalanced dataset, 416 Matrix multiplication, 58
Inconsistent data, 416 Matrix transposition, 72
Indefinite integral, 144 Matrix transposition properties, 73
Independent event, 327 Maximum likelihood estimation, 379
Independent variable, 36, 207 Mean, 268
Infinite sets, 24 Mean squared errors, 423
Integral, 142 Median, 336
Integrationb yp arts ,152 Method of Lagrange multipliers, 169, 191
Integration by substitution, 148 Missing data, 417


================================================================================
PAGE 485
================================================================================

Index 475
Mode, 337 Recall, 425
Monotonic function, 41 Rectified linear activation unit, 250
Multinomial distribution, 314 Regression, 3
Multivariate normal distribution, 316 Regressor, 207
Mutual exclusivity, 326 Regularisation term, 434
Relation, 33
Repeated data, 417
N Representations of simultaneous equations,
Neural network, 14, 227 64
Nominal level, 19 Residual, 224
Norm, 51 Ridge regression, 434
Normal distribution, 288 Right-singular vectors, 109
Normal equation, 218 Root mean squared errors, 423
O S
Odd function, 42 Sample mean, 99
Ordinal level, 20 Sampling distribution of estimators a˜ ., 399
Organised data, 17 Sampling distribution of the linear regression
Orthogonal matrix, 76 estimators, 394
Outlier, 343 Sampling distribution of variance σ˜2 ., 402
Overfitting, 433 Sampling distributions of means, 344
Sampling distributions of proportions, 347
Scalar multiplication, 57
P Scalar-vector multiplication, 49
Pearson correlation coefficient, 100, 341 Scatter plots of residual against predictions,
Period of function, 43 423
Permutation, 253 The second derivative, 137
Point estimation, 354 The second partial derivative, 158
Poisson distribution, 284 Sensitivity, 425
Polynomial function, 37 Set, 23
Power set, 25 Set complement, 29
Precision, 425 Set intersection, 27
Predictor, 207 Set membership, 24
Principal component, 101 Set subtraction, 28
Principal Component Analysis, 10, 99, 185 Sets written in comprehension, 30
Probability, 256 Set union, 26
Probability multiplication rule, 327 Sigmoid, 41
Procedure for conducting a hypothesis test, 365 Sigmoid function, 134, 407
The procedure of Data Science, 2 Singular matrix, 88
Proper subset, 25 Singular value, 109
Singular value decomposition, 109
Span, 82
Q Spanning set, 83
Qualitative, 18 Specificity, 425
Quantitative, 18 Square matrix, 66
Standard deviation, 100, 339
Standard errors, 349
R Standard normal distribution, 289
Range, 342 Student’s t-distribution, 350
Rank, 88 Subset, 25
Ratio level, 20 Subspace, 79
Reading a normal distribution table, 291 Supervised learning, 3


================================================================================
PAGE 486
================================================================================

476 Index
T V
Test set, 422 Validation set, 422
Trace, 67 Variance, 100, 186, 275, 339, 428
Trade-off between the bias squared and Vector, 47
variance, 433 Vector addition, 49
Training set, 422 Vector direction, 52
Trigonometric function, 39 Vector magnitude, 52
True negative rate, 425 Vector space, 78
T-test, 366 Venn diagram, 26
Type I error, 364
Type II error, 364
Types of relation, 33 W
Weights, 228
U
Underfitting, 433 Z
Unit vector, 55 Z-score, 290
Unstructured data, 18
Unsupervised learning, 5
Upper quartile, 342

