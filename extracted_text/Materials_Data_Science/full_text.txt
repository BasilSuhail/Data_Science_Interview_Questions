
================================================================================
PAGE 1
================================================================================

The Materials Research Society Series
Stefan Sandfeld
Materials
Data
Science
Introduction to Data Mining, Machine
Learning, and Data-Driven Predictions
for Materials Science and Engineering


================================================================================
PAGE 2
================================================================================

The Materials Research Society Series


================================================================================
PAGE 3
================================================================================

The Materials Research Society Series covers the multidisciplinary field of mate-
rials research and technology, publishing across chemistry, physics, biology, and
engineering. The Series focuses on premium textbooks, professional books, mono-
graphs, references, and other works that serve the broad materials science and engi-
neering community worldwide. Connecting the principles of structure, properties,
processing, and performance and employing tools of characterization, computation,
and fabrication the Series addresses established, novel, and emerging topics.


================================================================================
PAGE 4
================================================================================

Stefan Sandfeld
Materials Data Science
Introduction to Data Mining, Machine
Learning, and Data-Driven Predictions
for Materials Science and Engineering


================================================================================
PAGE 5
================================================================================

Stefan Sandfeld
Forschungszentrum Jülich GmbH
Institute for Advanced Simulation –
Materials Data Science and Informatics
(IAS-9)
52068 Aachen, Germany
ISSN 2730-7360 ISSN 2730-7379 (electronic)
The Materials Research Society Series
ISBN 978-3-031-46564-2 ISBN 978-3-031-46565-9 (eBook)
https://doi.org/10.1007/978-3-031-46565-9
© The Materials Research Society 2024
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
Theuseofgeneraldescriptivenames,registerednames,trademarks,servicemarks,etc.inthispublication
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Paper in this product is recyclable.


================================================================================
PAGE 6
================================================================================

For Ruth


================================================================================
PAGE 7
================================================================================

Preface
Since the early 2000s, many scientific disciplines, including materials science and
materials engineering, have been undergoing a disruptive change. This change is in
fact made up of a series of small and large changes, a few of which can be considered
real revolutions.
On the one hand, there is the advent of the digitization in the field of experi-
mentation and microscopy, where an increasing number of machines and devices
provide direct access to a wealth of digital data and information. In addition, there
is a growing awareness—on the part of both manufacturers and scientists—that
data from measuring devices should be easily accessible and reusable. Furthermore,
simulation methods in computational materials science have reached a very high
level of maturity today, making them a source for a wealth of digital data and
information.
On the other hand, there are amazing methodological developments in the field
of data science and computer science that have effectively “democratized” statistical
learning and deep learning approaches through a series of well-designed and mature
software libraries. These significantly lower the entry barrier and make the use of
methods of, e.g., deep learning possible even for non-computer scientists. When one
considers the rapid development of performance and the enormous drop in price of
CPU and GPU hardware, it becomes clear that a pivotal point has been reached.
Today, almost everyone can see that machine learning has the potential to change
the way we approach our research data in materials science and physics, whether
it is small datasets or “big data.” This became clear at the latest after the release of
OpenAI’s the “artificial intelligence chatbot ChatGPT” by the end of 2022 (which
took place during writing of this book). However, there still seems to be a significant
gap between what people believe machine learning and artificial intelligence can do
for us and what can actually already be done in science and engineering.
This book is about data science; it is about exploring the large methodological
toolbox of statistics, data visualization, classical machine learning, and deep
learning approaches with a special focus on scientific applications. However, this
book is not only about solving scientific problems and understanding data, but also
about understanding analysis methods—because it is often important for assessing
the quality of analysis results that we do not use analysis methods as black boxes.
With this in mind, the book focuses heavily on the derivation and numerical
implementation of the majority of all algorithms and methods presented. In addition,
vii


================================================================================
PAGE 8
================================================================================

viii Preface
the book teaches the most important basics in mathematics and statistics and
assumes that the reader has basic Python programming skills. I believe that a
program is worth more than a thousand equations. Therefore, almost every method
presented is also presented in form of Python code. The emphasis is not on
performance, compactness, or elegant software design, but clearly on the didactic
aspects, helping you, the reader, to understand basic concepts and equipping you
with the tools to experiment and explore.
Studying data science and machine learning using generic datasets is certainly
useful. However, transferring this to the materials science context can be difficult.
Therefore, the book provides numerous materials science examples and datasets,
most of which are included directly in the text or provided via the book’s website
supplementary webpage (https://MDS-book.org).
Last but not least, I would be very grateful about general feedback, if you found
a typo or error, or have a suggestion for including an additional topic in the book.
Please see the book’s website supplementary webpage (https://MDS-book.org) for how
to contact me.
About this Book: Goals and Structure
The aim of this book is to provide a readership with a physics or engineering
background with the knowledge required to use data science methods (which
includes machine learning) to solve problems.
Sometimes we use motivational descriptions that are rather useful to sharpen
intuition, but can sometimes be a little less rigorous, e.g., in terms of mathematical
proofs. Nevertheless, formal derivations can also be very useful, which is why we
show them either in advanced chapters or through step-by-step explanations in the
introductory chapters. Last but not least, having to “scrape together” information
and explanations fromdifferent sources (books, lecture slides,online resources, etc.)
can be counterproductive, as many different notations are used. One of the aims of
this book is also to introduce consistent notations that are used for all methods and
topics. Below we give a brief overview of the structure of the book.
Part I: Introduction and Foundations This part provides a broad overview of
many historical and current developments that eventually led to data science and
machine learning as we know it today. The consequences for materials science
are then presented. An important chapter for practical purposes is the third chapter
that covers all the basics about data, mathematics, as well as some computational
aspects. At the end of this part, all datasets used in this book are presented.
Part II: A Primer on Probabilities, Distributions, and Statistics The second
part is designed as an introduction to all relevant concepts and techniques, starting
with combinatorics and probabilities, random variables and probability functions, as
well as expectation values and moments. This is followed by the different types of
“statistics,” the underlying concepts, and a number of interesting consequences for


================================================================================
PAGE 9
================================================================================

Preface ix
data analysis. This part concludes with a larger section on exploratory data analysis
and an overview of common distributions in materials science and engineering.
Part III: Classical Machine Learning One of the most important tools in data
science is machine learning. In this part, we review basic machine learning concepts,
important techniques, and a number of supervised and unsupervised “classical”
machine learning methods, which often have their roots in the field of statistical
analysis.
Part IV: Artificial Neural Networks and Deep Learning The fourth, big part
continues with another kind of machine learning approaches, the artificial neural
networks. This part follows the historical developments of neural networks and,
building on this, motivates all the models presented. In the second half of this
chapter, we move on to deep learning network architectures and introduce state-
of-the-art approaches.
Finally, this part contains an overview of linear algebra for machine learning
methods. This is not a complete introduction, but rather serves as a refresher for
those who have some knowledge of vector and matrix calculus. This part also
contains many Python examples. It helps make the book almost completely self-
contained.
A “How to MDS-Book” for Getting a Quick Overview
For getting a quick overview of data science and machine learning, the first two
chapters could be skipped. Chapter 3 gives a detailed description of the notations
and conventions for data and datasets. The reader in a hurry might just glance at
Sect.3.3 and Fig.3.3, skip the rest, and return to these subsections whenever it is
necessary to refresh knowledge of some of the notations. The nomenclature at the
beginning of this book should also help to navigate the jungle of variables names,
notations, and conventions.
If the reader already has some knowledge of random variables and distributions,
the first chapters of the Part II can be skipped and only the chapters on statistics
(Chap.8) and exploratory data analysis (Chap.9) should be read. These are two
chapters with general “data science” subjects that are particularly helpful for
understanding the ground on which machine learning is introduced.
If a particular machine learning method is preferred, the reader can choose
whether to focus on “classical” (statistical) learning in Part III or on neural networks
and deep learning in Part IV, which is written, at least for the most part, such that
it can be read independently of Part III. In any case, for a reader without any prior
knowledge of machine learning, it is definitely advisable not to skip the introduction
Chap.11, which gives a comprehensive overview of machine learning methods and
related subjects.


================================================================================
PAGE 10
================================================================================

x Preface
Acknowledgments
I would like to thank the following people (in alphabetical order) for their great help
with this book:
• People who helped create, write or fine-tune examples, short passages of text,
exercises or code: Aytekin Demirci, Abril Guzmann, Bashir Kazimi, Binh Duong
Nguyen, Kishan Govind, Pavlo Potapenko, Sébastien Bompas, Tarek Iraki
More detailed information also on code contributions are given and updated on
the book’s supplementary webpage (https://MDS-book.org).
• People who reviewed one or more chapters, provided other support during the
writing process: Aytekin Demirci, Abril Guzmann, Bashir Kazimi, Binh Duong
Nguyen, Hariprasath Ganesan, Jens Bröder, Karina Ruzaeva, Katharina Immel,
Hengxu Song, Markus Stricker, Marc Legros, Sébastien Bompas, Tarek Iraki
• Ruth Schwaiger for reviewing chapters and carefully proofreading and editing a
significant part of the final draft of the book
• The Forschungszentrum Jülich GmbH for the opportunity to write this book.
• I would like to thank the community at https://tikz.net and https://texample.net/
tikz for providing numerous TEX examples for TikZ figures that helped create
many of the figures in this book.
Attribution
Some of the graphics used in this book are or contain emoji graphics licensed under
CC-BY 4.0: https://creativecommons.org/licenses/by/4.0/, Copyright 2019, Twitter,
Inc and other contributors.


================================================================================
PAGE 11
================================================================================

Contents
Part I Introduction and Foundations
1 A Brief History of Data and Data Science ............................... 3
1.1 Where Do All the Numbers Come From? ......................... 3
1.2 The Ancient Roots of Data Science................................ 4
1.2.1 Ancient Babylonia (1800–600 BCE) ................... 4
1.2.2 The Age of Classical Antiquity (≈750 BCE–600 CE). 6
1.2.3 The Medieval Period (approx. 500–1500 CE).......... 6
1.2.4 The Renaissance (approx. 1500–1700 CE) ............. 7
1.2.5 OCCAM’s Razor.......................................... 8
1.3 The First True Data Scientist ....................................... 9
1.4 The More Recent Roots of Data Science .......................... 10
1.5 Data Science and Machine Learning in the Twentieth Century .. 11
1.6 Summary and Conclusion .......................................... 12
References.................................................................... 12
2 From Data Science to Materials Data Science........................... 15
2.1 What Is Data Science and How Is It Related to Machine
Learning and Artificial Intelligence?............................... 15
2.1.1 Data Science ............................................. 15
2.1.2 The Relation to Artificial Intelligence .................. 16
2.1.3 From Machine Learning to Deep Learning............. 17
2.1.4 Domain Knowledge...................................... 17
2.2 Data Science and Machine Learning in Materials Science
and Engineering..................................................... 18
2.2.1 Materials Data Science Without Machine Learning? .. 18
2.2.2 A Brief Overview of Machine Learning in
Materials Science ........................................ 20
2.2.3 Example: Linkage of Analysis Methods for Data
Mining of Microscopy Images .......................... 21
2.3 From Data and Information to Knowledge ........................ 23
2.3.1 “Data and Information” from the Point of View
of Information Science .................................. 23
2.3.2 “Data and Information” from the Point of View
of Data Science .......................................... 26
xi


================================================================================
PAGE 12
================================================================================

xii Contents
2.4 The Curse of Dimensionality....................................... 26
2.5 Summary and Conclusion .......................................... 28
References.................................................................... 28
3 What You Should Know About Data, Math, and Computing ......... 31
3.1 General Conventions and Notations................................ 31
3.2 Sets, Tuples, Vectors, and Arrays .................................. 32
3.2.1 Sets ....................................................... 32
3.2.2 Tuple...................................................... 35
3.2.3 Intervals .................................................. 36
3.2.4 Matrices and Row and Column Vectors ................ 37
3.3 Representation of Data in Statistics and Machine Learning ...... 41
3.3.1 General Problem Formulation........................... 41
3.3.2 Structured Data .......................................... 43
3.3.3 Tabular Data and the Data Matrix....................... 45
3.3.4 Matrix and Vector Short Forms for Linear Algebra.... 47
3.4 Summary and Conclusion .......................................... 51
3.5 Exercises ............................................................ 51
References.................................................................... 52
4 Materials Science Datasets and Data Generation....................... 53
4.1 Introduction ......................................................... 53
4.2 Dataset MDS-1: Tensile Test with Parameter Uncertainties ...... 53
4.3 Dataset MDS-2: Microstructure Evolution with the Ising Model 57
4.4 Dataset MDS-3: Cahn-Hilliard Model............................. 58
4.5 Dataset MDS-4: Properties of Chemical Elements................ 60
4.6 Dataset MDS-5: Nanoindentation of a Cu-Cr Composite......... 60
4.7 Dataset DS-1: The Iris Flower Dataset............................. 62
4.8 Dataset DS-2: The Handwritten Digits Dataset ................... 63
4.9 Online Resource for Obtaining Training Data..................... 64
References.................................................................... 65
Part II A Primer on Probabilities, Distributions, and Statistics
5 Combinatorics and Probabilities.......................................... 69
5.1 Combinatorics....................................................... 69
5.1.1 Counting with the Additive Principle ................... 70
5.1.2 More Counting with the Multiplication Principle ...... 71
5.1.3 Factorials ................................................. 73
5.1.4 Permutations—Ordered Sets Without Replacement ... 74
5.1.5 Permutations of n items, k chosen at a time:
Generalization of the Multiplication Principle ......... 79
5.1.6 Permutations with Repetition—Ordered
Sets/Samples With Replacement........................ 80


================================================================================
PAGE 13
================================================================================

Contents xiii
5.1.7 Combinations Without
Replacement—Unordered Sets/Samples
Without Replacement.................................... 81
5.1.8 Combinations with Replacement—Unordered
Sets/Samples with Replacement ........................ 84
5.1.9 Summary: Permutations and Combinations ............ 85
5.2 Probabilities ......................................................... 86
5.2.1 What is Probability? ..................................... 87
5.2.2 Two Python Examples ................................... 90
5.2.3 Outcomes, Events, and the Sample Space .............. 92
5.2.4 Calculating with Events and Probabilities .............. 94
5.2.5 The KOLMOGOROV Axioms ............................ 97
5.3 Conditional Probabilities, Product rule, and Bayes’ theorem..... 99
5.4 Summary ............................................................ 100
5.5 Exercises ............................................................ 101
References.................................................................... 102
6 Random Variables and Probability Functions .......................... 103
6.1 Random Variables................................................... 103
6.1.1 Discrete Random Variables.............................. 105
6.1.2 Continuous Random Variables .......................... 106
6.1.3 Assigning Probabilities to Discrete Random Variables 106
6.1.4 Assigning Probabilities to Continuous Random
Variables.................................................. 107
6.2 Introduction of Probability Functions.............................. 108
6.2.1 Why Do We Need Probability Functions?.............. 108
6.2.2 Classification of Distribution Types..................... 109
6.3 Discrete Probability Distributions.................................. 110
6.3.1 Probability Mass Functions ............................. 110
6.3.2 Cumulative Frequencies ................................. 112
6.4 Continuous Probability Distributions .............................. 113
6.4.1 Probability Density Functions........................... 113
6.4.2 Cumulative Distribution Functions ..................... 115
6.4.3 The Difference Between a Function and a Distribution 120
6.4.4 A Brief Summary of the Most Important Properties ... 120
6.5 Multivariate Discrete and Continuous Distribution ............... 121
6.5.1 Generic Formulation of the Joint CDF.................. 122
6.5.2 Joint Probability Functions for Discrete Random
Variables.................................................. 123
6.5.3 Joint Probability Functions for Continuous
Random Variables........................................ 123
6.5.4 Marginal Probability..................................... 125
6.5.5 Conditional Probability.................................. 125
6.6 Bivariate Distributions as a Special Case ......................... 126
6.6.1 Joint Probability for Two Discrete Random Variables . 126


================================================================================
PAGE 14
================================================================================

xiv Contents
6.6.2 Joint Probability for Two Continuous Random
Variables.................................................. 126
6.7 Summary ............................................................ 128
6.8 Exercises ............................................................ 128
7 Expectation, Variance, and Moments .................................... 131
7.1 Expected Values of Discrete Random Variables................... 131
7.1.1 Definition and Examples ................................ 131
7.1.2 Calculating with Expectation Values.................... 133
7.2 Variance and Standard Deviation .................................. 137
7.2.1 Definition and Examples ................................ 137
7.2.2 The Standard Deviation ................................. 139
7.2.3 Calculating with Variances .............................. 139
7.3 Raw Moments....................................................... 140
7.3.1 General Formulation..................................... 140
7.3.2 The Zero-th Raw Moment............................... 141
7.3.3 The First Raw Moment or the Mean .................... 142
7.4 Central Moments.................................................... 142
7.4.1 General Formulation..................................... 143
7.4.2 Zero-th and First Central Moments ..................... 144
7.4.3 The Second Central Moment or the Variance .......... 144
7.5 Standardized Moments ............................................. 145
7.5.1 General Formulation..................................... 145
7.5.2 Aspects of Standardized Moments...................... 146
7.6 Exercises ............................................................ 146
8 Introduction to Statistics................................................... 149
8.1 And What Now Is Statistics?....................................... 149
8.2 The Sample and the Population .................................... 150
8.2.1 The Sample and the Population ......................... 150
8.3 Two Flavors: Descriptive and Inferential Statistics................ 152
8.3.1 What is “Sampling”? .................................... 153
8.4 Sampling Strategies................................................. 154
8.4.1 Simple Random Sampling............................... 155
8.4.2 Systematic Sampling .................................... 156
8.4.3 “Take from Top” Sampling.............................. 158
8.4.4 Weighted Sampling ...................................... 158
8.4.5 Stratified Sampling ...................................... 159
8.5 The Law of Large and Truly Large Numbers...................... 161
8.5.1 The Law of Large Numbers ............................. 161
8.5.2 The Law of Truly Large Numbers ...................... 163
8.6 Central Limit Theorem ............................................. 163
8.7 Relations Between Multivariate Variables: Covariance and
Correlation .......................................................... 165
8.7.1 Covariance ............................................... 165
8.7.2 Covariance Matrix ....................................... 167


================================================================================
PAGE 15
================================================================================

Contents xv
8.7.3 Correlation ............................................... 172
8.7.4 Correlation Matrix ....................................... 175
8.8 Exercises ............................................................ 177
Reference..................................................................... 178
9 Exploratory Data Analysis ................................................ 179
9.1 The Why, the When, and the How ................................. 179
9.2 Two Preliminary Steps.............................................. 180
9.2.1 Initial Exploration of Data File(s)....................... 180
9.2.2 A Quick Initial Visualization............................ 182
9.3 Descriptive Statistics................................................ 183
9.3.1 Simple Descriptions of Univariate Distributions ....... 184
9.3.2 Simple Descriptions of Multivariate Distributions ..... 185
9.3.3 Central Tendency: The Mean, the Median, and
the Mode ................................................. 187
9.3.4 The “Spread” of a Distribution: The Variance ......... 190
9.3.5 Example: Summary Statistics of the Iris Dataset....... 193
9.3.6 Skewness ................................................. 193
9.3.7 The Fourth Moment: Kurtosis........................... 195
9.3.8 Summary ................................................. 195
9.4 Data Visualization .................................................. 196
9.4.1 Scatter Plots .............................................. 197
9.4.2 Histograms ............................................... 198
9.4.3 Box-Whisker Plots....................................... 200
9.4.4 Advanced Exploratory Plot Types ...................... 202
9.5 Exercises ............................................................ 204
References.................................................................... 206
10 Commonly Encountered Distributions in Materials Science
and Engineering ............................................................ 207
10.1 About the Following Discrete and Continuous Distributions..... 207
10.2 Discrete Uniform Distribution ..................................... 208
10.3 Bernoulli Distribution .............................................. 210
10.4 Binomial Distribution............................................... 211
10.5 Geometric Distribution ............................................. 213
10.6 Poisson Distribution ................................................ 215
10.7 Normal Distribution ................................................ 217
10.8 Bivariate Normal Distribution...................................... 219
10.9 Multivariate Normal Distribution .................................. 222
10.10 The Relation Between Covariance Matrix and Multivariate
Normal Distribution ................................................ 223
10.11 Lognormal Distribution............................................. 227
10.12 Exponential Distribution............................................ 229
10.13 Logistic Distribution................................................ 230
References.................................................................... 231


================================================================================
PAGE 16
================================================================================

xvi Contents
Part III Classical Machine Learning
11 Introduction and General Concepts of Machine Learning and
Data Science................................................................. 235
11.1 The Definition(s) of Machine Learning............................ 235
11.2 How and What Do Machines Learn? .............................. 239
11.2.1 The Machine Learning Model........................... 239
11.2.2 A First Learning Algorithm: Instance-Based Learning 240
11.3 Introduction of the General Machine Learning Workflow ........ 243
11.4 Data Collection...................................................... 244
11.5 Data Preprocessing.................................................. 245
11.5.1 Data Cleaning ............................................ 245
11.5.2 Outlier Detection......................................... 246
11.5.3 Data Transformation..................................... 247
11.5.4 Other Transformations................................... 250
11.6 A Taxonomy of Machine Learning Models ....................... 252
11.6.1 Overview of Regression ................................. 253
11.6.2 Overview of Classification .............................. 254
11.6.3 Overview of Clustering.................................. 255
11.6.4 Overview of Dimensionality Reduction ................ 256
11.6.5 The Zoo of Names for Input and Output Variables..... 257
11.7 Error Measures for Numerical Data................................ 257
11.7.1 Introduction to the Problem ............................. 258
11.7.2 Mean Absolute Error .................................... 259
11.7.3 Mean Squared Error ..................................... 259
11.7.4 Root Mean Square Error................................. 260
11.7.5 Coefficient of Variation.................................. 261
11.7.6 Root Mean Squared Log Error .......................... 261
11.8 Similarity Measures for Classification Problems.................. 262
11.8.1 From True Positives to False Negatives................. 262
11.8.2 The Jaccard Similarity or Intersection Over Union .... 263
11.8.3 The Dice Coefficient..................................... 265
11.8.4 Precision and Recall ..................................... 266
11.8.5 Categorical Cross-Entropy .............................. 266
11.8.6 Example: Binary Segmentation of a TEM Image ...... 267
11.9 Exercises ............................................................ 269
References.................................................................... 270
12 A First Approach to Machine Learning with Linear Regression ..... 271
12.1 The Roots of Regression Analysis ................................. 271
12.1.1 A Historical Perspective ................................. 271
12.1.2 Statistics Versus Machine Learning..................... 273
12.2 General Concepts and Important Terminology .................... 275
12.2.1 Interpolation.............................................. 277
12.2.2 Extrapolation............................................. 278
12.2.3 True Functions and Noise ............................... 278


================================================================================
PAGE 17
================================================================================

Contents xvii
12.2.4 The Training Process and the Machine Learning
Model..................................................... 279
12.2.5 Hypothesis/Hypothesis Space and the Parameter Space 280
12.2.6 Predictions ............................................... 281
12.2.7 Inference ................................................. 281
12.2.8 Under-and Overfitting................................... 282
12.2.9 Notations for Input and Output Variables............... 283
12.2.10 Training and Testing: Datasets and Errors Types....... 283
12.3 Simple Linear Regression .......................................... 287
12.3.1 The Linearity in Linear Regression ..................... 288
12.3.2 Hypothesis Space and Parameter Space ................ 289
12.3.3 Estimating Deviations: Errors and Residuals........... 291
12.3.4 The Loss Function and the Loss ........................ 292
12.3.5 The Cost Function ....................................... 293
12.3.6 Cost Function Minimization: Analytical Derivation ... 296
12.3.7 Cost Function Minimization: The Method of
Gradient Descent......................................... 297
12.4 Computational Aspects of Vectorization........................... 302
12.4.1 The First Type of Vectorization: Partial Vectorization . 302
12.4.2 A Second Type of Vectorization: Full Vectorization ... 303
12.5 A Worked Example of Simple Linear Regression................. 304
12.5.1 Python Implementation.................................. 304
12.5.2 Visualization and Discussion of Training Results...... 306
12.5.3 Validation of the Trained Model ........................ 309
12.6 Multiple Linear Regression Models................................ 310
12.6.1 Hypothesis Formulation ................................. 310
12.6.2 Special Case: Multiple Linear Regression with
Two Features ............................................. 310
12.6.3 Generalization to an Arbitrary Number of Features.... 312
12.7 Exercises ............................................................ 313
References.................................................................... 315
13 Advanced Methods and Topics of Regression ........................... 317
13.1 Nonlinear Model Behavior with Linear Regression............... 317
13.1.1 Polynomial Regression .................................. 318
13.1.2 Other Types of Hypothesis Functions................... 321
13.1.3 Linearization of Nonlinear Hypotheses................. 322
13.2 Generalized Formulations and Vectorization for Multiple
Linear Regression................................................... 323
13.2.1 Hypothesis and Extended Feature Matrix .............. 324
13.2.2 Derivation of Cost Function and Cost Gradient ........ 325
13.2.3 Closed Form Solution ................................... 327
13.2.4 Summary ................................................. 328
13.3 Generalized Formulation of Linear Regression with Basis
Functions ............................................................ 329


================================================================================
PAGE 18
================================================================================

xviii Contents
13.3.1 Basis Functions .......................................... 329
13.3.2 Hypothesis with Arbitrary Basis Functions ............ 330
13.3.3 Extended Feature Matrix with Arbitrary Basis
Functions ................................................. 332
13.3.4 Derivation of Cost Function, Cost Gradient, and
the Closed Form Solution ............................... 332
13.3.5 Numerical Aspects and Discussion ..................... 333
13.4 Formulation of Chosen Cases in Terms of Basis Functions ...... 334
13.4.1 Simple Linear Regression ............................... 335
13.4.2 Multiple Linear Regression ............................. 335
13.4.3 Polynomial Regression .................................. 336
13.4.4 Multiple Polynomial Regression ........................ 339
13.4.5 Discussion of Polynomial Regression .................. 341
13.5 Semi-and Non-parametric Regression ............................ 342
13.5.1 Problem Formulation .................................... 342
13.5.2 Regression with Piecewise Polynomials................ 343
13.5.3 Piecewise Regression Splines ........................... 346
13.5.4 Kernel Regression with Gaussian Basis Functions..... 348
13.5.5 Kernel Regression with FOURIER Basis Functions .... 353
13.5.6 Other Commonly Used Basis Functions................ 353
13.6 Further Nonlinear Regression Models ............................. 354
13.6.1 Ridge-and LASSO Regression ......................... 354
13.6.2 K-Nearest Neighbors Regression ....................... 355
13.6.3 Support Vector Machine Regression.................... 355
13.6.4 Artificial Neural Networks .............................. 355
13.7 Summary and Conclusion .......................................... 355
13.8 Exercises ............................................................ 356
References.................................................................... 357
14 Supervised Classification .................................................. 359
14.1 Introduction to Supervised Classification.......................... 359
14.2 Rule-Based Classification Methods and Decision Trees .......... 361
14.2.1 The Idea Behind Classification Rules................... 361
14.2.2 The Relation of Rule-Based Methods and
Decision Trees ........................................... 364
14.3 Notions and Concepts for Classification Problems................ 366
14.3.1 Datasets, Models, and the Hypothesis Class............ 366
14.4 Nearest Neighbor Classifier ........................................ 369
14.4.1 Distance Measures ....................................... 369
14.4.2 The (First-or One-) Nearest Neighbor Method ........ 371
14.4.3 The General k-Nearest Neighbor Method .............. 373
14.4.4 Multi-class Classification................................ 375
14.4.5 Implementation .......................................... 377
14.5 Gaussian Naive Bayes Model ...................................... 378
14.5.1 The Class of Naive Bayes Models ...................... 379


================================================================================
PAGE 19
================================================================================

Contents xix
14.5.2 Maximum-Likelihood Estimation for the
Gaussian Naive Bayes Model ........................... 380
14.5.3 Algorithm and Python Implementation................. 381
14.6 Support Vector Machines........................................... 384
14.6.1 Linear Support Vector Machine ......................... 384
14.6.2 Nonlinear Support Vector Machine ..................... 386
14.7 Exercises ............................................................ 388
References.................................................................... 388
15 Unsupervised Learning .................................................... 389
15.1 Introduction to Dimensionality Reduction......................... 389
15.1.1 Example .................................................. 389
15.1.2 The extrinsic and intrinsic dimensionality.............. 390
15.1.3 Notations and Conventions.............................. 390
15.2 Principal Component Analysis: Theoretical Background
and Derivations ..................................................... 391
15.2.1 A Simple Motivation from Solid Mechanics ........... 392
15.2.2 Further Examples of “Higher”-Dimensional Data Sets 393
15.2.3 The Algorithm of Principal Component Analysis
in Brief ................................................... 395
15.2.4 Preparation: Orthogonal Projection of Data onto
aLine..................................................... 395
15.2.5 Maximizing the Variance along the First
Principal Direction....................................... 402
15.2.6 The Second Principal Direction ......................... 405
15.2.7 The k-th and the Best k Principal Directions ........... 407
15.3 Application Aspects and Examples of PCA ....................... 409
15.3.1 Image Data and the Principal Component Analysis.... 409
15.3.2 Accuracy of the Representation ......................... 415
15.3.3 Reconstruction ........................................... 417
15.4 Further Methods for Dimensionality Reduction................... 420
15.4.1 The Relatives of Principal Component Analysis ....... 420
15.4.2 Manifold Learning Methods............................. 420
15.5 Clustering ........................................................... 424
15.5.1 k-Means and k-Medians ................................. 424
15.5.2 Gaussian Mixture Model ................................ 425
15.5.3 Model Selection with the BIC Criterium ............... 428
15.5.4 Density-Based Methods: DBSCAN..................... 429
15.6 Materials Science Examples........................................ 430
15.6.1 Dimensionality Reduction for Microstructures
from the ISING Model ................................... 430
15.6.2 Dimensionality Reduction for Microstructures
from the CAHN-HILLIARD Model ...................... 432
15.6.3 Clustering of Indentation Data .......................... 433
15.7 Exercises ............................................................ 435
References.................................................................... 435


================================================================================
PAGE 20
================================================================================

xx Contents
16 Machine Learning Techniques ............................................ 437
16.1 Feature Engineering and Feature Importance ..................... 437
16.1.1 Feature Engineering ..................................... 437
16.1.2 Permutation Feature Importance ........................ 439
16.2 Data Splitting, Cross-Validation, and Statistical Resampling..... 440
16.2.1 The Difference Between the Testing and
Validation Dataset........................................ 441
16.2.2 The Holdout Method—Or the Train/Test Split......... 442
16.2.3 Leave-One-Out Cross-Validation ....................... 446
16.2.4 k-Fold Cross-Validation ................................. 447
16.2.5 Repeated k-Fold Cross-Validation ...................... 448
16.2.6 Stratified k-Fold Cross-Validation ...................... 449
16.2.7 Bootstrap ................................................. 450
16.3 Baseline Models .................................................... 452
References.................................................................... 453
Part IV Artificial Neural Networks and Deep Learning
17 From the Perceptron to Artificial Neural Networks .................... 457
17.1 A First Model of a Neuron ......................................... 457
17.1.1 The Biological Neuron .................................. 457
17.1.2 Introducing...the McCulloch-Pitts Model ............. 459
17.1.3 Boolean Decision Problems ............................. 461
17.2 The Rosenblatt Perceptron ......................................... 463
17.2.1 The Mathematical Model of the Rosenblatt Perceptron 463
17.2.2 Again: Boolean Decision Problems..................... 466
17.2.3 Geometric Interpretation ................................ 467
17.2.4 Supervised Learning with the “Perceptron
Learning Rule”........................................... 469
17.2.5 Geometric Interpretation of the Perceptron Training... 474
17.3 The ADALINE Model.............................................. 476
17.3.1 ADALINE Training Strategy............................ 476
17.3.2 Example: Comparison Between the Perceptron
and the ADALINE Model ............................... 478
17.3.3 The Relation to Gradient Descent Optimization ....... 479
17.3.4 Limitations of the Perceptron and the ADALINE
Model..................................................... 480
17.4 Increasing the Complexity: Assemblies of Neurons .............. 481
17.4.1 A First Network of Two Computational Neurons ...... 481
17.4.2 The Boolean XOR Decision Problem................... 484
17.4.3 Python Implementation of the Two-Neuron-Assembly 485
17.4.4 A Fully Connected Network with Input, Output
and a Single Hidden Layer .............................. 487
17.4.5 “Vectorized” Operations: The Equations for the
Network with a Hidden Layer........................... 490


================================================================================
PAGE 21
================================================================================

Contents xxi
17.4.6 Capabilities of the Networks for Classification
and Regression ........................................... 491
17.5 Summary and Historical Remarks ................................. 492
17.6 Exercises ............................................................ 493
References.................................................................... 494
18 A Gentle Introduction to Deep Learning ................................ 497
18.1 Overview of the Historical Developments......................... 497
18.2 Activation Functions................................................ 498
18.2.1 Step Function ............................................ 498
18.2.2 Linear Function .......................................... 498
18.2.3 Sigmoidal Function ...................................... 500
18.2.4 The Family of ReLU Functions ......................... 500
18.2.5 Hyperbolic Tangent Function ........................... 501
18.3 Back-Propagation: Introduction and Example..................... 501
18.3.1 The General Concept .................................... 501
18.3.2 A First Example.......................................... 501
18.3.3 A Complete Python Implementation.................... 506
18.4 General Formulation of Backpropagation ......................... 510
18.4.1 Forward Pass ............................................. 510
18.4.2 Backward Pass ........................................... 512
18.5 Python Implementation and Example for the Fully
Connected Network................................................. 516
18.5.1 Python Implementation.................................. 517
18.5.2 Application: Identification of Handwritten Digits...... 521
18.6 Further Concepts and Techniques.................................. 523
18.6.1 Mini-Batches............................................. 523
18.6.2 Batch Normalization..................................... 524
18.6.3 Weight Initialization ..................................... 525
18.7 Less Is More: The Concept of Dropout ............................ 527
18.8 Example: Microstructure Classification and Property Prediction 528
18.9 Exercises ............................................................ 530
References.................................................................... 532
19 Advanced Deep Learning Architectures and Techniques .............. 533
19.1 Convolutional Neural Networks.................................... 533
19.1.1 Motivation of the Architecture .......................... 533
19.1.2 Convolutional Layer ..................................... 534
19.1.3 The Pooling Layer ....................................... 538
19.1.4 The Receptive Field and Dilated Convolutions......... 539
19.1.5 Feed Forward Step and Backpropagation............... 540
19.1.6 CNN Architectures ...................................... 540
19.2 Deep Learning Techniques ......................................... 543
19.2.1 Optimization Methods................................... 543
19.2.2 Momentum ............................................... 544
19.2.3 Early Stopping ........................................... 544


================================================================================
PAGE 22
================================================================================

xxii Contents
19.2.4 Transfer Learning ........................................ 544
19.2.5 Data Augmentation ...................................... 544
19.3 Two Examples for Deep Learning in Microscopy................. 545
19.3.1 Semantic Segmentation of TEM Images of
Nanoparticles............................................. 545
19.3.2 Binary Segmentation Using Synthetic Training
Data for Grain Microstructures.......................... 546
19.4 Autoencoder: How to Learn with Networks Without Supervision 548
19.4.1 Single-Layer and Multilayer Architectures............. 548
19.4.2 The Relation to Principal Component Analysis ........ 549
19.4.3 Convolutional Autoencoder ............................. 550
19.5 Generative Adversarial Networks: How to Create Data? ......... 550
19.5.1 Model..................................................... 550
19.5.2 Training .................................................. 551
19.5.3 Loss Functions for the Min-Max Game ................ 551
19.5.4 Common Problems of GANs............................ 554
19.5.5 Alternative Architectures and Variations ............... 555
19.5.6 Python Implementation.................................. 556
19.5.7 Two Examples: Handwritten Digits and
Microstructures .......................................... 560
19.6 Physics-Informed Machine Learning and Beyond ................ 562
19.7 Summary and Conclusion .......................................... 563
19.8 Exercises ............................................................ 564
References.................................................................... 565
A Linear Algebra for Machine Learning ................................... 569
A.1 Vector Calculus ..................................................... 570
A.1.1 Definition of Vectors..................................... 570
A.1.2 Conventions: Row and Column Vectors ................ 570
A.1.3 Transposition of a Vector ................................ 571
A.1.4 Scalar Product of Vectors................................ 571
A.1.5 Orthogonal Vectors ...................................... 572
A.1.6 Euclidean Norm of a Vector............................. 572
A.1.7 Euclidean Distance ...................................... 573
A.1.8 Orthogonal Projection of a Vector ...................... 573
A.1.9 Gradient of a Scalar Field and a Vector Field .......... 574
A.2 Matrices and Matrix Operations.................................... 575
A.2.1 Basic Definition and Creation of Matrices in Python .. 575
A.2.2 Addition of Matrices..................................... 576
A.2.3 Multiplication of a Matrix with a Scalar................ 577
A.2.4 Matrix Product ........................................... 577
A.2.5 Multiplication of a Matrix with a Vector ............... 579
A.2.6 Hadamard Product of Matrices.......................... 580
A.2.7 Transposition of a Matrix................................ 581


================================================================================
PAGE 23
================================================================================

Contents xxiii
A.2.8 Neutral Element for Matrix Addition ................... 582
A.2.9 Neutral Element for Matrix Multiplication ............. 582
A.2.10 Square Matrix ............................................ 582
A.2.11 Inverse Matrix............................................ 583
A.2.12 Symmetric Matrix ....................................... 584
A.2.13 Triangular Matrices ...................................... 585
A.2.14 Trace of a Matrix......................................... 585
A.2.15 Rank of a Matrix ......................................... 586
A.2.16 Determinant .............................................. 587
A.2.17 Orthogonal Matrix ....................................... 588
A.3 Derived Properties, Further Theorems, and Advanced
Definitions........................................................... 589
A.3.1 Eigenvalues, Eigenvectors, and the
Characteristic Polynomial ............................... 589
A.3.2 Diagonal Matrices and Diagonalization of Matrices ... 592
A.3.3 Similarity Transformations .............................. 594
A.3.4 The Spectral Theorem ................................... 594
A.3.5 Eigendecomposition and Singular Value
Decomposition ........................................... 596
A.4 Matrix-Related Operation That Only Exists in Numpy ........... 597
A.4.1 Addition of a Scalar to a Matrix......................... 597
A.4.2 One-Dimensional vs. Two-Dimensional “Vectors” .... 598
B Proofs and Derivations..................................................... 601
B.1 Proofs of the Theorems About Expectation Values ............... 601
B.2 Proofs of Some Theorems About Variances ....................... 602
B.3 Simplification of Pearson Moment Coefficient of Skewness ..... 603
B.4 Proofs and Additional Derivations for Distributions .............. 604
B.4.1 Property of the Binomial Coefficient ................... 604
B.4.2 Binomial Distribution: Derivation of Mean and
Variance .................................................. 605
B.4.3 Isocontour Lines of a Bivariate Normal Distribution .. 606
References.................................................................... 608
List of Python Listings........................................................... 609
Index............................................................................... 611


================================================================================
PAGE 24
================================================================================

Acronyms
The page number indicates only the first occurrence of the acronym.
ADALINE Adaptive Linear Neuron.............................................. 476
AE absolute error ......................................................... 472
AI artificial intelligence.................................................. 15
AIC Akaike information criterion......................................... 429
AL activation layer ....................................................... 510
ANN artificial neural network .............................................. 534
BIC Bayesian information criterion....................................... 429
CDF cumulative distribution function ..................................... 116
CLT central limit theorem ................................................. 163
CV coefficient of variation ............................................... 261
CNN convolutional neural network ........................................ 533
DL deep learning.......................................................... 15
DoE design of experiment ................................................. 28
EDA exploratory data analysis............................................. 152
EBSD electron backscatter diffraction ..................................... 115
FEA finite element analysis................................................ 44
FEM finite element method ................................................ 59
FCL fully connected layer ................................................. 510
GAN generative adversarial network....................................... 550
GD gradient descent ...................................................... 504
GMM Gaussian mixture model ............................................. 425
GNB Gaussian naive Bayes ................................................ 378
IQR interquartile range .................................................... 200
SGD stochastic gradient descent........................................... 543
IID independent and identically distributed ............................. 137
IoU intersection over union ............................................... 264
k-fold CV k-fold cross-validation ............................................... 447
KDE kernel density estimation............................................. 202
kNN k-nearest neighbors................................................... 242
LLN law of large numbers ................................................. 161
LOO CV leave-one-out cross-validation ....................................... 446
LSQ least square............................................................ 274
xxv


================================================================================
PAGE 25
================================================================================

xxvi Acronyms
MAE mean absolute error................................................... 259
MCP McCulloch-Pitts ...................................................... 459
MD molecular dynamics .................................................. 44
ML machine learning ..................................................... 15
MLP multilayer perceptron................................................. 487
MSD mean squared deviation .............................................. 259
MSE mean squared error ................................................... 259
OLS ordinary least squares ................................................ 327
OHE one-hot encoding ..................................................... 438
PCA principal component analysis ........................................ 169
PC principal component.................................................. 395
PDF probability density function.......................................... 109
PDE partial differential equation .......................................... 562
PMF probability mass function ............................................ 109
PINN physics-informed neural network.................................... 563
ReLU rectified linear unit ................................................... 500
RMSE root mean squared error .............................................. 260
MSLE root mean squared logarithmic error................................. 261
RBF radial basis function .................................................. 349
SEM scanning electron microscopy ....................................... 546
SVD singular value decomposition ........................................ 392
SVM support vector machine............................................... 355
SRS simple random sampling ............................................. 155
TEM transmission electron microscopy ................................... 21
t-SNE t-distributed stochastic neighbor embedding........................ 421
RV random variable ...................................................... 134
UMAP Uniform Manifold Approximation and Projection.................. 421
TLU Threshold Logic Unit................................................. 459


================================================================================
PAGE 26
================================================================================

Part I
Introduction and Foundations


================================================================================
PAGE 27
================================================================================

1
A Brief History of Data and Data Science
Equipped with his five senses, man explores the universe around
him and calls the adventure Science.
Edwin Hubble (1889–1963),
American astronomer
1.1 Where Do All the Numbers Come From?
Natural science and engineering in general would not be possible without numbers
and without the ability to count (although in the discussions of where statistics end
and data science begins, it is claimed that data science is different because it does
not only work with numerical data, but that is a different story). Counting, and in
particular using our fingers as “the earliest calculating machine”[5], has been part
of humanity for more than 40,000 years. However, the first written records date back
to about 10,000 BCE.1
Beginning around 3,400 BCE, the Sumerians introduced the first known number
system, in which numerals were imprinted on clay tablets. They also attached a
version of what we would call today metadata or physical units: small symbols
that represented the quantity being counted (e.g., a sheep). Figure 1.1 shows one
of the earliest “data visualization” of this number system. Around 2,000 BCE the
Babylonians developed a first decimal number system and then already calculated
a first approximation of π ≈ 3.125. Around 300 BCE they then also invented the
.
abaqus as the first calculating tool. While the Roman numerals appeared as early
as 1,000 BCE, Europe had to wait another two centuries before the Indo–Arabic
1 BCE is the abbreviation for Before Common Era and CE is the abbreviation for Common Era.
These are year notations according to the Gregorian calendar. CE and BCE are identical to Before
Christ (BC) and Anno Domini (AD) but do not have their religious connotation.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 3
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_1


================================================================================
PAGE 28
================================================================================

4 1 ABriefHistoryofDataandDataScience
Fig. 1.1 One of the first data
visualization: summary
account of silver for the
governor written in Sumerian
Cuneiform on a clay tablet.
(From Shuruppak or Abu
Salabikh, Iraq, circa 2,500
BCE. British Museum,
London. BM 15826, courtesy
Gavin Collins [10])
numeral system (which we still use today) was finally introduced. Of course, there
are many more exciting events such as the invention of the number “zero” and the
first binary numeral system, but we will now go ahead and take a look at one of
the most important uses of numbers in ancient history besides trade and commerce:
astronomy and predicting the path of the celestial objects in the sky.
1.2 The Ancient Roots of Data Science
When we talk about the “ancient roots of data science” in this section, we do
not mean to imply that, for example, the ancient Babylonians did anything even
close to data science. But it is clear that data science would not exist without the
historical events presented below. Figure 1.2 provides an overview of the stages
in history that have played a crucial role in the development of modern science
in general and that have been of particular importance to data science. We begin
with the ancient Babylonians, who already demonstrated impressive creativity and
inventiveness above.
1.2.1 Ancient Babylonia (1800–600 BCE)
When the Babylonians observed the night sky, they recognized two types of celestial
objects. The first type are stars that move around in a circular pattern (it will be
seen that they move around Polaris, the North Star) and can “twinkle.” The photo
on the left in Fig.1.3 illustrates these circular orbits. The second type of celestial
objects are those that move in a more complex manner. This is known today as
retrograde motion (cf. Fig.1.3 right). This complicated motion gave them the name
“wandering stars,” which in Greek means “planetes.” The Babylonians counted five
such planets: Mercury, Venus, Mars, Jupiter, and Saturn. Together with the Sun and
the Moon, this gave seven objects, one for each day of the week, as they introduced
at that time. Since the Babylonians could not explain the movement of the planets
in any other way, they assumed that the planets must be pulled through the sky
by gods. The stars, on the other hand, were fixed on a hemispherical “shell” that


================================================================================
PAGE 29
================================================================================

1.2 TheAncientRootsofDataScience 5
≈ 700–300 BCE ≈ 360 BCE ≈ 1250 CE ≈ 1580 CE 1750 CE
Babylonia Eudoxus of Cnidus Thomas von Aquin Tycho de Brahe J. T. Mayer
gods propelling more rotating 5 proofs of god’s existence astronomical data 1st data scientific
stars and planets spheres & connection to astronomy collection model
≈ 400 BCE ≈120 CE ≈ 1330 CE ≈ 1500 CE ≈ 1610 CE 1805 CE
Anaximenes Claudius Ptolemy William of Ockham Nicolaus Copernicus Kepler A.-M. Legendre
planets attached to geocentric model with heliocentric data-based model method of least
rotating crystal spheres 80 circles & spheres ”Ockham’s Razor” model development squares
Fig. 1.2 Overview of important historical developments and concepts (up to . ≈ 1800) that
contributed toward shaping today’s data science
Fig. 1.3 Stars and planets: long-term exposure photograph, made during a complete night, has
captured the star trails around the south celestial pole, courtesy Kwon O Chul. The “temporal
averages” reveal the apparent motion of starts on circular paths (left). Superposition of snapshots
at equidistant time steps (right) reveals the complex forward-backward motion of planets called
“retrograde motion” (shown is a photo-montage of the motion of Mars)
constantly rotated to the east. Obviously, they could not properly explain the forces
acting between the planets, but their model still worked quite well: the goal of the
model was to create calendars that could provide information about when it was
best to plant, harvest, or go to war [8]. Obviously, any model necessarily contains
simplifications and idealizations, and therefore all models are flawed in some of
their aspects; the real question, however, is whether the model performs well at the
(predictive) task for which it was developed,2 and in this respect, the Babylonian
model worked perfectly.
2 The question of model performance is also a question that we will discuss in the context of
machine learning many times.


================================================================================
PAGE 30
================================================================================

6 1 ABriefHistoryofDataandDataScience
1.2.2 The Age of Classical Antiquity (≈750 BCE–600 CE)
After the fall of Babylon in 539 BCE, the Babylonian Empire ceased to exist
and much of its knowledge was at risk. Fortunately, some of their astronomical
knowledge had already reached Greece. In particular, ANAXIMENES (approx. 585–
525 BCE) based his work on the Babylonian cosmology. His approach removed
the gods from the picture and introduced instead a number of concentric half-
spheres spanning the flat “earth disk,” with which the stars and planets moved. Thus
his celestial mechanics was the first purely physics-based model. To explain why
no one could see the spheres, he also invented the invisible, crystal-like element
aether, which accompanied science for more than a millennium. However, unlike
the Babylonians, he could not explain the movement of the planets. The “remedy”
was provided by EUDOXUS OF CNIDUS (approx. 408–355 BCE), a student of
PLATO. He added more rotating spheres: three for the Sun and for the Moon and
four for each of the five visible planets, which could explain the retrograde motion
in parts by superposing the motion on two different spheres. In total, this model
required 27 spheres. But this was not the last word: ARISTOTLE (384–322 BCE)
extended the model to 56 spheres, because this was the only way to explain the
brightness variations of the stars. In the context of machine learning, this model
would have the taint of overfitting.
Unfortunately, even this model could not explain all the observed phenomena,
including the fact that the Moon goes through different lunar phases of waning and
waxing. It took several centuries before the Roman CLAUDIUS PTOLEMY (approx.
100–170 CE) arrived with new ideas (even though some of them had already been
mentioned by the ancient Greeks). In his model, the planets were attached to some
kind of “celestial wheel,” which in turn was attached to the spheres. In addition, the
Earth was placed in a slightly eccentric position so that the planets could move at
different velocities. This very complex geocentric model required about 80 circles,
spheres, and other geometrical parameters. However, it was extremely accurate and
could predict, among other things, the occurrence of eclipses. Figure 1.4a gives an
idea of the complex motion of the planets resulting from this model. PTOLEMY also
provided astronomical tables and other tools to easily compute the positions of the
Moon, Sun, and the other planets. For the next 1,000+ years, this was the state-of-
the-art despite the horrendous complexity and accumulation of completely wrong,
(un)physical assumptions.
1.2.3 The Medieval Period (approx. 500–1500 CE)
The Middle Ages were a dark time for the European people and for science.
Famines, war, and the plague were responsible for greatly reducing the population,
including scientists. Invasions and mass migrations created an environment in which
science struggled to thrive. THOMAS AQUINAS (1225–1274) is one of the relatively
few brilliant minds worth mentioning: he was a modern thinker who tried to


================================================================================
PAGE 31
================================================================================

1.2 TheAncientRootsofDataScience 7
reconcile astronomy and theology. As a first step, he stated five different proofs for
the existence of God [8]. This says something about that era, but actually it is more
interesting to know that they were all refuted by WILLIAM OF OCCAM. He and the
so-called Occam’s razor play a special role in data science and will be explained in
more detail below.
1.2.4 The Renaissance (approx. 1500–1700 CE)
NICOLAUS COPERNICUS (1473–1543) rejected the model of PTOLEMY and could
not bring himself to believe that the universe worked that way. He radically changed
all of astronomy by placing the Sun at the center of the model of the universe
(cf. Fig.1.4b). This was such a pivotal event that it subsequently became known
as the Copernican Revolution (although the Greek ARISTARCHOS OF SAMOS had
already proposed a heliocentric system around 250 BCE). COPERNICUS’ special
contribution was to represent his idea in the form of a mathematical description.
Even though the predictions of his model were not much more accurate than those
of PTOLEMY’S model, his model was much simpler; his model was the first to
Fig. 1.4 (a) Ptolemy’s prediction of the apparent movement of the Sun, Mercury, and Venus as
observed from Earth. The inner pentagram-like shape is part of the orbit of Venus. (b) Copernicus’
heliocentric model with the Sun in the center from the manuscript “De revolutionibus,” Book One,
Chapter 10: The Order of the Heavenly Spheres. Both models are comparable in terms of predictive
accuracy, but the heliocentric model is significantly simpler


================================================================================
PAGE 32
================================================================================

8 1 ABriefHistoryofDataandDataScience
consistently explain (and not just describe) the retrograde motion of the planets.
All this was the foundation for a new epoch of science.
TYCHO DE BRAHE (1546–1601) was a diligent astronomer who spent much of
his time collecting data from observing the stars. Coincidentally, he also observed
two truly unique events: a nova in 1572 and a comet in 1576. These “outliers” would
seem to contradict all that was believed or known about the universe, especially the
concept of crystal spheres, since the comet was able to cross the orbit of the planets.
Finally, his later assistant JOHANNES KEPLER (1571–1630) was one of the most
interesting personalities in the history of science, with unconventional creativity,
spirituality, and accuracy. He admired the simplicity of COPERNICUS’ model and
refined it by introducing the trajectories of the planets as ellipses; his three “Kepler
laws” are very simple mathematical expressions and are still valid today. Eventually,
KEPLER obtained much of the data BRAHE had collected during his lifetime.
KEPLER then began to analyze the data from the observed orbit of Mars, and he
did so for an impressive 25 years. He compared the data to the predictions of the
theoretical models of PTOLEMY and COPERNICUS and found significant deviations.
Because of his appreciation of TYCHO BRAHE’s data, this eventually led to a new
model in which he introduced the ellipsoid orbits of the planets.
This is a good time to pause our historical tour for a moment and recognize that
a defining point was reached with KEPLER. And why? Because after 25 years of
thorough data analysis, KEPLER should in some ways be regarded as the first data
analyst [7]: never before in human history has there been anyone who analyzed
a massive dataset over and over again, just for the sake of analyzing it, but also
to reveal structure in the data and identify parameters by comparing the data to
mathematical hypothesis equations. KEPLER was motivated by the belief that even
if the overall phenomenology of the motion of the stars and planets looks extremely
complicated, it is simple laws and simple principles that have a divine aesthetic
and that form the mechanics behind them. This is related to another principle that
is important in data science today and by which KEPLER was most likely already
influenced: the principle of Occam’s razor, which takes us back to the fourteenth
century.
1.2.5 OCCAM’s Razor
WILLIAM OF OCCAM (also spelled OCKHAM) lived from 1285 to 1347 and was an
English Franciscan monk as well as a philosopher and logician. He became famous
for his theo(logical) arguments, which he used to refute THOMAS AQUINAS’s
“proofs” of God [8] despite his own religious beliefs. His particular style of logical
argumentation also brought him some fame. This involved, in particular, a principle
known today as OCCAM’s razor. He did not “invent” this principle, but OCCAM used
in on several occasions.
Occam’s razor is a principle also known as the principle of parsimony .Itstates
that if two theories explain a given phenomenon, we should rather choose the one
based on the smallest possible number of “parameters,” i.e., the simplest one. A


================================================================================
PAGE 33
================================================================================

1.3 TheFirstTrueDataScientist 9
popular alternative formulation states that the simplest solution is best or most
probable. Although Occam’s razor is not a universal rule but rather a principle, many
scientists (including statisticians who make BAYESian inferences) are convinced
that this principle is something fundamental.
What does this have to do with the complexity of models? It is clear that a very
sophisticated model can represent a more complex physical behavior than a very
simple model. Take the heliocentric system as an example: the number of parameters
is much smaller than those of the PTOLEMYian model. However, if we fitted the
model parameters to measured data, the heliocentric model achieves much higher
accuracy because more data are available per parameter. We will encounter this
principle in a number of different situations. In the next chapter, we will encounter
similar points when we discuss the curse of dimensionality.
Nevertheless, it is only a principle that still needs to be supported by logic
and empirical evidence. WILLIAM OF OCCAM, for example, focused more on the
meaning of not making a theory more complicated than necessary . For KEPLER,
however, this principle certainly helped to shape his intuition and to take the bold
step of turning the universe almost “inside out.”
1.3 The First True Data Scientist
One could argue whether KEPLER really was the first data scientist. However,
there is another person who is certainly considered the first “real” data scientist:
a person, who used data in a way that is very close to the approach of today’s data
scientists. It was the German astronomer JOHANN TOBIAS MAYER (1723–1762)—
yet another astronomer! His work was mainly devoted to the study of the Moon
and in particular its “libration” (a kind of “wavering” behavior of the Moon: the
Moon always faces the Earth with the same face, but small fluctuations occur—the
libration). He became famous for his Lunar Tables and an accurate map of the Moon
[14]. To study the libration, he chose a point on the Moon’s surface, measured it,
and used a trigonometric equation that depended on three unknown parameters α,
.
β, and θ in addition to the measured position. (see [11] for further details).
. .
It would have taken only three measurements on three different days, resulting
in three individual equations, to approximate the free parameters. However, MAYER
had measurements from a total of 27 days and therefore had to come up with a
suitable strategy, as he did not want to throw away valuable data. At the time,
there was no theory for dealing with such overdetermined systems of equations
[4]. MAYER’s idea was to divide the 27 equations into 3 groups of 9 equations.
Then he added the equations of each group so that there were only three equations
and three unknowns. Now how do you choose which of the 27 equations goes
into which group? Since α was the most important parameter for the equation, he
.
decided that in the first group, all the equations had the largest positive value for
α. The second group consisted of the nine largest negative values of α, and the
. .
rest went into the third group. His intuition was to maximize the “contrast” of the
coefficientα, which should result in a good estimate. He wrote: “[...]because each
.


================================================================================
PAGE 34
================================================================================

10 1 ABriefHistoryofDataandDataScience
of these equations has been formed in the most advantageous manner [...]t hrough
the above division into three classes, the difference between the three sums is made
as large as possible.” He also provided an empirical evaluation of the accuracy;
however, he believed that since there were nine observations for each parameter, the
result should also be nine times more accurate, which is obviously false. However, it
took several more years for PIERRE-SIMON LAPLACE and later CARL FRIEDRICH
GAUSS to derive a correct error estimate.
MAYER’s work is particularly impressive given that the discovery of the least
squares method was still a long way off in 1750 (it took until 1805 for ADRIEN-
MARIE LEGENDRE to introduce this approach). MAYER had the great intuition at
that time not to cherry-pick the data (e.g., by selecting the “best” three measure-
ments), but to let the data “speak” for themselves by combining a set of equations
and thus measurements. In a sense, MAYER realized what is still true for data science
and machine learning today: the more data, the better—and the more data, the better
the model. EULER worked on a similar problem but could not solve it because he
approached the problem with the mind of a theoretician, while MAYER probably
benefited from his engineering sense and practical intuition. His data-based method
was completely new, was highly regarded by his colleagues, and greatly influenced
later work.
1.4 The More Recent Roots of Data Science
The foundations of data science are old and resulted from the desire to predict
phenomena and events and to obtain decision support, which has not changed
significantly to this day. For this reason, data science is closely linked to the
development of modern mathematics and statistics. In the next 250 or 300 years,
starting in the second half of the eighteenth century, many exciting developments
took place, of which we mention here only some of the most important aspects.
A major step was ADRIEN-MARIE LEGENDRE’s conceptualization of the method
of least squares in 1805, mentioned above. We will encounter his method in
numerous places in this text. There were also a number of significant developments
in the field of probability theory. One of the most important names was THOMAS
BAYES, who worked around the middle of the nineteenth century and is known,
among other things, for what is known as BAYES’s theorem. This theorem plays
an important role in probabilistic machine learning methods. From the first half of
the nineteenth century, probabilities and statistics were developed mainly as tools
in the social sciences, e.g., for analyzing death rates or conviction rates for different
geographical locations. The late nineteenth century is considered the cornerstone
of modern statistics, when scientist such as FRANCIS GALTON benefited from the
fact that experiments in the natural sciences were becoming more sophisticated and
reproducible and could generate larger amounts of data. Another name we will
encounter several times in this text is KARL PEARSON, who lived and worked until
the 1930s and is responsible for, among other things, the standard deviation and
the correlation coefficient. Not surprisingly, there are many more names and works


================================================================================
PAGE 35
================================================================================

1.5 DataScienceandMachineLearningintheTwentiethCentury 11
worth mentioning that we have skipped (see, e.g., the overview by Stigler [11]
for more details). We now fast-forward a few decades and jump directly to the
beginnings of the digital age.
1.5 Data Science and Machine Learning in the Twentieth
Century
The early 1960s marked another turning point in science and technology that also
gave significant impetus to the development of data science. Around 1960, the
transition from vacuum tubes to transistors and integrated circuits in computers
occurred. Shortly before 1960, magnetic tape drives were developed, and computer
memory chips (RAM) also became available shortly after 1960. The ability to
process and store larger amounts of data triggered a whole new scientific discipline
centered around “data.”
These new developments began in 1962, when JOHN W. TUKEY [13] published
an article entitled “The future of data analysis.” Therein, he defined the notion of
data analysis pretty much like what is also called data science today:
[...] procedures for analyzing data, techniques for interpreting the results of
such procedures, ways of planning the gathering of data to make its analysis
easier [...] and all the machinery and results of (mathematical) statistics
which apply to analyzing data.
Also in the mid-1960s, PETER NAUR created the term datalogy as the science of
data processes [12], but it did not catch on. A few years later, in 1974, NAUR, in
his book “Concise Survey of Computer Methods” [9], referred to data science as a
“science of dealing with data.”
In 1985, the term data science was coined by C.-F. JEFF WU [15] who proposed
it as an alternative name for statistics. It is said that in 1992, during the second
Japanese-French statistics symposium at the University of Montpellier in France, the
name data science was generally accepted, established, and from then on understood
as a new discipline. This was also the beginning of a long discussion about whether
and to what extent data science is just another name for statistics or whether one
does not need statistics at all for data science (see, e.g., the overview in [2])—a
discussion that is even today not entirely finished.
Closely related to data science is also the term data mining: this term first
appeared in 1999 when the article “Mining Data for Nuggets of Knowledge”
announced a data mining “short Executive Education course,” organized by
JACOB ZAHAVI [6]. There, it was noted that traditional statistical methods work
with small datasets, but data mining requires specialized tools for large datasets.
Data mining became another term for the process of identifying structure and


================================================================================
PAGE 36
================================================================================

12 1 ABriefHistoryofDataandDataScience
patterns in relatively large datasets, and this definition is still valid today, with the
availability of increasingly powerful machine learning methods that complement
“conventional” analysis methods.
With the advent of advanced machine learning methods and affordable high-
performance CPU/GPU computing infrastructures, and with the availability of
software libraries that have made sophisticated machine learning and data mining
methods accessible to virtually everyone (what is sometimes called the “democrati-
zation of AI”), the field of data science gained even more momentum and brought
us to where we are today.
1.6 Summary and Conclusion
This concludes the brief and perhaps somewhat unusual introduction of the histori-
cal roots and the development of data science. Clearly, data science is a “young”
disciplines but at the same time has a long history and a multitude of links
to different scientific fields. We will now continue by introducing a number of
different concepts from data science and information science while explaining their
connections to materials science and engineering.
References
1. N. Copernicus. “De revolutionibus”, p. 9 verso. Source: https://en.wikipedia.org.URLhttps://
commons.wikimedia.org/wiki/File:De_Revolutionibus_manuscript_p9b.jpg.
2. D. Donoho. 50 Years of Data Science. Journal of Computational and Graphical Statistics,
26(4):745–766, Oct. 2017. DOI https://doi.org/10.1080/10618600.2017.1384734.
3. J. Ferguson. Representation of the apparent motion of the sun and planets from the earth. Taken
from the “Astronomy” article in the first edition of Encyclopaedia Britannica (1771). Source:
https://en.wikipedia.org. URLh ttps://en.wikipedia.org/wiki/File:Cassini_apparent.jpg.
4. E. Gray Forbes. The Life and Work of Tobias Mayer (1723-62). Quarterly Journal of the
Royal Astronomical Society, 8:227, Sept. 1967.
5. G. Ifrah. The Universal History of Numbers: From Prehistory to the Invention of the Computer.
John Wiley & Sons, Inc., 2000.
6. Mining Data for Nuggets of Knowledge — knowledge.wharton.upenn.edu. URL https://
knowledge.wharton.upenn.edu/article/mining-data-for-nuggets-of-knowledge/, [Accessed 25-
01-2023]
7. A. E. Kossovsky. Johannes Kepler—The First Data Analyst, pages 23–24. Springer
International Publishing, Cham, 2020. ISBN 978-3-030-51744-1. DOI https://doi.org/10.1007/
978-3-030-51744-1_7.
8. J. McFadden. Life is simple. Basic Books UK, 2021. ISBN 9781529364958.
9. P. Naur. Concise Survey of Computer Methods. Petrocelli, 1975. ISBN 0-88405-314-8.
10. Gavin Collins (photographer). Sumerian account of silver for the govenor (back-
ground removed). British museum, London. BM 15826. Source: https://en.wikipedia.
org. URL ht tps://commons.wikimedia.org/wiki/File:Sumerian_account_of_silver_for_the_
govenor_(background_removed).png.
11. S. M. Stigler. The History of Statistics. The Measurement of Uncertainty before 1900. The
Belknap Press of Harvard University, 1986. ISBN 9780674403413.


================================================================================
PAGE 37
================================================================================

References 13
12. E. Sveinsdottir and E. Frøkjær. Datalogy — the Copenhagen tradition of computer science.
BIT, 28(3):450–472, Sept. 1988. DOI https://doi.org/10.1007/bf01941128.
13. J. W. Tukey. The Future of Data Analysis. The Annals of Mathematical Statistics, 33(1):1–67,
1962. DOI https://doi.org/10.1214/aoms/1177704711.
14. E. A. Whitaker. Mapping and Naming the Moon—A History of Lunar Cartography and
Nomenclature. Cambridge University Press, 2003. ISBN 9780521544146.
15. C.-F. J. Wu. Future directions of statistical research in China: a historical perspective.
Application of Statistics and Management, 1, 1986.


================================================================================
PAGE 38
================================================================================

From Data Science to Materials Data Science 2
Data is not information, information is not knowledge,
knowledge is not understanding, understanding is not wisdom.
Clifford Stoll (born 1950),
American astronomer, author and teacher
2.1 What Is Data Science and How Is It Related to Machine
Learning and Artificial Intelligence?
The previous chapter did not attempt to define what data science is. In the
following, we will choose a very intuitive definition and also clarify some common
misconceptions about artificial intelligence (AI).
2.1.1 Data Science
Most of us would probably agree that data science consists largely of statistics,
computer science, machine learning, data processing, and visualization (see, e.g.,
the overview in [8]). But how does all of this connect, and where does AI come
into play? Figure 2.1 shows a visualization of all the different fields that contribute
to data science. We can see that data science consists of contributions from many
different fields. First of all, data science does not necessarily mean that machine
learning (ML)or deep learning (DL) methods have to be used. These are just some
(but admittedly, some of the most successful) methods. However, the roots of data
science lie in statistics, which has been accompanied by explorative data analysis
for several decades. Both are introduced in the first part of this text.
Last but not least, approaches and tools from computer science and programming
are the bread and butter for data science, as the gray box in Fig.2.1 shows. While
it might well be that “programming” as we know and use it today will change
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 15
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_2


================================================================================
PAGE 39
================================================================================

16 2 FromDataSciencetoMaterialsDataScience
Fig. 2.1 Data science is at the intersection of a number of different disciplines, all more or
less loosely connected by topics in computer science and programming (symbolized by the gray
background). An important aspect of data science is its connection to domain knowledge, which
in this text is the point at which knowledge from physics, materials science, and engineering
integrated
completely in the near future—either through systems like ChatGPT [25] or through
graphical programming approaches—writing and reusing code and libraries is at
present probably the best and most instructive way to solve problems using data
science approaches.
What we do not seeinthissketchisA I. Does that mean that AI has nothing to do
with data science or even ML? We answer this question in the following.
2.1.2 The Relation to Artificial Intelligence
The relationship between the three buzzwords artificial intelligence, machine
learning, and deep learning probably needs some additional explanation. These
terms are everywhere today, very often they are used more or less interchangeably
today, and AI seems to be quite useful for marketing purposes. However, there is a
clear distinction, which will be discussed below.
AI methods are about mimicking human behavior, which requires certain cog-
nitive skills such as reasoning and logic. In addition, it is generally agreed that
an important component of AI is also the ability to interact with the environment
(see, e.g., the seminal text by Russell and Norvig [31]). “Interacting” encompasses
perceiving the environment and acting on the environment. AI does not automati-
cally imply that the program is intelligent in exactly the same sense as a human.


================================================================================
PAGE 40
================================================================================

2.1 WhatIsDataScienceandHowIsItRelatedtoMachineLearningand... 17
It is sufficient that it “looks like” it is solving the task in a way that would require
intelligent behavior.
By contrast, ML focuses heavily on learning algorithms and learning from
data examples. How is ML related to AI? Sometimes one comes across graphical
representations that suggest that ML is a subset of AI (e.g., by showing three
circles/sets where AI contains ML, which in turn contains DL, i.e., AI ⊃ ML ⊃
.
DL). Sometimes this is even understood in the sense that ML is a specialization of
AI, which is clearly a misinterpretation. ML is one of the means of AI to achieve its
goal. Reasoning and logic, while important disciplines of AI, are not part of ML.
Here are two examples of non-ML approaches that would nevertheless contribute
to AI: one example is a rule-based system in which, for example, a hundred
human experts define complex “if-then-else” clauses, enabling a system to make
complex decisions that can easily be considered “intelligent.” Another example is
genetic algorithms, which perform optimization tasks using algorithms that mimic
evolutionary processes.
2.1.3 From Machine Learning to Deep Learning
DL is a further specialization of ML as it uses a particular type of ML models,
namely, deep neural networks inspired by the structure and function of the brain.
The attribute “deep” results from the fact that such a network contains many layers
of artificial neurons, sometimes hundreds and more, as opposed to a shallow neural
network , which contains only very few of such layers. Again, DL is one of the
methods that contribute to an artificial intelligence “agent,” but it should not be
called AI.
In this text, Part III is dedicated to the “classical” ML methods, which have
their roots quite strongly in statistical analysis, while Part IV is dedicated to the
introduction of DL methods that are rather inspired by the biological neurons.
2.1.4 Domain Knowledge
Another key “ingredient” to data science, shown in Fig.2.1 and often neglected, is
domain knowledge. Not only does data science require data; most of which comes
from a specific application domain; also, many of the most interesting data science
questions originate from a specific problem, such as a scientific or technological
or societal problem, where data science is hoped to provide answers and new
insights. Although a large number of “off-the-shelf” methods can be used, the really
interesting problems are those where the methods need to be tailored to the specifics
of the problem, and combining them with domain-specific insights is an ideal (and
very fun) way to do this.


================================================================================
PAGE 41
================================================================================

18 2 FromDataSciencetoMaterialsDataScience
2.2 Data Science and Machine Learning in Materials Science
and Engineering
Combining data science with domain-specific aspects from materials science leads
us to define materials data science :
Definition 2.1 (Materials Data Science). Data science is positioned at the
interface between machine learning and a large part of statistics and classical
data analysis as well as data visualization. All this is additionally connected
by methods from computer science and programming. The application of
data science approaches to materials science problems or the tailoring of
data-based solutions to the specifics of such problems is what materials data
science is all about.
Such a close coupling between a domain science and data science can be observed
in many other fields as well, such as bio data science or geo data science.
When the focus is not only on data analysis and data science aspects, but
when also the technological aspects, e.g., compute or storage facilities, play a
more important role, then one can rather refer to names such as bioinformatics,
materialsinformatics,orgeoinformatics.However,allthesearestillrelativelyyoung
disciplines.
How and when did materials science and data science meet? There is no clear
point in time when this happened; rather statistical methods and data visualization
have always been present in some form, similar to many other engineering and
natural science disciplines. However, there are contributions that clearly stand out,
for example, by attempting to uncover hidden structures in data or fusing datasets
from different sources and performing “meta-studies.”
The Definition of Machine Learning?! (cid:2)
But wait, there is a definition of materials data science, which is based on
machine learning—where is the definition of machine learning (or artificial
intelligence)? For now, we just have to go with our intuition and on what was
said in Sect.2.1.2. A proper definition will be given in Chap.11.
2.2.1 Materials Data Science Without Machine Learning?
One of the best known examples in materials science are the material property
charts , which have become an often used way of communicating what the current


================================================================================
PAGE 42
================================================================================

2.2 DataScienceandMachineLearninginMaterialsScienceandEngineering 19
state of, e.g., a class of alloys is in terms of their properties and how this new alloys
might be superior to others. The material property charts are also known as Ashby
maps [2, 3], named after MICHAEL ASHBY. These are scatterplots that plot two
material properties (e.g., Young’s modulus and density) on two axes and categorize
each point by specifying the material class (e.g., metals, polymers, ceramics) as the
color of the point. In a next step, clusters of points belonging to the same material
class are then grouped and visualized as a colored area, as shown in Fig.2.2. The
motivation of such a representation of material properties was described in [3] as
follows: “the resulting charts are helpful in many ways. They condense a large
body of information into a compact but accessible form; they reveal correlations
between material properties that aid in checking and estimating data; and they lend
themselves to a performance-optimizing technique, [...], whichb ecomes the basic
step of the selection procedure.”
This contains a number of interesting ideas: the charts are not just a representa-
tion of the raw data; rather, they attempt to “distill” and visualize certain patterns
in the data. Guided by both the engineering knowledge and intuition, ASHBY has
chosen the most important pairs of properties, e.g., Young’s modulus vs. density
or strength vs. density, because in an engineering design process, the first quantity
is usually to be maximized and the second one minimized. In the context of ML
this is referred to as feature engineering. Additionally, this is supported by double
logarithmic scaling of the axes to make details visible at different scales. Sometimes
other information was also included, showing, e.g., guideline values for minimum
Fig. 2.2 Ashby map showing the location of different material classes in the density-strength
space. The light-colored “bubbles” indicate subclasses, e.g., within the gray metals region there
are the subclasses of, e.g., pure metals, super-alloys, or amorphous metals. (Adapted from [1]
under the CC-BY 4.0 license, see4 )
4 https://creativecommons.org/licenses/by/4.0/


================================================================================
PAGE 43
================================================================================

20 2 FromDataSciencetoMaterialsDataScience
weight design. The charts have a clear purpose: to facilitate the materials selection
decision process. The Ashby maps are an excellent example of a data science
approach that does not require ML.
2.2.2 A Brief Overview of Machine Learning in Materials Science
Nevertheless, ML methods have become increasingly important over the last three
decades, and materials scientists have been using neural networks since at least
the 1990s. One of the earliest publications is Ghaboussi et al. [16] with the title
“Knowledge-based modeling of material behavior with neural networks.” Initially,
neural networks were mainly used for simple optimization tasks and for “parameter
fitting” or parameter optimization. However, progress was relatively slow, and it
took until in the early 2000s for a number of conceptual and technical as well as
societal changes to take place.
The awareness that data science can be a means to transform research data into
even more valuable “knowledge” also went hand-in-hand with various “digitization
aspects” of materials science, such as in the integration of data analysis, modeling,
and experimentation [17, 26], and the increasing importance of the FAIR data
principles5 for research data[40]. This is also reflected in the funding programs of
many funding agencies and has led to new initiatives that address research data
management and data analysis with the goal of initiating a non-incremental change
in the way scientific research is conducted.
One such example is the Materials Genome Initiative ,6 which also increased the
awareness about theusefulnessofmaterialsinformatics, research data management,
and materials data science.
One of the first publications in this context by K. RAJAN [28] discussed the
challenges and opportunities of materials informatics and advocated the goal of
knowledge discovery by integrating high-quality materials science data, correlation
analysis, and validation through established scientific theories and experiments.
Not surprisingly, the field of high-throughput computational materials design, in
particular, has been very active from the beginning in realizing large databases
containing the computed thermodynamic and electronic properties of materials and
also analyzing them with statistical and ML methods [9]. Another, still growing
application field is machine learning of interatomic potentials for atomistic simula-
tions [5]. Interatomic potentials govern the interaction forces between atoms. They
are important components of atomistic simulations, and as the potential need to be
evaluated for every atom, computational performance is an issue. ML potentials can
produce high-quality and chemically accurate datasets faster (by several orders of
5 Data is denoted as “FAIR” if it is Findable, Accessible, Interoperable, and Reusable. This is seen
as an important prerequisite for making research data machine-readable and machine-actionable,
which could also be of great benefit to ML and data science.
6 https://www.nist.gov/mgi


================================================================================
PAGE 44
================================================================================

2.2 DataScienceandMachineLearninginMaterialsScienceandEngineering 21
magnitude) than density functional theory calculations, enabling the simulation of
larger systems.
Mesoscale simulation methods and simulations of defects have also benefited
from ML-based analyses [27,32,33]. The list of interesting computational materials
science topics and problems addressed in the spirit of data science is, of course,
incomplete and could easily be expanded; for other relevant reviews of materials
informatics and materials data science in the context of computational materials
science, see, e.g., [14,29,36,39].
The microscopy community benefits equally from the developments in the field
of data science, ML and DL. “Classical” data mining approaches are able to extract
patterns from microscopy images, e.g., [11,19] or a set of frames of movies acquired
during an in situ transmission electron microscopy (TEM) experiment, for example,
[20,34,42]. In the field of microscopy, DL has brought about great progress.
In particular, the development of the so-called convolutional neural networks and
generative adversarial networks (all of which will be introduced in Chap.19) has
initiated an ongoing revolution in the field of image analysis. Applications range
from segmentation of objects (such as atoms, defects, or grains, see, e.g., [18,21,
37]), to image denoising and restoration of, e.g., diffraction patterns (see [4,12] and
references therein) to resolution enhancement [35] and feature tracking. All of this
is becoming increasingly accessible to a wider audience [38]. Overviews are given
in [15,41].
2.2.3 Example: Linkage of Analysis Methods for Data Mining of
Microscopy Images
Anexample ofamorecomplex dataanalysisworkflowformicroscopydataisshown
in Fig.2.3. A SiC crystal was grown and a wafer was cut. The wafer was then treated
with KOH (potassium hydroxide, which is a strong base) so that dislocations that
penetrate the surface become visible as dark etch pits.
Interesting questions are as follows: What does the spatial distribution of
dislocations look like? What is their character (e.g., are they threading edge,
threading screw, or mixed dislocations)? What is their Burgers vector? What is the
total number of different types of dislocations? Can the spatial structure be related
to the growth conditions of the SiC crystal?
An X-ray topography scan of the entire wafer was performed in individual
steps. Subsequently, the individual images of a complete wafer are automatically
stitched together using the Python image processing library “openCV” [7]. From
this image, smaller image sections were extracted containing the dark etch pits and
were automatically classified as “good” and “bad” image sections (the bad ones
contain multiple etch pits or other defects).
Etch pits look different depending on the dislocation character and the length
of the Burgers vector. To automatically identify the different orientations of the
hundreds of thousands of images, dimensionality reduction using Uniform Manifold
Approximation and Projection (UMAP)[22] was performed. This method transforms


================================================================================
PAGE 45
================================================================================

22 2 FromDataSciencetoMaterialsDataScience
Fig. 2.3 Schematic of a data science workflow for extracting previously inaccessible information
from microscopy images. Three different machine learning methods are connected. (Unpublished
own work together with P. Wellmann and B. D. Nguyen [24])
the images into a different, two-dimensional space where images with similar
“features” (whatever that means exactly!) are close to each other. Note that the two
“coordinates” have no physical meaning. Next, the etch pits were divided into three
groups (threading screw, edge, and mixed dislocations) by automatic clustering. In
the figure, the coloring of the group elements indicates the orientation. This can
now either be used to create a high-quality training dataset for supervised learning
(where for each image the orientation is also known), or it can be used as input for
further microstructural analysis. In Fig.2.3 we show an example of computing the
local strain energy density distribution for a given defect structure (taken from [23]).
This example illustrates a number of aspects. First of all, a solid command
of classical computer vision methods (here used as preprocessor for stitching
and cleaning up the images) is very handy and avoids manual work. Then the
combination of different ML methods can be very useful. However, this requires
a certain understanding of the individual methods. Last but not least, answering
the domain-specific scientific questions to gain new knowledge is the goal and
requires at least a basic understanding of the domain or good communication with
the domain specialist in order to create this data analysis “pipeline.”


================================================================================
PAGE 46
================================================================================

2.3 FromDataandInformationtoKnowledge 23
2.3 From Data and Information to Knowledge
In the previous sections, we have already used the terms data, information, and
knowledge several times. What is commonly referred to as data plays a crucial role
in all natural sciences and engineering, and data science even contains data in its
name. Nevertheless, a clear and unambiguous definition is perhaps needed. We now
begin with the information science perspective on data and information and then
introduce the terms and definitions as they are used in this book.
2.3.1 “Data and Information” from the Point of View of
Information Science
In information science, the terms and concepts of data, information, and knowledge
play a fundamental role, but even there it is difficult to find a clear answer: for
example, a recent overview article [43] lists no less than 130 definitions for these
3 terms. Therefore, for this book, we have chosen a definition that follows the
definitions in [10,30,43] and best fits our intuitive understanding:
Definition 2.2 (Data). Data are symbols, the properties, or “raw” facts of
objects, events, and their environment. They are obtained from observations
ormeasurements made by scientistsand other people without (orvery limited)
contextual interpretation. Data are often quantitative and are presented in
numbers, but can also consist of non-numerical symbols or statements.
Data itself, without context, contains no information—it is just a collection of
facts. For example, a collection of numbers contains no meaning unless there is
context or additional information about the purpose of the numbers or the particular
ordering of the numbers. So what is required to make data useful? Data must be
organized and structured, and it must have context. This brings us to one of the
many possible definitions of information, which depends on the way how the term
“data” is defined:
Definition 2.3 (Information). Information is what is created when data is
organized, structured, processed, or analyzed, giving relevance to a particular
purpose or context; it is of value to the person who is to use the information.
Information is, for example, in the form of answers to questions.
Information can lead to decisions and trigger actions. Information itself, on another
level, can again be seen as data that needs to be further organized and processed to


================================================================================
PAGE 47
================================================================================

24 2 FromDataSciencetoMaterialsDataScience
obtaininformation.Thismakescleardefinitionsdifficult.Therefore,inExample2.1,
a number of examples are shown to elucidate these two concepts in the context of
materials science and engineering.
Example 2.1 (Data and Information) (cid:2)
Here are some examples of situations relevant to materials science:
• A spreadsheet of numbers: Here, every number is a raw data value; all data
are organized in rows and columns. Statistics summarizes and analyzes
data, turning it into information, e.g., you can learn about the mean value
of all data, which tells you something about the circumstances under which
the data was collected.
four.xls
three.ods
two.xls
one.ods
• The “data stream” from a video camera: Here you should distinguish
between the raw data, which could be the electrical signals, and the lightly
processed data in the form of files that contain a lot of numbers. Looking
at the latter file without knowing that it could be a video stream with a
certain compression type makes it almost useless. Adding context, e.g., in
the form of metadata (a comment line with a file description or a reference
to the file/compression format used), turns it into information, e.g., when a
histogram of brightness is computed.
• A text file with three columns of comma-separated numbers contains
data—the numbers and the comma, which is an important detail. If the
scientific context is that these data were recorded during a tensile test,
this is already interesting information, but may not be enough to turn the
data into information. We may need the additional hint that these are time-
force-displacement data given in physical units (e.g., in micro-Newtons
and nanometers)—another important distinction: data are just numbers,
while information often contains units.
four.csv
three.dat
two.txt
one.csv


================================================================================
PAGE 48
================================================================================

2.3 FromDataandInformationtoKnowledge 25
In information science, data and information are often viewed as parts of a
hierarchy of at least four different terms or concepts, which are typically visualized
in the form of the so-called DIKW pyramid (cf. [30] and references therein), shown
in Fig.2.4. There, data and information are accompanied by the two notions of
knowledge and wisdom. The origin of this hierarchy is most probably T.S. Eliot’s
poem The Rock in 1934 [13], which contains the lines:
Where is the wisdom that we have lost in knowledge?
Where is the knowledge that we have lost in information?
Not surprisingly, there are also a number of different definitions for the two terms
“knowledge” and “wisdom,” which might be even more difficult to harmonize. In
general,knowledgeisbasedoninformationandreferstothe“general understanding
and awareness garnered from accumulated information, tempered by experience,
enabling newcontextstobeenvisaged.,”toquoteadefinitionsfrom[43].Knowledge
enables one to act and make decisions based on information. Finally, the transition
to wisdom requires an understanding of the underlying principles, relationships,
and mechanisms that enable one to answer “why” questions, and wisdom has a high
level of abstraction.
These concepts and notions appear in many published works in the fields of
information science/systems and knowledge management and have proven useful
for basic classification and evaluation of objects, actions, and tasks in these fields
and their meaning and value. We now turn to the question of whether and how this
is also relevant to materials science and data science.
Fig. 2.4 The “DIKW pyramid,” showing the hierarchy of the four information science concepts
Data, Information, Knowledge, and Wisdom. DIKW (or at least IKW) is believed to have originated
from the play “The Rock” by T. S. Eliot in 1934. It symbolizes that each term is based on the terms
underneath. The top of the pyramid and the final goal is “wisdom”


================================================================================
PAGE 49
================================================================================

26 2 FromDataSciencetoMaterialsDataScience
2.3.2 “Data and Information” from the Point of View of Data
Science
From the previous section, it is clear that the information science definition of “data”
and “information” differs from our colloquial use of the terms and from the way
the terms are understood and used in materials science and engineering and even
the field of data science. When we talk about data, we usually imply that it is
not raw data, but that it is already organized and structured and that the context
is clear—even if most people are not aware of the latter. Context is usually defined
by describing the circumstances in which the data was created. This includes that
we usually know what the data “means,” e.g., in terms of physical quantities and the
units used.
2.4 The Curse of Dimensionality
Many problems in science and engineering depend on many different parameters,
some of which are known and—worse—some of which may be completely
inaccessible. Traditional computer simulations and theoretical models often use
idealizations to reduce the problem of interest to a manageable substitute problem.
The concept of a digital twin, which is a digital representation of an experimental
sample or device, instead seeks to capture as many details and parameters as
possible. This might look particularly promising in the context of ML approaches,
which should be able to handle large datasets and parameter sets with ease and
sometimes even be able to make predictions without knowing the underlying
physical parameters.
While this is at least partially true, there is a serious problem directly related
to the increase of parameters or the number of dimensions in general, known as
the curse of dimensionality, an expression coined already in the 1950s by the
mathematician R. E. BELLMAN [6]. We will explain this based on a simple case
study given in Example 2.2.
Example 2.2 (Parameter Study) (cid:2)
Imagine that we can perform a total of 35 experiments in which we can vary
any number of parameters, such as the magnitude of a force, a strain rate,
temperature, and so on. Suppose that for each parameter we decide that eight
different values are sufficient to capture a representative range of values. How
many parameters can you exhaustively explore with the 35 experiments?
Of course, the variation of only one parameter, e.g., the temperature, can
be performed. But what about two parameters? Then there would be already
8×8 = 64possible combinations (also see Chap.6) and 35 experiments are
.
not enough. For three parameters we would need 83 = 512experiments and
.
so on.


================================================================================
PAGE 50
================================================================================

2.4 TheCurseofDimensionality 27
In this example, it is clear that the more parameters are investigated, the more
data points are needed, and this number grows exponentially. A visualization can
be found in Fig.2.5, where now 35 points are randomly selected. Then their
representation in a one-, two-, and three-dimensional (parameter) space is shown.
We observe that with increasing dimensionality, the (parameter) spaces become
more and more “empty.” In the two-dimensional case, we can still say that there
is enough data to interpolate and thus fill the missing data. However, for the three-
dimensional case, this might already seem too bold. This is a very simple example
of the curse of dimensionality.
The consequences can be severe: on the one hand, we try to include more and
more parameter and variables, trying to represent as many details as possible; on
the other hand, this reduces the “degree of data coverage,” i.e., discretizing each
dimension into p individual intervals and keeping the number data points k constant;
in the best case, we would have a coverage ofk/(pn)where n is the dimensionality
.
of the considered space—the “degree of coverage” decreases exponentially with
discretization having the greatest impact.
8 8
7
7
6
6 5
5 4
4 3
2 3 1
0 1 2 3 4 5 6 7 8 2 0
1 7 8
0 5 6
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8 0 1 2 3 4
8
8
7 6 7 6 7
6 5 5
5 4
4 4 3
3 2 3
0 1 2 3 4 5 6 7 8 2 1 2 0 1
1 8
7
0 6
5
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8 0 1 2 3 4
lexov
rep
stniop
1D 2D 3D
stniop
atad
)smargotsih(
atad
dennib
Fig. 2.5 A dataset consisting of 35 records (the blue markers), shown in a one-, two-, and three-
dimensional space. The panels at the bottom are “density plots” and the colors correspond to the
number of data points in the 1D interval, the 2D pixel, or the 3D voxel similar to a histogram. Only
the non-empty pixel/voxel are shown


================================================================================
PAGE 51
================================================================================

28 2 FromDataSciencetoMaterialsDataScience
How can we deal with this problem? In the field of data science and machine
learning, there are a number of methods and techniques that help to reformulate the
problem. These include methods for dimensionality reduction, which we will cover
in Chap.15. There, the problem at hand is transformed into a lower-dimensional
representation that is then (hopefully) easier to be solved. Another approach to
mitigatingthecurseofdimensionalityisthe design of experiment (DoE),whichhelps
to plan in advance how to sample a potentially large parameter space.
2.5 Summary and Conclusion
This chapter has given a very broad overview of a range of different aspects
pertaining to data science and machine learning with a particular view on materials
science. We also tried to give a first idea about data and in particular how “raw data”
can be turned into something even more useful, i.e., information and knowledge.
Therefore, the general scene is set now, and in the next chapter, we will take a closer
look at “data” from a very practical perspective.
References
1. Material comparison strength vs density (licensed under cc by 4.0, https://creativecommons.
org/licenses/by/4.0/deed.en. URL https ://commons.wikimedia.org/wiki/File:Material-
comparison--strength-vs-density_plain.svg.
2. M. F. Ashby. Overview no. 80: On the engineering properties of materials. Acta metallurgica,
37(5):1273–1293, 1989.
3. M. F. Ashby. Materials Selection in Mechanical Design. Elsevier, 4 edition, 2011. ISBN
978-1-85617-663-7. DOI https://doi.org/10.1016/c2009-0-25539-5.
4. L. Banko, P. M. Maffettone, D. Naujoks, D. Olds, and A. Ludwig. Deep learning for
visualization and novelty detection in large x-ray diffraction datasets. npj Computational
Materials, 7(1), July 2021. DOI https://doi.org/10.1038/s41524-021-00575-9.
5. J. Behler. Perspective: Machine learning potentials for atomistic simulations. The Journal of
chemical physics, 145(17):170901, 2016.
6. R. Bellman. Dynamic Programming. Dover Publications, 1957. ISBN 9780486428093.
7. G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.
8. L. Cao. Data science. ACM Computing Surveys, 50(3):1–42, June 2017. DOI https://doi.org/
10.1145/3076253.
9. S. Curtarolo, G. L. Hart, M. B. Nardelli, N. Mingo, S. Sanvito, and O. Levy. The high-
throughput highway to computational materials design. Nature materials, 12(3):191–201,
2013.
10. A. Debons, E. Horne, and S. Cronenweth. Information Science: An Integrated View.
Professional librarian series. G.K. Hall, 1988.
11. B. L. DeCost, M. D. Hecht, T. Francis, B. A. Webler, Y. N. Picard, and E. A. Holm.
Uhcsdb: ultrahigh carbon steel micrograph database: tools for exploring large heterogeneous
microstructure datasets. Integrating Materials and Manufacturing Innovation, 6:197–205,
2017. DOI https://doi.org/10.1007/s40192-017-0097-0.
12. J. M. Ede. Deep learning in electron microscopy. Machine Learning: Science and Technology,
2(1):011004, 2021.
13. T. Eliot. The Rock. Faber & Faber, London, 1934.


================================================================================
PAGE 52
================================================================================

References 29
14. K. Frydrych, K. Karimi, M. Pecelerowicz, R. Alvarez, F. J. Dominguez-Gutiérrez, F. Rovaris,
and S. Papanikolaou. Materials informatics for mechanical deformation: A review of
applications and challenges. Materials, 14(19):5764, Oct. 2021. DOI https://doi.org/10.3390/
ma14195764.
15. M. Ge, F. Su, Z. Zhao, and D. Su. Deep learning analysis on microscopic imaging in materials
science. Materials Today Nano, 11:100087, Aug. 2020. DOI https://doi.org/10.1016/j.mtnano.
2020.100087.
16. J. Ghaboussi, J. H. Garrett, and X. Wu. Knowledge-based modeling of material behavior with
neural networks. Journal of Engineering Mechanics-asce, 117:132–153, 1992.
17. S. R. Kalidindi and M. D. Graef. Materials data science: Current status and future outlook.
Annual Review of Materials Research, 45(1):171–193, July 2015. DOI https://doi.org/10.1146/
annurev-matsci-070214-020844.
18. C. Kusche, T. Reclik, M. Freund, T. Al-Samman, U. Kerzel, and S. Korte-Kerzel. Large-area,
high-resolution characterisation and classification of damage mechanisms in dual-phase steel
using deep learning. PLOS ONE, 14(5):e0216493, May 2019. DOI https://doi.org/10.1371/
journal.pone.0216493.
19. M. I. Latypov, M. Kühbach, I. J. Beyerlein, J.-C. Stinville, L. S. Toth, T. M. Pollock, and
S. R. Kalidindi. Application of chord length distributions and principal component analysis
for quantification and representation of diverse polycrystalline microstructures. Materials
Characterization, 145:671–685, 2018. DOI https://doi.org/10.1016/j.matchar.2018.09.020.
20. S. Lee, M. J. Duarte, M. Feuerbacher, R. Soler, C. Kirchlechner, C. H. Liebscher, S. H. Oh,
and G. Dehm. Dislocation plasticity in fecocrmnni high-entropy alloy: quantitative insights
from in situ transmission electron microscopy deformation. Materials Research Letters, 8(6):
216–224, 2020a.
21. S. Lee, A. Vaid, J. Im, B. Kim, A. Prakash, J. Guénolé, D. Kiener, E. Bitzek, and S. H. Oh.
In-situ observation of the initiation of plasticity by nucleation of prismatic dislocation loops.
Nature communications, 11(1):1–11, 2020b.
22. L. McInnes, J. Healy, and J. Melville. UMAP: Uniform Manifold Approximation and
Projection for dimension reduction. The Journal of Open Source Software, 3(29):861, 2020.
DOI https://doi.org/10.21105/joss.00861,
23. B. D. Nguyen, M. Roder, A. Danilewsky, J. Steiner, P. Wellmann, and S. Sandfeld. Automated
analysis of X-ray topography of 4H-SiC wafers: Image analysis, numerical computations, and
artificial intelligence approaches for locating and characterizing screw dislocations. Journal of
Materials Research 38:1254–1265, 2023. DOI https://doi.org/10.1557/s43578-022-00880-z.
24. B. D. Nguyen, J. Steiner, P. Wellmann, and S. Sandfeld. Combining unsupervised and
supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer, 2024. DOI
https://doi.org/10.48550/arXiv.2402.13353.
25. OpenAI. Gpt-4 technical report, 2023.
26. A. Prakash and S. Sandfeld. Chances and challenges in fusing data science with materials
science. Practical Metallography, 55(8):493–514, 2018. DOI https://doi.org/10.3139/147.
110539.
27. M. H. Rafiei, Y. Gu, and J. A. El-Awady. Machine learning of dislocation-induced stress
fields and interaction forces. JOM, 72(12):4380–4392, Oct. 2020. DOI https://doi.org/10.
1007/s11837-020-04389-w.
28. K. Rajan. Materials informatics. Materials Today, 8(10):38–45, 2005. ISSN 1369-7021.
DOI https://doi.org/10.1016/S1369-7021(05)71123-8. URL h ttps://www.sciencedirect.com/
science/article/pii/S1369702105711238.
29. R. Ramprasad, R. Batra, G. Pilania, A. Mannodi-Kanakkithodi, and C. Kim. Machine learning
in materials informatics: recent applications and prospects. npj Computational Materials, 3(1),
Dec. 2017. DOI https://doi.org/10.1038/s41524-017-0056-5.
30. J. Rowley. The wisdom hierarchy: representations of the dikw hierarchy. Journal of
Information Science, 33(2):163–180, 2007. DOI https://doi.org/10.1177/0165551506070706.
31. S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, 3 edition,
2010.


================================================================================
PAGE 53
================================================================================

30 2 FromDataSciencetoMaterialsDataScience
32. M. Sarvilahti, A. Skaugen, and L. Laurson. Machine learning depinning of dislocation pileups.
APL Materials, 8(10), 10 2020. ISSN 2166-532X. DOI https://doi.org/10.1063/5.0020376.
101109.
33. D. Steinberger, H. Song, and S. Sandfeld. Machine learning-based classification of dislocation
microstructures. Frontiers in Materials, 6, 2019. ISSN 2296-8016. DOI https://doi.org/10.
3389/fmats.2019.00141.
34. D. Steinberger, I. Issa, R. Strobl, P. J. Imrich, D. Kiener, and S. Sandfeld. Data-mining of in-
situ TEM experiments: Towards understanding nanoscale fracture. Computational Materials
Science, 216:111830, Jan. 2023. DOI https://doi.org/10.1016/j.commatsci.2022.111830.
35. R. Strack. Deep learning advances super-resolution imaging. Nature Methods, 15(6):403–403,
May 2018. DOI https://doi.org/10.1038/s41592-018-0028-9.
36. I. Tanaka, K. Rajan, and C. Wolverton. Data-centric science for materials innovation. MRS
Bulletin, 43(9):659–663, 2018.
37. P. Trampert, D. Rubinstein, F. Boughorbel, C. Schlinkmann, M. Luschkova, P. Slusallek,
T. Dahmen, and S. Sandfeld. Deep neural networks for analysis of microscopy images—
synthetic data generation and adaptive sampling. Crystals, 11(3), 2021. ISSN 2073-4352. DOI
https://doi.org/10.3390/cryst11030258. URLh ttps://www.mdpi.com/2073-4352/11/3/258.
38. L. von Chamier, R. F. Laine, J. Jukkala, C. Spahn, D. Krentzel, E. Nehme, M. Lerche,
S. Hernández-Pérez, P. K. Mattila, E. Karinou, S. Holden, A. C. Solak, A. Krull, T.-O.
Buchholz, M. L. Jones, L. A. Royer, C. Leterrier, Y. Shechtman, F. Jug, M. Heilemann,
G. Jacquemet, and R. Henriques. Democratising deep learning for microscopy with Zero-
CostDL4mic. Nature Communications, 12(1), Apr. 2021. DOI https://doi.org/10.1038/s41467-
021-22518-0.
39. L. Ward and C. Wolverton. Atomistic calculations and materialsinformatics: A review. Current
Opinion in Solid State and Materials Science, 21(3):167–176, 2017.
40. M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Axton, A. Baak,
N. Blomberg, J.-W. Boiten, L. B. da Silva Santos, P. E. Bourne, J. Bouwman, A. J. Brookes,
T. Clark, M. Crosas, I. Dillo, O. Dumon, S. Edmunds, C. T. Evelo, R. Finkers, A. Gonzalez-
Beltran,A.J.Gray,P.Groth,C.Goble,J.S.Grethe,J.Heringa,P.A.’tHoen,R.Hooft,T.Kuhn,
R. Kok, J. Kok, S. J. Lusher, M. E. Martone, A. Mons, A. L. Packer, B. Persson, P. Rocca-
Serra, M. Roos, R. van Schaik, S.-A. Sansone, E. Schultes, T. Sengstag, T. Slater, G. Strawn,
M. A. Swertz, M. Thompson, J. van der Lei, E. van Mulligen, J. Velterop, A. Waagmeester,
P. Wittenburg, K. Wolstencroft, J. Zhao, and B. Mons. The FAIR guiding principles for
scientific data management and stewardship. Scientific Data, 3(1), Mar. 2016. DOI https://
doi.org/10.1038/sdata.2016.18.
41. C. Zhang, H. Song, D. Oliveros, A. Fraczkiewicz, M. Legros, and S. Sandfeld. Data-mining
of in-situ tem experiments: On the dynamics of dislocations in CoCrFeMnNi alloys. Acta
Materialia, 241:118394, 2022. DOI https://doi.org/10.1016/j.actamat.2022.118394.
42. C. Zhang, H. Song, D. Oliveros, A. Fraczkiewicz, M. Legros, and S. Sandfeld. Data-mining
of in-situ tem experiments: On the dynamics of dislocations in cocrfemnni alloys. Acta
Materialia, 241:118394, 2022.
43. C. Zins. Conceptual approaches for defining data, information, and knowledge. Journal of the
American Society for Information Science and Technology, 58(4):479–493, 2007. DOI https://
doi.org/10.1002/asi.20508.


================================================================================
PAGE 54
================================================================================

What You Should Know About Data, Math, and 3
Computing
It is a capital mistake to theorize before one has data.
Sir Arthur Conan Doyle (1859–1930),
creator of the character Sherlock Holmes
3.1 General Conventions and Notations
Let us start with a brief overview of the most important mathematical conventions
and notations used in this text. All used abbreviations and acronyms are listed on
page xxv. As we do not expect the reader to move linearly through this text from
the beginning to the end, we introduce the used acronyms for each chapter anew.
Concerning mathematical notations, we will make extensive use of scalars, vec-
tors, matrices, and other objects. We mainly stick to conventions that are commonly
used in engineering and physics. But since there is not the one convention, we state
the most important ones here:
Definition 3.1 (Basic Definitions and Notations)
(cid:129) Scalars, i.e., regular numbers, are given by italic Greek or Latin letters,
e.g., a =1.423or λ=2.
. .
(cid:129) Vectors are usually written as lowercase, bold, and italic Latin characters,
e.g., a and x, but sometimes we also encounter Greek letter, e.g., as in
. .
σ =[0.5,0.4,0.9].
.
(cid:129) Matrices are indicated by uppercase, sans-serif, bold Latin characters, e.g.,
Aor X.
. .
(continued)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 31
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_3


================================================================================
PAGE 55
================================================================================

32 3 WhatYouShouldKnowAboutData,Math,andComputing
(cid:129) The sets of real and integer numbers are denoted by R and Z, respec-
. .
tively. The latter consists of all positive as well as negative integers
including zero. The natural numbers Nare the set of positive integers.
.
(cid:129) Functions depend on variables, e.g., in f(x) the function is f and x is
.
the variable. To specify fixed parameters or constants p ,p , we use a
. 1 2
semicolon and write, e.g.,
f(x; p ,p )=p +p x2 with p =0.5andp =2.0.
. 1 2 1 2 1 2
This reads “function f depends on x for given parameters p and p .” An
. 1 . 2
alternative notation is f (x).
. p1,p2
(cid:129) Different types of parentheses and brackets need to be differentiated:
()are called round brackets or parentheses; []are square brackets or just
. .
brackets, as opposed to curlybrackets orbraces{}; the last type areangular
.
brackets, (cid:2)(cid:3), which are also sometimes called chevrons.
.
(cid:129) Python code is written with a light-gray background, e.g.,
print('x =', x) .
3.2 Sets, Tuples, Vectors, and Arrays
This section introduces definitions of sets, tuples, lists, and arrays. Our definitions
are not formal mathematical definitions but are rather definitions of how the terms
are commonly used and what the notations are. As many of these terms are used
across different disciplines, it is important to settle on “common grounds.” We also
show short Python examples as we believe that it is useful for the reader to see the
mathematical notation right next to the practical implementation. For those reader
who want to acquire some foundation in Python, e.g., [1,2] are two of many good
starting point for that.
3.2.1 Sets
When distinct items are to be collected in a group, the members of this group are
called elements. To describe the group, the mathematical set can be used, which is
defined as follows:


================================================================================
PAGE 56
================================================================================

3.2 Sets,Tuples,Vectors,andArrays 33
Definition 3.2 (Set) A mathematical set is an unstructured collection of dif-
ferent elements without repetitions. Elements can be, e.g., numbers, variables,
symbols, or also other sets. We indicate a set by curly braces, and elements
are separated by commas. For example, the set of the numbers 7, 1, 4, 2, and
8 is written as {7,1,4,2,8}.
.
7
1 2
4 8
A set can also be defined by describing the properties of the elements of the set S,
e.g.,
S ={n|n∈N, nisanoddnumber,and1≤n≤10} (3.1)
.
which consists of two parts: to the left of |1 stands a variable name, which is then
.
used to the right of |to describe properties. Equation (3.1) is pronounced “The set
.
of all n where n is an odd integer number between 1 and 10”—in other words,
S ={1,3,5,7,9}.
.
The fact that the set is an unstructured collection implies that the ordering of the
elements does not matter. The concept of a set also exists in Python. For example,
two sets of numbers can be compared by
In [1]: set([3, 4, 5]) == set([4, 3, 5, 3])
Out [1]: True
which here shows that both sets are identical despite the double occurrence of the
number 3, as a set does not contain duplicate elements (note the phrase “without
repetitions” in Definition 3.2). Additionally, the ordering of the elements in a set
does not matter (or rather: there is no order in a set).
Set Operations
A number of basic mathematical operations on sets exist, which can be visualized
using Venn-diagrams (closed geometrical shapes that may overlap and contain
certain items, used to visualize set operations). Assuming that A and B are sets,
then the most important operations between them are:
1 Sometimes, instead of the vertical line “. | ,” a colon “:” is used in definitions of sets.


================================================================================
PAGE 57
================================================================================

34 3 WhatYouShouldKnowAboutData,Math,andComputing
Fig. 3.1 Venn diagrams for three operations between the sets .A = 5{,3,1,8,6} and .B =
{9,5,0,6,2}are shown as the two circles in each plot. The shaded area of the left panel shows
the union of A and B, the shaded area of the middle panel is their intersection, and the right panel
shows the difference.B\A
(cid:129) Union: The union A∪B is the set containing all elements of A and B.
.
(cid:129) Intersection: The intersectionA∩B are those elements that occur both in the set
.
A and in B.
(cid:129) Difference: The difference A\B (or alternatively written as A−B)isthesetof
. .
all elements that are in A but not in B.
A special set is the empty set {}, which contains no elements. The empty set is
.
commonly denoted by ∅. If for two sets A and B their union is A∪B = ∅, then
. .
A and B are called disjoint sets. A visualization of sets, represented as a closed
curve, and some mathematical operations are shown as Venn diagrams in Fig.3.1.
From these visualizations, it can be seen that the union of A and B is A∪B =
.
{1,8,3,5,6,9,0,2}, their intersection is A∩B = 5{,6}, and the difference “B
.
without A”isB\A ={9,0,2}.
.
Python already has the most common set operations built in (and as always, there
is more than just one way of achieving the desired goal; see the comments in the
following code lines):


================================================================================
PAGE 58
================================================================================

3.2 Sets,Tuples,Vectors,andArrays 35
3.2.2 Tuple
In many situations, however, the ordering of elements is important. In these cases
an “ordered list” is useful. In math this is called a tuple:
Definition 3.3 (Tuple) A tuple is a collection of elements such as numbers,
variables, or symbols that occur in an ordered manner, where repetition
matters. This is indicated by parenthesis, and elements are separated by
commas. For example, the tuple of the letters m, d, and s is written as
(m,d,s). Sometimes also square brackets are used, e.g., [4,5,5,1].
. .
4 5 5 1
In Python there exist two slightly different data types, both of which correspond to
the mathematical definition of a tuple from Definition 3.3: the Python tuple and the
Python list . The difference between the two Python data types is that the values
of the elements of a list can be altered or elements can be deleted while a tuple
is “immutable,” i.e., the content cannot be changed.
As opposed to the set, Python’s tuple and list do consider repeated elements,
as shown in this example:
In [1]: list([1, 3, 5]) == list([1, 3, 3, 5, 5])
Out [1]: False
This line of code compares two lists, returning False as they are not the same.
The lists have a different number of elements; if they were contained in a set,the
same comparison would yield True . Note that in Python a list is also created by
[1, 3, 4] and a tuple is created by either tuple([1, 3, 5]) or just (1, 3, 5) .


================================================================================
PAGE 59
================================================================================

36 3 WhatYouShouldKnowAboutData,Math,andComputing
A number of operations exist for a Python list , such as appending or removing
elements or counting or sorting elements of a list. The Python tuple is quite
similar to a Python list concerning “read-only”—operations that do not change the
content. We will often encounter lists (and less often also tuples) in implementations
shown in this text.2
3.2.3 Intervals
Beware of how intervals, i.e., ranges of numbers, are written—some intervals also
use square brackets or parentheses and can be confused with tuples. To make the
difference clear, we also give the definition of an interval:
Definition 3.4 (Interval) An interval is denoted by [a,b] and contains all
.
numbers between the start and end points a and b, including both the start
point and the end point. To indicate that the start or end point are not included
in the interval, then a parenthesis is used for this point instead of the square
bracket.
For example, in the context of real numbers, (a,b)denotes the open interval of all
.
numbers x with a <x <band [a,b)denotes the half-open interval of all numbers
. .
x with a ≤x <b. The notation also applies for intervals of integer numbers. From
.
the context it should always be clear if [a,b]denotes a closed interval or a list.
.
Integer interval A similar notation using square brackets or parentheses also holds
for integer numbers while in that case the start and end values are always included.
Therefore, we prefer a more explicit notation using an ellipsis, e.g., instead of(i,i+
.
3), we write(i,...,i+3), which denotes the tuple of numbers(i,i+1,i+2,i+3).
. .
Alternatively, square brackets can be used. However, as opposed to intervals of real
numbers, a mixture of square and round brackets would not denote a half-open
interval and is therefore virtually never used.
In Python an integer interval is also called a range and is used, e.g., to specify or
index elements in a vector or matrix, as introduced below.
2 A good place for someone with intermediate Python skills for learning more about many features
of Python is the documentation. It can be found online at https://docs.python.org/.


================================================================================
PAGE 60
================================================================================

3.2 Sets,Tuples,Vectors,andArrays 37
Words of advice (cid:2)
(cid:129) In mathematics, an integer interval includes both the start and end point, while
for intervals of real numbers, this is not necessarily the case!
(cid:129) In mathematics, an interval of real numbers follows a different notation, e.g.,
in .(2,4), the end points are both excluded. Here, the context is important (i.e.,
knowing that these should be real values).
(cid:129) Examples:
.Itis (i,i+3)= (i,...,i+3)=(i,i+1,i+2,i+3)
and [−2,2]=[−2,...,2]=[−2,−1,0,1,2]
but [1.0,5.5) ={x|1≤ x <5.5}.
(cid:129) Python follows—for various reasons—a different convention: in the Python com-
mand range(i, i+3) , the end point is excluded by default, and range(4)
includes the numbers 0, 1, 2, and 3 as Python is “zero-based.”
3.2.4 Matrices and Row and Column Vectors
Is there a relation between tuples or lists and vectors? The answer is that they are
not related, at least not mathematically spoken, because vectors are the elements of
a vector space, which is a completely different story.
However, there is a related meaning in the context of matrices. Matrices play a
central role in linear algebra and can be used to represent systems of linear equations
(and we will extensively make use of that in the context of optimization in later
chapters). Matrices are also used in the spirit of “containers” for (typically) two-
dimensional data such as tabular data. But prior to that, we now give a definition of
what a matrix is.
Definition 3.5 (Matrix) A matrixA is an ordering of real-valued elements
.
into m rows and n columns
⎡ ⎤
a a ··· a
11 12 1n
⎢ ⎢a 21 a 22 ··· a 2n ⎥ ⎥
. A=⎢ ⎣ . . . . . . ⎥ ⎦ (3.2)
. . .
a a ··· a
m1 m2 mn
(continued)


================================================================================
PAGE 61
================================================================================

38 3 WhatYouShouldKnowAboutData,Math,andComputing
where an element in an arbitrary row number i (counted from the top) and
column number j (counted from the left) is denoted by a . The a are
. ij . ij
typically real numbers but can also be integer numbers. The matrix in Eq. (3.2)
is called a m×nmatrix.
.
A number of mathematical operations exist, such as matrix multiplication, addition
of two matrices, multiplication of a matrix with a scalar, to name a few. An
introduction of the most important rules and details can be found in Appendix A.
Also in Python, a number of methods for creating and processing of matrices
exist that are compatible with the above-introduced notations. E.g. a 3×4 matrix
.
consisting of zeros can be created using the numpy package by
In [1]: import numpy
A = numpy.zeros((3, 4))
A
Out [1]: array([[0., 0., 0., 0.],
[0., 0., 0., 0.],
[0., 0., 0., 0.]])
The element in the second row and third column can then be accessed by A[1, 2]
(note that Python indices start at 0, while indices in maths and MATLAB® start at
1). The indexing can be used to set a value of an element (e.g., A[1, 2] = 1.2)as
well as to retrieve its value (e.g., print(A[1, 2]) ):
In [2]: import numpy
A[1, 2] = 1.2
A
Out [2]: array([[0., 0., 0., 0.],
[0., 0., 1.2, 0.],
[0., 0., 0., 0.]])
Row and column vectors appear in numerous situations in machine learning
(ML). Additionally, the scalar product of two vectors as it is used throughout this
text requires the differentiation between row and column vectors.3 We define row
and column vectors as certain “sub-matrices” of a matrix as follows:
3 Having to differentiate between row and column vectors is a side effect of the chosen notation.
While other, more elegant and general notations exist, it turns out that this is the most accessible
approach at least for those who do not have a strong mathematics background in tensor calculus.
Additionally, the used notation is compatible with Python implementations.


================================================================================
PAGE 62
================================================================================

3.2 Sets,Tuples,Vectors,andArrays 39
Definition 3.6 (Row and Column Vectors) Given a matrix A as in Defini-
.
tion 3.5 a row vector is defined as a 1 × n sub-matrix of A. In analogy, a
. .
column vector is defined as a m×1sub-matrix of A. Furthermore:
. .
(cid:129) a row vector is written in the same way as a list, for example:
v =[a , a , ··· , a ]
. 21 22 2n
(cid:129) a column vector is given by the following expression:
⎡ ⎤
a
11
⎢ ⎥ (cid:8) (cid:9)
. w =⎢ ⎣ a · 2 · 1 · ⎥ ⎦ = a 11 , a 21 , ··· , a m1 T
a
m1
where the [·]T in the last expression is the transposition symbol, which
.
exchanges rows with columns. The commas in both expressions are written
in analogy to the above definition of a list but are often dropped for brevity.
In other words, a row or column vector consists simply of all elements of a particular
row or column of a matrix. Appendix A contains more general information about
notation in linear algebra. Python has a straightforward way of denoting a full row
or column using either the Ellipsis object ( ... ) or the colon ( : ), e.g., to set all
elements of row 2 to the value 3, one can write
Similarly, A[:, 1] or A[..., 1] addresses the elements of the second column.
However, care needs to be taken concerning the dimensions of the resulting object,
which is a one-dimensional array.


================================================================================
PAGE 63
================================================================================

40 3 WhatYouShouldKnowAboutData,Math,andComputing
Words of advice (cid:2)
Extracting a row or column from a matrix .A with m rows and n columns by, e.g.,
A[:, 1] results in a one-dimensional array, and it doesnot result in a column vector
with.m×1elements (which might be counter-intuitive, at a first glance).
This can be seen by looking at the “shape” of the result, i.e., the result’s dimensions,
e.g., A[:, 1].shape gives m,and A[2, :].shape gives n , which indicates one-
dimensional arrays with m elements for the first case and n elements for the second
case.
Use the function numpy.reshape to turn a one-dimensional array into a row
or column vector, e.g., the one-dimensional array c = numpy.array([1, 2, 3])
is turned into a two-dimensional matrix with three rows and one column by
numpy.reshape(c, (3, 1)) .
In the explicit formulation of most machine learning (ML) problems, a number
of other super- and subscripts are present. As this can make it difficult to directly
understand the notations, clear and practical definitions are particularly useful.
Therefore, in close analogy to Python or MATLAB® syntax, we introduce the
following short form for row and column vectors:
Definition 3.7 (Short Notation for the i-th Row or Column Vectors)
Given a matrix Awith the component a as in Definition 3.5, we introduce
. . pq
the following short notation for indexing using a colon (:):
(cid:129) for the i-th row vector: a i,: =[a i1 , a i2 , ··· , a in ],
(cid:129) for the i-th column vector: a:,i = [a 1i , a 2i , ··· , a mi ]T
In this mathematical definition, the indices start at 1. Note, however, that Python
indices always start at 0. Sometimes, when mathematical expressions get more
complicated, we will surround the above indices by brackets for better readability
(e.g.,
.
a[:,i]), but this will be explicitly mentioned.
With the above definitions and conventions, we are now ready to take the
next step and to investigate how data used in the context of statistics and ML is
represented.


================================================================================
PAGE 64
================================================================================

3.3 RepresentationofDatainStatisticsandMachineLearning 41
3.3 Representation of Data in Statistics and Machine Learning
The data that we are interested in for statistical analysis, data mining, or ML can
come in various forms: it could be time series data of a scalar quantity (such as
measurements of temperature over time), it can be vector-valued field data measured
at discrete points such as a strain field measured at points of an equidistant grid, or
it could be unstructured data, e.g., represented by a network-like structure.
We begin by introducing some general aspects of the mathematical shape of
typical problems with particular view on the data and data structures. Based on
this, a number of definitions are given that will form the basis of nearly all of the
following chapters.
3.3.1 General Problem Formulation
Regardless of the particular data type at hand, many of the physics and data science
problems considered in later chapters can be reduced to a simple albeit abstract
mathematical form:
y = f(x) (3.3)
. .
⇔ (y ,...,y ) = f(x ,...,x ), (3.4)
1 m 1 n
i.e., some input that is somehow related by a function to some output. An example
for a function with three input variables and one output is y = f(x,x ,x ), and
. 1 2 3
an example for a function with a single input and two output variables would be
(y ,y )= f(x).
. 1 2
Words of advice (cid:2)
Note that in Eq. (3.4), we use the typical notation from mathematical analysis where
it is not distinguished between row and column vectors. Furthermore, Eq. (3.3) has,
strictly speaking, a single function argument (the vector .x ), but Eq. (3.4) has n of
them. Most of the time, the meaning is clear from the context. However, when it
comes to implementing such equations in a computer program, we have to be careful
and will be as explicit as possible.
In Eq. (3.3), the variable x is an input variable possibly consisting of more
.
than one components x . The output variable y also may consist of a number of
. i .
components y . Both, x and y, may have different number of elements, depending
. j . .
on the respective problem under consideration.


================================================================================
PAGE 65
================================================================================

42 3 WhatYouShouldKnowAboutData,Math,andComputing
The Names of Variables ... (cid:3)
In mathematics, the x are also called independent variables, which suggests
. i
that they cannot be obtained as the result of any function of other variables.
However, in statistics and data analysis, there is no requirement of “indepen-
dency of input variables” (in fact, often input variables are correlated), and
hence, we typically avoid the mathematics notion of “independent variables.”
In statistics or ML, a number of different names other than “input variables”
are given to these two variables; an overview is given in Sect.11.6.5.
The function f can, in principle, take any arbitrary mathematical form, it can
be a linear or a nonlinear function, or it also might “hide” an ordinary differential
equation in which case the input consists of the initial value, while the output
consists of the resulting value after time integration. Furthermore, f need not be
a mathematical function; it even might be a black box model where, e.g., a lookup
table returns for each input the corresponding output. An example explaining the
relation of the input and output data is given in Example 3.1.
Example 3.1 (Problem formulation with input and output variables) (cid:2)
Why is a general mathematical function y = f(x) representative for many
.
problems that we encounter in materials science, engineering, and physics?
We often ask questions such as “What is the temperature at this particular
point?” or “What is the property of the material after a particular heat
treatment of the specimen?.” In both cases there are input variables and output
variables. For example, in the first case, the input variable is a coordinate in
space,(x ,x ,x ), while the output variable is a temperature T . In the second
. 1 2 3
example, the input could be the characteristics of the materials microstructure
after a certain heat treatment, while the output variable is the resulting
property.
If we know how to compute f(x) for any of the two cases, then we are
.
able to predict the temperature or the resulting material property!
Equation (3.3) serves as a template for almost all problems that we consider
throughout this text. To be able to understand and systematically explore the relation
between input data and output data, given by the function f, we will introduce
common terminologies and the relevant data structures in the following.


================================================================================
PAGE 66
================================================================================

3.3 RepresentationofDatainStatisticsandMachineLearning 43
3.3.2 Structured Data
Throughout this text we will be able to represent most of the data in a structured
format as tabular or “array” data. Nonetheless, notations can become complicated
when we need to differentiate between different spatial points, vector-valued
components, and data with different purposes (often indicated by various sub- or
superscripts). Therefore, we start from the very basics by giving definitions and
introduce notions that will be used to describe all kinds of data and related aspects:
the data point,thed ata record, and the dataset.
Definition 3.8 (Data pointP ) A data point is a single observation con-
.
sisting of the input variables x = (x ,...,x ) and the output variables
. 1 n
y = (y ,...,y )where the data pointP is given as a tuple of row vectors
. 1 m .
by
P=(x,y)=([x ,...,x ], [y ,...,y ]). (3.5)
. 1 n 1 m
If there is only one independent or dependent variable (n=1or/and m=
. .
1), then x and/or y reduce to the scalar variables x and/or y.
. .
In the context of experimentally acquired data, a data point is also called measure-
ment; in statistics and ML also the names instances or examples are used. Here is a
simple Python example for defining a data point P1 :
In [1]: import numpy as np
x = np.array([[2, 4, 6, 11]]) # directly write as 2D array
y = np.array([3, -1, 4, 0]).reshape(1, 4) # or: 1 row, 4 columns
P1 = (x, y)
P1
Out [1]: (array([[ 2, 4, 6, 11]]), array([[ 3, -1, 4, 0]]))
We use the script font letterP to indicate that a data point is not just a vector of
.
two elementsxandybut in fact a “compound” ofxandy(a tuple (x,y)). Usually,
. . . . . .
we can assume that each data point is a measurement of the same quantities such that
each data point has the same number of elements resulting in a structured dataset.
In other words, each int (or instance) number i is given byP(i)and has the structure
.
(cid:10)(cid:11) (cid:12) (cid:11) (cid:12)(cid:13)
P(i) = x (i) ,...,x(i) , y (i) ,...,y(i) . (3.6)
. 1 n 1 m
Note that in this equation, the data point consists of n input values (e.g., n = 3
.
spatial coordinates) and m output values (e.g., the m = 6 independent values of a
.
strain tensor), and in general it is not necessary that n=m; see Example 3.3.
.


================================================================================
PAGE 67
================================================================================

44 3 WhatYouShouldKnowAboutData,Math,andComputing
We can now define the dataset, a notion that we so far have used without properly
having defined it:
Definition 3.9 (DatasetD) A datasetD is the collection of all n data points
. .
Pthat belong to a statistical, computational, or real experiment. Assuming
.
that the ordering matters, the dataset is the tuple
(cid:10) (cid:13)
D= P(1), ...,P(n) . (3.7)
.
In continuation of the above Python code snippet, here is an example that details the
definition of the dataset:
In [2]: P2 = ... # need to be defined in analogy to P1
D = (P1, P2)
D
Out [2]: ((array([[2, 4, 6, 1]]), array([[ 3, -1, 4, 0]])),
(array([[3, 7, 0, 2]]), array([[ 5, -2, 4, 1]])))
Assuming that all data points have the same structure as in Eq. (3.6), a dataset can
easily be stored in a table or represented as a matrix of data. This is also motivated by
the previous Python example where we can already see that there are two “blocks”
of data, and each new data point adds a new row.
What if the data is not structured? (cid:3)
There may exist situations where the number of input features or outputs
is changing, e.g., if in a molecular dynamics (MD) simulation with a grand
canonical ensemble the number of atoms changes and atomic positions are
recorded, if the experimental measurement points are rearranged, or if in a
finite element analysis (FEA) the mesh is refined and the number of element
and thereby the number of, e.g., displacement values changes. In these cases
an averaging or “resampling strategy” (e. g., interpolation at regularly spaced
points) can be used to represent the data in a data matrix. In all other
cases, it even might become necessary to change, e.g., details of the planned
experiment.


================================================================================
PAGE 68
================================================================================

3.3 RepresentationofDatainStatisticsandMachineLearning 45
Example 3.2 (Data Points and Dataset) (cid:2)
Assume a metallic rod that has a length l = 1mand on which in equidistant
.
points n=5temperature sensors are attached. Their location is x =l·(i−
. . i
1)/(n−1)or x = [0,0.25,0.5,0.75, 1m], which is the input variable. Each
.
temperature sensor has as output a single value, the temperature T . Hence,
. i
the output variable is y = T[ , ..., T ]. The data point consists of the five
. 1 5
input and output values. Taking altogether four measurements (e.g., after a
certain time) then results in a dataset that consists of 4 pointsP(1),...,P(4),
.
each of which having ten items.
3.3.3 Tabular Data and the Data Matrix
After having introduced what a dataset is from a rather mathematical point of view,
we now turn to the more technical aspect of how to represent or to store such
structured data. The most common approach is to contain the data in form of a table
or array where each row contains, e.g., a single measurement, the data record, which
is sometimes also called instance or example. Writing the data records in rows on
top of each other results in tabular data shown in Fig.3.2. The shown dataset consists
of m records in rows, e.g., obtained from measurements. It has n input variables and
p output variables in columns, which result in them×(n+p)data matrix. The input
.
variables X , X , etc. are also called the features of the dataset. Often, the number
. 1 . 2
of outputs is much smaller than the number of input variables, and in many cases,
there is even only one output. In some ML methods (the so-called unsupervised
learning methods), the output data is even missing entirely. Example 3.3 illustrates
these notations and conventions with an example of how to create the data table for
an experiment.
Input Output
··· ···
1 2 1 2
Record No. 1 ( (1)) : ··· ···
11 12 1 11 12 1
Record No. 2 ( (2)) : ··· ···
21 22 2 21 22 2
. . . . . . .
. . . . . . .
. . . . . . ··· .
Record No. m ( ( )) : ··· ···
1 2 1 2
Fig. 3.2 Tabular data consisting of n input variables (or features) and p output variables.
Altogether m data records are contained in this dataset where record number i is given by the
data pointP . (i). The blue matrix of input data is denoted by .X , and the orange matrix of output
data is denoted by.Y


================================================================================
PAGE 69
================================================================================

46 3 WhatYouShouldKnowAboutData,Math,andComputing
Example 3.3 (Representation of a Measurement Dataset (Cooling of a
Specimen)) (cid:2)
An experiment investigates how the temperature during a cooling down
process of a specimen changes as a function of time. Besides temperature, also
the air pressure is recorded as there might be a correlation between pressure
and temperature. We obtain a measurement every 10s over a period of 1min.
For the first half of the measurement period, the pressure values are 1010hPa,
at t ≥ 30sand the values are 1013hPa. The temperature drops linearly from
.
180
◦
Catarateof1
◦
Cs
−1.
. .
Questions: (i) What are the independent and dependent variables? (ii) Write
down all components of the dataset in the same way as shown in Fig.3.2. (iii)
What is the data record number 2? (iv) What is the first feature? (v) What is
the input record number 3?
Answers: (i) Time and pressure are the independent variablesX andX , and
. 1 . 2
temperature is the dependent variable Y (or just Y). (ii) The table has seven
. 1
rows including the starting point in time:
Input/Features Output
X X Y
. 1 . 2
◦
Record No. timeins pressure in hPa temperature in C
.
1: 0 1010 180
2: 10 1010 170
3: 20 1010 160
4: 30 1010 150
5: 40 1013 140
6: 50 1013 130
7: 60 1013 120
(iii) Record number 2 is the data pointP (2) = [10,1010,170]; (iv) the
.
first feature is time: X = [0,10,...,60]T; (v) the input record number 3 is
. 1
the row vector x =[20,1010].
. 3


================================================================================
PAGE 70
================================================================================

3.3 RepresentationofDatainStatisticsandMachineLearning 47
Fig. 3.3 Sketch of the data matrix and the different types of variables. Each of the boldface .x i
denotes a possibly multidimensional data record (a row vector), the non-boldface.X jand.Y kdenote
several records or measurements of a scalar input or output variable (column vectors)..X and.Y are
sub-matrices containing all input and output data, respectively
3.3.4 Matrix and Vector Short Forms for Linear Algebra
Based on the dataset shown in Fig.3.2, we now introduce and summarize variables
and notations that will be frequently used throughout this text. These short forms for
special vectors and sub-matrices are in particular useful for concise linear algebra
formulations such as vector-matrix products, which will turn out to be helpful for
formulations as well as numerical implementations in statistics and ML. A sketch of
these quantities can be found in Fig.3.3.
First, we take a look at the individual feature X and output variable Y . Both
. i . j
of them are concerned with scalar quantities (e.g., X could be the first Cartesian
. 1
coordinate of an object and Y its temperature). However, we almost never operate
. 1
with just a single measurement or data point. Instead, we usually have several
measurements because, e.g., we record the position of an object as a function of time
as one of the input variablesX . This is why any featureX is in fact represented by
. i . i
a column vector of values [x , x , ...](cf. Example 3.3, question (iv)).
. 1i 2i
If we, on the other hand, take a look at the m-th data record, then this is given
by the vector x = [x , x , ...], e.g., this could be coordinates of an object
. m m1 m2
or point or even coefficients of quaternions (which are used to describe texture in
crystal plasticity). This is in fact a vector value that also explains the boldfacex as
. m
opposed to the non-boldface X (cf. Example 3.3, question (v)).
. j
Words of advice (cid:2)
Beware of the notations:.x iis a row vector (the i-th record), but.Xjis a column vector
(all records of the j-th scalar feature). Even though the boldface.x i/non-boldface.X j
convention might seem unusual, this notation is consistent with the commonly used
one in probability theory and statistics, and it represents the fact that any value of.X j
is a scalar, while.x ican be the three-dimensional coordinate of a point, i.e., a vectorial
quantity.


================================================================================
PAGE 71
================================================================================

48 3 WhatYouShouldKnowAboutData,Math,andComputing
Given these column and row names, we can now summarize this in terms of
vector notation. The data of a feature variable is specified by:
Definition 3.10 (All records of a specific input or output variable) The
variable of the k-th feature or input variable is X . The data is given by the
. k
column vector
.
X
k
=[x
1k
,...,x
mk
]T =x:,k . (3.8)
In analogy, all data of the r-th output variable is given by the column vector
.
Y
r
=[y
1r
,...,y
mr
]T =y:,r . (3.9)
The colon in
.
x:,k and
.
y:,k denotes all rows, similar to numpy notation. In
case that there is only one feature, we just write X or Y.
As before, please refer to Figs.3.2 and 3.3 for a visualization of these conventions.
The i-th data record or data point consists of input and output values:
Definition 3.11 (Specific data record or data point) The record number i
consists of the input data x = [x , x ,...,x ]and the output data y =
. j i1 i2 in . i
[y ,...,y ]. It is written in a compact form as
i1 ip
(cid:14) (cid:15)
P(i) = x ,y . (3.10)
. i i
Often it is useful to directly refer to the matrix of all input or of all output values.
This allows for a very concise formulation in terms of matrix-vector calculus and is
also beneficial for the compute time. Here is the definition of the input data matrix
or the feature matrix, as we will call it:
Definition 3.12 (Feature Matrix or Input Data Matrix:) The feature
matrixXcontains the data of all n features and is a m×nmatrix given by
. .
⎡ ⎤ ⎡ ⎤
x 11 ··· x 1n | |
. X= ⎢ ⎣ . . . ... . . . ⎥ ⎦ or: X=⎣ X 1 ···X n ⎦ . (3.11)
x ···x | |
m1 mn


================================================================================
PAGE 72
================================================================================

3.3 RepresentationofDatainStatisticsandMachineLearning 49
The feature matrix is in the statistics literature also called design matrix.Theo utput
data matrix is defined in full analogy:
Definition 3.13 (Output Data Matrix) The output data matrix, Y contains
.
the data of all p output variables and is a m×pmatrix given by
.
⎡ ⎤ ⎡ ⎤
y 11 ··· y 1p | |
. Y= ⎢ ⎣ . . . ... . . . ⎥ ⎦ or: Y=⎣ Y 1 ···Y p ⎦ . (3.12)
y ···y | |
m1 mp
And finally, the whole dataset is represented by the data matrixD:
.
Definition 3.14 (Data Matrix) The data matrix contains the whole dataset
and is given by the list of all data points or equivalently by the concatenation
of the feature matrix and the output data matrix:
(cid:8) (cid:9)
D=P(1) ···P(n) T =[X,Y] , (3.13)
.
which has as many rows as there are data records, and the number of columns
is the sum of the number of input features and the number of output values.
A Python example for how to use tabular data is given in Example 3.4, which is
the continuation of Example 3.3.
Example 3.4 (Representation of a Measurement Dataset (Part II)) (cid:2)
[This is the continuation of Example 3.3.] In this example we show how we
can use numpy to create the different variables and matrices. We start by
creating the feature vectors and the target vector as one-dimensional arrays:
In [1]: import numpy as np
X1 = np.array([0, 10, 20, 30, 40, 50, 60], dtype=float)
X2 = np.array([1010, 1010, 1010, 1010, 1013, 1013, 1013], dtype=float)
Y = np.array([180, 170, 160, 150, 140, 130, 120], dtype=float)
(continued)


================================================================================
PAGE 73
================================================================================

50 3 WhatYouShouldKnowAboutData,Math,andComputing
We convert them to column vectors, i.e., two-dimensional arrays with only
one column. The argument of reshape is the new shape of the array. “ . − 1”
can occur only once: then the value is automatically determined from the total
number of elements and the other dimensions.
In [2]: X1 = X1.reshape(-1, 1) # The argument of reshape is the dimension of
X2 = X2.reshape(-1, 1) # the reshaped array. 1 = 1 column
Y = Y.reshape(-1, 1) # -1 : as many rows as necessary
X1
Out [2]: array([[ 0.],
[10.],
[20.],
[30.],
[40.],
[50.],
[60.]])
Next, we “assemble” the feature matrix Xand the data matrixD :
. .
In [3]: X = np.hstack((X1, X2)) # concatenate X1 and X2 horizontally
D = np.hstack((X, Y)) # concatenate X and Y horizontally
D
Out [3]: array([[ 0., 1010., 180.],
[ 10., 1010., 170.],
[ 20., 1010., 160.],
[ 30., 1010., 150.],
[ 40., 1013., 140.],
[ 50., 1013., 130.],
[ 60., 1013., 120.]])
And finally, we use our variables for two queries (the questions (iii) and (v)
from Example 3.3):
In [4]: print("P(2) =", D[1]) # second record
print("x_3 =", X[2]) # third record of feature matrix
Out [4]: P(2) = [ 10. 1010. 170.]
x_3 = [ 20. 1010.]
Note that Python indexing starts at zero. Therefore, D[1] gives in fact the
second record.


================================================================================
PAGE 74
================================================================================

3.5 Exercises 51
3.4 Summary and Conclusion
This chapter is the technical foundation for the rest of this book. An important goal
was to motivate, clearly define, and explain all used notions and concepts that are
required to describe what a dataset is and to explain how we can interact with it
using Python or mathematical descriptions. We saw that tabular data is among the
most important data “structures” and introduced a number of notations that will be
used throughout the whole book. The following box summarizes some of the most
important aspects:
Things to Remember (cid:3)
1. Given an arbitrary matrix A that consists of the elements.a ij.Then...
(cid:129) the i-th row is given by.ai,: =[ai1,ai2,...].
(cid:129) the j-th column is given by.a:,j =[a1j,a2j,...]T.
2. The featurematrix.X consists of the input data in a “table-like” structure where
each row is a data record and each column is a feature:
(cid:129) the n-th feature is .Xn = [x1n,x2n,...,xmn ]Twhere m is the number of
records.
(cid:129) the k-th input data record is .xk = [xk1,xk2,...,xkn ] where n is the
number of features.
3. The output Matrix follows the same conventions as the feature matrix and is
represented by.Y .
4. The data matrixD . consists of the feature matrix .X and the output matrix .Y such
thatD
.
=[X,Y].
3.5 Exercises
3.1 (Sets) Write a Python script that calculates the three set operations shown in
Fig.3.1.
3.2 (Lists) Given are two lists, the first of which is a = 4[,8, 1,−3,5] and the
second b = 4[,3,2,1]. Use Python to show that the two lists are not identical.
Next, print the elements that the two lists have in common as well as those elements
that are only in a and only in b.
3.3 (Intervals) Use numpy’s function linspace to create the interval of real
numbers given by {x|1 <x ≤2.2 AND x =1+ 0.2n AND n ∈ N}.


================================================================================
PAGE 75
================================================================================

52 3 WhatYouShouldKnowAboutData,Math,andComputing
Table 3.1 The whole Input/Features X Output Y
dataset consisting of feature
matrix X and output matrix Y
X1 X2 X3 X4 Y1 Y2
3 7 9 4 1 −2
4 0 3 2 7 7
6 4 2 0 10 0
9 0 6 6 18 15
3.4 (Row and Column Vectors) Create a matrix A that consists of four rows and
three columns and contains only 1 as element values. Then set all elements of
column number 2 to 2, and set element in row 1 and column 2 to 4. Show the first
row of the resulting matrix. Compute the sum of all elements of A.
3.5 (Calculating with Input and Output Matrices) Given is a data matrix feature
X and output Y in Table 3.1. Also given is a vector-valued function f
⎛⎡ ⎤⎞
. f :X(cid:12)→Y ⇔ f ⎝⎣ X . . . 1 ⎦⎠= (cid:20) 2 X X 1 − − 2 X X 3 + + X x 4 (cid:21) = (cid:20) Y Y 1 (cid:21)
1 2 3 2
X4
Note that, in this case, the function also could be written as matrix multiplication
of X with a coefficient matrix. The following questions about X, Y and f help to
understand the structure of the used variables, matrices, and the mathematical and
computational operations:
1. How many records, feature variables, and outputs does the dataset have? Which
is the value of the third record of the second feature?
2. Write down X as numpy array and print the number of rows and columns.
3. How can a numpy function and array operations be used to compute Y?
References
1. C. Hill. Learning ScientificProgramming withPython. Cambridge University Press, 2nd edition.
ISBN 978-1108745918.
2. Q. Kong, T. Siauw, and A. M. Bayen. Python Programming and Numerical Methods. Elsevier,
2021. ISBN 978-0-12-819549-9. DOI https://doi.org/10.1016/c2018-0-04165-1.


================================================================================
PAGE 76
================================================================================

4
Materials Science Datasets and Data
Generation
4.1 Introduction
Machine learning (ML) and data science depend heavily on data. We need data to
explain mathematical relations, to investigate statistical properties, and, last but not
least, to train our ML models.
While every data scientist should know a few of the famous machine learning
datasets, such as the “Iris flower dataset” and the “MNIST dataset of handwritten
digits” (introduced below as DS-1 and DS-2), it is necessary to also have datasets
that have a direct relevance and interpretation to our scientific field of work—
materials science and the neighboring disciplines. For such datasets, we already
have some intuition about the meaning of certain aspects or variables. These are the
datasets introduced as MDS-1 up to MDS-5.
We intentionally introduce these datasets at this early stage, even before we
have introduced how to statistically characterize and describe datasets in general.
As a consequence, there are very few “summary statistics” and similar statistical
descriptions, because in one of the next chapters, we will use the datasets to
introduce what a summary statistic is. We will also describe and explore different
aspects of the datasets at various points in this textbook. For additional information
on the details of the datasets and the datasets themselves, the reader is referred to the
supplementary webpage (https://MDS-book.org). A summary of the datasets and
how they are used in this book can be found in Table 4.1. Additionally, in a number
of places throughout the text, the datasets are used in the text for explanations.
4.2 Dataset MDS-1: Tensile Test with Parameter Uncertainties
A tensile test is one of the most commonly used methods in materials science
to determine the effective mechanical material response of a whole specimen.
Although the details of the underlying grain, defect, or phase microstructures of
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 53
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_4


================================================================================
PAGE 77
================================================================================

54 4 MaterialsScienceDatasetsandDataGeneration
Table 4.1 Overview of the datasets and the usage or purpose
Used in
Dataset page section Purpose or Aspect Used
MDS-1 (Tensile Test) 240 11.2.2 instance-based learning
246 11.5.2 outlier detection
271 12 regression
356 13.8 regression (exercises)
452 16.4 baseline models
493 17.6 neural networks (exercises)
MDS-2 (Ising Model) and MDS-2 light (Ising Model, small)
208 10.2 distribution types
430 15.6.1 dimensionality reduction (PCA, t-SNE)
528 18.8 classification, regression (DL)
533 19.1 network architecture, counting weights
MDS-3 (Phase Microstructure Evolution with the Cahn-Hilliard Model)
432 15.6.2 dimensionality reduction (PCA, t-SNE)
449 16.2.6 cross-validation
MDS-4: Chemical Elements 204 9.5 visualization (EDA)
435 15.7 PCA, t-SNE, cluster analysis
437 16.1.1 feature importance
MDS-5: Nanoindentation 202 9.4.4 advanced data visualization (EDA)
433 15.6.3 cluster analysis (GMM)
DS-1: Iris Flowers 180 9.2.1 initial exploration (EDA)
202 9.4.4 plot with histograms (EDA)
175 8.7.4 correlation matrix
366 14.3 classification
391 15.2 dimensionality reduction (PCA)
DS-2: Handwritten Digits (MNIST) and DS-2 light (Alpaydin Digits)
389 15.1 dimensionality reduction ...
409 15.3 ... and reconstruction with PCA
435 15.7 dim. reduction (exercises)
521 18.5.2 classification (DL)
a sample are critical to its mechanical behavior, their effects are strongly averaged
in typical macroscopic samples of centimeter size. The material parameters obtained
from such tensile tests contain uncertainties that are usually expressed in terms of a
mean value and the standard deviation.
In the following, a synthetic dataset of temperature-dependent stress vs. strain
data is created. For this purpose, the model equations described in Section 4.2
of [11] are used to provide a phenomenological description of the relation between
various material parameters or constants and the stress-strain values. The empirical
model was created for structural steel with a nominal yield stress below 450MPa.


================================================================================
PAGE 78
================================================================================

4.2 DatasetMDS-1:TensileTestwithParameterUncertainties 55
It consists of three terms, the first of which describing the temperature-dependent
modulus of elasticity E:
( [ ( ) ( ) ])
1 AT e1 1 AT e2
E(T)=E · exp − −
. 0
2 e 2 e
3 4
with AT =T −T (4.1)
0
All parameter values used and assumed standard deviations are given in Table 4.2.
The second equation defines the temperature dependency of the yield stress F :
. y
( [ ( ) ( ) ])
1 AT r1 1 AT r2
F (T)=F r +(1−r )·exp − − (4.2)
. y y0 5 5
2 r 2 r
3 4
With these two relations, the true stress σ can now be written as a function of true
.
strain εand temperature T :
.
⎧
. σ = ⎨ ⎩ E F ( ( T T ) ) · + ε ( k −k F ) exp [ − ( T ) k1 ] ( ε−ε (T) ) n f f o o r r ε ε ≥ < ε ε y
y 3 4 y0 k2 y y
(4.3)
where
F (T)
ε = y (4.4)
. y
E(T)
is the temperature-dependent yield strain.
Using the mean values of all material and model parameters given in Table 4.2,
we obtain the average stress-strain response shown as solid lines in Fig.4.1. Each
data point was obtained by drawing random values from normal distributions given
by the mean and standard deviation in Table 4.2, resulting in varying degrees
of noise in the curves. This represents the (assumed) uncertainty with which
parameters used were determined or measured. Of course, this is an academic
example, but it is particularly useful for the following reasons:
. There is a mathematical function that describes the “true” stress-strain curve (i.e.,
the data obtained from the mean values)—and it is known to us.
. The resulting data can be used as a dataset with only one input variable (i.e.,
stress) and one output variable (i.e., strain) for a fixed temperature. It can also
be used with two input variables, e.g., stress and temperature, and one output
variable.
. The dataset can be divided into different parts that can be used separately. For a
simple regression model, for example, one would start with the linear part, while


================================================================================
PAGE 79
================================================================================

56 4 MaterialsScienceDatasetsandDataGeneration
Table 4.2 Parameters used to generate the synthetic stress-strain-temperature data. The mean
value and standard deviation define the range from which a parameter value is randomly selected
Material & model parameters
variable mean value std. dev. Description
.E0 206GPa 7GPa Young’s modulus at ambient temperature
.Fy0 345MPa 12MPa Yield stress at ambient temperature
.T0 20. ◦ C 0. ◦ C Ambient temperature
.e1 .3.768 – Material dependent model parameter for
.e2 .1.0 – Young’s modulus.E (T)
.e3 1639. ◦ C –
.e4 1650. ◦ C –
.r1 .7.514 – Material dependent model parameter for
.r2 .1.0 – Yield stress.F y(T)
.r3 588. ◦ C –
.r4 676. ◦ C –
.r5 .0.09 –
.k1 .7.82 – Material dependent model parameter for
.k2 540. ◦ C – True stress.σ
.k3 1006MPa –
.k4 .0.759 –
n 0.503 0.015 Strain hardening exponent
Fig. 4.1 The solid lines
show the average stress-strain
response, while the markers
show the stress-strain values,
when the material or model
parameter is sampled from a
normal distribution given by
the standard deviations in
Table 4.2
for more complicated models, the whole dataset consisting of both linear and
nonlinear parts, could be used.
. A number of characteristics of the dataset have a physical meaning, e.g., Young’s
modulus, yield stress, etc.
In the following, this dataset will be referred to as the “MDS-1 (Tensile Test)”
dataset. The Python code for producing the data as well as the data itself can be
obtained from the supplementary webpage (https://MDS-book.org).


================================================================================
PAGE 80
================================================================================

4.3 DatasetMDS-2:MicrostructureEvolutionwiththeIsingModel 57
4.3 Dataset MDS-2: Microstructure Evolution with the Ising
Model
The Ising model is a model of statistical physics originally developed by W.
LENZ [8] to represent aspects of ferromagnetism, namely, the evolution of the
ferromagnetic domain walls. ERNST ISING, to whom the model owes its name, was
the first to solve it mathematically for a one-dimensional situation [5]. However,
the two-dimensional case leads to much more complex behavior; in particular, it
can describe a disorder-order phase transition. The Ising model in general has been
studied by theoretical physicists for decades. It is also used as a simulation model
in a number of different use cases in physics and materials science.
The following simulation was performed in the context of predicting the
structure-property relation using ML approaches by NGUYEN et al. [9]. The dataset
is obtained from a two-dimensional simulation model of anN×N lattice (Fig.4.2).
.
A magnetic dipole is stored on each of the lattice points. Neighboring dipoles
interact with each other. The energy of a simplified system is given by the
Hamiltonian
E
H =−J σ σ , (4.5)
. i j
<i,j>
where we assumed that no external magnetic field is applied and that the interaction
strength J = 1 between all neighboring dipoles is the same. The indices i and j
.
include all lattice sites, with numbers ranging from 1 to N2, and with the additional
.
constraint that in the sum only the nearest neighbors (indicated by <i,j>) are
.
considered. σ ∈{−1,1}is the sign of the magnetic dipole at a given site.
.
To simulate such a system with periodic boundary conditions, we use the
Metropolis Monte Carlo algorithm. First, we randomly initialize the spin (up or
down, i.e., +1 and −1) of all dipoles. Then, we randomly select a site s(i,j)with
. . .
i,j ∈ 1,...,N, flip the corresponding dipole, and calculate the energy of the new
.
Fig. 4.2 Sketch of two states of an .N×N lattice during a simulation with the Ising model. The
left panel shows the initial state, where the spin directions (small arrows) are chosen randomly.
After a series of Monte Carlo update steps at a fixed temperature T , the domain walls between the
regions of the two spins become visible


================================================================================
PAGE 81
================================================================================

58 4 MaterialsScienceDatasetsandDataGeneration
Fig. 4.3 Ising model: the left
panel shows the evolution of
the effective magnetization
obtained from the
corresponding microstructure
as a function of temperature.
Examples of microstructures
for three different
temperatures are shown on
theright.Theimagesizeis
.64×64pixel
configuration. If the energy of the new configuration is smaller than the previous
one, we keep the new configuration; if the energy is greater than the previous one,
we only keep it with a probability
( )
AE
p =exp − , (4.6)
.
k T
b
where T is the prescribed temperature of the system, AE the energy difference
.
between the previous and the current state of the system, and k is the Boltzmann
. b
constant, scaled to unity for simplicity. We repeat those steps N3 times and
.
then terminate the simulations. The reasoning behind this stopping criterion is
the following: after N3 steps, every site of the N × N lattice has been visited
. .
approximately N times, so that the information had enough time to pass through the
whole lattice. Without such a stopping criterion, the two “phases” would eventually
merge completely and only one phase would occupy the entire area.
Each of the spin configurations is saved as a black and white image, in which a
value of 0 and 255 denotes a negative and a positive spin, respectively. The images
are generated from within the range of
.
[0, 2T
c
], where the CURIE temperature
.
T
c
(named afte√r the physicist MARIA SKŁODOWSKA-CURIE) is given as
.
T
c
=
2J/(k ln(1+ 2)). With the above scaling of k = 1 and J = 1, we obtain
B . B .
T ≈2.26. Figure 4.3 shows examples of resulting microstructures.
. C
The dataset used for data analysis in this text consists of two variants. The first
variant, MDS-2 (Ising model), consists of images of 64×64 pixel, given as PNG
.
files. Each file is also accompanied by the information regarding the temperature at
which the simulation was performed. A variant of this dataset, MDS-2 light (Ising
model, small), consists of a smaller number images that additionally are only16×16
.
pixel in size.
4.4 Dataset MDS-3: Cahn-Hilliard Model
Binary mixtures, such as alloys of two metals, may coarsen into two phases, a
mechanism that is described by the spinodal decomposition. This phenomenon may


================================================================================
PAGE 82
================================================================================

4.4 DatasetMDS-3:Cahn-HilliardModel 59
occur if an initially homogeneous phase becomes thermodynamically unstable. In
this case, small fluctuations quickly grow, as described by [2].
In contrast to the dataset MDS-2 (Ising model), there is no randomness in the
evolution of this system; it is completely governed by continuum equations, i.e.,
the Cahn-Hilliard equation, which governs the evolution of the concentration of a
phase:
∂c δE
=M ∇2 , (4.7)
. c
∂t δc
with the free energy E and a mobility coefficient of the interfaceM . The free energy
. c
density ψ consists of terms for the potential, gradient, and elastic energy density:
.
ψ =ψbulk+ψgrad+ψel (4.8)
.
where ψbulk = c c2(1 − c)2, ψgrad = 1k |∇c|2 and ψel = 1σ : εel. The
. 0 . 2 c . 2
two constants c and k are the density scale and the gradient energy density,
. 0 . c
respectively. σ is the stress tensor and ε the elastic strain tensor. The energy
. . el
functional is then
f f
1 1
E = ψdo= c c2(1−c)2+ k |∇c|2+ σ :εeldo (4.9)
. 0 c
2 2
o o
The elastic contribution is determined by fulfilling the mechanical equilibrium
equation, ∇·σ =∇·C:εel =0where Cis the fourth-order stiffness tensor.
. .
Finite element method (FEM) simulations with the above equations and with
periodic boundary conditions were carried out to create the dataset. The width and
the height of the domain are both assumed to be 20μm. The elastic constants are
.
assumed to be C = 198GPa, C = 138GPa and C = 97GPa. The density
. 11 . 12 . 44
scale is c = 50·10 −6Jμm −3, and the gradient energy density is k = 10Jμm −1.
. 0 . c
Examples of typical microstructures along with the corresponding total energy as
a function of time are shown in Fig.4.4. There, the “kink” at 2. is caused by
the microstructure undergoing rapid changes and coarsening (note the logarithmic
scaling of the time axis).
The simulation is performed for a duration of 20s with adaptive time stepping.
Initial values are sampled from a uniform random distribution with values between
0 and 1, which is the only reason for the random-looking microstructures since the
evolution equations are deterministic. The dataset consists of the images from the
entire simulation process. Each image contains the phase microstructure encoded as
grayscale values. In addition, there is also a text file containing the time and total
energy value for each image. The image data is exported at every step of the first part
and every 10th step of the second part of the simulation. Therefore, the generated
datasets contain about 60,000 images. For further details and discussions, see the
.
publications by NGUYEN et al. [9].


================================================================================
PAGE 83
================================================================================

60 4 MaterialsScienceDatasetsandDataGeneration
Fig. 4.4 Examples of
microstructures obtained
from the Cahn-Hilliard
model. Three different
snapshots in time are shown
together with the
corresponding values of the
total energy
4.5 Dataset MDS-4: Properties of Chemical Elements
This is a small dataset in which Ferreira et al. [3] collected the four periodic
properties (i.e., atomic radius, electron affinity, ionization energy, and the elec-
tronegativity) of a total of 38 chemical elements (22 metals and 16 non-metals) from
a number of available sources. The data is shown in Table 4.3. When analyzing the
data, one tries to find out whether two of these properties are related or whether it
is possible to predict if a material is a metal or not, based on some given properties.
The data can be simply copied either from Table 4.3 or from the supplementary
webpage (https://MDS-book.org).
4.6 Dataset MDS-5: Nanoindentation of a Cu-Cr Composite
This dataset was obtained during nanoindentation of different Cu-Cr composites
containing 25wt% Cr and 60wt% Cr corresponding to 29.95at% and 64.40at% Cr,
respectively, as well as pure Cu and Cr as reference samples. The aim of this study
was to investigate the extent to which the properties of the two materials can be
distinguished based on indentation experiments. Another question was how many
different “groups” of properties, i.e., elastic modulus and hardness, can be detected.
For example, whether there are only two distinct elastic property sets corresponding
to the two pure metals (because Cu-rich regions should have a different hardness
or elastic modulus than Cr-rich regions) or whether there are more than two such
property groups. The latter would indicate that there are additional effects, e.g.,
interphases and phase boundaries, contributing to the mechanical behavior.
The investigation pertaining to this dataset was published by ZHANG et al. in
[12]; some of the results and the datasets are shown here, with permission of the
authors. Figure 4.5 shows typical microstructures of the composites investigated.
An overview of the dataset is given in Fig.4.6, showing the full dataset as
experimentally acquired including “outlier.” The outliers were eliminated, resulting
in the dataset shown in the right panel of Fig.4.6. For each of the four materials,
the goal was to identify patterns and structures in the data distribution, which can
then be explained on the basis of microstructural aspects. The dataset itself consists


================================================================================
PAGE 84
================================================================================

4.6 DatasetMDS-5:NanoindentationofaCu-CrComposite 61
Table 4.3 Four periodic properties and group for a selection of chemical elements after [3]
Atomic Electron affinity Ionization energy Electro-
Element radius (pm) (kJ mol-1) (kJ mol-1) negativity Group
Metals
Li 1.82 59.63 520.22 0.98 1
Na 2.27 52.87 495.85 0.93 1
K 2.75 48.39 418.81 0.82 1
Rb 3.03 46.88 403.03 0.82 1
Cs 3.43 45.51 375.71 0.79 1
Fr 3.48 44.38 392.96 0.70 1
Be 1.53 . − 66.00 899.51 1.57 1
Mg 1.73 . − 67.00 737.75 1.31 2
Ca 2.31 2.37 589.83 1.00 2
Sr 2.49 4.63 549.47 0.95 2
Ba 2.68 13.95 502.85 0.89 2
Ra 2.83 9.65 509.29 0.90 2
Al 1.84 41.76 577.54 1.61 13
Ga 1.87 41.49 578.85 1.81 13
In 1.93 28.90 558.30 1.78 13
Tl 1.96 36.38 589.35 1.80 13
Ge 2.11 118.94 762.18 2.01 14
Sn 2.17 107.30 708.58 1.96 14
Pb 2.02 35.12 715.60 1.80 14
Sb 2.06 100.92 830.58 2.05 15
Bi 2.07 90.92 702.94 1.90 15
Po 1.97 183.30 811.83 2.00 16
Nonmetals
H 1.10 72.77 1312.05 2.20 1
B 1.92 26.99 800.64 2.04 13
C 1.70 121.78 1086.45 2.55 14
Si 2.10 134.07 786.52 1.90 14
N 1.55 . − 7 1402.33 3.04 15
P 1.80 72.04 1011.81 2.19 15
As 1.85 77.57 944.46 2.18 15
O 1.52 140.98 1313.94 3.44 16
S 1.80 200.41 999.59 2.58 16
Se 1.90 194.97 940.96 2.55 16
Te 2.06 190.16 869.29 2.10 16
F 1.47 328.17 1681.05 3.98 17
Cl 1.75 348.58 1251.19 3.16 17
Br 1.85 324.54 1139.86 2.96 17
I 1.98 295.15 1008.39 2.66 17
At 2.02 270.20 1037.00 2.20 17


================================================================================
PAGE 85
================================================================================

62 4 MaterialsScienceDatasetsandDataGeneration
Fig. 4.5 Cu-Cr composites produced by field-assisted sintering technique with 25wt% Cr shown
in the left panel, and 60wt% Cr shown in the right panel
Fig. 4.6 The experimentally obtained distributions of Young’s modulus E and hardness H for
0wt%, 25wt%, 60wt% and 100wt% Cr content. (Left panel: the experimental data in its original
form. Right panel: cleaned and preprocessed dataset used in the ML analyses (the rectangle in the
left panel indicates the region shown in the right panel)
of tabular data in which the hardness and the elastic modulus recorded for different
indentation depths are presented. The original dataset as shown in the left panel of
Fig.4.6) as well as the already preprocessed dataset with outliers removed (as shown
in the right panel of Fig.4.6) can both be downloaded from the supplementary
webpage (https://MDS-book.org) or obtained from zenodo [13].
4.7 Dataset DS-1: The Iris Flower Dataset
The “Iris Flower Dataset,” sometimes called “Fisher’s Iris Dataset” [4], is one of
the most famous datasets used as an introductory example for ML. It consists of the
measured width and length of petal and sepal of iris flowers (the features) together
with the corresponding name of the species (the label or target). The aim is to find
the name of the species based on the length measurements.
Some exemplary data is shown in the table in Fig.4.7a. There, additional
numbers (the class IDs) have been assigned to the three species names. This
mapping of text labels (e.g., “Iris-setosa”) to integer numbers makes it much easier
for ML models, since such models are based on numbers anyways.


================================================================================
PAGE 86
================================================================================

4.8 DatasetDS-2:TheHandwrittenDigitsDataset 63
Fig. 4.7 The Iris dataset: the table shows an excerpt from the dataset with the class ID as an
additional column. The photograph shows an iris setosa. The smaller violet “leaves” at the top are
the sepals and the larger ones the petals. (a) Extract from the “Iris Plant Dataset”. (b) Example of
an “iris setosa”
The dataset consists of 4 features, 50 records, and a single output variable, i.e.,
the name of the flower. To turn this textual label into a number, we assign an integer
number to each flower species (the class IDs 0, 1, and 2), as shown in Fig.4.7a. The
dataset can be obtained, e.g., through the Python package scikit-learn.1 It is also
available through the supplementary webpage (https://MDS-book.org).
4.8 Dataset DS-2: The Handwritten Digits Dataset
Identifying the correct number displayed in an image with handwritten digits has
long been one of the benchmark tests in ML. ML methods required benchmark
data to compare the performance of the different methods. Therefore, the MNIST
database of handwritten digits [6] was introduced in the 1990s by Lecun et al. [7]
in their seminal paper introducing a revolutionary convolutional neural network for
handwritten digit recognition. Since then this has become one of the standard dataset
for studying and benchmarking different ML approaches. The data itself consists of
70,000 training and test images with labels. The images are 28×28pixels in size,
.
and each contains a handwritten number from the range 0 to 9. For more details, such
as how the dataset was acquired and preprocessed, see Lecun et al. [7]. Figure 4.8
shows a number of examples from this datasets. For some deep learning (DL) tasks,
the dataset is already quite large. Therefore, sometimes a second dataset is used,
consisting of only 1797 examples, and the images are 8×8pixels. The dataset was
.
1 The Python ML package scikit-learn [10] can be found at https://scikit-learn.org


================================================================================
PAGE 87
================================================================================

64 4 MaterialsScienceDatasetsandDataGeneration
Fig. 4.8 The first 27 images
in the MNIST database,
which contains a total of
70,000 examples of
handwritten digits. Each
grayscale image has a
resolution of.28×28pixels
created by E. ALPAYDIN [1] and can also be obtained either from scikit-learn or
from the supplementary webpage (https://MDS-book.org).
The MNIST dataset is referred to as DS-2 (MNIST), while the dataset of
ALPAYDIN is referred to as DS-2-light (Alpaydin Digits).
4.9 Online Resource for Obtaining Training Data
What could you do to find new and exciting datasets for your ML experiments?
There is a wide range of different datasets available on the Internet. However, not
all datasets are well curated or suitable for ML, and only a small fraction of them are
from the field of materials science. Below we list some starting points for searching
for datasets. First, there are the repositories that contain generic ML data:
. The UC Irvine Machine Learning Repository at https://archive.ics.uci.edu/ con-
tains a large variety of documented datasets, including some of the “classical”
datasets. Datasets have a DOI and are referenceable.
. https://dasl.datadescription.com/ is another repository where you can search
different categories of datasets.
. A huge repository of annotated images mainly for DL can be found at https://
www.image-net.org/.
. Another interesting platform is https://paperswithcode.com/. When a particular
ML method has been published, the code and sometimes also the data from it
can often be retrieved there. The website also contains a section of a dataset
collection that can be browsed and searched.
. An alternative to the Iris dataset is the “Palmer Penguins” dataset, currently
hosted at https://github.com/allisonhorst/palmerpenguins.
If you are looking for (materials) science datasets, a little more time is usually
required to find suitable data:
. zenodo.org is a good starting point for published datasets. But these are not
uploaded with the machine learner in mind, so you need some patience and
a bit of browsing. Unfortunately, many people who upload their data have no
particular need for documentation.


================================================================================
PAGE 88
================================================================================

References 65
. Many scientific journals require or at least allow that along with a publication, the
data is also published, either in the form of a separate data publication or in the
form of supplementary material. This can sometimes be helpful, e.g., the journal
npj Computational Materials (https://www.nature.com/npjcompumats/) is one of
the journals where authors are more likely to publish their data and code than
not.
References
1. E. Alpaydin and C. Kaynak. Optical Recognition of Handwritten Digits. UCI Machine
Learning Repository, 1998. DOI https://doi.org/10.24432/C50P49.
2. J. W. Cahn. On spinodal decomposition. Acta metallurgica, 9(9):795–801, 1961. DOI https://
doi.org/10.1016/0001-6160(61)90182-1.
3. J. E. V. Ferreira, M. T. S. Pinheiro, W. R. S. dos Santos, and R. da Silva Maia. Graphical
representation of chemical periodicity of main elements through boxplot. Educación Química,
27(3):209–216, July 2016. DOI https://doi.org/10.1016/j.eq.2016.04.007.
4. R. A. Fischer. The Use of multiple Measurements in Taxonomic Problems. Annals of Eugenics,
7(2):179–188, Sept. 1936. DOI https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.
5. E. Ising. Beitrag zur theorie des ferromagnetismus. Zeitschrift für Physik, 31(1):253–258, Feb
1925. ISSN 0044-3328. DOI https://doi.org/10.1007/BF02980577.
6. Y. LeCun, C. Cortes, and C. J. Burges. The mnist database of handwritten digits. URL http://
yann.lecun.com/exdb/mnist/.
7. Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. DOI https://doi.org/10.1109/
5.726791.
8. W. Lenz. Beitrag zum verständnis der magnetischen erscheinungen in festen körpern. Z. Phys.,
21:613–615, 1920.
9. B. D. Nguyen, P. Potapenko, A. Demirci, K. Govind, S. Bompas, and S. Sandfeld. Efficient
surrogate models for materials science simulations: Machine learning-based prediction of
microstructure properties. Machine Learning with Applications, 2024. DOI https://doi.org/10.
1016/j.mlwa.2024.100544.
10. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
11. M. Seif, J. Main, J. Weigand, F. Sadek, L. Choe, C. Zhang, J. Gross, W. Luecke, and
D. McColskey. Temperature-dependent material modeling for structural steels: Formulation
and application. Technical report, Apr. 2016. DOI https://doi.org/10.6028/nist.tn.1907.
12. C. Zhang, C. Bos, S. Sandfeld, and R. Schwaiger. Unsupervised learning of nanoindentation
data to infer microstructural details of complex materials, 2023. DOI https://doi.org/10.48550/
arXiv.2309.06613.
13. C. Zhang, C. Bos, S. Sandfeld, and R. Schwaiger. Nanoindentation Datasets of Copper-
Chromium Composits, 2023. DOI https://doi.org/10.5281/zenodo.8336072.


================================================================================
PAGE 89
================================================================================

Part II
A Primer on Probabilities, Distributions,
and Statistics


================================================================================
PAGE 90
================================================================================

5
Combinatorics and Probabilities
“The true logic of this world lies in the calculus of probabilities.”
James Clerk Maxwell (1831–1879),
Scottish mathematician and scientist
Probability deals with predicting the likelihood of future events, while statistics
involves the analysis of the frequency of past events. Without these two fields,
it would neither be possible to characterize nor to understand datasets or certain
machine learning algorithms (e.g., Bayesian methods strongly rely on conditional
probabilities). To count events, to create combinations of events, and to compute
the probability of occurrence of particular events are important prerequisites for
probability and statistics. Therefore, in Sect.5.1, we start by introducing the founda-
tions of combinatorics together with the most important mathematical formulations.
We will then continue with deriving and computing probabilities (Sect.5.2) using
discrete and continuous random variables (Sect.6.1).
5.1 Combinatorics
Combinatorics is a field of mathematics and the foundation for many considerations
in probability theory. It is mainly concerned with counting and arrangement of
things (e.g., numbers, tokens, coins, and all other kinds of objects). The best-known
example is probably that of a coin toss: assuming that you are tossing a coin five
times, how many different outcomes (i.e., sequences of head or tail) are possible
and what are they? Throwing two dies is another example —which are the summed
values that can occur and how many times do they occur? A famous example from
physics and materials science comes from the context of BOLTZMANN’s statistical
thermodynamics. But first, we need to introduce some of our terminology:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 69
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_5


================================================================================
PAGE 91
================================================================================

70 5 CombinatoricsandProbabilities
Definition 5.1 (Experiment, Event, and Sampling).
(cid:129) A random experiment can have several outcomes. For example, blindly
choosing one token out of three (a red, a blue, and a green one) is an
experiment. Another experiment is to initially choose one token, keep it,
and choose another one (out of the remaining two), which leaves us with
two tokens in our hand.
(cid:129) An event is the possible outcome of a random experiment. For example, in
the first example, choosing a red ball is an event, as is choosing a blue one,
etc. In the second example, picking a red and a green ball also represents
one event.
(cid:129) The procedure of sampling (or to draw a sample) refers to picking an
element from a set of elements. Most of the time, random sampling is
performed, when elements are chosen (or “drawn”) at random.
We will encounter these terms again, when we introduce probabilities and will
still have to refine them a bit. But for now, the above definitions are sufficient to
get started. Counting things or events nearly always comes down to one of two
principles, i.e., the additive principle and the multiplication principle, which we
introduce next.
5.1.1 Counting with the Additive Principle
We start with a simple problem: assume you have three different types of vegetables
and four different types of drinks, and you want to have either a veggie or adrinkin
your picnic basket. How many items can you choose from? The answer is quite
simple: just add the two numbers, and you get 3 + 4 = 7 choices. Figure 5.1
.
shows the answer using a tree diagram. To demonstrate the additive principle,
a tree diagram is more complicated than necessary to understand the principle.
Fig. 5.1 How many
single-item choices are there
if you have three different
types of vegetables and four
different types of drinks? The
tree diagram shows the two
groups; the seven possible
choices are shown as small
framed boxes below the tree


================================================================================
PAGE 92
================================================================================

5.1 Combinatorics 71
However, in Sect.5.1.2, the usefulness of such a visualization will become clearer.
This example can be generalized to the additive principle:
Definition 5.2 (Additive Principle). Assume that an event or random exper-
iment A can occur in m different variants, and another event or random
experiment B can occur in n different and disjoint ways. Then the event “A
or B” can occur in m+ndifferent ways.
.
The word disjoint indicates an important aspect: A and B cannot happen at the same
time and are mutually exclusive. The above definitions of the additive principle can
easily be extended to more than two events or experiments in the obvious way.
5.1.2 More Counting with the Multiplication Principle
To explain the multiplicative principle, we again use a simple example: assume that
you want to pack your picnic basket and that you can choose one piece of vegetable
and one drink. The following veggies are available: broccoli, tomato, and eggplant.
The drinks you can choose from are hot tea, water, beer, and a glass of milk. How
many different snack combinations can you create?
To answer this question, one can arrange the two decisions in a tree diagram, as
shown in Fig.5.2. With this structure, the answer is rather straightforward. For each
of the three veggies, there are four possibilities for choosing a drink, which makes
altogether 3·4=12different combinations. In such a tree diagram, each complete
.
path from top (the “root” or “stem”) to bottom (the “leaves” that do not have any
Fig. 5.2 How many different combinations of one veggie and a drink are there? For each of the
three veggies, there are four possible drinks. The solid lines indicate one possible path each, i.e.,
a particular choice of veggie and drink. All possible combinations are shown in the small boxes
below the tree


================================================================================
PAGE 93
================================================================================

72 5 CombinatoricsandProbabilities
further connections) represents one possible combination (or event). We can more
formally define the multiplicative principle as follows:
Definition 5.3 (Multiplication Principle). Assume that an event or random
experiment consists of two sub-events or sub-experiments, A and B . The
multiplicative principle states that if A has m different outcomes, and each
of these m outcomes for A allows for exactly n different outcomes for B, then
the overall event or experiment “A and B” can occur in m·ndifferent ways.
.
This principle can also be generalized to more than two events or experiments,
as demonstrated in Example 5.1. Note that in the above definition, the word “and”
in “A and B” is not identical to the mathematical “times.”
Example 5.1 Multiplication Principle and Parameter Studies (cid:2)
Assume that you want to perform a parameter study using a simulation
software. There are five different values for parameter 1, three for parameter
2, and four for parameter 3 to be used in this study. How many simulation
runs need to be performed?
Just multiply the number of simulations per parameter. Altogether, we then
need to run
5·3·4=60
.
simulations to perform the study. Due to the multiplication, including more
parameter(s) can therefore quickly result in very large numbers of simulations
to be performed!
Why is it important to know how many different outcomes exist or how often
they occur? Because this kind of counting is crucial, when we need to calculate
a probability. For example, in many cases, probabilistic experiments result in
outcomes of equal likelihood. The probability then is also the same for each of
these outcomes, defined as 1/N, with N as the total number of possible outcomes.
.
In the case of rolling a six-sided die, for example, the probability p to throw any
value of 1 ...6is p =1/6. Here, “counting” refers to figuring out that six different
. .
outcomes exist. Once we use more than one die, it becomes obvious that “counting”
is involved.
We will now turn to introducing the mathematical basis, the factorials, and then
continue with the two big classes of “experiments” in combinatorics: permutations
and combinations.


================================================================================
PAGE 94
================================================================================

5.1 Combinatorics 73
5.1.3 Factorials
The factorial is a number that shows up in many different situations, in which
probabilities are involved, namely, whenever the product of the first n integers
is required. This multiplication operation is abbreviated by an exclamation mark
(which might seem like an odd choice):
Definition 5.4 (Factorial). The factorial n! of an integer number n > 0 is
. .
given by
n!=n·(n−1) ·····2·1. (5.1)
.
For n=0the definition 0!:=1is used.
. .
The definition of0!:=1might look surprising but, as it turns out, makes a lot of
.
calculations much easier. For example, with this definition the factorial can also be
written using the next smaller factorial:
n!=n·(n−1)! (5.2)
.
Factorials already have quite a long history and were discovered by a number of
ancient cultures a long time ago. Many different fields could not do without them.
The first seven factorials (starting with the definition for 0!) are as follows:
.
0!=1
.
1!=1
2!=1·2=2
3!=1·2·3=6
4!=1·2·3·4=24
5!=1·2·3·4·5=120
6!=1·2·3·4·5·6=720
It is quite obvious that with increasing n,thevalueofn!quickly explodes. Thus,
.
for large n, computations quickly become unfeasible. Stirling’s approximation gives
a good approximation of the values in such cases. One of the most frequently used
versions is the following:
√ (cid:2) (cid:3)
n n
n!≈ 2πn , (5.3)
.
e


================================================================================
PAGE 95
================================================================================

74 5 CombinatoricsandProbabilities
where e ≈ 2.7183 ...is the Euler number, and the approximation approaches the
.
true value for n →∞. Another form often used is:
.
ln(n!)≈nln(n)−n, (5.4)
.
which is also the form used in the famous Boltzmann equation from statistical
mechanics, where the so-called microstates and macrostates need to be counted.
In Python, factorials can be computed using a function from the math module:
In [1]: from math import factorial
In [2]: factorial(12)
Out [2]: 479001600
5.1.4 Permutations—Ordered Sets Without Replacement
Permutations of the objects in a set (the definition of a set is given in Definition 3.2)
denote the arrangement of these objects in particular sequences or orders. Tech-
nically, the result is a set of tuples, as for the resulting permutations the order
of elements matters. An important question is as follows: How many different
permutations of a particular set of objects do exist?
Quite unsurprisingly, also a concise mathematical definition exists. However, first
we explain the concept by giving a number of examples described in Example 5.2.
Example 5.2 Permutation Examples (cid:2)
1. For a set with one object, {42}, only one way of sorting a single element
.
exists: it is the set that consists of only one tuple, here:
{(42)}
.
(which clearly are a lot of brackets enclosing just a single number!).
2. For a set with two objects, {D,C}, two ways of sorting two elements exist
.
(here: the letters D and C); the permutations consist of the set of the two
tuples:
{(D,C),(C,D)}
.
(continued)


================================================================================
PAGE 96
================================================================================

5.1 Combinatorics 75
3. For a set with three objects, ,there are six different ways of sorting
three elements. It is the set that consists of the following tuples:
.
4. A set with four items {0,1,2,3}has the following 24 permutations:a
.
{(0,1,2,3), (1,0,2,3), (2,0,1,3), (3,0,1,2),
.
(0,1,3,2), (1,0,3,2), (2,0,3,1), (3,0,2,1),
(0,2,1,3), (1,2,0,3), (2,1,0,3), (3,1,0,2),
(0,2,3,1), (1,2,3,0), (2,1,3,0), (3,1,2,0),
(0,3,1,2), (1,3,0,2), (2,3,0,1), (3,2,0,1),
(0,3,2,1), (1,3,2,0), (2,3,1,0), (3,2,1,0)}
aTip: For writing down all permutations, it generally helps to do this in a systematic manner,
e.g., always keep the rightmost item as the one that changes “fastest” and the leftmost as
the one that changes the slowest, similar to four nested for-loops in Python.
From these examples, we can infer the rule for constructing the different
permutations. We summarized these steps in Algorithm 5.1 and have followed them
in the fourth example of Example 5.2. In the first column of tuples, the 0 is fixed,
and the numbers 1, 2, and 3 are permuted. In the second column, the 1 is kept on
the left, and again the remaining numbers 0, 2, and 3 are permuted. The two last
columns are processed analogously.
Algorithm 5.1: Systematic Approach for Constructing All Permutations (cid:3)
These are the steps required for systematically finding all possible permutations of a
set of n elements:
1. Choose one of the n elements and keep it, e.g., as the leftmost element “fixed”;
2. Fortheremaining“free”elements,theproblemtobesolvedissimplerandconsists
only of the permutation of.n−1elements;
3. Apply the same strategy as in 1. to the new problem until only one element is left.


================================================================================
PAGE 97
================================================================================

76 5 CombinatoricsandProbabilities
How can this algorithmic strategy be generalized systematically? The multipli-
cation principle again comes in handy. We introduce a variable P for the number
. N
of possible permutations1 k for a set of N elements as P = k. For example, for a
. N
set of only one element, only one permutation is possible and P = 1. A set with
. 1
three objects has P = 6, and P = 24. We already saw how a permutation of N
. 3 . 4
elements can be reduced to a permutation of n−1elements together with keeping
.
the first element fixed. Thus:
. P N =N ·P N−1
=N ·(N −1)·P
N−2
=N ·(N −1)·(N −2)·P
N−3
.
.
.
=N ·(N −1)·(N −2) ···2·P
1
=N ·(N −1)·(N −2) ···2·1=N! (5.5)
That’s why in the context of permutations, a factorial will always pop-up some-
where.
It is often useful to visualize and count the number of possible permutations using
a permutation tree as shown in Fig.5.3. It can be seen that for the set of numbers
{0,1,2,3}, initially, there are four possibilities—the four elements of the set. Then,
.
there are three and two possibilities and finally only one possibility. Altogether,
this makes 4 ! =24 combinations. This argument can easily be extended to more
.
complex situations, such as the one discussed in Example 5.3 and Example 5.4.
Example 5.3 Permutations of atom positions (cid:2)
Assume that in a crystal there are nine vacancies in which nine atoms should
be placed. How many different configurations exist for placing the atoms?
For the first atom, there are nine possibilities to place it, for the second one
there are 8, etc. Altogether there are 9!=362880different arrangements.
.
1 In later chapters, we will use P for probabilities, but it will always be clear from the context if
probabilities are meant or permutations.


================================================================================
PAGE 98
================================================================================

5.1 Combinatorics 77
Fig. 5.3 The permutation tree for all permutations of the set of numbers . {0,1,2,3}. It should be
read starting from the left, which is where the “root” is located. Each of the first four “siblings”
(i.e., . · ·on the level 1) has the leftmost element fixed. Moving toward the right, more and more
elements are already fixed. From level 3 to level 4, only one number is left to choose from, which
is why, e.g., from 012, there is only one “descendent”: 0123. For brevity, the numbers are not
separated by commas in this tree


================================================================================
PAGE 99
================================================================================

78 5 CombinatoricsandProbabilities
Example 5.4 Permutations of atom positions with constraints (cid:2)
Assume the same nine atoms and nine vacancies as in Example 5.3. This
time, five particular atoms should fill the first five vacancies in an arbitrary
sequence. The remaining four atoms can be placed in the rest of the vacancy
positions. How many different configurations exist?
For the first atom, there are five options to place it. For the second, there
are four, etc., until the first five atoms will have been placed. Altogether these
are 5!=120different arrangements.
.
For every one of the 5! arrangements, there are now another 4! ways to
. .
arrange the remaining four atoms. Using the multiplication principle, this
gives5!·4!=120·24=2880different configurations, which are significantly
.
fewer configurations compared to the case of having no constraints for the
first five atoms (which we computed as a total of 9! = 362880 different
.
combinations).
Python offers a number of ways for creating permutations, and we will use two of
them. In both approaches, we seek to obtain the permutations of these three items:
In [1]: all_items = ['A', 'B', 'C']
The first approach uses three nested for loops with the first one iterating through
all items of all_elements , while the second loop iterates through items that are not
contained in b and the innermost loop chooses an item if it is not contained in either
a or b :
In [2]: perms = []
for a in all_items:
for b in all_items:
if a == b:
continue # skip the rest of this loop iteration
for c in all_items:
if c in [a, b]:
continue
perms.append((a, b, c))


================================================================================
PAGE 100
================================================================================

5.1 Combinatorics 79
In the second cell, the number of nested loops corresponds to the length of the
permutation. Thus, increasing the length of permutations requires changing code
and adding more for loops. Alternatively, one can use the powerful Python
module itertools [3], which is part of the Python standard library and provides
functionality for, e.g., combinatorics. In order to obtain all permutations of the
letters A, B, and C, the code is as follows:
Printing them as above would give the same result. Note that itertools functions
typically return the so-called iterators, but neither lists nor arrays. Therefore, you
either have to use, e.g., a for loop as in the second cell above, in order to use
the results of permutations() , or you have to convert the result to a list first
(e.g., by using list(permutations(...)) ). Further details can be found in the
documentation of itertools .2
5.1.5 Permutations of n items, k chosen at a time: Generalization
of the Multiplication Principle
Assuming we have n different items, out of which we choose k at a time. How many
of these permutations can be formed? At the first position, n different items can be
placed, at the second and third positions,n−1andn−2items, respectively. For the
. .
final position, only n−(k−1)choices are left. Using the multiplication principle,
.
we find the total number of possibilities for placing n items in k positions:
n·(n−1)·(n−2)· ...·(n−(k−1)) (5.6)
.
A short derivation (cf. exercise 5.4) shows that this term can be summarized as
n!
P := (5.7)
. n,k (n−k)!
where P is the number of resulting permutations, n is the number of available items,
and k is the number of items in each resulting permutation. Typical examples that
use Eq.(5.7) are questions of the following type: if you have nine things, items,
or fruits, in how many ways can you arrange them in a group of 4? The answer is
P =9!/(9−4)!=9·8·7·6=3024.
. 9,4
2 https://docs.python.org/3/library/itertools.html


================================================================================
PAGE 101
================================================================================

80 5 CombinatoricsandProbabilities
Fig. 5.4 Tree of all ordered samples for the successive drawing of three samples from the set
. {0,1,2}with replacement
5.1.6 Permutations with Repetition—Ordered Sets/Samples With
Replacement
Creating ordered sets by sampling with replacement, i.e., returning the item that was
drawn from a dataset, is yet another special case of the multiplication principle. Let
us assume a set with N elements, from which k samples are to be drawn in a way
in which the order matters and each drawn element is always returned. For the first
sample, we have N possibilities, and for the second one, we have N possibilities
again—as for all k samples. Thus, in total there exist
.
P
n
(cid:6)
,k
=N(cid:4) ·N(cid:5) · (cid:6)...·N(cid:7) =Nk (5.8)
k−times
ways of drawing k items from the set of N elements with repetition.3 Note, that in
our notation, we do not differentiate between with replacement and without . The
tree diagram of this case is shown in Fig.5.4.
Python’s itertools module is also able to perform this operation using the
product function. To obtain the . 33 = 27 ordered sets for sampling with replace-
ment of the letters A, B, and C and printing them, we could use the following code:
(cid:8)k
3 An alternative notation for such a product is to use the product operator:. N =Nk
i=1


================================================================================
PAGE 102
================================================================================

5.1 Combinatorics 81
5.1.7 Combinations Without Replacement—Unordered
Sets/Samples Without Replacement
We now turn to the second big category in combinatorics: combinations. As opposed
to permutations, in a combination the order of elements does not matter.
The number of combinations that can be obtained from a given set of items
is denoted by C (while the number of permutations was denoted by P). More
specifically, we are looking for the resulting C combinations for a total number
. n,k
of n available items, placing k items in each of the resulting combinations.
When the order of elements does not matter and each element can occur only
once, we have a combination without repetition—or just combination.
A typical question in this scenario could be as follows: if you have a set of four
items, how many different unordered pairs of items can you choose? In Fig. 5.5 we
show all unordered pairs of the letters A, B, C, and D as a 4×4matrix. We start by
.
writing down all possible permutations and then remove the letters that should not
show up in a combination without repetition. Clearly, the repeated elements on the
main diagonal have to be excluded because our prerequisite is that each element can
occur only once. Additionally, due to symmetry reasons, the sets below the main
diagonal are identical to those above the diagonal (remember that we are looking
for unordered sets). We end up with the set {AB,AC,AD,BC,BD,CD} shown
.
in blue in Fig.5.5.
What is the system used for counting here? If we have a set of N items, then
altogether there areN2possible combinations of two letters. The main diagonal has
.
N items, all of which need to be excluded. Due to symmetry, we end up with


================================================================================
PAGE 103
================================================================================

82 5 CombinatoricsandProbabilities
Fig. 5.5 The matrix shows all pairs of items chosen from the set . {A,B,C,D}. As samples are
drawn without replacement, the double letters on the main diagonal are excluded. As we are
looking for unordered sets, all elements below the main diagonal can be excluded. Only the six
blue elements remain, which are the unordered sets without replacement
N2−N N(N −1)
= . (5.9)
.
2 2
Alternatively, we can start by filling only the first position—there are N ways
of doing this. The second position can then only be filled by one of the remaining
N −1 items. As we double counted, we have to divide by two, which then yields
.
the same result as above.
If, instead of pairs, we want to create unordered sets of k items, drawn without
replacement from N items, the situation is a bit more complex. However, we can
make use of what we already found out about the problem of permutations of n
items with k objects chosen simultaneously at a time, cf. Sect.5.1.5.
1. Consider a particular permutation p (which consists of k items) from the set of
all, in total P , possible permutations.
. n,k
2. p is also a valid combination and contained in the set of all, in total C ,
. n,k
combinations.
3. If we “converted” all permutations into combinations, the particular combination
c would be repeated several times—to be precise, it would be repeated P =k!
. k,k
times. We would answer the question: How many permutations of length k can
one create from a set of k different elements?
4. If each combination were repeated k! times, the number of permutations would
.
be greater than the number of combinations by the factor k!.Itfollowsthat
.
P n!
C = n,k = (5.10)
. n,k k! k!(n−k)!
Turning the task shown in Fig. 5.5 into a small Python example is again
straightforward, if you are familiar with the module itertools . Luckily, it has a
function called combinations that fits exactly for our problem:


================================================================================
PAGE 104
================================================================================

5.1 Combinatorics 83
In [1]: from itertools import combinations
In [2]: for c in combinations('ABCD', 2):
print(c)
Out [2]: ('A', 'B')
('A', 'C')
('A', 'D')
('B', 'C')
('B', 'D')
('C', 'D')
In [3]: len(list(combinations('ABCD', 2)))
Out [3]: 6
Note that in the last cell, we first had to convert the result of combinations to a list
in order to show the number of items.
As a short form in the expression in Eq.(5.10), the following definition is used
frequently:
Definition 5.5 (Binomial Coefficient). The binomial coefficient is defined
as
(cid:9) (cid:10)
n n!
:= (5.11)
. k k!(n−k)!
It gives the number of combinations, C . The combinations consist of k
. n,k
items that are taken (without replacements) from a set of n items. Often this
is referred to as “n choose k.”
The binomial coefficient can easily be used to compute the number of combi-
nations. A number of additional rules and applications for binomial coefficients
exist, for which, however, we refer the reader to the vast body of literature on
combinatorics.
A Python implementation uses the already introduced functions for factorials,
and, e.g., for the example in Fig.5.5, we could write:
In [1]: from math import factorial
In [2]: n, k = 4, 2
factorial(n) // ((factorial(n-k)) * factorial(k))
Out [2]: 6


================================================================================
PAGE 105
================================================================================

84 5 CombinatoricsandProbabilities
The // in the second cell denotes an integer division. The “regular” division
would yield a floating point number as a result.
5.1.8 Combinations with Replacement—Unordered Sets/Samples
with Replacement
Creating combinations with replacement is the second case, in which the order does
not matter. In contrast to the previous case, however, whenever an item is drawn,
it will be returned, i.e., each element may occur more than once. The number of
combinations with repetitions of k objects chosen from a total of n objects is denoted
(cid:6)
by C . Here, we just state the final result:
. n,k
(cid:9) (cid:10)
C (cid:6) =
(n+k−1)!
=
n+k−1
(5.12)
. n,k (n−1)!·k! k
An example would be rolling three dices, while the order of them being thrown does
not matter (e.g., { , , } is identical to { , , } ). This is equivalent to rolling
one die three times—or three times drawing a ball from a jar, which contains six
balls numbered from 1 to 6, and each drawn ball is returned. Figure 5.6 shows
all possible combinations for a dice being thrown three times, which results in
altogether1·6+2·5 +···+5·2+6·1=56combinations. Using Eq.(5.12) with
.
k = 3samplings out of n = 6numbers, we can calculate the same total number of
. .
combinations:
(6+3−1)! 8! 8! 8·7·6
C (cid:6) = = = = =56. (5.13)
. 6,3 (6−1)!·3! 5!·3! 5!·3! 3·2·1
We conclude this section again with a small Python implementation using the
module itertools for the task shown in Fig.5.6:
In [1]: from itertools import combinations_with_replacement
In [2]: comb = combinations_with_replacement([1, 2, 3, 4, 5, 6], 3)
for c in comb:
print(c)
Out [2]: (1, 1, 1)
(1, 1, 2)
(1, 1, 3)
...
(5, 5, 6)
(5, 6, 6)
(6, 6, 6)


================================================================================
PAGE 106
================================================================================

5.1 Combinatorics 85
Fig. 5.6 All combinations with replacement of rolling a die three times. In each column, the first
dice is fixed; in each section of each column, the second dice value is kept, while the third one is
changed. We start at the top left and continue toward the bottom and then switch to the top of the
next column. “Duplicates” (e.g., { , , }={ , , }) are skipped
5.1.9 Summary: Permutations and Combinations
This section introduced how to count and arrange items by permutations and
combinations. We have seen that there are two types of permutations and two types
of combinations, both either with or without replacement.


================================================================================
PAGE 107
================================================================================

86 5 CombinatoricsandProbabilities
Things to Remember Permutations and Combinations (cid:4)
(cid:129) Permutations are about the number of different ways items can be arranged
(cid:129) Combinations are about the number of ways items can be selected
(cid:129) After “drawing” items from a set ...
– the item can be returned (called “with replacement” or “with repetitions”) ...
– or the item can be kept (called “without replacement” or “without repeti-
tions”)
number of
permuta-
order of repetitions/ tions or
Selection items replace- combina- typical
type matters? ments? tions question
Permutations Yes No .Pn,k =
(n−
n!
k )!
1.
Yes Yes .P
n
(cid:6)
,k
= nk 2.
No No .Cn,k =
(n−
n
k
!
) !·k!
3.
Combinations No Yes .C
n
(cid:6)
,k
= (
(
n
n
+
−
k
1
−
)!·
1
k
)
!
!
)
4.
The set from which combinations or permutations are created has n Items; the
(cid:6) (cid:6)
resulting combinations or permutations contain k items. A prime (as in .P or .C )
indicates that samples are drawn with replacements. Typical questions asked for
these four cases are as follows:
1. How many possible sequences are there, if k cards are drawn from a stack
containing n unique cards?
2. How many sequences exist, if a card is k times drawn from a stack of n cards and
always returned to the stack?
3. How many sets of k cards can you draw from a stack of n cards, if their order does
not matter?
4. How many sets of k cards can you draw from a stack of n cards, if the order does
not matter and each card is always returned to the stack for the next draw?
5.2 Probabilities
In the previous section in Definition 5.1, we already introduced the notion of
sampling. Drawing random samples from a set often implies that each element
of a set has the same chance to be drawn, but this is not necessarily always the
case. Random sampling is one of the most important concepts for investigating
probabilities. Probabilities, in turn, can be used for modeling random processes.
The underlying uncertainty is then represented by random variables.


================================================================================
PAGE 108
================================================================================

5.2 Probabilities 87
Relationship Between Probabilities, Statistics, and Machine Learning (cid:2)
What is the relation between probabilities, statistics, and machine learning, and why is
probability theory important? Probability theory starts with an abstract or mathematical
model, e.g., that of a coin toss, and predicts the observations.
In statistics as well as, roughly speaking, in machine learning inference, it is the other
way round: we have a collection of observations and then try to figure out, which underlying
process or “model” could explain the observations.
Relationship to Machine Learning
Why should you know about probabilities, if you are interested in machine learning
(ML) in the first place? To start with, a solid understanding of probabilities
and related concepts helps to judge the quality and information content of data
and allows to make predictions, when purely analytical approaches fail. More
concretely, the direct relation to ML is that probabilities and random variables
help us to quantify uncertainty in the data of the machine learning model and its
predictive power. And last but not least, a number of ML methods exist that directly
make use of estimating uncertainties.
5.2.1 What is Probability?
Probability is a mathematical measure that gives the likelihood of a particular event
to happen in the future—or not. Thus, it is concerned with uncertainty. Probability
theory is the whole field that deals with probabilities. Questions of interest that can
be answered are as follows: (1) How many times do we get “Heads” (H) or “Tails”
(T) when flipping a coin? (2) What is the likelihood that a specimen has not failed
after a million loading cycles?
The answer to the first question is straightforward and can be figured out
analytically for a perfect coin: on average (i.e., when we flip the coin many times),
we expect it to land the same number of times on the Heads side as on the Tails side.
Hence, the probability P for both outcomes is 0.5 or 50% each. Unfortunately, this
. .
holds only on average and for a large, possibly infinitely large, number of tosses. For
only a few coin tosses, the results might suggest completely different probabilities,
e.g., when we get five times “Heads” (H)inarow.
The answer to the second question, which is of importance to materials research,
requires further details to be specified. But even then, it probably cannot be
answered based on mathematical considerations alone and needs a number of well-
designed experiments to be performed.
We will now continue with a number of definitions for commonly used notions
and formalize the most important concepts around probabilities.
Probability
Here is a definition and explanation of probability in plain words:


================================================================================
PAGE 109
================================================================================

88 5 Combinatorics and Probabilities
Definition 5.6 (Probability). The probability of an event represents the
likelihood of the event to occur, taking values between 0 and 1. A value of
0 indicates that the event will never occur, while 1 indicates that the event
occurs with certainty. The higher the value of the probability, the higher the
likelihood of the event to happen.
Often, P is used as variable for the probability of a specific event.
In other words, probability is the fraction of the number of times a specific event
occurs. We often denote a probability as a percentage, meaning a probability p of
x % is identical to p = x/100.
. .
A slightly different definition is related to our existing and possibly changing
knowledge about a situation: Probability is the degree of belief about an event. This
is a definition that we will use in the context of Bayesian statistics and ML.
Basic Random Experiments
The three best-known and most frequently used examples, for which we can
intuitively calculate probabilities, are presented in Example 5.5. Real or virtual
experiments for studying random behavior are called random experiments.
Example 5.5 Probabilities of Simple Experiments (cid:2)
(cid:129) The probably simplest example is that of a coin toss: if it is a fair coin and
a fair toss, then in half of all cases—at least on average— the coin will
show Heads (H) or Tails (T), and the corresponding probabilities for either
are 0.5 (or 50%).
.
(cid:129) The second, equally famous, example is that of rolling a six-sided die.
Here, one could ask what the probability was of rolling a three. There are
six potential outcomes of rolling the die (1, 2, 3, 4, 5, and 6), all of them
occurring with equal likelihood. Then, on average every number should
occur one sixth of the times, and therefore the probability is 1/6.
.
(continued)


================================================================================
PAGE 110
================================================================================

5.2 Probabilities 89
(cid:129) Another typical example is drawing a ball from a jar that contains six blue
and four red balls. The probability of drawing a blue ball is6/10, while the
.
probability of drawing a red ball is 4/10—on average.
.
A more complex type of random experiments, for which it is more difficult to
calculate probabilities, is to draw one or more cards from a deck of cards. This
is a more complex, because the 52 cards of a standard deck have multiple properties
(four different suits, each of them having three “face cards” and ten values). And
last but not least, querying your Python random number generator for one or more
random numbers is also a random experiment.
Observation
In probability theory, you will often read about observations, which means:
Definition 5.7 (Observation). Any information that is recorded, e.g., during
such a random experiment, is called an observation.
Many of the observations discussed in this chapter consist of categorical data that
can only take a limited number of values. For example, the result of a coin toss will
be either H or T, and the result of drawing a ball from a jar with blue, green, and red
balls will be the color of the ball drawn.
Probability and Equal Likelihood
We now introduce an important assumption, which significantly simplifies many
formulations: all n outcomes of a random experiment are equally likely. With
this assumption, we can now easily compute probabilities based on the following
definition:
Definition 5.8 (Probability of Equally Likely Outcomes). If a set of out-
comes consists of m different, equally likely outcomes, the probability is
given by
(continued)


================================================================================
PAGE 111
================================================================================

90 5 Combinatorics and Probabilities
number of desired outcomes m
P = . (5.14)
.
total number of possible outcomes n
The assumption of equal likelihood might be a bad approximation of reality. Hence,
it is advisable to choose the investigation such that the violation of the equal
likelihood is negligible.
In Eq.(5.14) it can be seen that counting and probabilities are closely related.
However, counting the total number of outcomes is not always trivial, which is
why in the previous section, we have looked at different types of permutations and
combinations.
Here is a simple example: if a fair six-sided die is rolled, and the number 2
is observed, then m = 1, n = 6. Thus, the probability is P = 1/6. Obviously,
. . .
conducting this experiment only once is not reasonable, as the event of observing
a 2 might not even occur at all. However, when the die is rolled many times—
whatever “many” exactly means!—one can expect that on average16.66 ¯ % (= 1/6)
.
of the time, we will observe a 2.
5.2.2 Two Python Examples
Example 1 Python together with the numpy library is also a useful tool to conduct
random experiments. For example, we can easily draw ten integer random numbers
from the set of numbers 1...6 with replacement and with equal likelihood—either
using numpy ’s legacy function random.randint() or with the more recent approach
by creating a random number generator and then calling its integers() function;
both approaches return random integer numbers from within a given range:4
4 When dealing with random numbers in a computer program, a few additional points need to be
considered: (i) there are no real random numbers, only “pseudo” random numbers, which will
repeat themselves after some time (actually: after a very long time). For practical purposes, this is
good enough. (ii) “Seeding,” i.e., initializing of the random number generator, determines whether
for each run of a program the same sequence of random numbers occurs or not. More details are
given in the numpy documentation at https://numpy.org and will also be given in the text when
needed.


================================================================================
PAGE 112
================================================================================

5.2 Probabilities 91
In [1]: import numpy as np
# create and initialize random number generator
rng = np.random.default_rng()
In [2]: N = 10 # number of samples
# create N random integer numbers in the range of [1,...,6]
values = rng.integers(1, 6, endpoint=True, size=N)
print("values: ", values)
Out [2]: values: [ 1 1 1 3 1 2 6 5 3 3]
This code creates N = 10 random numbers for rolling a die N = 10 times. We
observe that the resulting numbers do not occur with the same frequency, e.g., the
number 1 was thrown four times, while the number 4 did not show at all. Is the
random number generator defective? We cannot tell, as the numbers were drawn by
chance, and ten samples might not be sufficient to determine any “imbalance.”
Example 2 To demonstrate how the number of repetitions of an experiment
influences the accuracy compared to the behavior that is expected on average , we
perform another numerical experiment. We roll a fair die multiple times and record
the change of the average value for every additional throw as function of the number
of rolls (note that the current average value is the sum of all so far recorded values
divided by the current number of rolls). The first step for the implementation is again
to import the relevant packages, to initialize the random number generator, and to
get 50 samples:
In [1]: import numpy as np
import matplotlib.pyplot as plt
rng = np.random.default_rng(22524)
n_samples = 50
values = rng.integers(1, 6, endpoint=True, size=n_samples)
Note that this time we initialized the random number generator with a specific (but
arbitrary) seed value of 22524. This has the effect that the sequence of drawn random
numbers will always be the same. Next, we use cumsum to create an array with the
cumulative sum of values , i.e., the element i is the sum of all elements of values
up to position i:
In [2]: sample = np.arange(1, n_samples + 1)
averages = values.cumsum() / sample
The function np.arange creates all integer numbers starting from 1 up to and
excluding n_samples+1 (meaning [1,2,..., n_samples]). Exercise 5.7 will show
how the cumulative sum is computed in more detail. Finally, we are ready to plot
our 1D arrays:


================================================================================
PAGE 113
================================================================================

92 5 Combinatorics and Probabilities
Fig. 5.7 Rolling a fair die 100 times. Left: observed values as a function of the number of die
rolling. Right: average value of all die rolls that occurred during the first N rolls. The dashed line
shows the expected average value for a fair die
In [3]: fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
ax[0].plot(sample, values, '.')
ax[0].set(xlabel='N', ylabel='resulting value', ylim=(0.8, 6.2))
ax[1].plot(sample, averages, '-')
ax[1].axhline(3.5, linestyle='--' , c='0.5' , l=w1.5)
ax[1].set(xlabel='N', ylabel='average value', ylim=(0.8, 6.2))
This results in a figure similar to Fig.5.7 (add plt.show() at the end if you are not
using an interactive Jupyter notebook). Note that the random numbers drawn might
be different for different Python environments.
The left panel of Fig.5.7 shows the values from 1 to 6 during rolling a die 50
times. The right panel shows how the average changes accordingly. Initially, the
average value was quite low as there were mainly 1s and 2s. Already at N = 30,
the computed average value is getting closer to the expected average value of
(1 + ··· + 6)/6 = 3.5 with an error less than 10%. This behavior is typical
of random experiments demonstrating that these experiments are powerful for
predicting behavior of large ensembles of samplings only. They are nearly useless
for predicting the behavior of an individual event.
5.2.3 Outcomes, Events, and the Sample Space
To analyze random experiments, two concepts are required: the concept of the
random variable and, associated with it, the probability function, which measures the
probability of a certain result. These two concepts will be introduced in Sects.6.1
and 6.2. But first, we will introduce and define the terms outcome, sample space,
and event based on the simple random experiment of tossing a coin, in this case four
times, while recording the results in the order of occurrence.


================================================================================
PAGE 114
================================================================================

5.2 Probabilities 93
We have already used the notion of outcome in a colloquial manner, which is
already fairly close to the definition below:
Definition 5.9 (Outcome). Outcome denotes a possible result of a random
experiment or trial. They are the mutually exclusive “elementary” events of
an experiment that cannot be further broken down into a subset of events.
Each single coin toss has two outcomes: Heads (H) or Tails (T). In our case, though,
the experiment consists of altogether four coin tosses (it is quite important to be
aware of the exact nature of the experiment!). Therefore, the outcome consists of a
series of four items, each of which being either H or T. For example, THHH would
be an outcome.
(cid:6)
Using this example and considering all P permutations of H and T that result
. n,k
in valid outcomes brings us to the sample space:
Definition 5.10 (Sample Space). The set of all possible outcomes represent
the sample space. The sample space is denoted by the variable (cid:3).
.
For example, for four times tossing a coin, the sample space is the permutation of
the four {H,T},
.
(cid:3)={HHHH, HHHT, HHTH, HHTT , . . . , TTTT} (5.15)
.
The total number of elements in (cid:3)is, thus, P (cid:6) =24 = 16.
. . 2,4
To find the sample space in general, a tree diagram can again be helpful to
systematically construct all possible outcomes. In case of a very large or even
infinitely large sample space, the statement or rule method can be useful. For
example,
(cid:11) (cid:12)
(cid:3)= x|x2 > 4 (5.16)
.
denotes the set of all values x that are larger than +2 or smaller than −2 (note that
. .
this example is unrelated to the coin flipping and does not involve Heads or Tails).
Discrete values, e.g., the values from throwing a six-sided die, can also be used, e.g.,
(cid:3)={n|n ≥ 4}.
.
The definition of the sample space allows us to also define the meaning of an
event:


================================================================================
PAGE 115
================================================================================

94 5 Combinatorics and Probabilities
Definition 5.11 (Event). An event is a set of outcomes. We denote events
with capital letters, and each of the events is a subset of the sample space,
e.g., A ⊂(cid:3).
.
For example, in the case of our random (coin toss) experiment, the event A could
be “having at least two Heads”:
A ={HHTT, HTHT, HTTH, HHTH, HHHT, HTHH, HHHH}⊂ (cid:3). (5.17)
.
As a consequence, each experiment is always associated with the following two
events:
(cid:129) the event A of all possible outcomes, i.e., the whole sample space A =(cid:3)
.
(cid:129) the empty set A =∅r epresenting no possible outcome and, therefore, called the
.
impossible event
Note the difference between outcome and event, which might be somewhat counter-
intuitive: while the outcome of a random experiment is a single and the most basic
“entity,” an event corresponds to possibly several outcomes!
5.2.4 Calculating with Events and Probabilities
A few special cases help the calculation of probabilities, such as the situation that
two events do not have any common outcome.
Definition 5.12 (Mutually exclusive). Two or more events A are said to be
. i
mutually exclusive if they do not share any common outcome, i.e., either A
. 1
or A or ...can occur but not A and A and ... together.
. 2 . 1 . 2
This can also be written more formally as
A ∩ A =∅ for all i (cid:11)= j . (5.18)
. i j
The probability that any of mutually exclusive events occurs is the sum of all
individual probabilities,
P(A ∪ A ∪ A ∪··· ) = P(A)+ P(A)+ P(A) +··· . (5.19)
. 1 2 3 1 2 3


================================================================================
PAGE 116
================================================================================

5.2 Probabilities 95
Fig. 5.8 Visualization of different events together with their corresponding probabilities using
Venn diagrams
Often events are mutually exclusive, e.g., if the outcome is considered the event.
For example, rolling a die gives one out of six mutually exclusive events if a single
die roll is the considered experiment. An example for non-exclusive events during a
die roll is the event of all odd numbers, A = {1, 3,5} and the event of all numbers
.
greater than 2,B ={3,4,5,6}. The outcomes “throwing a 3” and “throwing a 5” are
.
contained in both events, A ∩ A = {3,5} (cid:11)= ∅. Figure 5.8 visualizes these relations
.
in terms of Venn diagrams.
The probability of a union of non-exclusive events can be calculated as:
P(A∪ B) = P(A)+ P(B) − P(A∩ B) . (5.20)
.
Why is this important or what is the relevance? We would now ask what the
probability is of either an odd number (event A) or a number larger than 2 (event B)
to occur upon rolling a die. As we already know, the events are
A ={1,3,5} and B ={3,4,5,6} .
.


================================================================================
PAGE 117
================================================================================

96 5 Combinatorics and Probabilities
Fig. 5.9 Visualization of the two “overlapping” events where the outcomes of each event are
indicated by connecting lines. The outcomes and are contained in both events. Out of six
possible outcomes of the single-die-throw (the sample space), five are “covered” by the two events
A and B (see text for further explanations)
The probabilities, thus, are
3 4
P(A) = = 0.5 and P(B) = ≈ 0.667 . (5.21)
.
6 6
The probability of getting an event either from A or from B is obviously not just
the sum of the two individual probabilities—this would result in 7/6 ≈ 1.667,
.
but probabilities larger than 1 are not possible. The problem is that we are double
counting the events A ∩ B, which is why the probability needs to be reduced by
.
P(A∩ B):
.
3 4 2 5
P(A∪ B) = + − = ≈ 0.833 . (5.22)
.
6 6 6 6
Figure 5.9 visualizes this situation, making it easy to simply count outcomes and
to ignore double counting. Note that this figure (which resembles a tree diagram)
does not reflect the properties of mathematical sets, but is rather a convenient way
of sorting and counting items. In terms of Venn diagrams reflecting the proper
mathematical representation, we would draw two overlapping sets, e.g., as circles,
that contain the respective events and that show and in their intersection.
As a final subtopic of this section, we consider independent events and their
probabilities. We already have a good intuitive understanding of this concept and
have used it, for example, when we investigated two coin tosses. We could either
have tossed a single coin two times or we could have used two coins and tossed
them at the same time just once. The results in terms of the probability of specific
outcomes are always the same (at least as long as the coins are identical). Generally,
this can be stated as follows:


================================================================================
PAGE 118
================================================================================

5.2 Probabilities 97
Definition 5.13 (Independent Events). Two or more random events A are
. i
said to be pairwise independent if for their probabilities it is
P(A ∩ A ) = P(A)P(A ) for i (cid:11)= j . (5.23)
. i j i j
The symbol “∩” denotes the intersection of two sets giving the elements that
.
are contained in both sets. In general, the eventsA , ..., A are independent
. 1 N
of each other, only if, for all events, it is
(cid:13)N
P(A ∩···∩ A ) = P(A ). (5.24)
. 1 N i
i=1
Here, we used the product operator, which is defined as(cid:4)N x := x ×···×
. i=1 i 1
x .
N
Further down, after the introduction of conditional joint probabilities , it will
become clear why this is indeed equivalent to the independence of events. Generally
speaking, this definition implies that the occurrence of an event does not change the
probability of the occurrence of any other event. In Sect.6.5 we will denote the
resulting probability of independent events as joint probability.
As an example, imagine that you roll a six-sided die and flip a coin. We could
define the events asA ={4,6} for the die andA ={H} as the event of interest for
. 1 . 2
the coin. It is
.
P({(4, H), (6,H)}) = 2/12, while for the individual probabilities, it is
.
P({4,6}) = 2/6, and
.
P({H}) = 1/2. The product of the two individual probabilities
is again
.
P({4,6}) × P({H}) =2/6 ×1/2 =2/12.
5.2.5 The KOLMOGOROV Axioms
Much of what we have just introduced can be condensed into or rather derived
from the Kolmogorov Axioms5 that were introduced in 1933 by the Russian
mathematician ANDREY KOLMOGOROV (see [2] for an English translation). The
following three axioms form the basis of probability theory as we know it today.
They help determine the types of mathematical operations that can be performed on
probabilities:
5 An axiom is a statement that cannot be proven and is assumed to be true. Axioms act as the point
of departure for constructing further theoretical frameworks.


================================================================================
PAGE 119
================================================================================

98 5 Combinatorics and Probabilities
1st Kolmogorov Axiom: The probability P of any event is a non-negative real
.
number (P ∈ R0+ ), also simply written as
.
P ≥ 0 . (5.25)
.
This is what we are used to in our intuitive understanding of the meaning of
probability. The higher the number, the higher the likelihood. However, there are
mathematical concepts and spaces, for which this does not hold automatically, thus
underscoring the importance of the axiom.
2nd Kolmogorov Axiom: The probability that at least one of the outcomes in
.
the entire sample space (cid:3)will occur is 1. This is also called the assumption
.
of unitary and can be summarized as
P((cid:3)) = 1 . (5.26)
.
From this axiom we understand that if a probability value is larger than 1 or
smaller than 0, it actually cannot represent a probability—at least not unless we
move to very strange mathematical theories and spaces. Everything that happens in
the “real world” has a likelihood to happen that is between (and including) absolute
certainty and no likelihood at all.
3rd Axiom: for any sequence of mutually exclusive events E , it holds that
. . i
the probability of the union of all events is equal to the sum of all individual
probabilities,
(cid:14) (cid:16)
(cid:15)∞ (cid:17)∞
P E = P(E ). (5.27)
. i i
i=1 i=1
The last axiom tells us that we can add probabilities (which also holds for both
finite and infinite sets). It also tells us that for mutually exclusive events E and E ,
. i . j
it is
P(E ∪ E ) = P(E)+ P(E ). (5.28)
. i j i j


================================================================================
PAGE 120
================================================================================

5.3 Conditional Probabilities, Product rule, and Bayes’ theorem 99
Starting from these three axioms, many more important probabilistic theorems and
relations can be derived and are covered in most text books on probability theory
and statistics; see, e.g., [1].
5.3 Conditional Probabilities, Product rule, and Bayes’
theorem
We conclude this chapter with a short overview of conditional probabilities and
some other formulations that often occur in the context of Bayes’ theorem.
As a prerequisite for Bayes’ theorem, we briefly summarize some important
notation and conventions:
(cid:129) If the probability of an event A does not depend on the outcome of any other
random variable, then we denote the probability byP(A) and call it the marginal
.
probability.
(cid:129) The joint probability that two events A and B take place simultaneously is written
as P(A∩ B)
.
(cid:129) The conditional probability of an event A, given that another event B takes place,
is written as P(A | B). It can be related to the joint probability through
.
P(A ∩ B) = P(A | B)· P(B) , (5.29)
.
which is also known as the product rule. This gives us a way to compute the
conditional probability
P(A∩ B)
P(A | B) = . (5.30)
.
P(B)
Words of advice (cid:3)
Note that, while the joint probability is symmetric,.P(A ∩ B) = P(B ∩ A), the same
does not hold for the conditional probability and, in general, it is.P (A | B)(cid:11)= P(B |
A).
With these notions and definitions, we can now state Bayes’ theorem (or, as it is
also sometimes called, Bayes’ rule). It is defined as:
Definition 5.14 (Bayes’ Theorem). Given are two events, A and B. Then,
for their conditional probability, it holds:
(continued)


================================================================================
PAGE 121
================================================================================

100 5 Combinatorics and Probabilities
P(B)
P(B | A) = P(A | B)· . (5.31)
.
P(A)
Bayes’ theorem helps to compute the probability that events belong to a certain
class, given some prior knowledge. Often, for the terms occurring in the above
definitions, specific names exist. For example, P(A) is the prior probability, while
.
P(A | B) is the posterior probability. Furthermore, P(B | A) is called likelihood
. .
and, last but not least,P(B) is the evidence. All of these are terms that we will again
.
encounter in some probabilistic ML models.
5.4 Summary
In this section, we introduced the concept of probabilities and random experiments.
After systematically defining outcomes, sample space, and what an event is, we
could then start to calculate with events and probabilities. There, Venn diagrams
helped us to visualize some of the set operations. An important aspect (also for
probabilistic machine learning methods) was the concept of independent events
and the corresponding probability. Last but not least, we introduced the notions of
conditional and joint probabilities, which are a prerequisite for Bayes’ theorem.
Things to Remember (Probabilities) (cid:4)
(cid:129) The probability P of any event is a number.0 ≤ P ≤ 1
(cid:129) The probability of any mutually exclusive events A and B is .P (A ∪ B) =
P(A)+ P(B).
(cid:129) For two events A and B, the following holds
(cid:129) A and B are independent. ⇔ P(A)· P(B) = P(A∩ B)
(cid:129) .P(A∩ B) is the joint probability that both events take place simultane-
ously.
(cid:129) conditional probability of event A given that event B takes place:
.P(A | B)
(cid:129) product rule:.P(A∩ B) = P(A | B)· P(B)
(cid:129) Bayes’ theorem:.P(B | A) = P(A | B)· P
P
(
(
B
A)
)


================================================================================
PAGE 122
================================================================================

5.5 Exercises 101
5.5 Exercises
5.1. Plot the first eight factorials together with their approximation by the Stirling
formula. Which is the largest integer that your computer can still compute the
factorial for? How do you explain this?
5.2. Write a small Python function that computes the factorial of a number based
on the “recursive” formula Eq.(5.2).
5.3. Formulations of the multiplication principle: use Eq.(5.6) to derive Eq.(5.7).
5.4. Draw the tree diagram for Fig.5.5.
5.5. The lattice structure of Cu Au is L1 . The unit cell consists of four atoms,
3 2
three of which are Cu and one Au. The lattice is a face-centered cubic type, with
Cu atoms on each face of the lattice and Au atoms at the corners. At higher
temperatures, often the structure can get disordered, and the individual species are
no longer bound to their ordered positions.
(a) If we consider a total of four lattice sites, how many possible orderings of the
atoms are possible? Consider that all three Cu atoms are identical.
(b) If all the possible orderings have equal energy, what is the probability of each
of the configurations?
5.6. A jar contains 15 marbles of the same size. Ten of them are blue and five are
yellow. Imagine that you are blind-folded and take one marble with each hand. Then
the colors are recorded.
(a) What is the name of this random procedure?
(b) Write down the sample space.
(c) How can you write the following event in terms of the sample space: “the marble
in the left hand is yellow”?
5.7. Write down the mathematical function for a cumulative sum of the first N
positive integer numbers. Write a Python function that computes the cumulative
sum of all elements of a given array. Test and compare your implementation with
numpy’s function numpy.cumsum() .
5.8. Which of the following options denote feasible sample spaces:
1. (cid:3) ={−1,0,1,2,3}
2. (cid:3) ={1,...,10}
3.


================================================================================
PAGE 123
================================================================================

102 5 Combinatorics and Probabilities
4. (cid:3) ={x | x >− 1 for allx ∈R}
5. (cid:3) ={}
5.9. The sample space (cf. Sect.6.1) can be systematically created using a tree
diagram. Assume a random experiment that consists of two steps: (1) flip a coin
and (2) if H was drawn, flip the coin a second time—but if T was drawn, throw a
six-sided die. Create a tree diagram of all possible outcomes and write down the
elements contained in the sample space S.
5.10. Given are two types of events. Event A is the set of numbers below 50,
and even B are all odd numbers below 100. We randomly draw 20 samples with
returning them from the set of all numbers from 1 to 100. Compute the likelihood
of event A, event B, and that both event A and B occur.
5.11. Assume the same events as in Exercise 5.10. Write a Python program that
draws n samples with returning them from the set of all numbers in between 1 and
N where n and N are parameter. Use the Python code with n = 20 and N = 100
and (based on the drawn random numbers) compute the likelihood of event A, event
B, and that both event A and B occur. Compare your results with the theoretical
values from Exercise 5.10 and discuss them.
References
1. M. Bonamente. Statistics and Analysis of Scientific Data. Springer New York, 2017. DOI https://
doi.org/10.1007/978-1-4939-6572-4.
2. A. Kolmogorov, A. Bharucha-Reid, and N. Morrison. Foundations of the Theory of Probability:
Second English Edition. Dover Books on Mathematics. Dover Publications, 2018. ISBN
9780486821597.
3. G. Van Rossum and F. L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA,
2009. ISBN 1441412697.


================================================================================
PAGE 124
================================================================================

Random Variables and Probability Functions 6
Anyone who attempts to generate random numbers by
deterministic means is, of course, living in a state of sin.
John von Neumann (1903–1857)
Hungarian-American mathematician and computer scientist
6.1 Random Variables
While the idea of the sample space, where we simply write down all possibilities,
might seem compelling, often this is not practical or contains many more (or too
many) details than actually needed. Instead, we use a random variable, which
represents the data in a compact numerical format (by definition the data has to
be numerical if it is to be used with a random variable). In the literature, random
variable is often abbreviated as RV. However, in this text, we write it out in order to
avoid the use of too many abbreviations.
Probability functions can be categorized depending on the type of the random
variable, i.e., whether it results in discrete or continuous data. Example 6.1 presents
the categorization based on a few examples.
Example 6.1 (Classification of Random Variables and Distributions) (cid:2)
(cid:129) Typical examples for discrete random variables involve counting items,
e.g., the number of cars passing by within a half hour. Upon a specific event
(continued)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 103
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_6


================================================================================
PAGE 125
================================================================================

104 6 RandomVariablesandProbabilityFunctions
only (a car driving by), the value of the random variable is changed, and
there are “gaps” between all events.
A materials science example is counting the number of defects observed
with a microscope on the surface of a specimen. Each sample results
in a value, and the defect numbers of all samples result in the discrete
distribution.
(cid:129) We encounter continuous random variables whenever a measurement or
observation takes place continuously, i.e., without “gaps” between individ-
ual measurements. Measuring the temperature w.r.t. time is a continuous
process (at least when observing a thermometer or neglecting the sampling
rate of the measurement device).
We often want to infer the most likely underlying continuous distri-
bution of a dataset because the shape of a distribution might give us
important information about the underlying physics. For example, the type
of grain size distribution (rather log-normal distribution or not) can give
information regarding the type of grain growth.
(cid:129) Categorical random variables represent qualitative data of different
classes or categories without any particular order. An example is drawing
colored marbles from a jar and recording the colors. Although we could
assign numbers to each color, e.g., red (cid:2)→1, blue (cid:2)→2, green (cid:2)→3, there
. . .
is no point in asking if 2 (green) is larger than 1 (blue)!
Materials science examples include the material type or chemical
element or if a specimen was fractured or not.


================================================================================
PAGE 126
================================================================================

6.1 RandomVariables 105
6.1.1 Discrete Random Variables
A random variable X reflects the relevant aspects of an experiment that we are
interested in. For example, when X represents the number of Tails thrown, X takes
the integer values 0 for HHHH, 1 for H HHT, or HHTH, etc., and ranges over all
numbers from the set {0,1,2,3,4}. The concept of random variables is defined for
.
a discrete sample space in Definition 6.1.
Definition 6.1 (Discrete Random Variable). A discrete random variable is
a function that maps each individual item from the sample space to a number.
Formally, this is written as
X :(cid:2)→Z (6.1)
.
where X is the random variable andZrepresents the set of all integer numbers
.
{...,−2,−1,0,1,2,...}. The “destination” of X is called image of the
.
random variable.
The purpose of a discrete random variable is to map a discrete, possibly non-
mathematical, description of a particular outcome to a mathematical number, such
that from then on mathematical calculus can be used. Note that even though X
is called random variable, it is not a classical variable, but rather a function or
“lookup table.” The naming often creates some confusion, but it is, nonetheless,
the commonly used notion. As opposed to continuous random variables, a discrete
variable can take countable values in a finite period of time, even though there may
be an infinite number of them (as, for example, in the set of integers, Z).
.
Another often used and probably slightly confusing notation is the following:
while the random variable is denoted by an uppercase letter X, each of its values is
denoted by a lowercase x, e.g., x . More details will be given below.
. 1
The purpose of the random variable is therefore twofold: (i) To condense all
outcomes of an event into a single variable (ii) To replace, e.g., the textual labels
such as “Heads” or HTTH by numerical values—to be more precise: in that case
by integer numbers. Only then are we able to perform calculations and compute
probabilities as demonstrated in Example 6.2 for the case of tossing a coin four
times.


================================================================================
PAGE 127
================================================================================

106 6 RandomVariablesandProbabilityFunctions
Words of advice (cid:2)
In machine learning we often encounter categorical variables that represent
a finite number of different classes. As categories are not necessarily num-
bers,theymightnothavealogicalorder.Inparticular,thereisnouniqueway
of mapping the categories to integer numbers. As categorical data cannot
be ordered, there is also no unique “cumulative distribution” (cumulative
distribution functions will be introduced in Sect.6.4.2).
Keeping this limitation in mind, our definition of the discrete random vari-
able in Definition 6.1 implicitly covers categorical data as well.
6.1.2 Continuous Random Variables
So far, the outcomes of a random experiment were always of categorical type,
and the number of different outcomes was usually countable. Continuous random
variables, on the other hand, can only take on continuous values
Definition 6.2 (Continuous Random Variable). A continuous random vari-
able is a function that maps each of the infinitely many elements of the sample
space to a real number. Formally, this is written as
X :(cid:2)→R (6.2)
.
where X is the random variable and R denotes the real numbers. The
.
“destination” of X is called image of the random variable.
The lifetime of a specimen is an example for a continuous random variable.
For example, the lifetime could have arbitrary values from within the range of 0
to 2 years (zero because some specimens are defective right from the beginning and
a maximum value irrespective of the kind of usage assuming, e.g., that after 2 years
some parts simply fail or are worn out).
6.1.3 Assigning Probabilities to Discrete Random Variables
Assigning probabilities to events is straightforward for simple experiments. For
example, for the case of 4 times tossing a coin, each of the 16 possible outcomes
is equally likely and has a probability of 1/16. Computing the probability of events
.
follows Eq. (5.14) and, therefore, requires counting the number of outcomes that


================================================================================
PAGE 128
================================================================================

6.1 RandomVariables 107
belong to a particular event. To denote the probability of event number 3, we would
write P(x ), while the collection of all x represents the random variable X. See
. 3 . i
Example 6.2 for a demonstration.
Example 6.2 (Random Variables and Event Probabilities (Part 1)) (cid:2)
Consider the random experiment of four times tossing a coin. The according
sample space (cid:2) consists of N = 16 elements. What is the random variable
. .
and how can we compute probabilities?
The random variable X could be the number of Heads occurring in the
experiment. Each “entry” or measurement of this variable (or mapping) is
denoted by a lowercase variable name, e.g., no Heads is represented by x =
. 1
0, one Head byx =1, etc., up tox =4. The subscript i inx does not stand
. 2 . 5 . i
for the number of Heads; the value of x does. For assigning probabilities to
. i
events, we count how many outcomes contribute to each entry of the random
variable:
Number of
contributing
Event x =? Outcomes outcomes n
. i . i
0 Heads x = 0 TTTT 1
. 1
1 Heads x = 1 TTTH, TTHT, THTT, HTTT 4
. 2
2 Heads x = 2 HHTT, HTHT, HTTH, THHT, 6
. 3
THTH, TTHH
3 Heads x = 3 HHHT, HHTH, HTHH, THHH 4
. 4
4 Heads x = 4 HHHH 1
. 5
As each outcome has an equal likelihood of 1/16, the probabilities of the
.
contributing outcomes for X are P(x ) = n /N = 1/16, P(x ) = 1/4,
. 1 1 . 2
P(x )=3/8, P(x )=1/4, and P(x )=1/16.
. 3 . 4 . 5
[This example is continued in Example 6.3]
6.1.4 Assigning Probabilities to Continuous Random Variables
In the case of discrete events, we could simply write down probabilities and assign
them to probabilities, e.g., in the form of a table as in Example 6.2. However, in the
case of continuous random variables, this is not feasible anymore because then the
random variable may assume infinitely many values. As an example, consider the
measurement of a temperature T : even if the temperature varies only within a small
range of[T−(cid:4)T,T+(cid:4)T], this interval still contains infinitely many real numbers,
.
and thus, we cannot write all of them down in a table. Mapping the values of the


================================================================================
PAGE 129
================================================================================

108 6 RandomVariablesandProbabilityFunctions
random variable to probabilities can be done by another function, the probability
function. Such a function can be defined for both discrete and continuous variables
and will be introduced in what follows.
6.2 Introduction of Probability Functions
The previous sections introduced the two different types of random variables,
and we have also seen that random variables are somehow linked to probabilities
(compare Example 6.2). We will now formalize this relation.
6.2.1 Why Do We Need Probability Functions?
While linking random variables to probabilities is straightforward for the case of
discrete random variables, the case of continuous random variables is more complex
because the image of the random variable (cf. Definition 6.1) is not countable
anymore. Thus, it is not possible to write down all values of the random variable
together with the respective probabilities. The remedy comes in the form of another
function, the probability function. A visual representation of how these concepts are
linked is shown in Fig.6.1.
In this figure, the probability mass function maps a discrete random variable
to the probability value, while for the case of a continuous random variable, the
mapping is achieved by the probability density function. Both functions will be
introduced in more detail below. We start with a general definition:
Definition 6.3 (Probability Function). A probability function is a mathe-
matical function that assigns a specific probability value to any outcome of a
random experiment.
Fig. 6.1 Visualization of how discrete random variables are linked to the sample space, on the
one hand, and to probabilities, on the other hand, using the example of two coin tosses. The four
discrete events from the sample space are mapped to integer numbers, each of which is associated
with a probability. The set in the middle is called the “image of the random variable”


================================================================================
PAGE 130
================================================================================

6.2 IntroductionofProbabilityFunctions 109
Hence, probability functions are closely related to the concept of random
variables: while the random variable assigns a value to each event or outcome, the
probability function gives us the probability value of the event represented by the
number.
Words of advice (cid:2)
Probability functions are also called probability distribution functions or in
short probability distributions. Note that a probability distribution function
is different from the probability density function, which is often abbreviated
as PDF and is a special type of a probability distribution. However, this
distinction is not always followed consistently in the literature.
Probability functions can be categorized depending on the type of the random
variable, i.e., whether it is a qualitative or quantitative data. A second way of
classification is based on the number of random variables. The knowledge about
these two categories will help us later to choose the appropriate data analysis or
machine learning (ML) method.
6.2.2 Classification of Distribution Types
The First Type of Classification
This classification is with regard to the type of random variable. Already in Sect.6.1,
we differentiated between discrete and continuous random variables. We will now
extend the classifications to probability functions. The following cases are to be
differentiated:
(cid:129) Discrete variables take only a countable number of particular values. A discrete
variable is always numeric, and therefore, their values can be sorted, e.g., from
the smallest to the largest number. A probability function with discrete variables
is called a probability mass function (PMF) and will be introduced in more detail
in Sect.6.3.
(cid:129) Continuous variables can take an infinite number of values. The corresponding
probability function that uses continuous variables is called a probability density
function (PDF) and will be introduced in more detail in Sect.6.4.
(cid:129) Categorical variables are a third type that has relevance for ML. They represent a
finite number of different classes. As categories are not necessarily numbers, they
might not have a logical order. In particular, there is no unique way of mapping
the categories to integer numbers. A probability function with categorical vari-
ables is called a categorical distribution or a generalized Bernoulli distribution.
As such, it is also a kind of discrete distribution.
The difference between discrete and categorical variables is not always pointed out
clearly in the ML literature. It is often assumed that simply numbering the different


================================================================================
PAGE 131
================================================================================

110 6 RandomVariablesandProbabilityFunctions
categories is sufficient to turn categorical variables into discrete variables. While for
many ML classification problems this works well, there is a fundamental difference,
as already highlighted in the previous chapter: as categorical data cannot be ordered,
there is also no unique cumulative distribution (cf. Sect.6.4.2).
A Second Type of Classification
An entirely different type of categorization relates to the number of random
variables:
(cid:129) Univariate distributions are the simplest type of probability functions that contain
only one random variable. Univariate distributions are often encountered and
serve as the fundamental “building blocks” for distributions with more variables.
(cid:129) Bivariate distributions are distributions that are based on two random variables.
Many real-life problems are represented as bivariate distributions because they
can still be easily visualized.
(cid:129) Multivariate distributions are the logical extension of the univariate distribution
toward probability distributions with more than one variables and, therefore, are
the most general case.
In what follows, we will focus on the first type of classifications and introduce the
respective distribution types.
6.3 Discrete Probability Distributions
When the random variable(s) take only discrete values, the discrete probability
functions are fully described by simply stating the corresponding probability.
6.3.1 Probability Mass Functions
Discrete probability functions result in the corresponding probability for discrete
values. Such a probability for each possible outcome x is usually called relative
. i
frequency
.
P(X = x
i
). This function is then called PMF,
.
P(X), which performs the
following mappings:
x (cid:2)→p , ..., x (cid:2)→p . (6.3)
. 1 1 n n
Here, the x are particular values of the random variable X (e.g., different mea-
. i
surements), and the p are the corresponding probabilities. For practical purposes,
. i
this mapping can be represented in form of a table, which contains the summa-
rized recordings of a random experiment. The recordings consist of the absolute
frequencies (i.e., the number of times) with which each specific event occurs. Such a
table is called frequency distribution table . An example is shown for a coin tossing
experiment in Fig.6.2. This table is a complete representation of the probability


================================================================================
PAGE 132
================================================================================

6.3 DiscreteProbabilityDistributions 111
Result Absolute Frequency
Heads 4
Tails 3
Fig. 6.2 Frequency distribution table for seven rounds of a coin toss. Recorded were the number
of times the event Heads or Tails occurred
function. Additionally, from such a table, the information of the probability as the
relative frequency P(X = x)can be derived. It is given by
.
n
P(X =x )= i (6.4)
. i
N
with N as the total number of samples and n the number of times that i occurs.
. i
A table that shows the probabilities for all events x is called probability table . It
. i
is a purely “data-based” method to represent random experiments and does not say
anything about possible underlying functional relations.
An important property of the probability P is that the sum of over all events must
equal 1,
(cid:2)
P(X =x )=1. (6.5)
. i
i
This also serves as a good test if a dataset is in a consistent state.
Example 6.3 (Random Variables and Event Probabilities (Part 2)) (cid:2)
[This is the continuation of Example 6.2] Summing up all individual proba-
bilities, we get the probability of 1:
(cid:2)5
1 1 3 1 1
P(x )= + + + + =1, (6.6)
. i
16 4 8 4 16
i=1
which means that our list of events indeed covers all possible cases. Two of
the many possible visualization types of a random experiment with discrete
events are shown in Fig.6.3. The plots show the results of conducting the
above experiment of coin flipping 500 times. The left panel shows the number
of times each item of the sample space was tossed. The dotted horizontal line
indicates the ideal “theoretical” value—each of the 16 outcomes should have
the same value of 500/24. The panel on the right shows how often each of
.
the five events has occurred. Small deviations from the theoretical values can
be seen. It is expected that these deviations would vanish as the sample size
approaches infinity.


================================================================================
PAGE 133
================================================================================

112 6 RandomVariablesandProbabilityFunctions
Fig. 6.3 A random experiment of four coin tosses. The plots show the data points as small circles.
The vertical columns are used to guide the eye. Left: number of times that each possible outcome
has occurred; the dotted horizontal line indicates the “theoretical” value. Right: number of times
that0,...,4Headshasoccurred.Theshortorangedashesindicatetheidealortheoreticalvalues
Relative
Absolute Relative Cumulative
Result cumulative
Frequency Frequency ( ) Frequency
frequency
Heads 4 0.571 4 0.571
Tails 3 0.429 7 1.000
= ( )=1
Fig. 6.4 Frequency distribution table and derived quantities (highlighted in light blue) for .N =7
rounds of a coin toss experiment
6.3.2 Cumulative Frequencies
Further derived quantities are the cumulative frequencies. In this case, the i-th value
is obtained by summing up all values of the absolute frequency to and including
the i-th event. If instead the relative frequencies are summed up, the relative
cumulative frequency is obtained. With these definitions, the above frequency
distribution table can be extended, as shown in Fig.6.4. As we will see later,
the cumulative distribution function is defined for both discrete and continuous
variables, in contrast to the PMF, which is only defined for discrete random variables.


================================================================================
PAGE 134
================================================================================

6.4 ContinuousProbabilityDistributions 113
6.4 Continuous Probability Distributions
Whenever the possible outcomes of a random experiment can take on continuous
values within a certain range of numbers, the relation between values of the
random variable and the corresponding probabilities are typically described by the
probability density function.
6.4.1 Probability Density Functions
A continuous probability density function (PDF) is often given by the function
f. Probability density functions are ubiquitous in materials science and a direct
consequence of the continuous nature of many phenomena. For example, “nature
never skips” any temperature values between two observed temperature values, but
rather varies smoothly—even though quick changes are possible.
Transferring what we have already learned about PMFs to continuous situations
is not entirely trivial because measuring the probability “P(x)” (i.e. the relative
.
frequency) of a continuous value is not possible. The reason is that the probability
with which a random variable takes on a specific value x is infinitesimally small.
Why? Because continuous random variables can take on infinitely many different
values (recall that an interval with real numbers contains infinitely many real
numbers), such that encountering any of them has a vanishing probability.
Therefore, we normally consider the probability that x is within a certain range
. [a,b]. This is exactly what can be obtained from integrating the PDF f.Thisisalso
the reason why the PDF is called a probability density (a proper definition will be
given in Definition 6.5). Mathematically, this is written as
(cid:3)b
P(a ≤x ≤b)= f (x)dx (6.7)
. X .
a
(cid:3)b (cid:3)a
= f (x)dx − f (x) dx (6.8)
X X
−∞ −∞
In other words, the probability P that x is within the interval [a,b]is given by the
.
corresponding area under the curve of the PDF . f X . In this text, we use an explicit
notation by including the name of the random variable as a subscript, e.g., the
random variable X in f (x). Then, x denotes a particular sample. An alternative
. X
notation is f(x), which can also be found in the literature and requires the name of
.
the random variable to be given in the context.
We directly find that in Eq.(6.7), the probability of a point is obtained from
computing P(x) ≡ P(a ≤ x ≤ a). As a consequence, the two integral limits have
.
the same value and, thus,P(x)=0. This consideration is critical to understand how
.
to interpret PDFs. In terms of a geometrical interpretation, we could say that the area


================================================================================
PAGE 135
================================================================================

114 6 RandomVariablesandProbabilityFunctions
Fig. 6.5 Probability density function (PDF): the light-shaded region graphically represents the
area that corresponds to the probability that a random value is between . −1and +3. The shaded
area would be roughly estimated to be a bit more than half of the total area under the curve, and
therefore, the associated probability is somewhat larger than .50%(the exact, computed value is
.62.5%)
of a line is zero. An additional consequence is that
P(a ≤x ≤b)= P(a <x <b), (6.9)
.
because the interval “end points” do not contribute any area to the integral. Finally,
the area under the whole PDF must be 1,
(cid:3)+∞
f (x)dx =1. (6.10)
. X
−∞
This is the continuous analogon to the discrete case, in which the sum of the
probabilities of all possible events has to be 1, cf. Eq.(6.5).
Figure 6.5 shows a visualization of a PDF. We see that the likelihood of observing
a value ofx ≈2is the highest (even though we cannot interpret the maximum value
.
of 0.2 directly). Finding very large positive and negative values, e.g., |x| > 10, is
. .
quite unlikely. In this way, we can use the PDF to compare different likelihoods,
while we cannot quantify them (meaning: we cannot directly express the values of
the PDF in terms of a percentage as for probabilities).
Words of advice (cid:2)
In Fig.6.5, the probability of finding a value of X = 2is not 0.2 (or 20%). A
. .
PDF needs to be interpreted differently: you always need to associate areas
under the curve with probability and notindividualpoints!


================================================================================
PAGE 136
================================================================================

6.4 ContinuousProbabilityDistributions 115
However, we can use Eq.(6.7) to compute the probability of finding a value of, e.g.,
−1 ≤ x ≤ 3: we compute the integral in the limits of a = −1 and b = 3, which
. . .
in Fig.6.5 is highlighted as the shaded area under the curve. Here, this would give
a value of ≈0.625 or 62.5%. For example, if this random variable represented a
. .
number of displacement measurements in units of millimeters, the probability that
the displacement is within −1...3mmwould be 62.5%.
. .
Now assume that the range of displacements is narrowed down to, say,
−1...1mm. Obviously, the area under the curve in this range is smaller, and
.
thus, the probability to find a displacement value between −1 and 1mm is also
.
smaller (it is only ≈24% to be precise). Shifting the whole range by 1mm again
.
increases the probability. Hence, the probability depends first and foremost on the
characteristics of the situation that we describe with a PDF, i.e., the shape of the
distribution. Second, the probability also depends on the size and the location of the
range within which the area under the curve is calculated.
How continuous is nature? (cid:3)
Many phenomena in physics and materials science can be modeled as contin-
uous functions, which are usually even considered as always differentiable.
However, the length and time scales considered (or the resolution with which
a problem is viewed) are often decisive in materials science and physics
problems:
For example, during a martensitic transformation, the atomic positions
of a specimen are switched nearly instantaneously resulting in a different
crystal symmetry. If we studied this phenomena by, e.g., taking an electron
backscatter diffraction (EBSD) image every other second, the transformation
would be observed as a time-discrete event with only two possible outcomes
(the two states of the material). However, on a nanoscale level, the atomic
positions still change only gradually (within a few nanoseconds). Similar
scale aspects can be observed in crystalline defects, such as dislocations or
when we look at shock waves in solids or fluids.
6.4.2 Cumulative Distribution Functions
An alternative to the PMF and the PDF is the cumulative distribution function (CDF).
We start by giving a general definition:


================================================================================
PAGE 137
================================================================================

116 6 RandomVariablesandProbabilityFunctions
Definition 6.4 (Cumulative Distribution Function). The cumulative distri-
bution function F of a random variable X is defined as
. X
F (x)=P(X ≤x) forall x ∈R. (6.11)
. X
where X is the random variable and Rdenotes the real numbers.
.
In this definition,F (x)is the probability, and the random variable X has a value
. X
smaller than or equal to x. In this text, we use an explicit notation by including
the name of the random variable X. This is equivalent to the shorter variant F(x),
.
which can also be found in the literature. In Example 6.4 an example is given,
which shows how to obtain the cumulative distribution function (CDF) for a discrete
random variable. “Jumps” at positions, at which the PMF is non-zero for a discrete
random variable, are typical features of the CDF.
Example 6.4 (CDF of Two Subsequent Coin Tosses) (cid:2)
Assume that we perform two coin tosses in a row and that we are interested
in the number of Tails, which represents our random variable X. What is the
corresponding CDF?
We already know the frequency and can easily infer the probabilities for
the three events (i.e., the number of Tails):
.
P (X =0)=1/4, P(X =1)=1/2, andP(X =2)=1/4,
from which we directly could plot the PMF. To obtain the CDF we need to
look at three different cases:
forx <0: F (x)=P(X ≤x)=0
. X
for0≤x <1: F
X
(x)=P(X ≤x)=P(X =0)=1/4
for1≤x <2: F
X
(x)=P(X ≤x)=P(X=0)+P(X =1)=1/4 +1/2 =3/4
forx ≥2: F (x)=P(X ≤x)=P(X=0)+P(X=1)+P(X=2)
X
=1/4 +1/2 +1/4 =1.
Unlike for continuous variables, there is a difference between “<” and “≤,”
. .
whichinFig.6.6 is reflected by two different types of markers. Also observe
that even though the PMF consists of only three points at the coordinates of
.
(0;0.25),
.
(1;0.5), and
.
(2;0.25),theCDF consists of lines—even outside the
region, in which the PMF “has data.”


================================================================================
PAGE 138
================================================================================

6.4 ContinuousProbabilityDistributions 117
Fig. 6.6 Cumulative distribution function for two coin tosses, where X denotes the number of
times we get Tails. The full circle indicates that the value is included, and the open ones indicate a
“jump,” i.e., the value is not included as in an open interval
In the case of a continuous random variable, the CDF is continuous, and if the
CDF is differentiable, we can easily obtain the PDF as a derivative. In fact, this is one
of the definitions of the PDF:
Definition 6.5 (Probability density function). Assume that X is a contin-
uous random variable and
.
F
X
(x) is the corresponding continuous CDF. If
. F X (x)is differentiable, the PDF . f X (x)is defined as
dF (x)
f (x)= X . (6.12)
. X
dx
With this definition, it follows that the CDF of a continuous random variable X is
the integral of its PDF . f X , and the CDF gives the area under the PDF from . −∞to x,
(cid:3)x
F (x)= f (t)dt . (6.13)
. X X
−∞
The probability that the random variable is in the interval (a,b]is
.
(cid:3)b
P(a <X ≤b)=F (b)−F (a)= f (t)dt . (6.14)
. X X X
a
Integrating over the complete Rfrom −∞to +∞the integral must be 1
. . .


================================================================================
PAGE 139
================================================================================

118 6 RandomVariablesandProbabilityFunctions
Fig. 6.7 The top panel
shows the probability density
function (PDF),.f X,andthe
bottom panel shows the
cumulative distribution
function (CDF),.F X.Both
distributions are evaluated at
the point x, which is chosen
slightly off the maximum of
.f X. The value of the CDF at
.X=xis obtained as the
red-shaded area under the
PDF from. −∞up to x.The
CDF has an inflection point at
.x =2,whilethePDF reaches
its maximum. Values of the
variable X at the maximum of
the PDF are most likely. Any
CDF must approach 1, when
the random variable goes
toward infinity
(cid:3)∞
f (t)dt =1, (6.15)
. X
−∞
which is equivalent to stating that the total area under the PDF must be 1. The
relationship between PDF and CDF is visually explained in Fig.6.7.
While, in principle, PDF and CDF contain the same information, a CDF always
looks “smoother” because it is obtained as an integral of the PDF. Generally,
during integration (or also during summation), small values and large values are
added up, which effectively is a kind of “averaging.” Visually, but sometimes even
computationally, it is difficult to identify the peak of a PDF based on a CDF because
inflection points are more difficult to spot than extrema (again: because F (x) is
. X
smoother). If the shape of a distribution is of interest and we start by looking at a
the CDF an additional step is required, e.g., we have to try to create the derivative
of the CDF in our mind, which basically shows us the PDF. A convenient property
of the CDF, though, is that we can easily tell the probability for a random variable
to be below or above a certain value, as this property is, in a way, directly contained
in the definition of the CDF.InFig.6.7 the probability that
.
X ≤ 3is approximately
70%.
.
Example 6.5 is a complete example that shows how to use the above axioms and
equations and how to derive the CDF from a PDF. It also shows how to compute
probabilities from the CDF and, therefore, serves as an important template for other
problems as well. Exercise 6.3 is a variation of this problem.


================================================================================
PAGE 140
================================================================================

6.4 ContinuousProbabilityDistributions 119
Example 6.5 (Computing with PDFs and CDFs) (cid:2)
Assume that for the random variable X, the PDF of the negative exponential
function (with the scalar parameter λ)isgivenby
.
(cid:4)
λexp(−x) forx ≥0andλ>0
f (x)= (6.16)
. X
0 otherwise
Task 1: Determine . λ such(cid:5) that . f X (x)is indeed a PDF For this, we have to
find λsuch that Eq.(6.15), ∞ f (t)dt =1, holds. Insertion of f gives
. . −∞ X . X
(cid:3)0 (cid:3)∞
f (t)dt + f (t)dt = −λexp(−x)|∞ =λ, (6.17)
. X X 0
−∞ 0
from which follows thatλmust be 1. Additionally, we havef (x)≥0for all
. . X
x.
Task 2: Derive the formulation of the CDF We start from the definition
(cid:5)x
on Eq.(6.13), . F X (x) = f X (t)dt and insert the PDF obtained above. For
−∞
x <0it is F (x)=0, and for the case that x ≥0,itis
. . X .
(cid:3)x
F (x)= f (t)dt = −exp(−t)|x =1−exp(−x). (6.18)
. X X 0
0
Task 3: What is the probability that a randomly drawn sample will have
a value in the range of [1,3] Thus, we are looking for P(1 < x ≤ 3)(the
. .
<or ≤does not matter here):
. .
P(1<x ≤3)=F (3)−F (1)=(1−exp(−3))−(1−exp(−1))≈0.319
. X X
(6.19)
The probability is approximately 32%. The figure below shows the PDF and
.
CDF together with some values relevant for Task 3 (Fig.6.8).


================================================================================
PAGE 141
================================================================================

120 6 RandomVariablesandProbabilityFunctions
Fig. 6.8 PDF and CDF of the negative exponential function. The vertical difference between the
dashed lines is.P (1 <x≤3)
6.4.3 The Difference Between a Function and a Distribution
We conclude this section by answering the question why we differentiate between a
function and a distribution. Let’s first summarize what a function is: A function in
general is a mapping from an n-dimensional space Rn to an m-dimensional space
.
Rm. For the special case ofm=n=1, it is a function that takes a scalar and results
. .
in a scalar, e.g.,
f(x)∈R with x ∈R,
.
meaning f is the mapping of a value x to a value f(x), which is written as
.
f :x (cid:2)→ f(x). (6.20)
.
A distribution can be understood as a more general object: some distributions
(i.e., the continuous ones) are, in fact, represented by functions, but others may
consist of discrete data and are represented by a table as in the case of the PMF.
Because we want to explicitly state this difference in this text, we use the notation
f(x) for functions and f (x) for distributions. Last but not least, our notation for
. . X
samples, data records, and the data matrix, as introduced in the ML chapters, also
reflects this difference.
6.4.4 A Brief Summary of the Most Important Properties
Here is a brief summary of the most important properties and abbreviations used:


================================================================================
PAGE 142
================================================================================

6.5 MultivariateDiscreteandContinuousDistribution 121
Things to Remember (PDFs and CDFs) (cid:3)
The most important properties of a PDF.fX(x)for a continuous random variable X
can be sorted in analogy to the three Kolmogorov axioms, cf. Sect.5.2.5:
(cid:129) corresponding to the.1 staxiom: .f X(x)≥0 forall x ∈R
(corresponds to Eq.(5.25))
+(cid:3)∞
(cid:129) corresponding to the.2ndaxiom: . fX(t)dt =1
−∞
(corresponds to Eq.(5.26))
(cid:129) corresponding to the .3rdaxiom: .P (a < X ≤ b) = FX(b)−FX(a) =
(cid:3)b
fX(t)dt
a
(corresponds to Eq.(5.27))
The relation between a PDF and a CDF is given by the equation
(cid:3)x
.FX(x)= fX(t)dt . (6.21)
−∞
6.5 Multivariate Discrete and Continuous Distribution
So far, everything has been formulated in terms of a single random variable. In many
situations, however, the relation between several (i.e., ≥2) random variables is of
.
interest. For example, consider a Monte Carlo simulation of point defects in metals:
we might be interested in how the rate of point defect nucleation is correlated with
the effective heat conduction of the material.
We will now start with the most general case of multivariate distributions.
Instead of a single random variable, we encounter N random variablesX ,...,X ,
. 1 N
which we abbreviate1 as X. The joint probability is a notion that we will always
.
encounter in this context. It is closely related to the independence of events (compare
Definition 5.13) and defined as follows:
1 In probability theory, a boldface .X is often used as a short form for a set of random variables
.X i. In this text, we use this notation for convenience (and brevity) but in particular for consistency
with the later chapters.


================================================================================
PAGE 143
================================================================================

122 6 RandomVariablesandProbabilityFunctions
Definition 6.6 (Joint Probability). Given are two or more random variables
X , ..., X , each of which representing the occurrence of independent
. 1 N
events (cf. Definition 5.13), E , ..., E , such that
. 1 N
P(E ∩···∩E )=Π P(E )). (6.22)
. 1 N i i
The joint probability is the probability that all N events happen at the same
time.
The property of events being independent of each other implies that the events
do not influence each other. For example, if two events were the elongation of
a specimen in a tensile test and the temperature value, the first event would not
influence the latter (if this is different, we have to be very careful when analyzing
the data). Note that temperature and elongation can still be correlated, though—this
is a completely different cup of tea!
Joint probability distributions, or sometimes joint probability functions, are the
multivariate pendant to the PMF and the PDF.
Multivariate Distributions vs Joint Distributions (cid:3)
A multivariate distribution is also called joint distribution. Usually, when
we talk about a particular example of a distribution, we say, e.g., “the joint
distribution of X and X has a maximum at ...” and only when we talk
. 1 . 2
about multivariate distributions in general, then we would rather say, e.g.,
“multivariate distributions are based on more than one random variable.” Last
but not least, when we refer to probabilities of a multivariate distribution, we
typically denote this as “joint probability.”
6.5.1 Generic Formulation of the Joint CDF
The joint cumulative distribution function can be defined in a way that holds both for
continuous and discrete random variables. Assume a given samplex =[x ,...,x ]
. 1 N
of N random variables, based on which a CDF is evaluated. This can be written as
F (x)=P(X ≤x ,...,X ≤x ), (6.23)
. X 1 1 N N
which is the probability that each of the N random variables X has a value ≤ x .
. i . i
Recall that the argument of P in this equation is a compact notation for


================================================================================
PAGE 144
================================================================================

6.5 MultivariateDiscreteandContinuousDistribution 123
P((X ≤x )and(X ≤x )and ...and(X ≤x )) (6.24)
. 1 1 2 2 N N
⇔ P((X ≤ x ) ∩ (X ≤x )∩ ... ∩(X ≤ x ))
1 1 2 2 N N
Each expression(X ≤x )defines a set of points, and the resulting joint probability
. i i
is the probability of finding the data in the intersection of all N sets. For brevity,
however, we use the shorter form from Eq.(6.23) most of the time.
6.5.2 Joint Probability Functions for Discrete Random Variables
For the joint CDF, the generic formulation from Sect.6.5.1 holds. A more concrete
formulation can be given in terms of summation, as shown below. Subsequently,
also the joint mass function is introduced and demonstrated using a formulation
with two random variables.
Joint Probability Mass Function (Joint PMF)
The joint probability mass function (or sometimes just joint function or joint prob-
ability) is the multivariate pendant to the univariate PMF. It returns the probability
that the random variable X has the value x , random variable X has the value x ,
. 1 . 1 . 2 . 2
etc. It is given by
P(X =x ,...,X =x ). (6.25)
. 1 1 N N
The comma between the “X = x ” terms should be understood as an “and”
. i i
condition between the terms. As a shorthand notation, we sometimes do not
explicitly state the name of the random variable, and only write p(x ,...,x ),
. 1 N
when it is clear from the context that the x are the values of random variables
. i
instead of mere function arguments.
As in the univariate case (cf. Sect.6.3), the joint PMF can be described by a
“probability table.” For N random variables, the table effectively becomes an N-
dimensional array of data. An example for the bivariate case is shown below.
Joint Cumulative Distribution Function (Joint CDF)
The joint cumulative distribution function can be obtained from summing joint PMFs
p, in analogy to the univariate case:
(cid:2) (cid:2)
F (x)=P(X ≤x ,...,X ≤x )= ··· p(x˜ ,...,x˜ ) (6.26)
. X 1 1 N n 1 N
x˜
1
≤x1 x˜
N
≤xN
6.5.3 JointProbabilityFunctionsforContinuousRandomVariables
As in the discrete case, also for continuous random variables, the generic formula-
tion of the joint cumulative distribution function from Sect.6.5.1 holds.


================================================================================
PAGE 145
================================================================================

124 6 RandomVariablesandProbabilityFunctions
Joint probability density function (Joint PDF)
As in the univariate case, we use f to indicate that this is a density distribution that
needs to be integrated in order to obtain a probability. The joint probability density
function of N random variables is then given by
f (X =x ,...,X =x ). (6.27)
. X 1 1 N N
f is a PDF of the random variables . X 1 ,...,X N , if (and only if) the following three
conditions are met:
(cid:129) Probability values are always positive or zero:
f (X =x ,...,X =x )≥0 forallx ∈RN . (6.28)
. X 1 1 N N
(cid:129) The integral of all probabilities must be 1:
(cid:3)+∞ (cid:3)+∞
··· f (x˜ ,...,x˜ )dx˜ ×···×dx˜ =1. (6.29)
. X 1 N 1 N
−∞ −∞
(cid:129) For any subset A:=A ×···×A ⊂R×···×R=RN,itis
. 1 N
(cid:3) (cid:3)
P((X ,...,X )∈A)= ··· f (x˜ ,...,x˜ )dx˜ ×···×dx˜ .
. 1 N X 1 N 1 N
A1 AN
(6.30)
These relations look somewhat more complicated due to a number of variables
and integrals; they are, however, the same as those presented in Sect.6.4.2 for the
univariate case.
Joint cumulative distribution function (Joint CDF)
In analogy to the univariate case, the CDF of a continuous random variable . X is
obtained as the integral of the PDF . f X . For the multivariate case, one has to perform
integration for each of the N random variables:
(cid:3)x1 (cid:3)xN
F (x)= ... f (t ,...,t )dt ×...×dt . (6.31)
. X X 1 N 1 N
−∞ −∞


================================================================================
PAGE 146
================================================================================

6.5 MultivariateDiscreteandContinuousDistribution 125
6.5.4 Marginal Probability
Assume that a random variable X assumes the value x regardless of which value
the other random variables have. This probability is then written as P(X = x)or,
.
in short, as p(x). As an example, in Fig.6.9 the number of events contributing to
.
p(x ) is indicated by the highlighted vertical column. The marginal probability is
. i
computed as
(cid:2)r
1
P (X =x )= n withN thetotalnumberofevents (6.32)
. i i,j
N
j=1
The marginal probability that Y takes the value y is computed in analogy by
. j
summing over all elements of the respective row,
(cid:2)s
1
P (Y =y )≡ p(y )= n . (6.33)
. i j ij
N
i=1
6.5.5 Conditional Probability
Assume that we now only consider the events with X = x. The probability
.
of those events, for which additionally Y = y, is the conditional probability
.
P (Y =y|X =x). It is written in short form as p(y|x) (read: “probability of y
. .
given x”). In terms of the distribution shown in Fig.6.9,p(y|x)can be computed as
.
follows:
(i) restrict the set of all considered outcomes to the highlighted column y , i.e., we
. i
only consider the absolute frequencies n ,...,n ;
. 1j qj
(ii) then, compute the probability of y only w.r.t. the events in that column.
. i
Writing this mathematically gives
(cid:6) (cid:7) n
P Y =y |X =x ≡ p(y |x )= ij , (6.34)
. j i j i (cid:8)s
n
ij
j=1
and in full analogy for the other conditional probability
(cid:6) (cid:7) n
P X =x |Y =y ≡ p(x |y )= ij . (6.35)
. i j i j (cid:8)q
n
ij
i=1


================================================================================
PAGE 147
================================================================================

126 6 RandomVariablesandProbabilityFunctions
Isp(y |x )the same asp(x |y )? (cid:3)
. j i . i j
No, p(y |x )and p(x |y )are not the same. To see this, we can take a look
. j i . i j
at Fig.6.9. In the first case, the relative frequency of the event at row i and
column j is computed with respect to the elements highlighted in the row,
while in the second one, it is computed with respect to the column. As the
random variables can have different numbers of states, the two conditional
probabilities generally are not the same.
As a concrete example, assume that all n have the value 1, but X has
. ij
six possible states and Y has seven. For this case we get p(y |x ) = 1/7and
. i i
p(x |y )=1/6.
. i j
6.6 Bivariate Distributions as a Special Case
6.6.1 Joint Probability for Two Discrete Random Variables
If we have more than one random variable, the table representing the PMF is
a multidimensional array. For two variables, X and Y, this can still be easily
visualized. The probability for event x and event y both to happen is the joint
. i . j
probability
(cid:6) (cid:7) n
P X =x ,Y =y = ij (6.36)
. i j
N
where again N is the number of all possible outcomes and n the number of
. ij
outcomes of event x and event y . An alternative notation, which is commonly
. i . j
used and borrowed from set theory, is
P(X =x ∩Y =y ). (6.37)
. i j
A visualization is shown in Fig.6.9.
6.6.2 Joint Probability for Two Continuous Random Variables
A joint probability density function of two random variables, X and Y, is the
continuous extension of the PDF in the univariate case. The probability that two
variables X and Y are located within a certain value range [a ,b ]and [a ,b ]is
. X X . Y Y
obtained from the joint PDF . f XY (x,y)as
(cid:3)bX (cid:3)bY
P(X,Y)= f (x,y)dxdy . (6.38)
. XY
aX aY


================================================================================
PAGE 148
================================================================================

6.6 BivariateDistributionsasaSpecialCase 127
··· ···
1 2 3
··· ···
1 11 12 13 1
. . .
2 21 2
. . .
. . .
. . .
··· ··· ···
1 2
. . .
. . .
. . .
1
Fig. 6.9 Bivariate probability mass function: visualization as table of absolute frequencies of a
discrete distribution for two random variables X and Y with q and r differen(cid:6)t states, respectiv(cid:7)ely.
The number of events with states .x i and .y j are .n ij; the joint probability .P X=xi,Y =yj =
p(xi,yj)is.nij/(q·r)and highlighted with the dark shading
Fig. 6.10 Bivariate
probability density function
of the variables X and Y.
.p(X)and.p(Y)indicate the
marginal distributions
Geometrically, this can be integrated as the section of a surface integral, which
results in the probability P(X,Y). If the considered value ranges of both variables
.
cover the whole R, the probability has to be 1,
.
(cid:3)+∞(cid:3)+∞
f (x,y)dxdy =1. (6.39)
. XY
−∞−∞
Figure 6.10 shows a bivariate distribution together with the two marginal distribu-
tions.


================================================================================
PAGE 149
================================================================================

128 6 RandomVariablesandProbabilityFunctions
6.7 Summary
As this was a chapter with two large thematic blocks, we also summarize it with two
boxes. Some of the most important aspects of probability theory were:
Things to Remember (Notations in Probability Theory) (cid:3)
(cid:129) For random variables, we use upper case roman letters, e.g., X, Y,etc.
(cid:129) A particular value of the random variable X is written as a lower case roman
letter, e.g., x; several measurements or samples of the same random variable
X are differentiated by subscripts, e.g.,.x 1 ,.x 2 , ...
(cid:129) Probabilitydensityfunctions(PDFs)andProbabilitymassfunctions(PMFs)are
both written as.f(x); or by additionally indicating the random variable, they
are denoted by.fX(x).
(cid:129) A somewhat sloppy but alternative notation for PMFs is the.p(x), which gen-
erally stands for a probability value (and which is different from a distribution
function). Similarly,.P(x)is used for the (discrete) CDF.
These are the most important notions about discrete and continuous distributions:
Things to Remember Discrete and Continuous Distributions (cid:3)
(cid:129) For a discrete variable, the probability function is called probability mass
function (PMF). Its values are the actual probabilities.
(cid:129) For a continuous random variable, the probability function is the probability
densityfunction (PDF). The values of the PDF are probability densities.
(cid:129) PMFandPDFarebothobtainedasderivativesfromthecumulativedistribution
function (CDF).
(cid:129) Some care must be taken for computing the CDF from a PMF as the CDF
will contain “discontinuities,” at which the value left of the discontinuity is
typically excluded.
(cid:129) The probability value corresponding to a single value of the continuous
random variable is zero.
6.8 Exercises
6.1. Assume that the toss of a fair coin was done three times. What is the probability
(i) of obtaining three Heads as an outcome; (ii) that the outcome of all tosses are
identical; and (iii) that one of the coins lands on the rim?
6.2. Rolling of two dies—what are their summed values, how often do they
occur?


================================================================================
PAGE 150
================================================================================

6.8 Exercises 129
(a) What is the PMF or frequency distribution table that describes the probability
distribution for the resulting sum of S from the two dies?
(b) Implement a Python function that simulates rolling the two dies and that returns
the values of two dies.
(c) Plot the frequency distribution from 100 dice rolls for sum values from 1 to 14
and compare it to your reference solution from (a).
(d) Plot the PMF that corresponds to the data from (c)
6.3. Here, we will now use the negative exponential distribution as one of the
simplest distributions to compute probabilities. The exponential distribution is given
by
1
f(z)= exp(−z/λ) (6.40)
.
λ
where λ is a real constant.
(a) Plot the PDF for λ = 1/2 and for λ =2.
(b) Derive the position of the maximum of the distribution f(z). What happens for
z = 0 and for z→∞?
(c) What is the probability that a value, randomly chosen from numbers that follow
the exponential distribution, will have z in the range of 1/4 to 1/2?
6.4. For the given data, classify them into one of the three categories: discrete
random variables (RVs), continuous RVs, and categorical RVs:
1. A Geiger-Müller tube is used to detect radioactivity, and the measuring device
gives the “count per minute” (CPM) of the ionization events due to radioactivity
(this can be made audible as the “clicking” sounds). What kind of RV is the CPM?
2. During an experiment, the elongation d of a rod is measured at distinct points in
time—what kind of RV is d?
6.5. The Maxwell-Boltzmann equation defines the distribution of speed for an ideal
gas at a given temperature.
(cid:9) (cid:10) (cid:6) (cid:7)
3
m 2 −mc2
.
f(c)=4πc2 e 2kBT (6.41)
2πk T
B
c is the scalar quantity of speed, m is the mass, T is the temperature, k is the
B
Boltzmann constant.
(a) Plot the PDF for He at room temperature. Note that m =4amu.
He
(b) Find the most probable speed.
(c) How does the PDF change with increasing temperature?


================================================================================
PAGE 151
================================================================================

130 6 RandomVariablesandProbabilityFunctions
(d) Additionally, consider different noble gases, such as Ne (m = 20 amu), Ar
Ne
(m = 40 amu), and Xe (m = 131 amu). How does the most probable speed
Ar Xe
change?
6.6. Two artists in a circus claim that they are able to communicate telepathically.
Since they insist that you test their claim, you ask them each to go to a different
room, far away from each other. Then you show person A a number of cards, each
of which either shows an apple, a cherry, or a banana. Each of the fruits are equally
likely to show up. For each card that you show to person A, person B writes down
the name of the fruit, e.g., “cherry,” according to what she believes the other person
is seeing. What is the probability that person B correctly “saw” at least one card
after ten cards were shown?


================================================================================
PAGE 152
================================================================================

7
Expectation, Variance, and Moments
“Life is so constructed, that the event does not, cannot, will not,
match the expectation.”
Charlotte Brontë (1816–1855)
English novelist and poet
7.1 Expected Values of Discrete Random Variables
In the previous chapters, we have already encountered a few situations, in which
a number of numerical outcomes of random experiments were to be simplified and
described by a single number. For example, we mentioned a number of times phrases
such as “on average the outcome is ... ”or“the mean of throwing a die 10 times is
...” and assumed that the reader intuitively knew the meaning. In what follows, we
will introduce the expected value and the variance, both of which have the purpose
of representing important features of distributions of numbers as a simple number.
Subsequently, we will then introduce the concept of moments as the generalization
of expected value and variance.
If we were to summarize the most important aspects of a distribution of numerical
data through a single value, this would most likely be the expected value (or
expectation value) of the outcome of a random experiment, more commonly known
as the average or mean. The subsequent formulations and examples are based on
discrete random variables. However, the formalism can be easily generalized to
continuous random variables, as will be done in the section about moments.
7.1.1 Definition and Examples
Assume the random experiment of tossing a coin four times, as also investigated
in Example 6.2. While the outcomes of the experiment in Example 6.2 was non-
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 131
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_7


================================================================================
PAGE 153
================================================================================

132 7 Expectation,Variance,andMoments
numerical (e.g., the sequence HHHT), we obtained numerical values with the
mapping of the random variable X (which differentiated between five different
types of events). Then the expectation value (also called expected value or just
expectation) of a random variable X is, roughly said, computed as the arithmetic
mean of a large number of outcomes of X. Different values of the random variable
could occur with different probabilities, in which case the weighted arithmetic mean
would be used:
Definition 7.1 (Expected Value) Let X be a discrete random variable with n
possible outcomes given by the x ,...,x . Each of the outcomes may occur
. 1 n
with the probability p . Then the expected value of X, written as E[X], is
. i .
defined as
(cid:2)n
E[X]=x p +x p +···+x p = x p . (7.1)
. 1 1 2 2 n n i i
i=1
E[·] is called expectation operator. If all outcomes are equally likely, the
.
expected value reduces to the “regular” arithmetic mean:
(cid:2)n
1
E[X]= x . (7.2)
. i
n
i=1
There are a number of different notations for the expectation value. A capital
upright “E,” as used in the above definition, is one of the most commonly used
.
notations in statistics and probability theory. To indicate that E is an operator, we
.
use square brackets instead of regular parentheses. A short form isμ or, if there is
. X
no ambiguity, just μ. Another often used notation in statistics is E[·]. An example
. .
illustrating how to compute the expected value is given in Example 7.1 (which is
the continuation of the above example Example 6.2).
Example 7.1 Random Variables & Event Probabilities (Part 3, Expected
Value) (cid:2)
[...continued from Example 6.2] What is the expected value of four times
tossing a coin? Since our random variable gives the number of Heads, we
have a numerical random variable (even though H and T are not numbers!),
for which it is reasonable to compute an expected value.
x =0, x =1, x =2, x =3, x =4
1 2 3 4 5
. p =1/16, p =1/4, p =3/8, p =1/4, p =1/16,
1 2 3 4 5
(continued)


================================================================================
PAGE 154
================================================================================

7.1 ExpectedValuesofDiscreteRandomVariables 133
from which follows for the expected value
1 1 3 1 1
E[X]=0× +1× +2× +3× +4× =2. (7.3)
.
16 4 8 4 16
In other words: the expected value of this random process would beX =2or
.
the event of 2 Heads.
Even though the expectation value for outcomes of equal likelihood is computed
in the same way as the “arithmetic average,” there is a slight conceptual difference:
the average value is typically computed after the data was produced, while the
expectation value suggests that we investigate a random experiment; the experiment
to obtain the values of the random trials is still to be performed. Beware that
there is also the notion of the mean, which we will sometimes use in the context
of distributions and which comes in three different “flavors”: the arithmetic, the
harmonic, and the geometric mean (note that usually the arithmetic mean is referred
to if someone talks about “the mean”).
The way we interpret the word “expected” in expected value in our daily
language is slightly different from the mathematical meaning. Imagine you throw
a six-sided die six times and you get 6, 6, 3, 1, 6, 5. Based on this experiment,
throwing a 6 would be considered the most likely event. However, the expected
value is (1+3+5+3×6)/6 = 4.5, which in fact is a number that can never
.
occur (as the die shows only integer numbers). The expected value of 4.5 has to be
interpreted in terms of a prediction for a much larger number of trials, e.g., if we
consider the situation where we only take a look at the summed up results of 1000
trials, then this would result in a value of 4500. Clearly, finding this exact value is
fairly unlikely to happen as there are many other possible sums.
7.1.2 Calculating with Expectation Values
A number of basic properties for expectation values help to perform calculations.
In the following we only list the most important properties and sketch the proofs,
which helps to see how the relations can be used for calculations with expectation
values.
Additivity
The expectation operator E[·] is a linear operator, and it holds for two random
.
variables X and Y:
E[X+Y]=E[X]+E[Y] . (7.4)
.


================================================================================
PAGE 155
================================================================================

134 7 Expectation,Variance,andMoments
The short proof uses the definition of the expectation operator for the case of equal
likelihoods and discrete random variables:
(cid:2)n (cid:2)n (cid:2)n
1 1 1
E[X+Y]= (x +y )= x + y =E[X]+E[Y] . (7.5)
. i i i i
n n n
i=1 i=1 i=1
The property of additivity of expectation values can be useful, in particular when
a random variable can be written as a sum of random variables that are easier to
compute. On the other hand, when computing the individual expectation values is
difficult, it might be possible to compute the expectation of the sum.
Homogeneity
Assume that c is a real valued constant and X is a random variable. Then, it is
cE[X]=E[cX] . (7.6)
.
The short proof again uses the definition of the expectation operator and assumes
equal likelihood of events and discrete random variables:
(cid:2)n (cid:2)n
1 1
cE[X]=c x = cx =E[cX] . (7.7)
. i i
n n
i=1 i=1
Thus, the property Eq.(7.6) tells how the expected value changes upon scaling
of a random variable (RV) by a factor: also the expected value is scaled by the
same factor. As a special case, we obtain the expectation value of a constant as the
constant itself,
E[c]= c. (7.8)
.
This is intuitively clear, e.g., by visualizing some data points on a coordinate axis
and considering, e.g., a “zooming in” on the axis scale.
Linearity
From the property of additivity and homogeneity, we conclude that the expectation
operator E[·] is a linear operator and can in general be written as E[aX+bY] =
. .
aE[X]+bE[Y]. Finally, by induction, it can be easily shown that linearity (and
therefore additivity) also holds for sums of arbitrary numbers of random variables.
Note that we did not have to make any assumptions about the independence of the
two random variables. In other words, linearity holds always, irrespective of whether
the random variables are independent or not.
Example and Application of the Linearity Theorem
How is this relation useful? Imagine a random process: two dices are thrown, and
we record their sum. For computing the expected value of the sum of rolling the two


================================================================================
PAGE 156
================================================================================

7.1 ExpectedValuesofDiscreteRandomVariables 135
dices, we could either consider the6+5+···+1=21combinations and compute
.
their expected value—or we make use of the already known expected value for each
of the two dices, which is 3.5. We then conclude that 3.5+3.5=7is the expected
.
value of the sum.
Translating
Last but not least, there is a theorem that shows how a shift of a RV influences the
expectation value. The proof is omitted here, but follows the same idea as before
and can be done as an exercise. Assume events of equal likelihood described by a
random variable X and additionally a constant a. Then the shifting (or: translation)
of the random variable is given by
E[X+a]=E[X]+a . (7.9)
.
This relationship denotes that shifting the values of a random variable by a constant
also shifts the corresponding expected value, which is intuitively clear.
Expected Value of a Product
A last theorem requires that the two random variables, X and Y, are independent
variables. Then, it can be shown (cf. Theorem B.1 in Appendix B) that the
expectation of the product of the two random variables is equal to the product of
the expectations:
E[XY]=E[X]·E[Y] . (7.10)
.
An example together with a Python solution is given in Example 7.2.
Example 7.2 Expected Value of a Product (cid:2)
A simple example is to throw a six-sided die and additionally to flip a coin.
Let us furthermore map the resulting symbols by the two random variables X
and Y as follows:
symbol: H T
.
↓
.
↓
.
↓
.
↓
.
↓
.
↓
.
↓
.
↓
value: .x1 = 3, .x2 = 4, .y1 = 5, .y2 = 6, .y3 = 7, .y4 = 8, .y5 = 9, .y 6 = 10
The values x and y are chosen arbitrarily. Then, the expected values for
. i . i
the two variables are E[X] = 3.5 and E[Y] = 7.5. Altogether, there are
. .
2 × 6 = 12 outcomes with equal likelihood. E.g., (H, ) corresponds to
.
(continued)


================================================================================
PAGE 157
================================================================================

136 7 Expectation,Variance,andMoments
x y = 3×5 = 15, (H, ) corresponds to x y = 3×6 = 18, etc., up
. 1 1 . 1 2
to (T, ) corresponds to x y =4×10=40.
. 2 6
Instead of computing the terms and products manually, we can use the
following Python code. We start by setting up two dictionaries (our “maps”):
In [1]: X = {'H': 3, 'T': 4}
Y = {i: i+4 for i in range(1, 7)}
print('X =', X, ' \nY =' , Y)
X = { 'H': 3, 'T': 4}
Y = { 1: 5, 2: 6, 3: 7, 4: 8, 5: 9, 6: 10}
Next, create pairs of the results from the coin and the die using a
Cartesian product. Summing up the x y and dividing by the total number
. i j
of combinations gives E[X·Y]:
.
In [2]: from itertools import product
sum_XY = 0
for x, y in product(X.values(), Y.values()):
sum_XY += x * y
E_XY = sum_XY / (len(X) * len(Y))
print("E_XY =", E_XY)
E_XY = 26.25
As an alternative we compute the expectation value for the individual
random variables
In [3]: E_X = sum(X.values()) / len(X)
E_Y = sum(Y.values()) / len(Y)
print("E_X * E_Y =", E_X * E_Y)
E_X * E_Y = 26.25
..., and their product gives the same value as before. Obviously the
calculation of E[X] · E[Y] is much easier than computing E[X·Y], in
. .
particular for larger numbers of items or for increasing numbers of variables.


================================================================================
PAGE 158
================================================================================

7.2 VarianceandStandardDeviation 137
Independent and Identically Distributed Random Variables
An important notion is introduced in the following definition:
Definition 7.2 (Independent and Identically Distributed (IID))
A number of random variables is called independent and identically
distributed if (i) each random variable is associated with the same probability
distribution and (ii) all random variables are mutually independent.
In this case, it is
E[X +···+E ]=NE[X] , (7.11)
. 1 N
where X is any of the random variables X to X . As a consequence, if we can
. 1 . N
show that for a set of random variables Eq.(7.11) holds, we can conclude that they
are IID. Making the assumption that variables are IID simplifies many formulations
in statistics and probability theory.
7.2 Variance and Standard Deviation
Imagine two random variables: the first one has the outcomes [3,4,5,6,7]and the
.
second one the outcomes [−5,0,5,10,15]. Each of them has the same expectation
.
value of 5, but the values of the second random variable are clearly much more
“spread out.” A random variable can be characterized by various measures other
than the expectation value alone. One of such commonly used characteristics in
probability theory is the variance, which quantifies the “spread” of values and the
statistical “uncertainty.”
7.2.1 Definition and Examples
Similar to the expectation value, the variance is also an operator; it is defined as
follows:
Definition 7.3 (Variance) The variance Var(X) of a random variable X is
.
given by
(cid:3) (cid:4) (cid:3) (cid:4)
Var(X)=E (X−E[X])2 =E (X−μ)2 (7.12)
.
(continued)


================================================================================
PAGE 159
================================================================================

138 7 Expectation,Variance,andMoments
where we used the already above-introduced abbreviation μ=E[X].
.
From the definition of the expectation value, Definition 7.1, it follows: if
x ,...,x are the possible outcomes, and each outcome x may occur with the
. 1 n . i
probability p , then
. i
(cid:2)n
Var(X)= p ·(x −μ)2 . (7.13)
. i i
i=1
Why does it measure the “spread” of a variable? First, the values are “centered”
around zero due to the subtraction of the “mean.” This allows the comparison of
different variables because, as a consequence, the mean will not have an influence
on the value of the variance. The squared term then acts as a nonlinear “weighting”;
thus, values further away from the expectation value have a more pronounced
influence on the value of the variance. Additionally, the squared term makes the
variance independent of the sign of the values (Fig.7.1).
Note that in the context of statistics (c.f., Chap.8), we will encounter two
ways to compute variances: one of them considers all possible data, i.e., the
population variance, and the other one considers only a subset of the data, i.e., the
sample variance. There is a small mathematical difference between them, which is
important, in particular, when we deal with small amounts of data. We will discuss
this in detail in Chap.8.
Fig. 7.1 Visualization of the
variance for the values of
. {1,2,3,4}: due to the squared
term in the definition, the
distance of the data points
from the mean of 2.5 is
represented as the square
areas


================================================================================
PAGE 160
================================================================================

7.2 VarianceandStandardDeviation 139
7.2.2 The Standard Deviation
The squared term contained in the equation for the variance has the effect that it is
more difficult to interpret than the expected value. This is the reason for introducing
the standard deviation as the square root of the variance:
Definition 7.4 (Standard Deviation) The standard deviation σ of a random
.
variable X is defined as
(cid:5)
σ(X)= Var(X). (7.14)
.
The standard deviation has the same unit as the random variable. An example for
computing variance and standard deviation is given in Example 7.3.
Example 7.3 Variance of Results from Rolling a Six-Sided Die (cid:2)
What is the variance and standard deviation of throwing a fair, six-sided die?
How can this be interpreted?
Starting from Definition 7.3 with the expectation value E[X] = 3.5 we
.
have
(cid:3) (cid:4) (cid:2)6
1
Var(X)=E (X−3.5)2 = (x −3.5)2 . (7.15)
. i
6
x=i
For the six-sided die, it is x =1, x =2, ... and therefore
. 1 . 2
(cid:6) (cid:7)
1
Var(X)= (1−3.5)2+···+(6−3.5)2 =2.92. (7.16)
.
6
The standard deviation σ is then, according to Definition 7.4
.
(cid:5) √
σ = Var(X)= 2.92≈ 1.71 (7.17)
.
7.2.3 Calculating with Variances
For calculating with the variance of random variables, a number of useful theorems
and properties can be stated. We will summarize them in the following, while some
of the proofs are given in Appendix B.2.


================================================================================
PAGE 161
================================================================================

140 7 Expectation,Variance,andMoments
(cid:129) Non-negativity: The values of the variance are non-negative,Var(X)≥0, which
.
is a side effect of the squaring in the definition of the variance.
(cid:129) Constant values: The variance of a constant c is zero. This implies in turn that
if the variance of a random variable is zero, all values x are identical.
. i
(cid:129) Additivity: For two independent random variables, it is
Var(X+Y)=Var(X)+Var(Y).
.
While this was obvious for the case of the expected value of two random
variables, it is not straightforward for the variance because the variance is not
a linear operator as the expected value operator.
(cid:129) Scaling: For a random variable X and a scalar value c,itis
Var(cX)=c2Var(X)
.
(cid:129) Translation: The variance is invariant w.r.t. a shifting of the random variable
values, Var(X+a)=Var(X).
.
7.3 Raw Moments
In this and the following sections, the concepts of different moments are introduced
for both discrete and continuous RVs. We will find that the expectation value
and the variance are both measures that can be obtained as special cases from
particular types of moments; thus, moments can be understood as the more general
concept. Additionally, the formulations for multivariate distributions can also be
easily obtained. In the following we start with the most basic variant, i.e., the raw
moments.
7.3.1 General Formulation
The n-th moment about 0 of a probability function is the expected value of the
random variable Xn; it is called a raw moment or (less commonly) also crude
.
moment:
Definition 7.5 (The n-th raw moment of X) The n-th raw moment of a
random variable X is denoted asE[Xn]. It is defined for a discrete probability
.
mass function p with the discrete values x as
. i
(cid:2)
E[Xn]= xnp (7.18)
. i i
i
(continued)


================================================================================
PAGE 162
================================================================================

7.3 RawMoments 141
and for a continuous probability density function f(x)as
.
(cid:8)+∞
E[Xn]= xnf(x)dx . (7.19)
.
−∞
Moments of multivariate distributions can also be obtained. However, in most
cases it is sufficient to characterize each random variable separately. This can
be done through marginalization and the marginal probability, as introduced in
Sect.6.5.4. For example, for a bivariate continuous probability density function with
the random variables X and Y,the n-th raw moment of X and Y, respectively, is
(cid:8)+∞(cid:8)+∞
E[Xn]= xnf(x,y)dxdy , (7.20)
. .
−∞−∞
(cid:8)+∞(cid:8)+∞
and E[Yn]= yn f (x, y)dx dy . (7.21)
−∞ −∞
These formulations can be generalized to more RVs by performing the additional
integrations. The formulations for the discrete case follow accordingly with the
definition of the raw moment and the joint probability from Sect.6.5.2.
Moments in Physics and Mechanics (cid:2)
The reader might have come across “moments” already in a different context,
e.g., in mechanics or in physics. In general, a moment is a mathematical
measure for the particular shape of a mathematical function. Translating
this into the world of physics or solid mechanics, the mathematical function
becomes a physical density in units weight per area. In that case the zero-th
moment gives the total mass, the first moment divided by the total mass is the
center of mass, and the second moment is the rotational inertia.
7.3.2 The Zero-th Raw Moment
The zero-th moment of any probability function is obtained by setting n = 0: then,
.
the sum or integral is formed over the whole probability, which is 1 (already per
definition):


================================================================================
PAGE 163
================================================================================

142 7 Expectation,Variance,andMoments
(cid:3) (cid:4) (cid:2)n
.
E X0 =E[1]= p
i
=1 fordiscreteRVs. (7.22)
i=1
(cid:3) (cid:4) (cid:8)+∞
E X0 = E[1] = f(x)dx = 1 for continuous RVs (7.23)
−∞
7.3.3 The First Raw Moment or the Mean
Even though in principle also higher raw moments can be used, in practical
situations only the first raw moment is relevant. It is obtained for n = 1 and as
.
the expectation value
(cid:3) (cid:4) (cid:2)N
.
E X1 ≡E[X]= x
i
p
i
fordiscreteRVs. (7.24)
i=1
(cid:3) (cid:4) (cid:8)+∞
E X1 ≡ E[X] = xf (x) dx for continuous RVs . (7.25)
−∞
These two equations are usually used to define the mean μ = E[X]. The mean
.
of a discrete RV has the property that each data point contributes the same fraction
of its value as the other ones, including potential outliers (i.e., rare values further
away from the mean of the other values). For example, in the dataset {2,4,6,50},
.
each of the four numbers contributes with a weighting of 0.25, and therefore the
.
“outlier” value of 50 increases the mean strongly: the mean of the first three values
is 4, whereas the mean of all values would be 15.5. Geometrically, the mean can be
.
understood as the point at which the sum of the data to the right is exactly balanced
by the sum of the data to the left, as depicted in Fig.7.2. This can be also transferred
to continuous random variables, for which the mean divides the total area into two
sub-areas of equal size.
7.4 Central Moments
Central moments are the moments w.r.t. to the mean. As opposed to the raw
moments, they describe aspects of the spread or shape of the function independent
of translations and achieved by subtracting the mean.


================================================================================
PAGE 164
================================================================================

7.4 CentralMoments 143
Fig. 7.2 Visualization of the
mean as the point at which
the dataset is “balanced.” The
dataset consists of the
following points:. {−1,2,
-3.5,0,3,3,-2.5,4,-2.5,2,
.3 }. The height of the blue
bars corresponds to the
different frequencies with
which the values occur. The
sketch in the middle shows
the point that balances the
data at the mean value of
. ≈0.54. Each blue bar can be
interpreted as a mass, which
creates an angular momentum
proportional to the number of
rectangles (the “force”) and
the distance from the mean
7.4.1 General Formulation
In the following the definition of arbitrary degrees of central moments of a
distribution is given:
Definition 7.6 (n-th central moment) The n-th central moment of a random
variable X is denoted by μ[Xn] = E[(X−μ)n]. In the discrete case it is
.
given for the N data points by
(cid:2)N
μ[Xn]= (x −μ)np , (7.26)
. i i
i=1
while in the continuous case, it is given by
(cid:8)+∞
μ[Xn]= (x−μ)nf(x)dx . (7.27)
.
−∞
In this definition,μ=E[X]is the mean, as before. All other quantitiesx , x, and
. . i
f are defined as in Definition 7.5. As in the case of raw moments, also the central
moments can be computed for multivariate distributions.


================================================================================
PAGE 165
================================================================================

144 7 Expectation,Variance,andMoments
7.4.2 Zero-th and First Central Moments
It follows directly that the 0-th central moment of any probability density distribu-
tion is 1. Furthermore, the first central moment (n = 1)ofanydistributioniszero
.
because it is the expectation value of a variable that was already “centered” at the
mean.
7.4.3 The Second Central Moment or the Variance
The second moment is the variance of the observations,
(cid:3) (cid:4)
Var[X]≡Var[X]=E (X−μ)2 =σ2 , (7.28)
.
which can also be calculated from the following relation:
(cid:3) (cid:4)
Var[X]=E X2 −(E[X])2 . (7.29)
.
The variance characterizes the spread from the average: a low value of variance
implies that all data are close to the mean, while a high variance indicates that the
data are strongly spread out.
For a discrete variable, the variance is given by
(cid:2) (cid:2)
Var[X]= (x −μ)2p = x2p −μ2 (7.30)
. i i i i
i i
and in case of a continuous random variable by
(cid:3) (cid:4) (cid:8)+∞
E (X−μ)2 = (x−μ)2f(x)dx . (7.31)
.
−∞
Example 7.4 Implementation details in terms of linear algebra (cid:2)
If we define the column vectoro=[1,...,1]T(with n entries), and the dataset
.
is given by the column vectorx =[x ,...,x ]T, the mean can be represented
. 1 n
by scalar products
oT·x
x¯ = , where oT·o=n
. oT·o
(continued)


================================================================================
PAGE 166
================================================================================

7.5 StandardizedMoments 145
which is the projection of x onto o. For the (population) variance, we can
. .
write
(cid:8)x−x¯o(cid:8)2
Var(x)= where(cid:8)y(cid:8)2 =yTy
.
n
This can now be “translated” to Python code, which follows the same
structure as the equations.
import numpy as np
1
x = np.random.rand(5)
2
o = np.ones_like(x)
3
mean = o.dot(x) / o.dot(o)
4
5 var = np.linalg.norm(x - mean * o) ** 2 / (o.dot(o))
np.testing.assert_allclose(mean, x.mean())
6
np.testing.assert_allclose(var, x.var())
7
The last two lines check if the computed mean and variance are, within some
tolerance, identical to the “reference values” directly obtained from numpy.
Such formulation in terms of scalar products of vectors is also used for
efficient implementations of many numerical methods.
7.5 Standardized Moments
The attribute standardized (or also normalized) implies that moments are calculated
after the distribution was normalized with the standard deviation. This makes nor-
malized central moments dimensionless and, thus, scale invariant. Also standardized
moments allow to indirectly compare the shapes of different distributions.
7.5.1 General Formulation
We start again with the general formulation of the n-th standardized moment:
Definition 7.7 (StandardizedCentralMoment) The standardized n-th cen-
tral moment of the random variable X is defined as
μ E[(X−μ)n]
μ˜ = n = . (7.32)
. n σn σn
In this definition,σ denotes the variance, andσnis the variance to the n-th power.
. .


================================================================================
PAGE 167
================================================================================

146 7 Expectation,Variance,andMoments
7.5.2 Aspects of Standardized Moments
As for the first central moment, also the first standardized moment is zero, μ˜ =0,
. 1
as it is the first moment of a centered variable. The second standardized moment is
always 1 because the second centered moment (the nominator of Eq.(7.32))isthe
variance of X. The third and fourth moments are dimensionless measures for the
skewness and kurtosis of distributions and will be discussed in more detail in the
context of descriptive statistics in Sect.9.3.
7.6 Exercises
7.1 Show that the first central moment of any distribution is zero.
7.2 This is the continuation of Exercise 6.5:
(a) Based on the Maxwell-Boltzmann distribution calculated for He at room
temperature, obtain the mean and standard deviation.
(b) How does the standard deviation change with increasing temperature?
7.3 A fair,n -sided die can be modeled as a discrete random variable, X, with
outcomes 1 ...n, each with equal probability 1/n. What is the general formula for
the variance of the outcome, X,ofann-sided die?
7.4 Consider a two-dimensional water molecule, as schematically drawn in the
figure below. Assume that the masses of the H and O atoms are 1 and 16,
respectively (dimensionless).
(a) Calculate the first and second moment.
(b) How do these moments relate to the center of mass and moment of inertia?
7.5 In quantum mechanics, one of the most fundamental concepts is that of a
particle in a box. The wave function (or simply put, a function that describes the
ground-state position of a particle) is given by:


================================================================================
PAGE 168
================================================================================

7.6 Exercises 147
(cid:9) (cid:10) (cid:9) (cid:10)
1
2 2 πx
f(x)= sin
.
L L
where L is the length of the box. The corresponding probability density is:
(cid:9) (cid:10) (cid:9) (cid:10)
2 πx
p(x)= sin2
.
L L
(a) Calculate the expectation value of the position of the particle.
(b) Calculate the uncertainty in the position of the particle, given by the standard
deviation.


================================================================================
PAGE 169
================================================================================

Introduction to Statistics 8
Statistics is the grammar of science.
Karl Pearson (1857–1936)
English Statistician
8.1 And What Now Is Statistics?
To start with, probability theory and statistics are both subfields of mathematics.
However, they approach problems from opposite directions: in probability theory
the point of departure is some random process, and based on a mathematical
description of this, we try to derive the result—typically using purely mathematical
reasoning and theoretical derivations. In statistics, we roughly take the opposite
approach: instead of using theoretical derivations, we rather observe certain phe-
nomena and, based on that, try to infer what the underlying process might be, as
demonstrated in Example 8.1.
What is the benefit of statistics and probability theory for data science? In later
chapters it will turn out that in fact much of machine learning (ML) is statistics and
probability theory. Additionally, statistics helps to decide if the data used for training
of a machine learning model is in “a good state” (or not) and if differences between
ML experiments are real or just effects of noise in the data.
In this text, we will consider statistics as a collection of mathematical tools and
concepts that are able to help us explore and answer questions about data (e.g., what
does the data look like? what is the most common observation?) and that will guide
us during the process of gathering data. We try to reduce the amount of technical or
theoretical considerations to a reasonable amount; for more in-depth considerations,
the reader is referred to the great body of literature.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 149
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_8


================================================================================
PAGE 170
================================================================================

150 8 IntroductiontoStatistics
Example 8.1 (Probability Theory vs. Statistics) (cid:2)
Imagine a jar filled with colored marbles. Typical perspectives are in ...
(cid:129) ...Probability Theory: Start by thinking about a possible underlying
probabilistic model that, e.g., could mathematically describe the random
filling process of the jar. And based on that, we could then predict the
probability to draw either a red or a blue marble.
(cid:129) ...Statistics: Draw a number of marbles from the jar (which is the process
of “sampling”) and assume that the rest of the marbles in the jar have colors
in proportion to what we got in our sample. Additionally, by describing
properties of our sample, we would conclude how the property of all
marbles in the jar (e.g., the number of marbles with the same color) are.
8.2 The Sample and the Population
One of the strengths of statistics becomes clearly visible once the number of objects
in which we are interested becomes larger. For example, in the above example, we
might be interested in finding out what the ratio between the number of blue marbles
and the total number of marbles is.
8.2.1 The Sample and the Population
The straightforward approach is to first count all blue marbles, then count all
remaining marbles so that we can compute the total number of marbles, and finally
divide the number of blue marbles by the total number of marbles. While this
gives an accurate answer to the investigated question, in most situations this is a
highly impractical approach. In statistics we take a different approach and rather
randomly draw a number of marbles from the jar. Instead of investigating the whole
jar, only this subset of marbles is then investigated. The hope of the statistician is
that this subset is a representative set from which the property of the whole jar
can be estimated. The subset is called sample, and the whole jar of marbles is the
population. Let us now define the notion of the population:


================================================================================
PAGE 171
================================================================================

8.2 TheSampleandthePopulation 151
Definition 8.1 (Population) In statistics, a population is the aggregations of
all objects of interest.
There are two noteworthy aspects: the beginning of the definition “in statistics”
suggests that other fields might have different definitions of populations. For
example, in everyday language usage, we would associate “population” with the
inhabitants of a city or a country. The population in statistics does not necessarily
have anything to do with people. The second interesting aspect in the short definition
above concerns the “objects of interest.” Once we are interested in different aspects
or properties, this part of the sentence implies that the relevant population can
change and is nothing that is fixed. Example 8.2 highlights the populations of two
different statistical experiments.
Example 8.2 (Population) (cid:2)
(cid:129) In Example 8.1 the population consists of the content of the whole jar.
(cid:129) If we performed 400 mechanical tests (60% bending tests and the rest
tensile tests) and are interested in the experiments in which specimens fail
during the tensile loading, then the population consists of 160 experiments
or specimens. If we are interested in how many specimens fail in total, then
the population consists of all 400 specimens.
In many situations we are not able to investigate an entire population, e.g.,
because it is too expensive, it would take too long, or because the population consists
of infinitely many members. In statistics, this is a well-known and fundamental
problem. Therefore, at the heart of statistical analysis is the concepts of the sample:
Definition 8.2 (Sample and Sampling) A sample is a subset of the entire
population. The process of picking the data from the population is called
sampling.
The purpose of a sample is that it represents the populations while at the same time
being a smaller and much easier to handle subset of the population.


================================================================================
PAGE 172
================================================================================

152 8 IntroductiontoStatistics
8.3 Two Flavors: Descriptive and Inferential Statistics
This is then the point of departure for two different statistical approaches: By
computing statistical measures such as the mean value or the variance of the data in
the sample, we ...
(cid:129) characterize and describe the sample through descriptive statistics.
(cid:129) hope to be able to predict statistical properties of the population based on the
properties of the sample. This is the purpose of inferential statistics.
A significant part of Chap.9 is dedicated to various aspects of the so-called
exploratory data analysis (EDA), which includes descriptive statistics as a major
topic. Aspects of inferential statistics will be discussed in the context of machine
learning starting from Chap.11.
A visualization of the concepts of the population and the sample is given in
Fig.8.1, which also gives an overview of the task for which the different sub-
disciplines within statistics are responsible. There, the two circles within the
population indicate two different samples. Sample #1 results in an equal number
of blue triangles, orange squares, and red circles. Making inference then is the
process of extrapolating the properties of the sample to the whole population.
Here, one could infer from sample #1 that also the whole population will consist
of equal number of triangles, squares, and circles—which is already pretty close
Fig. 8.1 Depending on if and how a sample from a population is used, different disciplines
become relevant: Probability theory would try to mathematically derive how the population is
composed and what its properties are. Instead, statistical methods operate on subsets of the
population: (i) statistical sampling is responsible for the sampling itself; (ii) descriptive statistics
characterizes properties of the sample, e.g., visually or by computing simple measures such as the
mean; finally, (iii) inferential statistics takes one or more samples and makes predictions about
properties of the entire population. The two circles on the left highlight two possible samples (see
text for further details)


================================================================================
PAGE 173
================================================================================

8.3 TwoFlavors:DescriptiveandInferentialStatistics 153
(there are nine rectangles, eight triangles, and eight circles, so we “guessed” only
one rectangle wrong). A sample that is representative for the population is called
a representative sample. However, if the population is not known, then it can be
difficult to judge if a sample is truly representative or not—which is one of the
fundamental challenges in statistics.
8.3.1 What is “Sampling”?
Therefore, an important subject in statistics is the process of sampling . It is
important because the type of sampling, i.e., the way how we choose the data
contained in the sample, may strongly influence the results. For example, if we
wouldhaveusedsample#2inFig.8.1, we would have concluded (i) that the entire
population consists of equal numbers of orange squares and blue triangles and (ii)
that the population does not contain any red circles. If in a sample certain outcomes
are over-represented it is said that the sample is biased toward that particular
outcome (in our example the second sample would be biased toward triangles
and rectangles). A famous example for such a kind of selection bias is given in
Example 8.3.
Example 8.3 (Selection Bias) (cid:2)
“Selection bias” can come in different ways; one of these occurs when
the particular sample represents some members of the population better
thanothers, e.g., because there are many more of the one type than the other.
This is called “undercoverage.”
A textbook example is the “Literary Digest voter poll,” which became
famous for its disastrous prediction of the victory of the Republican
Alfred Landon: from the poll data, it was predicted Alfred Landon would
beat Franklin Roosevelt in the 1936 presidential election. Afterward, the
commonly accepted explanation for the catastrophic failure was that poll
participants were mainly taken from phone books and car registration lists.
Back in 1936 only the better-off population (this time “population” really
means “people”!) was able to afford cars and telephones—and those would
have typically voted for the Republicans. Thus, the sample obtained during
the voter poll suffered from an undercoverage of a big group of voters, who
rather would have voted for a Democrat.
Nonetheless, there has been some discussion and investigation if this was
in fact the true reason. Part of this discussion can be found in [1].
A good choice of the sample size together with a suitable “sampling strategy” can
help to avoid erroneous approximations of the populations. Also in ML this will play
an important role and a whole chapter is dedicated to the topic of “cross-validation”


================================================================================
PAGE 174
================================================================================

154 8 IntroductiontoStatistics
(Chap.16) which, among others, also helps to make sampling problems visible. We
finish this section by some thoughts about the meaning of sample in the context of
(real) experiments:
Words of Advice (cid:2)
Often, in Materials Science and Physics, the notion of sample and specimen are used
interchangeably. However, once you have a bit of a statistics background, the word
sample tends to have a specific connotation. A material sample then refers to a small
amount of material that is taken from a much larger piece of material; the purpose
of the sample is to represent details of the larger amount of material, i.e., by testing
the sample, we can infer how this particular material would behave in general. As
opposed to this, a specimen often r efers to a particular individual, e.g., for which
geometry details or boundary conditions are specified. Nonetheless, keep in mind
that in many cases there is no commonly agreed definition of these terms. Luckily,
often the context resolves any ambiguity.
Before we continue with an overview of different sampling strategies in the
next section, here is a short summary of the four most important notions that we
encountered here:
Things to Remember (Important Notions & Concepts from Statistics) (cid:3)
(cid:129) Descriptive Statistics vs Inferential Statistics: The distinction between descrip-
tive and inferential statistics is important because it determines the goal of
“doingstatistics”:descriptivestatisticsobservesanddescribes,e.g.,theshape
of distributions, whereas the goal of inferential statistics is to establish a
relation between the data and any underlying causality.
(cid:129) SamplevsPopulation:Theseconceptsareabsolutelyfundamentalinstatistics.
A sample is a subset of the possibly very large dataset consisting of all data,
the population. The sample has the goal to represent the relevant aspects of
thepopulation.Atthesametime,thesampleshouldhavea“convenient”size.
8.4 Sampling Strategies
Sampling is a fundamental concept of statistics and therefore, a number of strategies
for sampling data were developed. The goal is always to create samples that are as
representative as possible. In the context of ML we will encounter related tasks
when we discuss cross-validation in Sect. 16.2. In the following a few of the most
common strategies for sampling are introduced. The list of possibilities, however, is
rather long, which is why we left out in particular techniques for highly specialized
situations.


================================================================================
PAGE 175
================================================================================

8.4 SamplingStrategies 155
Fig. 8.2 Visualization of the simple random sampling strategy. Here, the numbers 6, 22, 1, 3, 20,
and 15 were randomly drawn
8.4.1 Simple Random Sampling
The easiest approach for sampling is simple random sampling (SRS). It is a method
that is based on assigning the same probability to each item that could be chosen
during sampling. Then, the items contained in the sample are chosen without
replacement (also see Chap.6). Note that random sampling with replacement is
possible as well but has slightly different properties; for larger populations, however,
the difference becomes small. A visualization of this sampling technique is shown
in Fig.8.2.
Here is an example where we compute the mean value of a population and
a random sample. We start by importing numpy and creating a population that
consists of the numbers from 1 to 99:
In [1]: import numpy as np
population = np.arange(1, 100)
population.sum() / population.size
Out [1]: 50.0
Next, a random number generator rng is created. It comes with the convenient
method choice that chooses from the given population a number of items (here:
10) without replacement:
In [2]: rng = np.random.default_rng()
sample = rng.choice(population, size=10, replace=False)
sample
Out [2]: array([21, 64, 38, 74, 19, 13, 77, 12, 58, 90])
Now, we can also compute the mean value of the sample


================================================================================
PAGE 176
================================================================================

156 8 IntroductiontoStatistics
In [3]: sample.sum() / sample.size
Out [3]: 46.6
From this random sample, we estimate the population mean to be 46.6, which is
close to the true value of 50.0. Keep in mind that typically we do not know the true
value of the population.
If more than one sample is to be created, then each sample is chosen indepen-
dently from the other samples, i.e., for each sample items are chosen from the full
set of items. Thus, the same item can occur in more than one sample, but no more
than once in a single sample. Thus, based on the above code, we could create three
samples s1 , s2 , and s3 by
In [4]: n = 10
s1 = rng.choice(population, size=n, replace=False)
s2 = rng.choice(population, size=n, replace=False)
s3 = rng.choice(population, size=n, replace=False)
print("mean values:", s1.sum() / n, s2.sum() / n, s3.sum() / n)
mean values: 33.6 58.7 62.2
where we observe that each sample is characterized by a different sample mean. In
other words, we have now three different estimates of the population mean.
The advantage of this simple probabilistic sampling method is that it does not
require any knowledge about the populations. On the other hand, this also implies
that typically more data are required as compared to methods that are able to make
use of knowledge about the distribution of data within the population.
8.4.2 Systematic Sampling
Systematic sampling or linear sampling is a very simple methodology where from
an ordered list of N elements the first sample is chosen randomly. Subsequently,
every k-th element is chosen. After the end of the list was reached, one can return to
the beginning of the list. k is called sampling interval or skip and can be computed
by
N
k = , (8.1)
.
n
where n is the prescribed sample size, and k should be rounded to the next larger
integer.
As a consequence, every element in the list (i.e., the population) has the same
probability to be picked. The probability is commonly called epsem. In this respect,
system sampling is comparable to simple random sampling. One of the prerequisites
is that the population needs to be homogeneous. Additionally, if the data contained


================================================================================
PAGE 177
================================================================================

8.4 SamplingStrategies 157
Fig. 8.3 Visualization of the systematic sampling strategy. The randomly chosen start item was
number 3, and the sampling interval was 6
in the population follows a certain pattern, then systematic sampling might hide this
pattern (e.g., imagine that every k-th materials science experiment was done by a
certain person who is known as a particularly meticulous experimenter resulting
in small errors. Then, always picking the experiment from this particular series
might introduce a bias). Note that time series data often contains repeated patterns,
which is why systematic sampling is in general not a good choice for such data. A
visualization of this strategy is given in Fig.8.3.
A small Python example is as follows: we import numpy and create a random
number generator
In [1]: import numpy as np
rng = np.random.default_rng()
Next, a population is created with 25 items ( range(N) ), to and we append a copy
(note that 2 * list(...) duplicates and concatenates the list):
In [2]: N = 25
population = np.array(2 * list(range(N)))
population
Out [2]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
17, 18, 19, 20, 21, 22, 23, 24, 0, 1, 2, 3, 4, 5, 6, 7, 8,
9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])
Appending the copy is used, in case that we reached the end of the list. Because then
we have to start from the beginning—there are more general ways of doing this, and
out “appending” strategy only works as long as it is clear that we will not exceed
the length of the two lists. We now can choose the sampling interval,


================================================================================
PAGE 178
================================================================================

158 8 IntroductiontoStatistics
In [3]: n = 6
k = int(N / n)
k
Out [3]: 4
and pick elements starting from start every k elements. The second square
bracket chooses only the first n elements.
In [4]: start = rng.integers(0 , N)
population[start::k][:n]
Out [4]: array([24, 3, 7, 11, 15, 19])
8.4.3 “Take from Top” Sampling
As opposed to systematic sampling, take from top is a technique that is well suitable
for time series data. It uses the first n elements from a population and thereby
preserves the temporal structure of time series data. However, generally there is
no guarantee that the first chunk of data is in fact statistically representative of the
population.
8.4.4 Weighted Sampling
Sometimes, during data acquisition, certain observations are recorded with non-
uniform probabilities or left out completely (“gaps”), leading to a biased dataset.
Using simple random sampling then leads to skewed descriptions (e.g., of the
mean), which does not properly approximate the population. Weighted sampling
counterbalances this behavior by assigning different probabilities when items are
chosen during sampling. Effectively, in a region with dense data points, the sampling
probability then is lower as in sparsely populated regions. This should result in an
unbiased sample.
As an example, assume that a dataset consists of data that corresponds to two
distinct classes, and one class has significantly more data than the other class. As
a strategy for compensating for this, e.g., oversampling duplicates samples from
the class with the smaller amount of data, the minority class. Another approach
is undersampling, which simply ignores some data records from the class with
more data, the majority class. An example for oversampling is also given in
Sect. 16.2.7 where a technique called bootstrap is introduced. Effectively, both over-
and undersampling introduce a bias by their selection procedure that is supposed to
counteract the (unwanted) bias contained in the dataset.


================================================================================
PAGE 179
================================================================================

8.4 SamplingStrategies 159
8.4.5 Stratified Sampling
Stratification denotes the process of splitting the whole population into “sub-
populations,” the so-called strata (plural of stratum). The partitioning has to be
non-overlapping, i.e., each element is exactly in one of the strata. Subsequently,
simple random sampling (Sect.8.4.1) can be employed, which has to be done for all
strata.
The goal of the partitioning is that elements of each of the strata are as similar as
possible (i.e., the variability within each stratum is minimized). At the same time,
the mean of each stratum should be as different as possible (i.e., the variability
between strata is maximized). Thus, stratified sampling is applicable in particular
when data can be separated into different categories. The advantage is that such a
stratified sample is more representative of the population than the sample obtained
from simple random sampling. A visualization of the concept is shown in Fig.8.4,
and two examples are presented in Example 8.4. Note that for each individual
stratum, different sampling techniques can be employed, e.g., one stratum can be
sampled using simple random sampling and the other one using weighted sampling.
Example 8.4 (Stratified Sampling) (cid:2)
1. Stratified sampling is commonly used when strata can be formed based on
geography since there the strata have a direct meaning: data from different
cities or countries end up in different strata.
2. In materials science such a partitioning could make sense if a number of
experiments are conducted, each of which results in a different number
(continued)
Fig. 8.4 Visualization of the stratified sampling strategy. From each of the three strata, a number
of samples is drawn in proportion to the stratum size


================================================================================
PAGE 180
================================================================================

160 8 IntroductiontoStatistics
of measurements, e.g., experiment A has 30 measurements of a yield
stress, experiment B has 60 measurements, and experiment C has 120
measurements. Each experiment might be performed with a different
machine and therefore affected by different errors. We could take 30
random samples from these 210 measurements, but there is the possibility
that one of the experiments is by chance over-represented, introducing a
bias. Considering each experiment as a stratum and randomly taking 10
elements from A, 20 from B, and 40 from C ensures that the sample
contains data in proportion to the subpopulations A, B, and C.
However, in many cases it is not directly clear how to choose the criteria or
variables for splitting the population into strata. Then stratified sampling might
not be the method of choice. Additionally, the overall computational cost for
implementing this method is significantly higher than implementing one of the
simpler methods.
Here is a simple Python implementation that, in a simplified approach, randomly
assigns the population data to strata. We again start by creating a population.
In [1]: import numpy as np
rng = np.random.default_rng()
N = 25
population = np.arange(N)
population
Out [1]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
17, 18, 19, 20, 21, 22, 23, 24])
This is then split into four strata:
In [2]: strata = np.array_split(population, 4)
strata
Out [2]: [array([0, 1, 2, 3, 4, 5, 6]),
array([ 7, 8, 9, 10, 11, 12]),
array([13, 14, 15, 16, 17, 18]),
array([19, 20, 21, 22, 23, 24])]
In a compact approach, we create a list of all random samples from each stratum.
In [3]: s = [rng.choice(stratum, size=2, replace=False) for stratum in strata]
sample = np.concatenate(s)
sample
Out [3]: array([ 0, 3, 9, 8, 14, 18, 23, 22])


================================================================================
PAGE 181
================================================================================

8.5 TheLawofLargeandTrulyLargeNumbers 161
This “inline for loop” is called list comprehension and adds for each element of
the for loop the result of rng.choice to the list.
8.5 The Law of Large and Truly Large Numbers
8.5.1 The Law of Large Numbers
The law of large numbers (LLN) is one of the important concepts in probability
and statistics. It states that if a random experiment is repeated “sufficiently” many
times and all results are averaged, we will obtain a result that is approximately the
expected value. In Fig.8.5 we can observe this during repeatedly rolling a die and
monitoring how the deviation from the mean value changes as more and more results
from die rolling are added. This is shown for altogether five runs with different seed
values of the random number generator, giving five different sequences of (pseudo)
random numbers shown as the colored lines in the right panel. The left panel shows
the random values that correspond to the blue line.
The Python code for producing this figure is similar to that already shown in
Sect. 5.2.2. It starts with importing the required modules and with initializing the
random number generator:
In [1]: import numpy as np
import matplotlib.pyplot as plt
rng = np.random.default_rng(42)
Then, we obtain 1000 random values and use the cumulative summation to obtain
the means for all lengths of random number sequences:
Fig. 8.5 The Law of Large Numbers during rolling of a die. Left: observed values as a function
of the number of die rolling. Right: deviation of the mean value of all die rolls that occurred during
the first N rolls from the expected value


================================================================================
PAGE 182
================================================================================

162 8 IntroductiontoStatistics
In [2]: n_samples = 1000
values = rng.integers(1, 6, endpoint=True, size=n_samples)
sample = np.arange(1, n_samples + )1
averages = values.cumsum() / sample
Finally, we plot the deviations of the mean values from the expected value of 3.5 in
a semilogarithmic plot. This scaling of the axis helps to reveal more details of the
left region of the plot, where variations are very large:
In [3]: fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
ax[0].plot(sample, values, 'o' , mfc='C0', mec='none', alpha=0.6)
ax[1].plot(sample, averages - 3.5, '-')
Generally, we observe the behavior of “the larger the sample sizes, the smaller
error”—if the opposite would be the case, then the concept of sampling would be
obsolete.
We can also see that in all of the five sequences, some sample means overestimate
the expected value, while others underestimate it, depending on the particular
sequence of random numbers. There is no way to predict such details of the plot.
Words of Advice (cid:2)
Sometimes, people think that the Law of Large Numbers is also applicable in situa-
tions where the samples are small and that this helps to estimate the expected value.
This is no correct, and there is no such thing as a “Law of Small Numbers.”
The law of large numbers is directly related to what will be introduced in
Example 12.1 as regression to the mean. In the context of ML the LLN tells us that
training data must be representative of the true population. This might require that
some data ranges are explicitly excluded, e.g., during data cleaning, so that there are
not too many different phenomena contained in the dataset and the sample mean
Example 8.5 (Diffusion and the Law of Large Numbers) (cid:2)
The LLN can be applied to coarse-graining of (idealized) diffusive processes.
Imagine that we investigate how diffusion takes place, if one half of a
container contains idealized atom(s), separated by a barrier from the other
half, and the barrier is removed.
Then, the trajectory of a single atom follows a “random walk,” which
exhibits strong fluctuations. If there would be a few handfuls of atoms in
the one half of the container, we could observe that more and more atoms
are moving into the other container until both halves contain very roughly
speaking the same density of atoms. However, we would observe many
fluctuations about an equilibrium state, and never would see a truly stationary
(continued)


================================================================================
PAGE 183
================================================================================

8.6 CentralLimitTheorem 163
state. If there were incredibly many atoms, such that we would not be able
to see them individually any more, as, e.g., in a fluid in a centimeter-sized
container, then we would only observe a continuous process that follows
FICK’s law and all processes would follow the “expected trajectory” without
fluctuations (obviously, we have to assume that adding more atoms does
not change the interactions or introduce new physical phenomena). The LLN
guarantees that such a stable, stationary state exists.
8.5.2 The Law of Truly Large Numbers
An extension of the Law of Large Numbers is the Law of Truly Large Numbers.It
states that if larger and larger samples of observations are investigated, the likelihood
of seeing something unusual (e.g., a particularly small or large value, rare events, or
even entirely implausible observations) increases.
Such unusual events have the property that we tend to notice and remember them
much more than we would notice average events (e.g., people tend to remember
the big lottery wins but never the many failed betting attempts). In extremely
large samples, it is therefore paradoxically rather easy to find correlations. While
a statistician would typically always agree that larger samples are better, it is
important to beware that inference based on single events in very large samples
can be very deceptive. Cherry picking of data, i.e., choosing only those data that fit
to a hypothesis, is generally problematic. The law of truly large numbers warns us
that this can also happen unintentionally.
8.6 Central Limit Theorem
Assume that a number of n random experiments are performed, for each of which
the mean value x of the random variable X is recorded. Then, the central limit
. i
theorem (CLT) states that the shape of the distribution of the n values of the sample
means x always approaches the shape of a Gaussian distribution.
. i
As an example, consider an experiment of sampling integers from a uniform
random distribution with values in between −10and 10, all of which have the same
.
likelihood to appear. For these, we will compute the mean using a Python program.
As a preparation we import the used Python modules. Additionally, we define the
Gaussian distribution function that is given by the location loc and the standard
deviation s :


================================================================================
PAGE 184
================================================================================

164 8 IntroductiontoStatistics
In [1]: import numpy as np
def gaussian(x, loc, s):
return (1 / np.sqrt(2 * n p.pi * s ** 2) *
np.exp(-(x - loc) ** 2 / (2 * s ** 2)))
Next, we define the size of a sample, for which the mean is to be computed, as well
as the number of means to be calculated.
In [2]: sample_size = 10000 # larger values = narrower distributions
n_means = 500 # larger values = more "Gaussian-like"
Now, we create a list of the mean values of all samples:
In [3]: means = []
for i in range(n_means):
sample = np.random.randint(-10, 11, size=sample_size)
sample_mean = np.mean(sample)
means.append()
which can then be visualized as a histogram
In [4]: import matplotlib.pyplot as plt
plt.hist(means, bins=30, density=True)
x = np.linspace(-0.4, 0.4, 200)
plt.plot(x, gaussian(x, 0, 0.06))
as shown in Fig.8.6. We also superimposed a line plot of a Gaussian function
for which we identified the best fitting value of the standard deviation manually.
Figure 8.6 shows the results for two different numbers of mean values. It is
quite remarkable and rather surprising how well the agreement with the normal
distribution is. Note that the distribution from which the samples were taken was
a uniform random distribution, i.e., all values within the given range occur with
equal likelihoods. It is even more surprising as this behavior is in fact independent
of the statistical distribution from which the samples are drawn—as long as trials
are independent and identically distributed (IID). This is quite a useful theorem, and
there are (at least) two direct applications:
(cid:129) We can use the theorem to make inference about the distribution of the means of
samples; we also have a way of estimating how reliable the size of the collected
dataset might be.
(cid:129) The CLT can be used to generate Gaussian distributed random numbers. All that
is needed are random numbers sampled from any distribution. However, this
strategy is relatively slow, as compared to other methods, such as the Box-Muller
method.


================================================================================
PAGE 185
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 165
Fig. 8.6 Visualization of the consequence of the central limit theorem (see the text for further
explanations)
Words of Advice (cid:2)
Sometimes, the central limit theorem is confused with the Law of Large Numbers.
These are, however, two entirely different theorems: The Law of Large Numbers is
intuitive and tells us that agglomerating more data helps to obtain more accurate
estimates of the population. The central limit theorem, however, is not intuitive at
all and also does not conclude anything about a single mean. It is a great tool for
analyzing a larger set of sample means.
8.7 Relations Between Multivariate Variables: Covariance and
Correlation
In many experiments and simulations, variables are used or recorded that are not
entirely independent of each other. An example is the obvious relation between force
and displacement during a tensile test: as the displacement increases, so does the
force. Another example is the radius r of a circle and its circumference S: there
exists a (in this case linear) relation between the two variables as the circumference
is proportional to the radius. Thus, if the radius increases, so does S ;ifr decreases
also, S becomes smaller; the two variables have a positive relationship. However,
dependencies between variables can also be “weak” dependencies where we even
might not be aware of a clear mathematical relation between variables.
8.7.1 Covariance
Covariance is a measure for the degree of relationship between two variables and
is related to variance: while variance is a measure for the spread of distribution,
covariance is a measure of how strongly these two variables vary together. Here is
the formal definition:


================================================================================
PAGE 186
================================================================================

166 8 IntroductiontoStatistics
Definition 8.3 (Covariance) The (true) covariance is defined for two uni-
variate random variables X and Y (each of which consists of N records) as
the expected or mean value of the product of the random variable deviations
from the respective expected values:
Cov(X,Y)=E[(X−E[X])(Y −E[Y])]. (8.2)
.
If the expected value E of the population is unknown (which is usually the
.
case), then the sample means x and y are used as estimators
. .
(cid:2)N (cid:3) (cid:4)(cid:3) (cid:4)
1
Cov(X,Y)= x −X y −Y . (8.3)
. N −1 i i
i=1
which explains the scaling factor of 1/(N − 1) resulting from Bessel’s
.
correction, cf. Sect. 9.3.4.
From this definition it is directly clear that the covariance is symmetric in its
arguments such thatCov(X,Y)=Cov(Y,X). Similar to the mean and the variance,
.
also the covariance can be written in terms of linear algebra operations, as shown
in Example 8.6. This approach is then generalized in Sect. 8.7.2 in form of the
covariance matrix.
Example 8.6 (Covariance Using a Vectorized Formulation) (cid:2)
Equation (8.3) can also be rewritten using the vector product between the two
column vectors X = x[ , x ,...,x ]T and Y = y[ , y ,...,y ]T each of
. 1 2 N . 1 2 N
which contains N records/measurements:
1
Cov(X,Y)= (X−X)T·(Y −Y) (8.4)
. n−1
Consider X = [1,2,3]T and Y = [9,7,5]T. The mean values are X = 3and
. . .
Y =10.5. Then, the covariance is computed as
.
1
Cov(X,Y)= ((1−2)(9−10.5)+(2−2)(7−10.5)
. 3−1
+(3−2)(5−10.5))
1
= ((−1)(−1.5)−5.5)=−2
2
(continued)


================================================================================
PAGE 187
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 167
The vectorized Python implementation follows Example 7.4:
In [1]: import numpy as np
X, Y = np.array([1, 2, 3]), np.array([9, 7, 5])
mean_X = np.mean(X)
mean_Y = np.mean(Y)
covXY = 1 /(l en(X) - )1 * (X - mean_X).dot(Y - mean_Y)
covXY
Out [1]: -2
8.7.2 Covariance Matrix
Often, the situation occurs where we need to investigate the correlation of more than
two random variables. In those cases, a compact mathematical form can be obtained
when the individual random variable (RVs) are represented in form of a feature
matrix. The feature matrix X represents a multivariate random variable, which is
.
given by the following column vectors, joined into a matrix (cf. Eq.(3.11)),
⎡ ⎤
⎡ ⎤ x x ··· x
11 12 1n
. X=⎣ X | 1 ···X | n ⎦= ⎢ ⎢ ⎢ ⎣ x 2 . . 1 x 2 . . 2 ··· x 2 . . n ⎥ ⎥ ⎥ ⎦ (8.5)
| | . . .
x x ···x
21 22 mn
where each of the X is a random variable and consists of m records.
. i
Recall that the covariance of two variables is a scalar value. Now, we have
altogether n RVs.The covariance matrix consists of the covariance values evaluated
for each pair {X ,X },Eq.(8.3), summarized in form of a matrix:
. i j
⎡ ⎤
Cov(X ,X ) Cov(X ,X ) ··· Cov(X ,X )
1 1 1 2 1 n
⎢ ⎢Cov(X 2 ,X 1 ) Cov(X 2 ,X 2 ) ··· Cov(X 2 ,X n ) ⎥ ⎥
. Cov(X)=⎢ ⎣ . .
.
... . .
.
⎥ ⎦ . (8.6)
Cov(X ,X ) Cov(X ,X ) ··· Cov(X ,X )
m 1 m 2 m n
Thus, the covariance matrix is a compact representation of the correlations between
all variables. In what follows we derive a formulation that can easily be applied to
the whole feature matrix using only a computationally efficient and mathematically
compact matrix-matrix multiplication.


================================================================================
PAGE 188
================================================================================

168 8 IntroductiontoStatistics
Step-by-Step Derivation of the Structure of the Covariance Matrix S
A number of later chapters such as the “principal component analysis” in Sect. 15.2
will require the covariance matrix and will be using it as part of more complex
formulations. Since this is the first time that we are calculating with the feature
matrix, we will do it in more detail than usual.
As before, we have to differentiate between the population covariance matrix,
abbreviated by (cid:2), and the sample (or data) covariance matrix, abbreviated by S.
. .
S serves as an estimator for the population covariance matrix (cid:2). The following
. .
derivations will be written only for the sample covariance matrix, but they hold for
the population covariance matrix as well, if the factor from Bessel’s correction is
adapted.
The first step according to Eq.(8.3) requires that the data of each feature needs
to be centered around the mean value. For this, the sample mean of each variable
X is computed by
. j
(cid:2)m
1
X = x . (8.7)
. j ij
m
i=1
These values are then subtracted from the records of each feature, resulting in the
centered data matrixX:
.
⎡ ⎤ ⎡ ⎤
x −X x −X ··· x −X x x x ··· x
11 1 12 2 1n n 11 12 13 1n
⎢ ⎢x 21 −X 1 x 22 −X 2 ··· x 2n −X n ⎥ ⎥ ⎢ ⎢x 21 x 22 x 23 ··· x 2n ⎥ ⎥
. X =⎢ ⎣ . . . . . . ⎥ ⎦ =⎢ ⎣ . . . . . . . . ⎥ ⎦ ,
. . . . . . .
x −X x −X ··· x −X x x x ···x
m1 1 m2 2 mn n m1 m2 m3 mn
(8.8)
where in the second matrix we introduced the abbreviation x := x −X . With
. ij ij j
this we can now compute the sample covariance matrix Sas
.
S= 1 X T X (8.9)
. n−1
which is a quadratic n × n matrix that additionally is also symmetric (cf.
.
Appendix A.2.12). To elucidate the structure of this product and the correspondence
to Eq.(8.3), we explicitly write down the elements of the matrix product as shown
in Fig.8.7 and then compute two particular elements of S. The first one is on the
.
main diagonal (highlighted in blue in Fig.8.7):
(cid:11) (cid:12)
1
s = x2 +x2 +···+x2 . (8.10)
. 11 n−1 11 21 m1
Observe that s is the variance of the first variable or feature. Each other element
. 11
s on the main diagonal of Sgives the variance of the i-th feature. Next, we take a
. ii .


================================================================================
PAGE 189
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 169
Fig. 8.7 Matrix product for computing the covariance matrix. Two examples of involved rows and
columns are highlighted in the two different colors
look at an element that is off the diagonal. The corresponding rows and columns are
highlighted in light orange color in Fig.8.7:
1
s = (x x +x x +···+x x ) (8.11)
. 23 n−1 12 13 22 23 m2 m3
s is the covariance of variables X and X , or in general, s is the covariance
. 23 . 2 . 3 . ij
of variable X and variable X . For i = j, we obtain the elements on the main
. i . j .
diagonal,
(cid:11) (cid:12)
1
s = x2 +x2 +···+x2 (8.12)
. jj n−1 1j 2j mj .
(cid:2)n (cid:3) (cid:4)
= 1 x − X 2 (8.13)
n − 1 ij j .
i=1
= Var[X ] , (8.14)
j
each of which is identical to the variance of the RV . X j . Examples of the resulting
covariance matrices for different datasets are discussed below in Example 8.7.
Where Is the Covariance Matrix Typically Used? (cid:3)
In later chapters (e.g., Chap.12), we will “fit” a regression line to given data
and identify the covariance matrix as one of the main ingredients for finding
this line in a closed form solution.
The covariance is a measure for how strong the linear relationship between
two variables is. In the context of principal component analysis (PCA) in
Sect. 15.2, this is used to estimate the redundancy between variables: if a
variable X is a linear function of a variable X , then it does not add new
. 2 . 1
information to a dataset and is said to be redundant. Thus, a small value
(continued)


================================================================================
PAGE 190
================================================================================

170 8 IntroductiontoStatistics
1.0
0.5
0.0
0.0 0.5 1.0
X
1
X 2
0.076 0.007 0.004 0.000 0.066 −0.039
S = S = S =
0.007 0.002 0.000 0.006 −0.039 0.031
0.0 0.5 1.0 0.0 0.5 1.0
X X
1 1
Fig. 8.8 Three different datasets together with the resulting covariance matrix.S
of covariance implies a low redundancy, while a high value signifies a high
degree of redundancy.
Furthermore, it is the main constituent of the correlation matrix, which
will be introduced next.
Python Implementation and Example
A Python implementation is straightforward, thanks to the convenient form of the
equation derived in Eq.(8.9). The following code lines demonstrate this for an
artificially created dataset. We start by importing the numpy module and define
the number of records:
In [1]: import numpy as np
rng = np.random.default_rng() # create a random number generator
n = 100 # number of records
Next, we create values for X and randomly “jitter” them a bit. X is a linear
. 1 . 2
function of . X 1 with . X 2 = 0.5+0.1X 1 . RVs are superimposed with some noise.
To create the data matrix X we stack the two row vectors on top of each other and
.
transpose the resulting matrix such that each feature is a column:
In [2]: X1 = np.linspace(0.03, 97, num=n) + rng.normal(loc=0, scale=0.01, size=n)
X2 = 0.5 + 0.1 * X1 + rng.normal(loc=0, scale=0.03, size=n)
X = np.vstack((X1, X2)).T
This dataset is shown in the left panel of Fig.8.8.Xin Eq.(8.9) denotes the centered
.
data matrix that is obtained by subtracting the mean of X and X from the two
. 1 . 2
columns of X:
.
In [3]: Xc = X - np.mean(X, axis=0)
where axis=0 indicates that the mean along the first axis is to be computed, such
that the result is a numpy array with two elements, the mean ofX and the mean of
. 1


================================================================================
PAGE 191
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 171
. X 2 . Now we can write down Eq.(8.9) as a Python expression where @ represents
the matrix-matrix multiplication of two numpy arrays, cf. Appendix A.2.4.
In [4]: 1 / (n - 1) * (Xc.T @ Xc)
Out [4]: array([[807.54640804, 80.78520835],
[ 80.78520835, 8.08245467]])
A shortcut exists in form of the numpy function cov , which also takes care
of centering the data. Note, however, that this function expects that features are
contained in rows and not in columns. This can be changed by setting the parameter
rowvar=False :
In [5]: np.cov(X, rowvar=False)
Out [5]: array([[807.54640804, 80.78520835],
[ 80.78520835, 8.08245467]])
giving an identical result as for the “manual” calculation.
We are now able to compute the covariance matrix for arbitrary datasets.
Example 8.7 shows three examples and discusses the resulting covariance matrices.
Furthermore, these examples elucidate that the covariance represents average
features of a dataset: visually we can easily detect a predominant orientation of
the data, and this orientation determines then the relation between two variables on
average.
Example 8.7 (Interpretation of the Covariance Matrix Values) (cid:2)
In Fig. 8.8, three different datasets are shown along with the corresponding
covariance matrices.
The left dataset has a large spread along the X direction and a small
. 1
spread along X which results in s (cid:2) s because the diagonal elements
. 2 . 11 22
represent the variance of X and X , respectively. The value s is very small
. 1 . 2 . 12
as there is only a very small dependency between the two variables (when
X changes, X changes only slightly). s > 0 implies a positive relation
. 1 . 2 . 12
between the two variables: if X increase, then also X increases.
. 1 . 2
In the middle dataset, most of the points are “clustered” around the
position (0.75,0.75). This explains why the two off-diagonal covariance
.
values are approximately zero: there is no other relation between the two
variables than that of the concentration around the point. As the spread of
the cluster is approximately the same for theX andX directions, the values
. 1 . 2
of the variances are also approximately the same, s ≈ s . The spread is
. 11 22
roughly five times as large as the spread ofX in the left dataset which is also
. 2
consistent with the two values of s .
. 22
(continued)


================================================================================
PAGE 192
================================================================================

172 8 IntroductiontoStatistics
The rightmost dataset has a strong spread into both directions, which
shows in the appreciable values of both s and s . The latter is about half of
. 11 . 22
the values ofs because the range of the values projected on theX direction
. 11 . 2
is much smaller than the range of the values projected on the X direction.
. 1
Here, s has a large value where s <0implies a negative relation between
. 12 . 12
the two variables: if X increase, then X decreases (on average).
. 1 . 2
8.7.3 Correlation
A main difficulty with interpreting the covariance is that the value does not exactly
tell us how strong the dependency between two variables is. For example, a value of
350 does indicates a positive correlation. However, for comparing the “strength of
correlation” between two pairs of RVs, a comparison of the covariance value is not
meaningful. The reason is that the one pair of variables might be scaled differently
or using different units both of which will affect the mean values that are contained
in the calculation of the covariance. The remedy to that problem is the concept of
correlation.
To be more precise, the remedy consists in dividing the covariance by the
standard deviations of both variables, i.e., to normalize the covariance. This results
in the correlation coefficient:
Definition 8.4 (CorrelationCoefficient) Pearson’s correlation coefficient of
the random variables X and Y is given by the normalized covariance
Cov(X,Y) E[(X−E[X])(Y −E[Y])]
Cor(X,Y)≡ρ = = (8.15)
. X,Y
σ σ σ σ
X Y X Y
The population correlation coefficient is typically denoted byρ(or explicitly:
.
ρ ) whereas the sample correlation coefficient is denoted by r (or more
. XY
explicitly by r ) and is written in terms of the n records of X and Y:
. XY
(cid:13)n (cid:3) (cid:4)(cid:3) (cid:4)
x −X ¯ y −Y ¯
i i
r = (cid:14) i=1 (cid:14) (8.16)
. X,Y
(cid:13)n (cid:3) (cid:4) (cid:13)n (cid:3) (cid:4)
x −X ¯ 2 y −Y ¯ 2
i i
i=1 i=1
As compared to the covariance, the correlation coefficient is advantageous as its
value range is limited to [−1,1]. The reason is the normalization: the covariance
.


================================================================================
PAGE 193
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 173
is always smaller than the product of the two standard deviations. Then, +1 and
.
−1 denote the strongest (positive or negative) linear correlation, while a value
.
close to zero denotes the strongest disagreement. Additionally, the normalization
also removes the units from the covariance and makes the correlation coefficient a
dimensionless measure. The correlation coefficient is not affected by shifting the
data along one direction or by scaling of the variables.
For an interpretation of the correlation coefficient, we first take a look at the
extreme cases: a value of 1 implies a linear relationship between the two variables
and all points would be located on a line with positive inclination such that y = x.
.
For -1 the line would have a negative inclination and we have y = −x. If the
.
correlation coefficient is 0, then there is no linear relationship between the two
variables. In turn, this also implies that if two variables are related to through a
purely nonlinear relation, then the correlation coefficient will be zero. Figure 8.9
shows several examples for RVs with linear and nonlinear relationships together with
the respective correlation coefficients and cross-covariance value. Comparing the
first two columns, it becomes clear that the change of scale for X has a tremendous
impact on the covariance, while the correlation is unaffected. Additionally, we
clearly observe that a negative (or positive) slope results in a negative (or positive)
correlation coefficient. The less noise is “fogging” the linear function the higher the
correlation coefficient; the left and middle figure in the top row approaches already
the limits of ±1. Finally, the rightmost column shows data that are nonlinearly
.
related. For those, the correlation coefficient is nearly zero.
Interpretation of Pearson’s Correlation Coefficient (cid:3)
Quite conveniently, the magnitude of the correlation coefficient, ρ or r,
.
is dimensionless and can be directly interpreted. As a rule of thumb, the
following value ranges can be differentiated:
coefficient . −1 . −0.8 . −0.5 0 . +0.5 . +0.8 . + 1
value:
type and perfect strong moderate variables moderate strong perfect
strength of negative negative negative uncorre- positive positive positive
correlation: corr. corr. corr. lated corr. corr. corr.
Correlation and Causation
There is a well-known phrase “correlation does not imply causation,” which is a
warning about how to interpret correlations, e.g., governed by PEARSON’s correla-
tion coefficient. It implies that just because two variables seem to be correlated, this
does not mean that there is indeed an underlying relationship where the change of
one variable is caused by a change in the other variable.


================================================================================
PAGE 194
================================================================================

174 8 IntroductiontoStatistics
Fig. 8.9 Correlation coefficients and covariance values for different data distributions of two
random variables. The left and middle column shows data with increasing randomness from the top
to the bottom; note the different ranges of the X values that strongly influence the covariance value.
The right column shows that the correlation coefficient is insensitive w.r.t. nonlinear relationships
of the two variables, hence the (nearly) zero value for.C or(X,Y)
For example, consider the oscillation of a pendulum with mass m. We observe
that the frequency ν reduces as m increases—there is a clear causality which we
.
know from physics. A different situation is if we collect data about the average
amount of steel production and the number of sold video games per year. Then, we
would find that both are increasing over the years resulting in a positive correlation.
Obviously, this does not imply that buying more video games increases the steel
production. As there is correlation without causation, this type of correlation is
called spurious correlation. In this case, there might be a third variable that was


================================================================================
PAGE 195
================================================================================

8.7 RelationsBetweenMultivariateVariables:CovarianceandCorrelation 175
unknown and the links of the two variables: the increasing size of the population
on the earth (this time, population does imply people) results in a higher demand
for metallic components and also lets the absolute number of game buyers increase.
This “hidden” variable is also called confounding variable. Furthermore, spurious
correlation can also occur by chance (e.g., due to a too small sample size).
What are the options for avoiding spurious correlations? The importance of a
solid physical intuition about the problem at hand should not be underestimated.
Ensuring that the sample is statistically representative, e.g., by using suitable
sampling strategies together with a careful design of the experiment are other
important strategies.
8.7.4 Correlation Matrix
For multivariate RVs the correlation coefficients can also be represented in form of
a correlation matrix, in analogy to the covariance matrix. The normalization of the
correlation coefficients has the convenient effect that all values are in the range
of [−1,1] and can therefore easily be visualized. Example 8.8 shows a Python
.
implementation and interprets the correlation matrix for the “DS-1 (Iris Flowers,
Sect. 4.7)” dataset.
Words of Advice (cid:2)
Anoftenencounteredconfusionarisesinthecaseofavalueofzeroforthecorrelation
coefficient. This does not mean that two variables are not related at all. It rather
indicates that there is nolinear relationship between them.
Example 8.8 (Matrix of Correlation Coefficients for the Iris Dataset) (cid:2)
The Iris dataset has four features that might be correlated with each other.
As a small Python example, we compute the covariance matrix and plot a
visualization of the values. For this, we start by importing the Iris dataset (see
Python Listing 9.1 on page 181 for more details).
In [1]: # This function is a just placeholder for reading the dataset and
# creating the feature matrix with the four features in columns.
X, y = import_iris_dataset()
print(f"X has {X.shape[0]} records and {X.shape[1]} features.")
X has 150 records and 4 features.
For computing the matrix of correlation coefficients, we use the numpy
function corrcoef which, in analogy to the covariance matrix, takes the
feature matrix and needs to be informed that the features are in columns:
(continued)


================================================================================
PAGE 196
================================================================================

176 8 IntroductiontoStatistics
In [2]: import numpy as np
cor = np.corrcoef(X, rowvar=False)
cor
Out [2]: array([[ 1. , -0.10378415, 0.87128294, 0.81697087],
[ 0.87128294, -0.41521773, 1. , 0.9623143 ],
[ 0.81697087, -0.35073314, 0.9623143 , 1. ]])
[-0.10378415, 1. , -0.41521773, -0.35073314],
The matrix can now be visualized, e.g., using imshow :
In [3]: import matplotlib.pyplot as plt
im = plt.imshow(cor, vmin=-1, vmax=1)
plt.colorbar(im)
Figure 8.10 shows a visually enhanced version of this plot. There, only half
of the symmetric matrix is shown in order to avoid redundancy. The strong
positive correlation between petal length and width implies that the longer the
petal, the broader they are. On the other hand, a larger petal width sometimes
implies a smaller sepal width; however, this relationship is rather moderate.
Sepal width and sepal length are (almost) not correlated, i.e., if the length
changes, this has, one average, either no effect on the width or the two features
are nonlinearly related.
Fig. 8.10 Matrix of
correlation coefficients for the
iris dataset


================================================================================
PAGE 197
================================================================================

8.8 Exercises 177
8.8 Exercises
8.1 (Variance) Show that the following relationship for a random variable X holds:
(cid:15) (cid:16)
Var[X]=E X2 −(E[X])2 (8.17)
.
8.2 (Correlation) Assume that for two random variables, the correlation coeffi-
cient is zero. Does this imply that there is no relationship between the two variables
at all?
8.3 (Correlation) What are the correlation coefficients for data that obey the
following functions: (a) X = 4 − 0.1X , (b) X = X2 + 1, (c) X = 527 ·
2 1 2 1 2
log(X2 + 103)
1
8.4 (Correlation) Compute the covariance matrix and Pearson’s correlation coef-
ficient for the following random variables:
X =[−2,−1,0,1,2] (8.18)
. .
Y =[5,2,4, 1,3] (8.19)
8.5 (Random Sampling) Calculate the value of π using random sampling. An
approach to estimate the value of π is randomly sampling points inside a square
of length 1 (r = 1) within which a quarter circle is inscribed.
1. Device a method to estimate the value of π by comparing the area of the square
and circle.
2. Write a Python code to sample points randomly and calculate the value of π.
3. How many data points are needed to accurately estimate the value up to 4 decimal
points.
4. Is there a strategy to improve the sampling, for example, using smaller number
of data points, but performing the experiment multiple times?
8.6 (Sampling Techniques) Given below is the data from molecular dynamics
simulation of Al using an EAM potential. The instantaneous temperature measured


================================================================================
PAGE 198
================================================================================

178 8 IntroductiontoStatistics
(by applying a thermostat) at 0 pressure is given in K, and the temperature is
recorded in intervals of 0.1 picoseconds.
T(K)=600, 232, 280, 215, 429, 358, 340, 271, 219, 286, 258, 360, 298, 316, 254,
287, 250, 325, 262, 363, 312.
1. What is the mean temperature of the simulation?
2. Compare different sampling techniques:
a. Use “take from top” sampling and choose the first ten values and measure the
average temperature.
b. Use systematic sampling with n =10 samples and measure the average.
Reference
1. D. Lusinchi. “President” Landon and the 1936 Literary Digest Poll: Were automobile and
telephone owners to blame? Social Science History, 36(1): 23–54, 2012. DOI https://doi.org/
10.1017/S014555320001035X.


================================================================================
PAGE 199
================================================================================

Exploratory Data Analysis 9
Exploratory data analysis is an attitude, a state of flexibility, a
willingness to look for those things that we believe are not there,
as well as those we believe to be there.
John Tukey (1915–2000)
American mathematician and statistician
9.1 The Why, the When, and the How
Assume that a colleague did an experiment for you and, after she was finished,
provided you with a data file. Obviously, you are curious to see what is in there and
how the experiment went. This is exactly the point of departure for any exploratory
data analysis (EDA).
The next two sections start with a short introduction of techniques for performing
an initial exploration of the data. The purpose is to ensure that we actually read the
file content correctly and that we mainly know what we are looking at. While in
a way trivial, these steps are crucial for avoiding many difficult to detect follow-
up mistakes. These efforts are in a way also “explorative” steps but are not always
considered as part of EDA.
The subsequent steps are then always considered part of any EDA: we analyze the
data with the purpose to describe and characterize the whole dataset in the spirit of
an initial inspection of the data. Among others, we are interested in finding out
if there are any patterns, e.g., is the data clustered around a certain point? Are
there correlations between variables? This can be either done purely descriptive,
i.e., using mathematical measures such as the mean or the variance that could be
then summarized, e.g., in form of a table. Or it can be done through visualization of
the data itself where the challenge is to reduce the typically multi- and often even
high-dimensional data to simpler representations that can be plotted in a two- or
three-dimensional figure. Both approaches will be introduced below for univariate
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 179
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_9


================================================================================
PAGE 200
================================================================================

180 9 ExploratoryDataAnalysis
and multivariate distributions in Sects.9.3 and 9.4. These two methods meet when
the mathematical measures are visualized themselves (e.g., plotting the distribution
of mean values for various samples).
After completing the EDA, we know what the state of the dataset is: we know
about the size, the “data coverage” of regions of interest, we might know about
outliers, missing data, and other errors. However, EDA may go beyond characterizing
distributions by simple mathematical measures that are typically found within
descriptive statistics: it may even employ advanced methods such as clustering of
data, dimensionality reduction, or multivariate regression analysis. Many of those
methods will be introduced in later chapter in the context of machine learning (ML)
(even though there will be no particular focus on EDA); for now we focus on the
“classical” EDA approaches such as initial data exploration, descriptive statistics,
and data visualization, each of which are performed prior to any further ML analysis.
With this, we are then able to make a decision if, e.g., more or different datasets are
required and which data analysis or ML approach might be the most suitable one for
further investigating of the dataset.
9.2 Two Preliminary Steps
Before the “real” EDA starts, it is very useful to do a quick screening of the files
and data and to ensure that everything is ready for the subsequent exploratory data
analysis.
9.2.1 Initial Exploration of Data File(s)
The very first step, that is often not mentioned because it is not a statistical data
analysis step, is a technical one: it aims at trying to understand the file format, the
character encoding of the file content, or the structure of the data in the file and to see
if the dataset seems to be in a good state. The easiest but nonetheless very effective
way in doing this is to simply “look at the files and the data.” Figure 9.1 shows
an extract from the DS-1 (Iris Flowers, Sect.4.7) dataset. Even though without any
additional information we do not know what the numbers mean, we still can observe
a number of important details. First of all, we observe, that this is a text file, that we
can read with a text editor; thus the data exists in form of ASCII characters and is
not binary data. Next we see that each line consists of four floating point numbers
with only one decimal place, followed by some text string (without spaces between
the words—this might be important if we are to split the row content into individual
items). Furthermore, numbers and the string are separated by commas; each row
has the same structure of four numbers and a string. Altogether, there are only three
different text strings. We don’t observe any unusual characters or “strange symbols”
which might indicate an unusual character encoding.
In this case, we most likely already know from additional documentation that the
first four numbers are the four attributes in centimeter (sepal length and width, petal


================================================================================
PAGE 201
================================================================================

9.2 TwoPreliminarySteps 181
Fig. 9.1 Extract from a file
containing the data of the
famous “Iris Dataset”: the
“==//==” denotes a number
of rows that were skipped for
this figure. The gray column
are line numbers shown by
the used text editor and are
not part of the file itself
length and width) followed by the “class” (Iris setosa, Iris versicolor, Iris virginica).
Nonetheless, through this quick look at the file content (i), we could ensure that the
file is in a good state, and (ii) we have an idea how the data should be imported.
Python Listing 9.1 shows an example how this could be done using numpy. Then,
Python Listing 9.1 A Python function for reading the file iris.csv. The function returns the
feature matrix and the target vector. Note the specification of the dtype when the two
np.array s are created. Due to the mixed data type (numbers and strings) in the file iris.csv,
we import all of it as strings and then create X as a float and y as a string . Section 4.7
provides more information on how to obtained the file
this function can be used to obtain the data in form of two numpy arrays. Next we
can then print some basic information, such as the number of rows and columns:
This can be quite useful information: we see that there are 150 data records of the
4 features—if we would have expected more (or less) records, then we know that
something must have gone wrong during reading the data file. We then can also take
a look at, e.g., the first four rows by


================================================================================
PAGE 202
================================================================================

182 9 ExploratoryDataAnalysis
Comparing this to the beginning of the data file is a good “sanity check” which helps
to ensure that the importing of the file data went well.
Words of advice (cid:2)
Better check twice that your numpy variables indeed contain the data from
the file that was imported. Sometimes, due to some peculiarities of the data
representation in the file, you might have missed the first line, the delimiter
that separates columns has a uncommon format resulting in “merging” of
numbers, or the feature matrix could have been transposed. “Looking at the
data” in the file and the numpy variables helps to avoid these problems.
As always in Python, there is not just one way of importing data files, there are
in fact many of them. An alternative to using numpy is pandas1 [3] which is one of
the Python packages of the Scientific Python Stack and offers much flexibility for
importing tabular data.
Last but not least, if you find in this initial investigation that the file does not
contain tabular data but unstructured data, then you most probably have to use
different ways for importing and storing the data.
9.2.2 A Quick Initial Visualization
This initial visualization is only a first glimpse at the data through some basic plots.
This could be, e.g., a line plot of, e.g., the total displacement vs. time, a histogram
of a temperature distribution, or a scatter plot of two features of the above “Iris”
example:
There, X[:,0] and X[:,1] give a vector with the data of the first and second
feature (sepal length and width), respectively. This results then in the plot shown
in Fig.9.2. The figure could be improved in several ways to reveal more or different
information; however, this is not the purpose of the initial visualization, and we
postpone all more sophisticated visual analysis to Sect.9.4.
1 https://pandas.pydata.org/.


================================================================================
PAGE 203
================================================================================

9.3 DescriptiveStatistics 183
Fig. 9.2 Scatter plot of the 4.5
first two feature of the “Iris
4.0
dataset.” Obviously, this plot
lacks labels and might not be 3.5
well formatted, but it serves
the purpose of a quick initial 3.0
visualization with just one 2.5
line of Python code
2.0
4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0
As this scatter plot is a two-dimensional projection of the dataset into the plane
spanned by the first two features, it cannot visualize the structure of the whole
dataset, e.g., we do not know if the data points are distributed homogeneously along
the other two directions (or, rather, features) or not. However, this plot already might
be good enough to get a first impression of at least an excerpt from the data and to
visually make sure that the dataset is in a good shape without any defects and that it
can be reliably imported for the following “proper” exploratory data analysis.
9.3 Descriptive Statistics
Descriptive statistics is used to compute summarizing properties of distributions of
data (the sample) which results in numerical values. Data visualization is a different
tool for understanding the data but is not considered to be part of descriptive
statistics.2 Making predictions about the population is not the goal of descriptive
statistics, and therefore it is not based on the mathematical foundation of probability
theory as in the case of inferential statistics. Descriptive statistics always operates
with characteristics that are “simpler” than the original data and that summarize
important aspects of distributions of data, hence the other, often encountered,
term of summary statistics. The most commonly used measures for describing and
characterizing distributions of data are:
(cid:129) Measures of the frequency:They describe how commonly specific events occur.
This can be the number of data points with a certain characteristics or a
percentage (e.g., “25% of all data records for the first attribute is smaller than
221.0”).
.
(cid:129) Measures of the central tendency:These are the mean, the median, or the mode,
all of which reveal something about the position of the “center point” of a
distribution.
2 Among statisticians and data scientists, this distinction is not always followed consistently, and
sometimes, data visualization is considered as part of descriptive statistics as well.


================================================================================
PAGE 204
================================================================================

184 9 ExploratoryDataAnalysis
(cid:129) Measures of dispersion or the variability: These are measures such as the
standard deviation, the minimum and maximum values of the variables. They
reveal information about the value range or how “spread out” the data are.
(cid:129) Measures of the shape: The shape of a distribution is described by the skewness
and the kurtosis which characterize the asymmetry and the “tail” of a distribution,
respectively;
(cid:129) Measures of the dependency of variables: In case that two or more variables
are considered: measures of the statistical dependence of variables, such as a
correlation coefficient.
A number of these measures can be conveniently obtained as moments of distribu-
tions, introduced in Chap.7.
9.3.1 Simple Descriptions of Univariate Distributions
Performing descriptive statistics of univariate data (i.e., data consisting of only
one variable) is the simplest way of characterizing data. As an example consider
a sample with the following measurements of the temperature in degrees Celsius:
[20.5,19.8,20.1,22.4,20.9]. Here is our summary: “The sample consists of 5 data
.
points that have a mean value of 20.74, the minimum is 20.1, the maximum is 22.4”.
The values of the minimum and maximum (i.e., the range of a distribution) are
the simplest way of characterizing the spread of the distribution. They are strongly
influenced by extreme values, though. There are a number of other measures that
describe, e.g., the shape of a distribution which will be introduced in Sect.9.3.3 ff.
Another way of presenting a small dataset is a frequency table. An example
is shown in Table 9 .1 which is based on a number of experimental specimens
which have different thicknesses. The shown table summarizes how many times
a sample with a particular thickness was used by simply listing the “feature” (e.g.,
the measured variable) and how many times this measurement occurred. From this
table we can easily see that the most common specimen has a thickness of 55μm.
.
If the data is not discrete but rather continuous, then we would count the number
of data points for given intervals that without gaps cover the whole range of interest.
Table 9.1 Frequency table
Thickness Frequency
showing the number of
measurements taken from .
[μm]
.
[ −]
specimens of different 50 6
thicknesses (only discrete 55 10
values in steps of.5μm could 60 9
be measured). The
65 8
information of the total sum is
70 3
optional but can be useful for,
e.g., computing percentages Sum: 36


================================================================================
PAGE 205
================================================================================

9.3 DescriptiveStatistics 185
In analogy to what was done in the context of discrete probabilities, we can also
show the relative frequencies which then should sum up to 1.
9.3.2 Simple Descriptions of Multivariate Distributions
Most of the above univariate measures can also be used for characterizing mul-
tivariate distributions: counting data in categories and computing minimum and
maximum values can be directly done, but also computing various other measures
of univariate distributions can be computed if the data is projected onto a single
axis, similar as what would be done in the context of probabilities with a marginal
distribution. Example 9.1 shows how this can be applied in practical situations.
Example 9.1 (Using “Data Projections” for Simple Data Analysis) (cid:2)
How can we obtain a “projection” of the dataset onto a single axis and how
can we perform calculations with it?
Consider the example of the “Iris dataset” from Sect.9.2.1 where the
feature matrix X consisted of four columns each of which contained the
.
data for one variable X ,...,X which makes it a four-dimensional data
. 1 4
distribution:
By only considering one variable, the other three “dimensions” are ignored,
effectively projecting all data on the first variable.
which shows that X1 is a vector with 150 records. We can now compute, e.g.,
the mean of the first and the second variable:
(continued)


================================================================================
PAGE 206
================================================================================

186 9 ExploratoryDataAnalysis
In this example, we obtained two mean values, one along each of the first two
“directions.” From Fig.9.2 we would say that the mean along the horizontal
direction (X ) is around 6.0 and the mean of the vertical direction (X ) is
. 1 . . 2
approximately at 3.0. This fits very well to the first two mean values that we
.
obtained from mean_X1 and mean_X2 as . 5.843and . 3.054, respectively.
For small datasets a contingency table can be used that shows the frequency
of individual data categories; this type of a table is also known as cross tab; see
Table 9.2. This representation gives an easy overview over the sample and helps
to see that, e.g., specimen type C is underrepresented (5 vs. 10 or 11 data points).
A contingency table only works for two-dimensional data, in rare cases also three-
dimensional data can be shown by printing “slices” of the dataset, but for higher
dimensional datasets or large amount of data this becomes impractical. In such
cases, descriptions of conditional or marginal distributions can still be given in such
a tabular structure.
Often, the relationship between two or more variables is of interest. A simple
means to characterize this is the slope of a regression line (the best fitting curve). A
more general and quantitative measure of the dependence between two variables is
the correlation introduced below.
Table 9.2 Contingency
Cu content Total by
table that shows the number
of experiments performed
Specimen type .<0.2% 0.2–0.5% .> 0.5% specimen
with a specific specimen type A 3 1 6 10
and for a certain Cu content. B 2 4 4 10
The numbers denote C 3 2 0 5
frequencies; the right column
D 4 6 1 11
and bottom row are the sums
Total by Cu 12 13 11
of rows and columns,
respectively content


================================================================================
PAGE 207
================================================================================

9.3 DescriptiveStatistics 187
9.3.3 Central Tendency: The Mean, the Median, and the Mode
The central tendency of a distribution of data tells something about where the
“middle” or the “average” is located. It is able to reduce an entire dataset to a single
number.
The Mean
The mean of a population is typically not known, and we almost always operate with
the sample mean. The mean is obtained in the exact same way as the expectation
value (cf. Chap.7 and in particular Sect.7.3.3) with the only conceptual difference
that now we are operating with sample data; all theorems about calculating with
expectation values (Sect.7.1.2) such as the addition of mean values still hold.
Definition 9.1 (Sample Mean) The sample mean of the observations x is
. i
indicated by a line on top of the variable, x, and is given by
.
(cid:2)n
1
x = x . (9.1)
. i
n
i=1
There, the n instances of the sample dataset is the set {x , ..., x }.
. 1 n
In case that the dataset contains more than one feature, one can compute the mean
for each feature separately.
How about the Mean of a Continuous Variable? (cid:3)
Why don’t we also show the mean for a distribution of a continuous random
variable (RV) using integration as in Sect.7.3.3? Because the data obtained
from a file is automatically discrete data, we never will get a mathematical
function from the file. However, in cases where a specific probability density
function was fitted to a dataset, then we typically operate with mathematical
functions and need to either (i) perform an integration on that or again
“discretize” the data and perform a numerical integration which usually
implies again summation. Later, we will also consider such a case.
Note that there are different types of means: the arithmetic mean from Eq.9.1
(which is the one that we usually refer to when we speak about the mean), the
geometric mean (a measure based on the product of the data) as well as the harmonic
mean (which is based on the reciprocal of the sum of the reciprocal data), to name
only the most well-known means.


================================================================================
PAGE 208
================================================================================

188 9 ExploratoryDataAnalysis
The Median
The median is an alternative description of the central tendency. It is computed as
summarized in Algorithm 9.1.
Algorithm 9.1: Computing the median (cid:3)
1. Sort all N data points;
2. Locate the middle value in the sample, i.e., count the first.N/2data points;
3. Incasethat N isanevennumber:taketheaverageofthemiddletwoobservations.
There is no particular symbol that is commonly used for the median. It is also
called 50-th percentile because it splits the amount of data into two sets of equal size
(other percentiles are defined analogously). The median is much less sensitive w.r.t.
outliers than the mean. For the example dataset of the four numbers{2,4,6,50},the
.
median would be 5. The median also would be 5 regardless if the value of the last
number is 50 or 5000! An example showing how to compute the median for even
and odd numbers of values is given in Example 9.2.
Example 9.2 (Mean and Median) (cid:2)
As an example consider the two datasets, dataset #1 consisting of
{7,1,3,6,3,9,8} and dataset #2 consisting of {9,8,4,6,5,4,3,2,9,1}.
. .
Then, the sorted datasets are shown with the “middle” values marked in
boldface:
(cid:129) dataset #1: {1, 3, 3, 6, 7, 8, 9} has the median 6 and the mean of ≈5.29.
.
(cid:129) dataset #2: {1, 2, 3, 4, 4, 5, 6, 8, 9, 9} has the median (4+5)/2=4.5and
.
a mean of 5.1.
.
A typical application in materials science and physics is the analysis of data that
contains extreme outliers (values that are very far off due to different kinds of errors
or because they correspond to “rare events”), as shown in Fig.9.3 which could be
the velocity distribution of atoms in the vicinity of a surface. There, we observe
that the data is highly non-symmetrical. We already saw above that mean values are
strongly influenced by outliers and also here are expected to be too large. Here, the
mean is by a factor 10 larger than the median which also by visual inspection seems
to better represent the data.
As a general rule of thumb, one can conclude that mean values are suitable if the
data distribution is approximately symmetrical, while medians are more suitable for
data that is skewed and/or has a large number of outliers.


================================================================================
PAGE 209
================================================================================

9.3 DescriptiveStatistics 189
Fig. 9.3 The data (obtained from a lognormal distribution) is highly asymmetric (“skewed”) with
many large outliers. The mean is 1.021 (full line), while the median is 0.358 (dashed line). By
visual inspection, the median clearly seems to represents the data better
Fig. 9.4 Visualization of
four distribution types that
result in different types of
modes. The multimodal
distribution has three maxima
and is therefore also called
“trimodal” distribution
The Mode
The mode of a dataset is the category or value that has the highest frequency, the
modal value. The definition of the mode is not mathematically strict because it
requires that a category or value needs to have significantly more elements than
the others; however, there is no clear rule how large that difference has to be. In case
that each value or category occurs the same number of times, then there is no mode.
In case of continuous random variables, that data first needs to be binned. Then the
mode can be obtained from the frequency distribution or histogram by identifying
the maxima. A visualization can be found in Fig.9.4.
As the modal value is used to characterize the distribution in terms of the
“number of peaks,” there are also a number of notions that describe these situations:
a unimodal distribution has one peak, a bimodal distribution has two peaks, and any
distribution with more than two peaks is summarized as multimodal distribution,
while a distribution without distinct maxima is called a uniform distribution.


================================================================================
PAGE 210
================================================================================

190 9 ExploratoryDataAnalysis
Example 9.3 (Mode) (cid:2)
For the following two dataset we first have to count the number of occurrences
of different items, e.g., using a frequency table. Then the mode can be directly
obtained as the number with the highest frequency(ies):
(cid:129) {5,6,5,7,5,8,9,5}: the mode is 5 because 5 occurs 4 times while any
.
other number occurs fewer times.
(cid:129) {5,6,5,6,5,8,6,5,6}: 5 and 6 are both modes as both occur the same
.
number of times and more often than any other number.
There is no commonly used variable name or symbol for the mode, and most of
the time it is just mentioned in text form, e.g., “the mode of the dataset is 2,” or
Modeis used in the equation as well.
.
9.3.4 The “Spread” of a Distribution: The Variance
In Sect.7.4.3, we already encountered the second centered moment and identified
it as the variance. The variance is a measure for how strongly “spread out” a
distribution is, the dispersion of a distribution. It is a measure that can occur in two
slightly different mathematical formulations, depending on whether the population
or the sample variance is considered.
Sample Variance and Bessel’s Correction
The variance, or to be more precise the population variance, is often either
represented by Var[X]or by σ2. Similar to the case of the mean, also the variance
. .
can usually not be computed for the population. Instead, we use the sample variance
as an estimate for the population variance. However, the equation for the sample
variance is slightly different, due to Bessel’s correction.
The reason why a “correction” is needed for computing the sample variance
is related to the fact that we are (at least indirectly) using the sample mean, x,
.
instead of the population mean, μ. As a consequence, any value of the sample is
.
automatically closer to the sample mean than to the population mean. Thus, for the
squares of the distance from the mean values, it is
(cid:2)N (cid:2)N
(x −μ)2 ≥ (x −x¯)2 . (9.2)
. i i
i=1 i=1
As a remedy, dividing the left term by n while dividing the right term only byn−1
.
(this is Bessel’s correction) effectively increases the value of the right term, and
therefore the approximation of the population variance becomes more accurate. Of


================================================================================
PAGE 211
================================================================================

9.3 DescriptiveStatistics 191
course, this can also be shown with mathematical rigor, but we will not show this
here.
For a given discrete random variable, where all N values have equal likelihoods,
the sample variance s is
(cid:2)N
1
s = (x −x¯)2 (9.3)
. N −1 i
i=1
The factor of 1/(N − 1) from Bessel’s correction occurs in many statistics
.
formulations and makes them more accurate. However, for larger sample sizes, the
difference to using N becomes negligible.
The following example shows how to compute the two types of variances:
Example 9.4 (Population and Sample Variance) (cid:2)
Assume that the population is given by the dataset {16,11,9,8,1}. The
.
population variance is the computed by the following steps:
1. Find the mean, μ : μ=(16+11+9+8+1)/5=9.
. x .
2. Subtract each data point from the mean; then square the result:
(16−9)2 =49 , 11(−9)2 =4 , 9−( 9)2 =0
.
(8−9)2 =1 , 1−( 9)2 =64.
3. Add up all of the squared differences from Step 2:
(16−9)2+(11−9)2+(9−9)2+(8−9)2+(1−9)2 =118.
.
4. Divide Step 3. by the number of items which gives a population variance
of 23.6.
.
Assuming that the dataset is a sample of an unknown population, we would
compute the sample variance by dividing by 4 instead of 5, giving a sample
variance of 29.5
Standard Deviation
The standard deviation was already introduced in Definition 7.4 in the context
of random variables. In analogy, we can also compute the standard deviation for
distributions as the square root of the sample or population variance.


================================================================================
PAGE 212
================================================================================

192 9 ExploratoryDataAnalysis
Words of advice (cid:2)
The variable name “s” is used for the sample variance and sometimes also
for the sample standard deviation. In this text we only useσ for the standard
.
deviation; it is either clear from the context whether this is a sample or a
population measure, or we explicitly state which one is used.
Example 9.5 (Measurement of the Speed Light (A. A. Michelsen, 1882))
(cid:2)
A physics example is based on a dataset published by Stigler [5], who
revisited the work done in 1882 by A. A. Michelson: Michelsen performed
5 experiment each of which resulted in 20 measurements which were used
to determine the speed of light. To compare this dataset to today’s accurately
known speed of light in vacuum (299,792.5kms
−1),
one still would need to
.
convert the data.
A visualization of the data is shown in Fig.9.5. The bottom plot shows
the data as scatter plot where each marker is slightly translucent such that
overlapping marker creates a darker color. There, we observe two outliers at
the left and one at the right. The top plot is a histogram that shows how many
measurements fall within each bin.
The mean is computed as 299852.4kms
−1
(it is shown by the red line in
.
the figure), and the median is 299850.0kms
−1.
The only minor difference
.
indicates that the distribution of data is approximately symmetric.
Fig. 9.5 The plots show 100
measurements of the speed of
light by A. A. Michelsen in
1882, data taken from [5]


================================================================================
PAGE 213
================================================================================

9.3 DescriptiveStatistics 193
Table 9.3 Summary
Sepal length Sepal width Petal length Petal width
statistics of the Iris dataset
Count 149 149 149 149
Mean 5.84832 3.05100 3.77450 1.20537
std 0.82859 0.43350 1.75965 0.76129
Min 4.30000 2.00000 1.00000 0.10000
25% 5.10000 2.80000 1.60000 0.30000
50% 5.80000 3.00000 4.40000 1.30000
75% 6.40000 3.30000 5.10000 1.80000
Max 7.90000 4.40000 6.90000 2.50000
9.3.5 Example: Summary Statistics of the Iris Dataset
Often, a typical summary statistics includes the number of records (the “count”),
measures for the central tendency (at least the mean and the median), measures for
the dispersion, as well as the shape of the distribution (e.g., in form of the standard
deviation, the minimum/maximum values, and the 25th and 75th percentiles). These
values are then computed for each feature of interest and summarized in a table such
as the one shown in Table 9.3.
9.3.6 Skewness
Comparing the Mean to the Median
Comparing the mean to the median also helps to characterize if a distribution is
symmetrical or not, cf. Fig.9.6. If the mean is left of the median, then this implies
that the distribution is negatively skewed. The reason is that the median splits the
dataset into two sets with equal numbers of points. If the mean is left of the median,
this implies that for every point in the right set of points, there is a point in the left set
of points that is further away from the median. Thus the left half of the distribution
stretches further away from the “middle” of the curve. A perfect normal distribution
has a skewness of zero because the mean equals the median. However, in practical
situations, distributions of data are never perfectly normal distributed. In these cases
one can use the skewness to get an estimate of how much the distribution deviates
from a normal distribution: for values of the skewness in the range of [−2,+2]one
.
can assume that the data are normally distributed. This, however, requires that the
sample size is sufficiently large and has, e.g., at least ≈300data points.
.
PEARSON’S Moment Coefficient of Skewness
The third standardized moment μ˜ is the skewness of the distribution, which is
. 3
a measure for the asymmetry of a data distribution. It is also called PEARSON’S
moment coefficient of skewness and quantifies the amount and the direction of the
deviation from the symmetry. It is obtained from letting n=3in Eq.(7.32) as
.


================================================================================
PAGE 214
================================================================================

194 9 ExploratoryDataAnalysis
Fig. 9.6 Comparing mean, median, and mode also helps to determine the skewness of a
distribution. A distribution is called “negatively skewed” if its tail points into negative direction; a
perfectly symmetrical distribution has no skew
(cid:3)(cid:4) (cid:5) (cid:6)
X−μ 3
γ ≡μ˜ =E with μ=E[X]. (9.4)
. 1 3
σ
Often, the variable γ is used which is also called “Pearson’s moment coefficient of
. 1
skewness.” Appendix B.3 shows an alternative formulation for Eq.(9.4). Effectively,
skewness describes how much a distribution differs from a normal distribution,
either to the left or to the right. Hence, the skewness value can be positive (if the tail
is to the left of the maximum), negative (tail to the right of the maximum), or zero
(for a symmetric distribution).
Further Skewness Measures
As a last method for estimating the skewness, we briefly introduce PEARSON’s mode
skewness (which is also called Pearson’s first skewness coefficient). It is given by
μ−Mode
Skew[X ]= (9.5)
.
σ
where “Mode” is the mode. Sometimes, it is difficult to estimate the mode, for
.
example, in cases where the difference between the maxima and the value range
some noise are small.
Pearson’s second coefficient of skewness (also called median skewness) is an
alternative measure and defined as
3(μ−Median)
Skew[X ]= , (9.6)
.
σ
where “Median” denotes the median. Both equations give their results in units of
.
the standard deviations, which are therefore dimensionless numbers.


================================================================================
PAGE 215
================================================================================

9.3 DescriptiveStatistics 195
Discussion
Measuring the skewness based on comparing the mean to the median or based
on higher order moments might seem to be intuitively reasonable if one is only
considering a distribution function given by a mathematical equation. However,
most of the time, we are not using mathematical formulation for distribution
function but rather the data themselves. The data might be noisy and contain outliers.
As we saw above, the mean as well as all moments are very sensitive w.r.t. outliers.
As all above skewness measures are either directly or indirectly based on moments,
they are sensitive w.r.t. outliers as well. Therefore, in these cases, it is advisable to
either first identify and remove outliers or to use measures that are not based on
moments, e.g., see the approaches presented in [2].
9.3.7 The Fourth Moment: Kurtosis
Finally, the fourth central moment contains information about the “tail” of a
distribution. It is given by
(cid:3)(cid:4) (cid:5) (cid:6) (cid:7) (cid:8)
X−μ 4 E (X−μ)4 μ4
Kurt[X ]=E = (cid:9) (cid:7) (cid:8)(cid:10) = , (9.7)
. σ E (X−μ)2 2 σ4
which is called PEARSON’s measure of kurtosis. A high value of kurtosis can either
imply that the distribution has a “fat” tail which contains a significant part of the
probability mass or a distribution is centered around the mean but has many outliers.
Below, a visualization method (the box-whisker plot, Sect.9.4.3) is introduced
which helps to identify outliers.
9.3.8 Summary
This section covered many different techniques and descriptions relevant for
descriptive statistics. Additionally, we had to differentiate between population and
sample properties. To recall the most important variables and operators that were
introduced in this section, we summarize them as follows:


================================================================================
PAGE 216
================================================================================

196 9 ExploratoryDataAnalysis
Things to Remember ... about notations (cid:4)
Let X and Y denote two features of a dataset where the.x iand.y iare the i-th data
record. Then in general we use Latin letters for sample quantities and Greek letters
for population properties. In particular:
(cid:129) the sample mean of a random variable X is given by.X , while the population
mean is given by.E[X]or.μ .
(cid:129) the sample variance is.s 2, while the population variance is denoted by.σ2 =
Var[X]
(cid:129) the covariance between two RVs X and Y is .σXY or .Cov(X,Y) for the
population, and the sample covariance for a number of measurements is.s
x
2
y
(cid:129) covariance matrix for a population and a sample is given by .(cid:4) and .S ,
respectively.
9.4 Data Visualization
Data visualization is an extremely powerful tool for EDA as well as for understand-
ing and analyzing data in general (see the work by Edward R. Tufte, e.g., [6]). In
fact, even in the previous section of descriptive statistics, a few plots were shown in
order to show what one or the other numerical measure (such as the median) means.
Additionally, we also introduced in Sect.9.2.2 a quick type of “initial visualization,”
to be used to get a very brief overview. As opposed to this, in the current section, the
data visualization strategies are somewhat more involved, and some of those plots
even might need some explanations so that the reader becomes familiar with how to
read and interpret some of the plots.
One of the famous and very educative examples which shows why descriptive
statistics necessarily needs to be complemented by data visualization is Anscombe’s
quartet. The statistician F. ANSCOMBE created in 1973 four datasets, each of which
had the same statistical properties (such as mean values, variances, the correlation
coefficient) as the three others [1]. The dataset is shown in Table 9.4. Upon
investigation, one finds that each of the four datasets has the following properties:
(cid:129) each of them has n=11observations
.
(cid:129) the mean of each X is exactly x =9
.
(cid:129) the mean of each Y is x =7.50which is accurate up to two decimal places
.
(cid:129) the sample variance of each X is exactly s2 =11
. x
(cid:129) the sample variance of each Y is s2 =4.125(±0.003)
. y .
(cid:129) the correlation coefficient between any pair of X and Y is Cor(X,Y) = 0.816
.
with an exact match of the first three decimal places
(cid:129) fitting a line to the data (see linear regression) always gives the equation Y =
.
3.00+0.500·X, where the two constants are up to two and three decimal places
accurate, respectively.


================================================================================
PAGE 217
================================================================================

9.4 DataVisualization 197
Table 9.4 The data of
I II III IV
Anscombe’s quartet given as
X Y X Y X Y X Y
a table similar to the one
presented in the original 10.0 8.04 10.0 9.14 10.0 7.46 8.0 6.58
publication [1]. The four 8.0 6.95 8.0 8.14 8.0 6.77 8.0 5.76
datasets are indicated by the
13.0 7.58 13.0 8.74 13.0 12.74 8.0 7.71
roman numerals I–IV
9.0 8.81 9.0 8.77 9.0 7.11 8.0 8.84
11.0 8.33 11.0 9.26 11.0 7.81 8.0 8.47
14.0 9.96 14.0 8.10 14.0 8.84 8.0 7.04
6.0 7.24 6.0 6.13 6.0 6.08 8.0 5.25
4.0 4.26 4.0 3.10 4.0 5.39 19.0 12.50
12.0 10.84 12.0 9.13 12.0 8.15 8.0 5.56
7.0 4.82 7.0 7.26 7.0 6.42 8.0 7.91
5.0 5.68 5.0 4.74 5.0 5.73 8.0 6.89
Fig. 9.7 A visualization of the data of Anscombe’s quartet using four scatter plots: while the
datasets have nearly identical statistical properties, the data distributions are upon visual inspection
entirely different. The roman numbers indicate the number of the dataset
While the coincidence of the properties for all datasets suggest that the data
itself are identical, visualizing the data as in Fig.9.7 reveals that the distributions
look entirely different: the left dataset I shows roughly a linear distribution, while
dataset II shows a nonlinear relation for which Pearson’s correlation coefficient is
not a suitable measure. The second plot from the right, dataset III, again shows a
linear behavior; however, the slope should be different from dataset I—except for
the outlier withY =12.74. Last but not least, dataset IV does not show any relation
. 3
between the two variables, but the one outlier is sufficient to strongly impact the
result, e.g., in terms of the correlation coefficient or the regression line.
9.4.1 Scatter Plots
Scatter plots are the most commonly means of data visualization. There, each datum
is represented by a marker, often a circle, a star, or any other shape that clearly
indicates the coordinate of the point. These plot types can be used for univariate
(cf. Fig.9.5) as well as for bivariate (shown in Fig.9.9 as well as in Figs.9.7 and


================================================================================
PAGE 218
================================================================================

198 9 ExploratoryDataAnalysis
8.9) or even for trivariate data distributions. However, scatter plots in EDA are
most commonly used for bivariate variables. Scatter plots can be also used for
datasets with more features by simply showing a projection of the data into a two-
dimensional plane. Beware that the marker are not interconnected by lines. If lines
show up in these plots, then usually it is to indicate trends or highlight, e.g., a mean
value.
We can also “annotate” such plots, e.g., by indicating the mean as a point (e.g.,
with a different color or a different marker type) or by showing a regression line3 as
in Fig.9.7.
9.4.2 Histograms
Histograms are related to scatter plots and represent a “coarsened” picture of the
scatter data. The main aspect of histograms is that the whole spatial domain is
subdivided into a number of sub-domains, the “bins” which are usually of equal
size. Then the number of values that fall into each sub-domain is counted, which
is why continuous data and not categorical data is required. The number of data
points per sub-domain are the numbers that are visualized. In the limit case of
infinitesimally small bins, the histogram would look like a scatter plot. Histograms
are useful for observing where, e.g., maxima are located (allowing to infer if it is a
uniform, bimodal, or multimodal distribution) and for getting an impression of the
structure and shape of distributions (is it a symmetric distribution or is it skewed to
one directions?).
HistogramsforUnivariateData
Binning in the context of univariate data implies that the whole range of interest
[a,b] of an input variable X is subdivided into an appropriate number of n bins,
.
i.e., intervals. For simplicity, we assume that all bins have the same size (cid:5)x =
.
(b−a)/N. Bin number i is then given by the interval [i·Δx,(i+1)·Δx].Next,
.
the number of points that are located in each interval are counted. The histogram
contains rectangular bars where one side has the length Δx and the other side’s
. i
length is proportional to the number of data points in the bin. This visualization
directly corresponds to the frequency tables that we encountered in Sect.9.3.
An important parameter for a histogram is the “bin size.” In principle, it can
be chosen arbitrarily; however, it has a strong impact on the visual representation,
cf. Fig.9.8, and therefore could have been accidentally chosen such that the
relevant feature of a dataset might not become apparent. As a rule of thumb,
the bin size should be chosen such that most of the bins contain several data.
Additionally, the bin size does not necessarily have to be the same for each bin.
If the frequency is divided by the total number of data, then relative frequencies
3 This is, in a nutshell, the line that fits best to the data, typically minimizing the sum of the squared
errors.


================================================================================
PAGE 219
================================================================================

9.4 DataVisualization 199
Fig. 9.8 Histograms of the values .X =[10, 3, 3,-2,-5, 9, 5, 8, 9, 4, 9, 9, 15, 11, 1,-4,-9, 13,-7,
3. ] for a bin size of 5 (left panel) and . ≈1.5(right panel). A too small bin size (right panel) results
in many empty bins or bins with very few data. In this visualization there is a tiny gap between the
bars; however, these are just for guiding the eye
are depicted. Histograms help to estimate the probability density function; kernel
density estimation plots are an alternative visualization method that operates with
smoothed data.
Words of advice (cid:2)
Eventhoughbardiagramslooklikehistograms(inparticularwhenthe“bars”
areslightlyseparatedforvisualpurposesasinFig.9.8),thereisafundamental
difference: a box plot always represents data of different categories, while
a histogram represents a distribution of non-discrete and quantitative data
where individual “bars” have a fixed order.
Histograms for Bivariate Data
In principle, bivariate variables could be visualized as a histogram in a (pseudo)
three-dimensional style, i.e., two variables and the frequency are the three axes.
However, it is difficult to quantitatively read the three-dimensional projection
into the two-dimensional paper plane. As a better alternative, one can map the
frequencies to color values (middle panel of Fig.9.9) from which we can easily
identify the first and second largest maxima. This representation also makes it
possible to quantitatively compare to bivariate distributions. There are numerous
applications in materials science and physics, among them are dislocation densities
which, if obtained from electron backscatter diffraction (EBSD) measurements
or simulations, are shown and compared in such a “coarse-grained” histogram
representation; see, e.g. [4,8,9].
A variant of such a histogram quantifies the frequencies by the size of markers (a
kind of scatter plot that allows scaling the marker size) as shown in the right panel
of Fig.9.9. There, the area of the marker is proportional to the frequency and not
the radius. Such kind of diagram also allows to use colors to represent other aspects
of the data, e.g., for the Iris dataset this could be used to indicate the four different
classes of flowers.


================================================================================
PAGE 220
================================================================================

200 9 ExploratoryDataAnalysis
Fig. 9.9 Visualization of bivariate data (the first two features of the “Iris dataset”: scatter plot
(left), 2D histogram (middle), and histogram using different marker sizes instead of colors
Even if in a scatter plot two points coincide, in a histogram plot they can be
differentiated due to the different frequency values. As an variant of the two-
dimensional histogram, it is also possible to use bins of hexagonal shapes. As
rectangular array-like grids emphasize the horizontal and the vertical directions,
hexagons can help disturb these strict patterns. They are additionally more similar
to circles which means that the data aggregation is more efficient and balanced.
However, as this kind of data binning is not commonly used, it might be more
difficult to “read.” matplotlib provides the command hexbin for this type of plot.
9.4.3 Box-Whisker Plots
A box-whisker plot (or only: box plot) is a very compact representation of several
statistical properties of a univariate distribution. The below used version was
initially developed by the statistician JOHN TUKEY [7]. A main ingredient are the
quartiles which partition the whole dataset into four subsets, each of which contains
the same number of data. For example, the first quartile ending at Q contains
. 1
25%-th percentile of the data with the lowest values. The 50-the percentile (Q )
. 2
splits the whole dataset into two halves, one where all data have a value smaller
than Q and one where all values a larger than Q . The second quartile (= the 50-
. 2 . 2
th percentile) corresponds to the above-introduced median; the 75%-the percentile
.
(Q ) is defined following the same strategy. For determining all three quartiles, the
. 3
same rules concerning odd and even number of data as in Algorithm 9.1 applies.
The box plot (a sketch of all relevant quantities is shown in Fig.9.10) consists
of a rectangle that covers the data from the first quartile to the third quartile. The
median is shown as a line inside the rectangle; for a symmetric distribution, that line
is exactly centered within the box. Often, the mean is also indicated, e.g., by a small
marker. The difference between
.
Q
3
and
.
Q
1
defines the interquartile range (IQR),
IQR=Q −Q , (9.8)
. 3 1


================================================================================
PAGE 221
================================================================================

9.4 DataVisualization 201
Fig. 9.10 Sketch of the elements contained in a box-whisker plot (see the text for more
explanations)
which is a length and gives the extent of the rectangle. TheIQRis used in a box plot
.
as a measure for the dispersion of a distribution. Unlike using the standard deviation
or the range of the variable (i.e., the maximum and minimum values), the IQR is
.
much less affected by outliers.
As an extension of the box plot, whiskers are added (which gave this plot type
its name), a line that stretches away from the box starting from Q or Q . In the
. 1 . 3
simplest version, the line then terminates at the data point with the minimum or
maximum value, respectively, excluding outliers. The minimum is also denoted as
theQ percentile and the maximum as theQ or 100th percentile. Following Tukey
. 0 . 4
et al. [7] a more robust approach is to use a length of 1.5IQRstarting from Q
. . 1
into negative direction which terminates at the smallest data point that falls into this
range. Similarly for the whisker on the positive side, the whisker ends at the largest
point that falls into the interval given byQ andQ +IQR. The fact that the whisker
. 3 . 3
ends at a data point and not exactly after the distance of 1.5IQRis the reason why
.
the two whiskers may have different lengths even in cases where coinciding mean
and median suggest a symmetric distribution. There are other conventions as well
which is why it is important to clearly state which kind of quantity the whiskers
denote. In this text we follow the “1.5IQR” convention.
.
The position ofQ −1.5IQRis called lower fence; that ofQ +1.5IQRis called
. 1 . 3
upper fence . Thefe nces are the points past which data are considered as extreme
values called outlier. Those might be the result of measurement errors or can be due
to a new phenomenon so far not seen in the data, to name but two possibilities.
Sometimes, an additional fence is considered at Q − 3IQRand at Q + 3IQR,
. 1 . 3
the outer fences. Points beyond the outer fences are then called extreme outlier,
points in the intervals[Q −3IQR,Q −1.5IQR)and[Q +1.5IQR,Q +3IQR)
. 1 1 . 3 3
are called mild outlier, and the corresponding fences are called inner fences. Outliers
are the only ones in a box plot where data points themselves and not summarizing
measures are shown. All of the above quantities are condensed in a box-whisker plot
as shown in Fig.9.10.


================================================================================
PAGE 222
================================================================================

202 9 ExploratoryDataAnalysis
Fig. 9.11 Box-whisker plot
of the four features of the Iris
dataset. The red dot shows the
mean; the orange line inside
the box shows the median
The box-whisker plot allows to compare several random variables and is a very
compact data representation. As an example, Fig.9.11 shows all four features of the
Irisdataset.From this plot, we can easily observe that the value range of the sepal
lengths and the petal widths do not overlap. We also see that the sepal length, unlike
the petal length, is approximately symmetric because the median and the mean
coincide. The sepal width contains a few outliers, but compared to the petal length,
its distribution is very narrow. Thus, box plots are very information-rich and very
well able to visualize some of the important numerical measures of distributions.
9.4.4 Advanced Exploratory Plot Types
The above-introduced plot types are the most important plot variants that are used
in EDA. There are a few less commonly and more specialized plot types which
were skipped—mainly because many of them require more advanced statistical
analysis approaches, such as the kernel density estimation (KDE) plot or the so-
called Andrews curves. There are, however, also commonly plot types that consist
of a combination of two or more of the above-introduced plot types which will
briefly be introduced in the following.
Scatter Plot with Marginal Histograms
A compact way of visualizing bivariate data (or a respective projection) is to show
the data as a scatter plot together with two histograms of the marginal distributions
(i.e., one-dimensional “projections” of the data along one direction). Figure 9.12
shows this for a part of the dataset MDS-5 (Nanoindentation). It becomes very
well visible that the hardness distribution is a bimodal distribution, while this is
not so obvious for the Young’s modulus. This is difficult to see in the scatter plot,
even though we already made the marker slightly translucent such that overlapping
markers result in darker points, helping to estimate density variations. We have also
added contour lines from a kernel density estimation (KDE) which estimates the
distribution using a number of Gaussian kernels and which also helps to visualize


================================================================================
PAGE 223
================================================================================

9.4 DataVisualization 203
Fig. 9.12 Two-dimensional
scatter plot of the MDS-5
(Nanoindentation) dataset in
which additionally the
marginal distributions are
shown as one-dimensional
histograms
the structure of the data. Of course, we could continue to add more and more
information into the figure, such as the mean, median, and mode of the bivariate
distribution and of the marginals; however, in most cases such a plot has the purpose
to highlight a few specific aspects, and it is advisable not to “overload” such a plot.
Scatter Plot Matrix with Histograms
In those situations where a dataset contains more than two features, we already
introduced the scatter plot as a means to visualize a projection of the data onto two
selected features. This kind of approach can be done for all possible combinations
and results in a scatter plot matrix , where each row and each column is one
particular combination of two features. In Fig.9.13 a scatter plot matrix is shown for
the whole dataset “DS-1 (Iris Flowers, Sect.4.7),” where different colors highlight
different species. Scatter plots that are located on the main diagonal of the matrix
use the same features for both coordinates, and thus, all data points fall on a line.
Usually, this does not reveal much information. Instead, often histograms are shown
for this feature instead. As the matrix is symmetric, the scatter plots above the
diagonal can be removed to avoid redundancy. With this, we have a very compact,
two-dimensional representation of a whole, multivariate dataset. For example, one
can observe that the Iris versicolor and Iris virginica are the two classes that overlap
most, while the Iris setosa is rather separated from the other two classes. We also
can easily see the ranges of the different variables (i.e., the interval from minimum
to maximum), and by looking at the scatter plots and the histograms, we get an
idea about the spread and correlation of the different variables. As an extension of
this plot, type one could also plot the individual distributions of all three classes in
each histogram which can reveal information about the intersection of the different
classes as well as on the amount of data per class. Yet another extension is to
replace histograms by KDE plots; this is done by default, e.g., by the Python package
seaborn which is part of the Scientific Python Stack.


================================================================================
PAGE 224
================================================================================

204 9 ExploratoryDataAnalysis
Fig. 9.13 Scatter plot matrix of the Iris dataset. Additionally, histograms are shown on the main
diagonal
9.5 Exercises
9.1 (Box-Whisker Plot) After the length of dislocations in a pileup observed
during an in situ TEM experiment was analyzed, the following line lengths in
nanometer were found: 47, 49, 56, 59, 61, 63, 66, 69, 69, 70, 70, 70, 71, 71, 71,
72, 73, 75, 75, 76, 76, 78, 79, 81.
Calculating the following quantities: minimum, maximum, median (Q2), first
quartile (Q1), and third quartile (Q3) as well as the IQR. In a second step, create
a box plot, and compare it with your calculated values to ensure that you (or the
plotting code) did not make any mistakes.
9.2 The following DSs contain the grain size distributions of an iron alloy which
has been subjected to two different heat treatments.
1. Visualize the data.


================================================================================
PAGE 225
================================================================================

9.5 Exercises 205
Table 9.5 Grain size
Relative Grain
distribution. Heat treatment
up to 1100◦C frequency (%) size (μm)
2.1 40
9.5 60
20.7 80
33.0 100
20.8 120
7.6 140
5.1 160
1.2 180
Table 9.6 Grain size
Relative Grain
distribution. Heat treatment
up to 1400◦C frequency (%) size (μm)
1.3 40
2.0 60
4.5 80
6.2 100
10.0 120
15.6 140
15.9 160
13.5 180
9.1 200
9.5 220
5.5 240
5.0 260
1.9 280
2. Calculate mean and median. Hint: be careful with the median of a histogram; use
cumulative frequencies (Tables 9.5 and 9.6).
9.3 Materials design is a discipline that seeks to improve specific properties of
the material, such is the case of polymer design according to the molecular weight
distribution. Visualize the following data and comment on the modal values:


================================================================================
PAGE 226
================================================================================

206 9 ExploratoryDataAnalysis
9.4 Create a box plot to analyze the data from the dataset “MDS-4 (Properties
of Chem. Elements).” For each properties (such as the atomic radius or the
electronegativity), there should we one box plot. What can you tell about the
distribution shapes and about outlier?
References
1. F. J. Anscombe. Graphs in statistical analysis. The American Statistician, 27 (1): 17–21, 1973.
DOI https://doi.org/10.1080/00031305.1973.10478966.URLhttps://www.tandfonline.com/doi/
abs/10.1080/00031305.1973.10478966.
2. G. Brys, M. Hubert, and A. Struyf. A robust measure of skewness. Journal of Computational
and Graphical Statistics, 13(4): 996–1017, 2004. ISSN 10618600. URL http://www.jstor.org/
stable/27594089.
3. W. McKinney et al. Data structures for statistical computing in python. In Proceedings of the
9th Python in Science Conference, volume 445, pages 51–56. Austin, TX, 2010.
4. S. Sandfeld and G. Po. Microstructural comparison of the kinematics of discrete and continuum
dislocations models. Modelling and Simulation in Materials Science and Engineering, 23(8):
085003, 2015. DOI https://doi.org/10.1088/0965-0393/23/8/085003.
5. S. M. Stigler. Do robust estimators work with real data. Annals of Statistics, 5: 1055–1098, 1977.
6. E. R. Tufte. The Visual Display of Quantitative Information. Graphics Press, Cheshire, CT, 2
edition, 2001. ISBN 978-0-9613921-4-7.
7. J. W. Tukey et al. Exploratory data analysis, volume 2. Reading, MA, 1977.
8. D. Wallis, L. N. Hansen, T. Ben Britton, and A. J. Wilkinson. Geometrically necessary disloca-
tion densities in olivine obtained using high-angular resolution electron backscatter diffraction.
Ultramicroscopy, 168: 34–45, 2016. ISSN 0304-3991. DOI https://doi.org/10.1016/j.ultramic.
2016.06.002. URLhttps://www.sciencedirect.com/science/article/pii/S0304399116300821.
9. C. Zhang, H. Song, D. Oliveros, A. Fraczkiewicz, M. Legros, and S. Sandfeld. Data-mining
of in-situ tem experiments: On the dynamics of dislocations in CoCrFeMnNi alloys. Acta
Materialia, 241: 118394, 2022. DOI https://doi.org/10.1016/j.actamat.2022.118394.


================================================================================
PAGE 227
================================================================================

Commonly Encountered Distributions in 10
Materials Science and Engineering
10.1 About the Following Discrete and Continuous
Distributions
All different types distributions can be subdivided into two broad classes: discrete
and continuous distributions. The first half of this chapter introduces the most
commonly used discrete distributions, characterized by their probability mass func-
tion (PMF) and cumulative distribution function (CDF). These are in particular: the
discreteuniform,Bernoulli,Binomial,GeometricaswellasthePoissondistribution.
They will be introduced in Sects.10.2–10.6.
The second half considers continuous distributions, given by their probability
density function (PDF) and CDF. One of the most important distributions is the
normal distribution (including the bivariate and multivariate normal distribution),
for which a number of useful mathematical relations can be derived. Further notable
distributions are the lognormal, exponential, and the logistic distribution. These will
be introduced below in Sects.10.7–10.13.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 207
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_10


================================================================================
PAGE 228
================================================================================

208 10 CommonlyEncounteredDistributionsinMaterialsScienceandEngineering
10.2 Discrete Uniform Distribution
PMF: P(x) = 1/n
CDF: F(x) = x−a+1
n
for x ∈[a, b]
Mean: (a + b)/2
Median: (a + b)/2
Mode: –
Variance: (n2− 1)/12
Skewness: 0
6(n2+1)
Kurtosis: −
5(n2−1)
Additional Details One of the most well-known discrete distribution is the discrete
uniform distribution. There, all n valuesx ,...,x have the same likelinessP(x)=
. 1 n .
1/n to be observed. In the “canonical formulation,” the distribution would start at
x = 0, and the values of the random variable would all be integer numbers. For
.
this, formulation requires only a single parameter, n, to describe the shape of the
distribution. n is the number of discrete values of the random variable X.
The formulation shown above is a generalization, where (i) the random variable
is not restricted to only integer values and (ii) the distribution is shifted. For this
formulation, three parameters are required: the lower bound a, the upper bound b,
and the number of values n.
It is easy to recover the canonical form, which is non-zero in the interval [1,n],
.
by lettinga =1andb=n, resulting, for example, in a mean value of(n+1)/2.
. . .
A commonly used notation for drawing random numbers from the interval[a,b]
.
is
X ∼U(a,b) (10.1)
.
Note, however, thatUdenotes a uniform random distribution which is a continuous
.
distribution.


================================================================================
PAGE 229
================================================================================

10.2 DiscreteUniformDistribution 209
Examples and Application Examples include throwing a fair, six-sided dice for
which x = 1,...,6 and P(x) = 1/6 for all i. Other examples are tossing a coin
i i
or drawing a random card from a deck of cards. Furthermore, whenever an integer
number, e.g., as index for an array needs to be drawn from a range of numbers
with equal likelihood, this distribution would be used. In materials science and
engineering, Monte Carlo simulations are based on this type of random sampling.
An example is the dataset MDS-2 (Ising Model), where the site of the magnetic
dipole to flipped is chosen from a uniform random distribution of all integers from
[0,N2) where N is the size of the system.
Python/SciPy As the scipy library does not contain the discrete uniform distribu-
tion, we show how to plot the distribution and how to draw uniformly distributed
discrete random numbers.
A minimal example for plotting the distribution:
There, the function np.ones_like() creates a numpy array that has the same shape
as the argument x . Specifically, multiplying with the factor 1/n creates an array
with n = 5 elements all of which have the value 1/n.
A minimal example for drawing samples from the distribution uses the (legacy)
numpy function randint :


================================================================================
PAGE 230
================================================================================

210 10 CommonlyEncounteredDistributionsinMaterialsScienceandEngineering
10.3 Bernoulli Distribution
(cid:2)
1 − p if X = 0
PMF: P(X) =
p if X = 1
⎧
⎪⎨
0 if X <0
CDF: F(X) = 1−p if0≤X<1
⎪⎩
1 if X ≥ 1
Mean: p
⎧
⎪⎨0
if p <1 /2
Median: [0,1] if p = 1/2
⎪⎩
1 if p >1 /2
⎧
⎪⎨0
if p <1 /2
Mode: [0,1] if p =1/2
⎪⎩
1 if p >1 /2
Variance: p(1− p)
(cid:7)
Skewness: (1− 2p)/ p − p2
1 −6p(1−p)
Kurtosis:
p(1− p)
General Description The BERNOULLI distribution is the simplest of all discrete
distributions and is sometimes colloquially called the “yes/no distribution.” It
represents a random process that has only two possible outcomes during each trial—
the Bernoulli trial. The outcomes are typically called “Success” and “Failure” and
are represented by the integer numbers X = 1 for Success and X = 0 for Failure.
. .
Theprobabilitiesofobtainingoneortheotheroutcomes maybedifferent:“Success”
is expected with the probability of p (because P(X = 1) = p), while “Failure”
.
occurs with the probabilityq =1− p (becauseP(X =0) = 1− p). An alternative
. .
formulation of the above PMF for the case that the random variable X takes only
thevalue0or1is
p(x) = px q1−x , x = 0,1. (10.2)
.
The Bernoulli distribution is also sometimes called “point binomial distribution.”


================================================================================
PAGE 231
================================================================================

10.4 Binomial Distribution 211
Examples For the coin toss, the two values would be Heads and Tails with equal
probability, p = 0.5. Getting a 1 or 2 from a fair dice could be value 1 with a
probability of p = 2/6, while 3, 4, 5, and 6 are the second value with a probability
of 4/6. But even throwing two dies and defining Success as getting two 1s is an
experiment that can be represented by this simple statistical model.
A number of more complex distributions contain the BERNOULLI distribution as
a basic building block such that the BERNOULLI distribution can be considered as a
special case if there is only one experiment. Among those are the binomial distribu-
tion in Sect.10.4 (which gives the probability for a certain number of “successes”
during a number of Bernoulli trials), the negative binomial distribution introduced
in (giving the probability of a number of failures during several BERNOULLI trials),
and the geometric distribution from Sect.10.5 (predicting the probability of a certain
number of “Failures” until the first success occurs).
Application in Science, Engineering, and Machine Learning An engineering
example is the failure of a specimen which, in the simplest case, also can be
modeled by such a binary distribution. In machine learning (ML), decision trees are
based on sequences of probability-based binary decisions, modeled by BERNOULLI
distributions.
10.4 Binomial Distribution
(cid:8) (cid:9)
PMF: P(k) = n pk (1− p)n−k
(cid:8)k(cid:9)
or: b(x) = n px (1− p)n−x
x
(cid:6)(cid:10)k(cid:7)(cid:8) (cid:9)
CDF: F(x) = n pi(1− p)n−i
i
i=0
Mean: np
Median: (cid:8)np(cid:9)
Mode: (cid:8)(n+ 1)p(cid:9)− 1
Variance: np(1− p)
Skewness:
√1−2p
np(1−p)
Kurtosis: 1− np 6 ( p 1 ( − 1− p) p )
See the nomenclature (page xxiii)forthe
definition of(cid:8)y(cid:9)and(cid:6)y(cid:7).
General Description The binomial distribution is one of the distribution types
that is based on Bernoulli trials and the Bernoulli distribution. More specifically,


================================================================================
PAGE 232
================================================================================

212 10 Commonly Encountered Distributions in Materials Science and Engineering
there are n mutually independent Bernoulli trials, each of which can take the
outcomes Failure or Success. Then, the binomial distribution gives the probability
of observing, e.g., the outcome k times Success in a during performing n Bernoulli
experiments. In each individual experiment, the probability for Success is p, and, as
a consequence, the probability for Failure is 1− p. Such a distribution may result
.
from random sampling of a finite population where each drawn sample is returned
(“with replacement”) or from a population that is infinitely large. In the latter case,
it does not matter if the drawn sample is returned or not. (cid:8) (cid:9)
The equation of the PMF can be easily understood: recall, that . n k gives the
number of possible combinations Cn that consist of k items, taken from a set
. k
of n items with replacement (cf. Definition 5.5). Here, each of the k Success
outcomes occur with a probability of p, while then − k Failures have a probability
.
of
.
1 − (cid:8)p (cid:9)each. Often, the binomial probability distribution is also denoted by
b(x) = n px (1− p)n−x. In other words, the random variable counts the number
. x
of successes in n independent trials (“sampling with replacement”) of a random
experiment where the probability of success is p. The shape of the distribution is
given by the two parameters n and p. Due to its definition, it can also be represented
as the sum of n independent Bernoulli random varia(cid:10)bles with parameter p such that
the resulting random variable Y is given by Y = n X . A consequence is the
. i=1 i
“compound probability,” where, for computing the probability of getting at most k
successes,wemayaddallthedifferentsuccessoptionsusingtheindividualbinomial
distributions. For example, it is P(x ≤ 3) = P(x = 1) + P(x = 2) + P(x = 3).
.
For obtaining the probability of getting at least k successes, we use the binomial
distribution function and add together all relevant success options. Last but not least,
it might seem to be a strong limitation that we only differentiate between Failure and
Success events. However, it turns out, that this is sufficient to cover a large range of
different cases.
Examples The mean of the distribution is simply np and gives the number of
expected successes μ in n Bernoulli trials which is in line with our intuition, e.g.,
if each Bernoulli trial has a probability of p = 0.5, then for 100 trials we would
expect on average to get 50 Successes, which is exactly n· p = 100· 0.5= 50.
Application in Science, Engineering, and Machine Learning Binomial distri-
butions are often used in situations where a specimen or device is tested whether
it is still intact or whether it is faulty. When investigating specimens from a
production batch, it is important that the batch is sufficiently large because otherwise
the above requirement (infinite population) would be violated. In such cases, the
hypergeometric distribution would have to be used instead.


================================================================================
PAGE 233
================================================================================

10.5 Geometric Distribution 213
Example 10.1 (Probability of Faulty Specimens) (cid:2)
A certain type of metallic samples are very prone to cracking. About 25% of
them show small fracture networks after the surface treatment. Altogether 15
samples undergo a visual inspection. Before they should be shipped to various
labs, the following questions arise:
What are the values of the PMF?
It is P(X = 0) = 0.25 for a faulty specimen from which follows that P(X =
1) = 0.75 for an intact specimen.
What is the probability that more than 2 specimen have cracks?
P(X > 2) = 1− P(X = 0)− P(X = 1)− P(X = 2) = 1−0.236 = 0.764.
What is the expected value of broken specimens and their standard deviation?
Starting from the definition of the expectation value, we sum up products of
frequencies and probabilities, resulting in
E[X] = 0·0.01336+1· 0.06682+2 ·0.15591+ ... + 15· 0=3.75
.
With this, the variance follows as
(cid:11) (cid:12)
Var[X ]=E X2 −E[X]2
.
(cid:13) (cid:14)
= 02· 0.01336+12·0.06682+22· 0.15591+ ... +152· 0 −3.752
(10.3)
=16.875− 3.752 = 2.813
√
The standard deviation s is the s = Var[X ]= 1.677.
10.5 Geometric Distribution
Assuming that the values of the random variable are x ∈ N (i.e., positive integer
.
numbers excluding zero), it is


================================================================================
PAGE 234
================================================================================

214 10 Commonly Encountered Distributions in Materials Science and Engineering
PMF: P(x) = p(1− p)x−1
.
CDF: F(x) = 1 − (1− p)x
.
Mean: 1/p
.
Median:
Mode: 1
1− p
Variance:
. p2
2− p
Skewness: √
. 1 −p
p2
Kurtosis: 6+
. 1− p
General Description The geometric distribution is a type of discrete probability
distribution which gives the probability that, in a Bernoulli trial (cf. Sect.10.3),
the first time that the event “Success” occurs required k independent trials before
“Success.” As in the Bernoulli trial, the probability of success of an individual trial
is p, and the probability of failure is 1− p. The geometric distribution represents
.
the probability that after x trials for the first time “success” occurs. For example,
in the above figure, the probability that after three trials “success” occurs is P(x =
.
3) ≈ 0.1 if the probability for “success” isp = 0.2.
.
The above equations for the PMF and CDF assume that the random variable k is
an integer number k ∈ N where N denotes the set of all positive integer numbers.
. .
An alternative formulation exists where x does not start at 1 but rather at 0 (the
number of trials up to and including the first “success”). The geometric distribution
is a special case of the negative binomial distribution.
Examples A typical example is related to the number of defective specimens:
assume that we know that 3% of all samples or specimens have a defect. We can
then determine the probability of finding 0, 1, 2, or more specimens that are non-
defective using the geometric distribution. For example, the probability that we find
two good samples before a defective sample occurs is obtained from k = 3 and
.
p = 0.03 as P(k = 3) = 0.03(1 − 0.03)3−1 = 0.0282: in about 1 out of 30
. .
“experiments” (where the single experiment consists of looking at samples until we
find a defective sample), there are two good samples before we find a bad one.
This distribution is often also called “failure time distribution.”


================================================================================
PAGE 235
================================================================================

10.6 Poisson Distribution 215
Application in Science, Engineering, and Machine Learning The geometric
distribution can be encountered in Markov chain models which are used for making
predictions in the field of statistical mechanics or engineering physics, e.g., when
representing a random walk as in the context of Brownian motion of atoms or
molecules.
Python/SciPy The above example can be also easily computed using SciPy:
10.6 Poisson Distribution
μx exp(−μ)
PMF: P(x) =
x!
(cid:15)(cid:6)x(cid:7)
μi
CDF: F(x) = exp(−μ)
i!
i=0
Mean: μ
Median: ≈(cid:6)μ+1/3− 0.02/μ(cid:7)
Mode: (cid:8)μ(cid:9)−1 ,(cid:6)μ(cid:7)
Variance: μ
1
Skewness: √
μ
1
Kurtosis:
μ
See the nomenclature (page xxiii)for the
definition of(cid:8)y(cid:9)and(cid:6)y(cid:7). In those cases where x
is an integer number (e.g., the number of
events),(cid:6)·(cid:7)can be removed.


================================================================================
PAGE 236
================================================================================

216 10 Commonly Encountered Distributions in Materials Science and Engineering
General Description The Poisson distribution plays an important role in many
different scientific fields. It is obtained from the binomial distribution in case that the
sample size is very large, i.e., n → ∞, where, at the same time, the probability for
.
Success tends to zero,P → 0, such that nP stays constant. A vanishing probability
.
for the event Success means that this event happens only very rarely. This is why the
Poisson distribution is the distribution of choice whenever it comes to rare events.
The main application is to help to count such events.
In general, Poisson distributions model events that are typically occurring at
discrete points in time. There, the underlying assumption is that any event occurs
statistically independent of any previous event. Examples for such cases are the
number of cars passing by a checkpoint or the number of phone calls received.
These examples (and those, presented below in the “Applications” paragraph) have
in common that all events represented by the random variable are the result of a
Poisson process. As this is an important concept, we now introduce it briefly.
Poisson Process One of the most important random processes1 is that of the
Poisson process, which is also called Poisson point process . This describes
situations in which random events (i.e., “points”) occur in space and in time. There,
the occurrence of one event does not affect the probability of other points or events,
which is also called the memoryless property. The most important properties of such
random processes can be summarized as follows:
• Any two subsequent events in time occur independent from each other.
• In any time interval, the average number of events that occur is proportional to
the size of the interval; there is no dependency on where or when these events
occur.
• The probability to find exactly one event in an time interval (t, t +(cid:2)t) is λ((cid:2)t);
the probability to find two or more events in this interval is zero.
The variable X = (X ,X ,...) denotes the inter-arrival time of events, i.e., the time
1 2
(cid:2)t between two events. For a given time span, the number of occurring points is
taken as the random variable, which is why the Poisson process is in fact a counting
process. The distribution of the time between any two subsequent points is given by
the Poisson distribution.
Application in Science and Engineering A surprisingly large number of appli-
cations exists: whenever it is about counting the number of independent events
occurring during a certain random process, then the odds are that it is related to
Poisson distribution. For example, the number of scratches per area found on a
specimen’s surface or the number of radioactive particles per volume follows a
Poisson distribution. Poisson processes are also at the heart of percolation processes,
as encountered in, e.g., fracture processes.
1 Recallthatarandom processisamechanismthatgeneratesrandomeventswithcertainproperties.


================================================================================
PAGE 237
================================================================================

10.7 Normal Distribution 217
This distribution is also used to find the probability of a rare event to happen
during a particular period of time, a certain length, area, or volume. An example
would be a particle detector with the purpose of counting events.
10.7 Normal Distribution
1
PDF: f(x) = √
(cid:13) 2πσ(cid:14)2
exp −(x−μ)2 =N(μ, σ)
2σ(cid:13)2 (cid:13) (cid:14)(cid:14)
CDF: F(x) = 1 1+ erf x− √μ
2 σ 2
Mean: μ
Median: μ
Mode: μ
Variance: σ2
Skewness: 0
Kurtosis: 0
A normal distribution is given by two parameters, the location μ and the scale σ.
. .
It is often abbreviated by the calligraphic “N”, N(μ, σ). μ is also the mean or, in
. .
the context of probabilities, the expectation value, σ is the standard deviation, and
.
σ2 is the variance. The normal distribution is also known as Gau(cid:13)ssian dist(cid:14)ribution
.
because it consists of the Gaussian function, i.e., f(x) = a exp −(x−b)2 .Inthe
. 2c2
above equation for the cumulative distribution function, erf is the error function
.
defined as
(cid:16)z (cid:13) (cid:14)
2
erf(z) = √ exp −t2 dt, (10.4)
.
π
0
which is a sigmoid function frequently occurring in statistics as well as in deep
learning as an activation function. The PDF can also be characterized in terms of


================================================================================
PAGE 238
================================================================================

218 10 Commonly Encountered Distributions in Materials Science and Engineering
moments: the 0-th moment is the total probability (=1). The first raw moment isμ;
. .
the first central moment is 0. The second non-central moment isμ2+σ2, the second
.
central moment isσ2.
.
Generally speaking, if your data is symmetrical, bell-shaped, centered, and
unimodal, then it is very likely that a normal distribution is given. The normal
distribution is very well understood; many data distributions that we encounter
follow a normal distribution but not all of them do (e.g., the IQ’s of people follows
a normal distribution, while their income fits an exponential function).
The normal distribution is ubiquitous, and therefore it also hides in many
mathematical aspects of inferential statistics and the calculation of probabilities.
Manyoftheirmethodsassumethatanormaldistributionisgiven.Thisimpliesthatif
thedatais not normallydistributed,thenonehastobeverycarefulsincethemethods
could lead to wrong conclusions. A good “counter-example” is the Gaussian naive
Bayes classification method—a ML method that requires the training data to be
normal distributed; however, the method turns out to be surprisingly robust even
in cases when the data is not normal distributed.
The normal distribution has a large number of properties; here we only list some
of the most important ones:
• The “width” of the normal distribution is defined as√ the distance between the two
points where the function values areMAX(f ) · 1/ e
.
• ≈68% of the total area under the Gaussian curve (which represents the proba-
.
bility) lies in the range [μ − σ, μ + σ]. In other words, there is a 68% chance
.
for obtaining a value of x that is located within one standard deviation from the
meanμ
.
• The two parameter μ and σ can be used to shift and scale the Gaussian
. .
distribution. This results in the “standardized” form of the distribution:
exp(−x2/2)
f(x) = √ .
.
2π
Application in Science and Engineering Many properties in nature are normal
distributed, such as people’s height, their IQ scores, or body temperatures. Further-
more,itcanbeobserved thattheproperties ofmany samplesifplottedinahistogram
take the shape of the normal (or Gaussian) distribution. We already encountered this
in the context of the central limit theorem , Sect.8.6.
Python/SciPy Python Listing 10.1 shows how the PDF and CDF can be obtained
from Python’s scipy library.


================================================================================
PAGE 239
================================================================================

10.8 Bivariate Normal Distribution 219
1 import numpy as np
2 from scipy import stats
3 import matplotlib.pyplot as plt
4
5 x = np.linspace(-8, 8, 200)
6 mu, sigma = -,2 1
7 rv = stats.norm(loc=mu, scale=sigma)
8
9 plt.plot(x, rv.pdf(x), label='PDF')
10 plt.plot(x, rv.cdf(x), label='CDF')
11 plt.legend()
PythonListing10.1 A minimal example for plotting the PDF and CDF of a normal distribution
Drawing, e.g., 50 random numbers that follow the probability given by a normal
distribution can be then done with rv.rvs(50) (after the random variable has been
create by rv = stats.norm(loc=mu, scale=sigma) ; see Python Listing 10.1).
10.8 Bivariate Normal Distribution
Parameters: standard deviation σ ,σ ,
1 2
and mean μ ,μ
1 2
PDF: f(x ,x ) = 1
1(cid:17) 2 2πσ1σ2 (cid:18)
exp −(x1 −μ1)2 − (x2 −μ2)2
2σ2 2σ2
1 2
=N(x ,x ; μ ,μ ,σ ,σ )
1 2 1 2 1 2
CDF: (usually constructed
numerically)
Mean: (μ ,μ ) The bottom plane shows the projection
1 2
into the x-y-plane; the two line plots
Median: (μ ,μ )
1 2
show the contours of the marginal
Mode: (μ ,μ )
1 2 distributions.
Variance: σ2 and σ2 along
1 2
x and x direction
1 2
Skewness: 0
Kurtosis: 0


================================================================================
PAGE 240
================================================================================

220 10 Commonly Encountered Distributions in Materials Science and Engineering
The bivariate normal distribution is a normal distribution in two random variables.
It is a special case of the general multivariate normal distribution, introduced in
Sect.10.9, but it can be still written out without having to use linear algebra and
the covariance matrix. The main idea is that using the identity exp(a + b) =
.
exp(a)exp(b), the bivariate normal distribution can be decomposed into a product
of univariate normal distributions:
N(x ,x ; μ ,μ ,σ ,σ ) =N(x ; μ ,σ )N (x ; μ ,σ ) (10.5)
. 1 2 1 2 1 2 1 1 1 2 2 2
Therefore, for general aspects we refer the reader to the respective sections on
the univariate and multivariate normal distribution. Here, we only explain aspects
pertaining to this bivariate distribution.
Additional Details In science, this function is of some importance as it is a
function in two variables, which therefore can still be easily visualized. For
example, its isocontour lines can be used to highlight the quartile of all normally
distributed data points that are located within a certain region. The isocontour
lines can either be plotted (e.g. using matplotlib this can be done using the
function matplotlib.contour ), or these lines can be mathematically computed; see
below.
Isolines of a Bivariate Normal Distribution Visualizing bivariate normal distri-
butions can, e.g., be done by showing lines that connect points of equal values. In
***Appendix B.4.3, it is derived that the points that have the same values c are
located on contour lines of elliptical shape. As a result the two semi-axes of the
ellipse are given by
(cid:19) (cid:19)
(cid:17) (cid:18) (cid:17) (cid:18)
1 1
a = 2σ2 ln and b = 2σ2 ln . (10.6)
. 1 2πc σ σ 2 2πc σ σ
1 2 1 2
The major semi-axis is given by the larger of the two values, max(a; b), while the
.
minor semi-axis is min(a; b). With these abbreviations the equation governing the
.
elliptical isolines is
(x −μ )2 (x − μ )2
1 1 + 2 2 = 1 , (10.7)
. a2 b2
where μ and μ shift the center of the ellipse along the two direction. Thus, this
. 1 . 2
equation describes all constant values c of the PDF N . (x 1 ,x 2 ; μ 1 ,μ 2 ,σ 1 ,σ 2 ) = c.
As a demonstration, we now create a dataset, for which we will show how an
elliptical isoline can be computed. The dataset is obtained by sampling a normal
distribution, X ,X ∼ N, which is defined through the mean and the 2 × 2
. 1 2 .
covariance matrix.


================================================================================
PAGE 241
================================================================================

10.8 Bivariate Normal Distribution 221
In [1]: import numpy as np
from scipy.stats import multivariate_normal
In [2]: true_mean = np.array([5, 3], dtype=float)
true_cov = np.array([[2., 0.], [0., 0.3]], dtype=float)
rv = multivariate_normal(mean=true_mean, cov=true_cov)
X = rv.rvs(size=2000)
We then compute the sample mean, which is used to define the center of the ellipse,
and the sample covariance matrix, which is used to obtain the standard deviations.
In [3]: sample_mean = np.mean(X, axis=0)
sample_cov = np.cov(X, rowvar=False)
mu1, mu2 = sample_mean
sigma1, sigma2 = np.sqrt((sample_cov[0,0], sample_cov[1,1]))
The next two cells implement the above equations for the ellipse:
In [4]: def ellipse(x1, mu1, mu2, a, b):
y = np.sqrt((1 - ((x1 - mu1) ** 2) / (a ** 2)) * (b ** 2))
x1 = np.hstack((x1, x1[::-1], [x1[0]]))
return x1, mu2 + np.hstack((y, - y, y[0]))
In [5]: c = 0.01
fac = np.log(1 / ( 2 *n p.pi * c * sigma1 * sigma2))
a = np.sqrt(2 * s igma1 ** 2 * fac)
b = np.sqrt(2 * s igma2 ** 2 * fac)
x1 = mu1 + 0.999 * np.linspace(-a, a, 200 , )
x1, x2 = ellipse(x1, mu1, mu2, a, b)
There, the np.hstack functions are used to create data of the upper and lower half
of the ellipse in a format suitable for plotting.
In [6]: fig, ax = plt.subplots()
ax.plot(X[:, 0], X[:, 1], 'o', mfc='C0', mec='none', alpha=0.3)
ax.plot(x1, x2, '--')
ax.set_aspect('equal')
ax.set(xlim=(0, 9), ylim=(0, 6), xlabel='X1' , ylabe=l'X2');
The resulting figure is similar to the one shown in Fig.10.1. Note that the covariance
matrix of the dataset X might have small, non-zero off-diagonal entries. This would
imply that the ellipse, that fits best to the data, would be slightly rotated. This
is, for simplicity, ignored here. The next subsection introduced the most general


================================================================================
PAGE 242
================================================================================

222 10 Commonly Encountered Distributions in Materials Science and Engineering
Fig. 10.1 Elliptical
contourline of a bivariate
normal distribution. The
parameter of the normal
distribution function was
obtained from computing the
mean and the covariance
matrix of the dataset shown
as blue markers
representation of multivariate normal distributions and will further elucidate the link
to the covariance matrix.
10.9 Multivariate Normal Distribution
1 1
PDF: f(X; μ,(cid:2)) =
(2(cid:13)π)D/2 (det((cid:2)))1/2 (cid:14)
exp −1 (X − μ)T(cid:2) −1 (X − μ)
2
=N (X; μ,(cid:2))
Parameter: covariance matrix(cid:2)
vector of mean values μ
dimensionality D
Mean: μ
Median: μ
Mode: μ
Variance: e.g., Var(X ,X ) from covariance matrix
i i
Skewness: 0
Kurtosis: 0
This is a distribution in multiple random variables. The notation benefits particularly
from the vector and matrix notation (e.g., μ is a vector of the mean values along
.
all directions, and (cid:2) is the population covariance matrix). In all generality, the
.
multivariate normal distribution is not just a superposition of normal distributions
along the two EUCLIDian coordinate axes but additionally might be subjected to


================================================================================
PAGE 243
================================================================================

10.10 The Relation Between Covariance Matrix and Multivariate Normal... 223
a rotation. Using the covariance matrix as introduced in Sect.8.7.2 is a particular
concise representation.
As for the univariate normal distribution, the calligraphic letterN is used to
.
indicate thenormal distribution, e.g.,N(x; μ,(cid:2)).The above formulation isnot just
.
a formulation in arbitrary dimensions (i.e., arbitrary number of random variables),
but it also allows to represent normal distributions in transformed coordinate
systems, which, e.g., in two dimensions can be a rotation for the distribution about
its center.
10.10 The Relation Between Covariance Matrix and Multivariate
Normal Distribution
At the heart of the multivariate normal distributions is the covariance matrix which
is introduced in details in Sect.8.7.2. It represents correlations between variables
in an average sense and thereby elegantly connects data samples from a normal
distribution with some of their statistical properties that are represented through the
shape of the distribution.
In many ML methods such as the principal component analysis, the Gaussian
naive Bayes method, or the Gaussian mixture model, normal distributions play an
important role; this is the reason why we dedicate a later part to this subject. In the
following we will explore some aspects of the covariance matrix that are strongly
related to concepts from linear algebra as introduced in Appendix A; these will be
used to show that the eigenvectors of the data covariance matrix are the directions
of the axes of the corresponding ellipse or ellipsoid.
For simplicity, we consider the case of only two random variables, X ,X ∼
. 1 2
N(X ,X ; S, μ), whereS is the data covariance matrix2
1 2 .
S = 1 X T X , (10.8)
. n− 1
where X is the centered data matrix which has zero mean values. S is then a 2× 2
. . .
matrix with the components
(cid:20) (cid:21)
S S
S = 11 12 (10.9)
.
S S
12 22
where the diagonal terms are the squared standard deviations along the two
Euclidean axes, and S is symmetric. For this 2 × 2 matrix, we can compute the
. .
eigenvalues from the characteristic polynomial Eq.(A.58) which results in
2 Reminder: the data covariance matrix .S is an estimate of the “true” covariance matrix .(cid:2) .The
latter characterizes the distribution function from which the data was sampled, cf. Eq. (8.9).


================================================================================
PAGE 244
================================================================================

224 10 Commonly Encountered Distributions in Materials Science and Engineering
(cid:19) (cid:19)
tr(S) tr2(S) S2 + S2 (S −S )2
λ = ± − det(S) = 11 22 ± 11 22 −S2 ,
. 1/2 2 4 2 4 12
(10.10)
where det(S) = S S − S2 . Without loss of generality, we assume that λ ≥ λ .
. 11 22 12 . 1 2
Following Appendices A.3.1 and A.3.2, we then also can obtain eigenvectorsu and
. 1
u :
. 2
(cid:20) (cid:21) (cid:20) (cid:21)
1 −1
u = and u = , (10.11)
. 1 (−S + λ )/S 2 (S − λ )/S
11 1 12 11 2 12
which in the following we assume to have unit length, i.e., they are scaled asu ←
. 1
u /(cid:15)u (cid:15)andu ← u /(cid:15)u (cid:15). With the help of Eq.(A.62) the data covariance matrix
1 1 . 2 2 2
S can be diagonalized:
.
D =P −1SP (10.12)
.
whereD is a diagonal matrix that has the eigenvalue on its main diagonal
.
(cid:20) (cid:21)
λ 0
D = diag(λ ,λ ) = 1 with λ ≥ λ , (10.13)
. 1 2 1 2
0 λ
2
and the columns ofP consist of the two unit eigenvectors
.
P =[u u ] . (10.14)
. 1 2
According to the spectral theorem Appendix A.3.4, these eigenvectors are mutually
orthogonal, and hence, P is an orthogonal matrix with P −1 = PT. It follows that
. .
D governs the possibly anisotropic “stretch” of a vector while P governs a rotation
. .
of a vector. This holds also for a bivariate normal distribution: S transforms this
.
distribution by a stretch and a rotation. The angle of rotation in this two-dimensional
example can be obtained by comparing the elements of P with a rotation matrix:
.
there, it is θ = cos −1(P ) where P denotes the element in the first row and
. 11 . 11
column ofP.
.
This result can be generalized: the covariance matrix is decomposed into orthog-
onal and diagonal matrices by means of eigenvalues and eigenvectors. This can
be used for finding the isocontours of arbitrary, multivariate normal distributions:
the isocontour lines are formed by ellipsoids in three dimensions or by hyper-
ellipsoids in case of more than three dimensions. In the case of two dimensions,
the isocontours are isolines forming ellipses as discussed in Sect.10.8. Those points
located on the contour line or (hyper-)surfaces, respectively, are the lines of constant
density values c and therefore have equal likelihood to be drawn. The ellipsoids are
given by


================================================================================
PAGE 245
================================================================================

10.10 The Relation Between Covariance Matrix and Multivariate Normal... 225
(X− μ)TS −1(X− μ) =c2 (10.15)
.
Mahalanobis Distance (cid:2)
Note that the quadratic term on the left hand is called the squared Maha-
lanobis distance. The Mahalanobis distance is a useful and intuitive measure:
a value of two indicates a distance of two standard deviations away from the
mean.
The ellipsoids are centered at μ, and their axes are parallel to the directions of
.
the eigenvectors o√b
.
S . In the bivariate special case, the major and minor semi-axes
have the lengthsc λ whereλ andλ are the two eigenvalues ofS.
. 1/2 . 1 . 2 .
Example The following example demonstrates how the above results can be used
to show how the covariance matrix determines details of the distribution. We start
by creating a data set from sampling a given normal distribution:
In [1]: import numpy as np
true_mean = np.array([3, 1], dtype=float)
true_cov = np.array([[0.8, 0.3],
[0.3, 0.2]], dtype=float)
X = np.random.multivariate_normal(true_mean, true_cov, size=800)
The prescribed covariance and mean are the “true” quantities of the population.
Next, the sample mean and sample covariance are computed from the data—for a
large enough sample size, they should be only slightly different from the true values
and serve as an estimate for them. Additionally,
In [2]: sample_cov = np.cov(X, rowvar=False)
sample_mean = np.mean(X, axis=0)
Now we can compute eigenvectors. For this, the numpy function eig is used which
returns eigenvalues and-vectors:
In [3]: eigenvec1 = np.linalg.eig(sample_cov)[1][:, 0]
eigenvec2 = np.linalg.eig(sample_cov)[1][:, 1]
We have now obtained eigenvectors that are aligned with the axes of an ellipse
that would represent isocontour lines of the PDF. The sample data along with the
directions of the two eigenvectors can now be visualized:


================================================================================
PAGE 246
================================================================================

226 10 Commonly Encountered Distributions in Materials Science and Engineering
In [4]: import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.set(aspect='equal', xlim=(-1, 7), ylim=(-1.5, 3.5),
xlabel='$X_1$', ylabel='$X_2$')
ax.scatter(X[:,0], X[:,1], s=10, alpha=0.3, edgecolors='none')
xa, xb = np.linspace(-1, 1), np.linspace(-0.2, 0.2)
ax.plot(xa + sample_mean[0],
eigenvec1[1] / eigenvec1[0] * xa + sample_mean[1], c='0.3')
ax.plot(xb + sample_mean[0],
eigenvec2[1] / eigenvec2[0] * xb + sample_mean[1], c='0.3')
The resulting plot is shown in Fig.10.2. There, the two dashed lines show the direc-
tions of the eigenvectors; they intersect at the sample mean, which is observed to
be very close to the true mean. From the eigenvectors, we can compute the rotation
θ of the data resulting from the cross-covariance terms as θ = tan −1(u /u )
. . 1y 1x
where u is the first component of the first eigenvector and u is its second
. 1x . 1y
component. We observe that the direction of the largest and smallest “spread” of the
data coincide with the directions of the eigenvectors; a fact that we will heavily rely
on in Sect.15.2, when we derive the method of principal component analysis (PCA)
from this.
Note that this is just a basic visualization where we compute the inclination of
the two lines from the components of the eigenvectors. Here, the length of the two
lines is chosen for visual purposes and does not reflect, e.g., the magnitude of the
eigenvalues.
Fig. 10.2 The markers show samples from a given bivariate normal distribution. The directions
of the eigenvectors of the data covariance matrix are indicated by the two dashed lines. The
eigenvectors are aligned with the semi-axes of the ellipse which represents the isovalues of the
corresponding PDF. Note that the lines’ lengths are not up to scale


================================================================================
PAGE 247
================================================================================

10.11 Lognormal Distribution 227
10.11 Lognormal Distribution
(cid:13) (cid:14)
PDF: f(x) = √1 exp −(ln(x)−μ)2
xσ 2π 2σ2
(see alternative formulation below)
(cid:11) (cid:13) (cid:14)(cid:12)
CDF: F(x) = 1 1 +erf (ln x−μ)2
2 2σ2
Mean: exp(μ+ σ2/2)
Median: exp(μ)
Mode: exp(μ− σ2)
(cid:8) (cid:9)
Variance: exp(σ2)− 1 exp(2μ+ σ2)
(cid:8) (cid:9)(cid:7)
Skewness: exp(σ2)+ 2 exp(σ2) −1
Kurtosis: exp(4σ2)+ 2exp(3σ2) +···
···+ 3exp(2σ2)−6
General Description A lognormal distribution is a continuous probability dis-
tribution of a random variable whose logarithm is normally distributed. As a
consequence, if the random variable X is log-normal distributed, then Y = log(X)
.
has a normal distribution. Likewise, if Y follows a normal distribution, then X =
.
exp(Y) is characterized as a log-normal distribution. As opposed to the normal
distribution, the lognormal distribution is strongly skewed and has a long tail;
additionally, it is defined for positive value of the random variable only. In situations
that are related to waiting times between events, this is useful as time cannot become
negative.
Words of Advice (cid:3)
In case that your data exhibit a truly fat tail, then the lognormal description is
not appropriate to represent your data. Rather, power law-type distributions are
more suitable as, e.g., used for describing strain burst statistics of dislocations in
intermittent plasticity [1,2].


================================================================================
PAGE 248
================================================================================

228 10 Commonly Encountered Distributions in Materials Science and Engineering
There is an alternative formulation for the lognormal PDF
(cid:17) (cid:18)
1 1 (ln(x/μ˜))2
f(x ;μ,˜ σ) = √ exp − (10.16)
. xσ 2π 2 σ2
which is obtained by the transformation of μ = log(μ˜). The advantage of this
formulation is that it becomes visible that μ˜ is the scale of the distribution, which
can be useful information in view of a particular physical problem. Additionally,
this formulation has the beneficial aspect that the argument of the log function is
dimensionless. A convenient side effect of the fact that the standard deviation shows
up in the formulation of the mean is that the equation can be easily used to estimate
the two distribution parameters from a dataset: from computing the mean follows μ,
and with this we obtain σ from the mean of the lognormal distribution; no further
“curve fitting” is required.
Python/SciPy SciPy has an implementation of Eq.(10.16) where the above μ˜ is the
location of the function. The first cell creates a lognormal random variable “object”
that in the second cell can then be used to create the PDF. Additionally, we also
create 800 lognormal distributed random numbers:
In [1]: import numpy as np
from scipy.stats import lognorm
mu = 1
sigma = 0.85
# create a continuous lognorm RV
rv = lognorm(sigma, mu)
# generate lognorm distribtued random values
r = lognorm.rvs(sigma, loc=mu, size=800)
Plotting the PDF and a histogram of the random numbers, we observe that they
agree (note, the parameter density=True , used to ensure that the total area equal to
one):
In [2]: fig, ax = plt.subplots(dpi=200)
x = np.linspace(lognorm.ppf(0.01, sigma),
lognorm.ppf(0.99, sigma), 100)
ax.hist(r, bins=np.linspace(0, 7, 60), density=True);
ax.plot(x, rv.pdf(x))
The resulting plot is similar to the one shown in Fig.10.3.
Application in Science and Engineering The lognormal distribution can be
encountered in many engineering and scientific applications. For example, it is
used to model the lifetime of specimens during fatigue conditions, the particle size
distribution of aerosols or metal powders, or the reliability of electronic devices.


================================================================================
PAGE 249
================================================================================

10.12 Exponential Distribution 229
Fig. 10.3 Continuous PDF
and histogram of lognormal
distributed random numbers
Furthermore, many growth phenomena are also well approximated by a lognormal
distribution, such as grain growth. A related distribution is the Weibull distribution.
10.12 Exponential Distribution
PDF: f(x) = λe −λx
CDF: F(x) = 1 − e −λx
Mean: 1/λ
Median: (ln2)/λ
Mode: 0
Variance: 1/λ2
Skewness: 2
Kurtosis: 6
General Description We encountered the exponential distribution already in the
context of the POISSON point process where it is the probability distribution of


================================================================================
PAGE 250
================================================================================

230 10 Commonly Encountered Distributions in Materials Science and Engineering
the time between subsequent events. There, this function is also described as being
“memoryless”: any successful event should not influence the outcome of another
successful event, or the waiting time until an event occurs does not depend on
how much time has elapsed already. There exist only two distribution types that
are “memoryless”: the only distributions that are “memoryless” are exponential
distributions of non-negative real numbers and the geometric distributions of non-
negative integers.
Python/SciPy See the Python example for the lognormal distribution in
Sect.10.11. All that needs to be done is to replace lognorm with expon and
adjust the respective parameters accordingly.
Application in Science and Engineering Typical applications are failure, lifetime
prediction, and reliability engineering (also see the respective section of the Poisson
distribution, Sect.10.6).
10.13 Logistic Distribution
PDF: f(x) = (cid:13) exp(−x− s μ ) (cid:14)
s
1+exp(−x−μ
)
2
s
CDF: F(x) = (cid:13)1 (cid:14)
1+exp −x−μ
s
Mean: μ
Median: μ
Mode: μ
Variance:
s2π2
3
Skewness: 0
Kurtosis: 6/5
General Description The logistic distribution looks somewhat similar to a normal
distribution—it also is symmetric and looks bell-shaped—however, it has a larger
variance as well as a larger kurtosis. The latter is a consequence that the logistic


================================================================================
PAGE 251
================================================================================

References 231
distribution has a heavier tail than the normal distribution. The CDF is the logistic
function, which appears in logistic regression and feedforward neural networks.
Python/SciPy See the Python example for the lognormal distribution in
Sect.10.11. All that needs to be done is to replace lognorm with logistic and
adjust the respective parameters accordingly.
Application in Science and Engineering This distribution occurs in semiconduc-
tors and in metals in the context of electron transport properties. There, it has the
same functional shape as the derivative of the Fermi function. However, the most
important application is in the context of machine learning methods.
References
1. M.-C. Miguel, A. Vespignani, S. Zapperi, J. Weiss, and J.-R. Grasso. Intermittent dislocation
flow in viscoplastic deformation. Nature, 410(6829): 667–671, Apr. 2001. DOI https://doi.org/
10.1038/35070524.
2. Q. Rizzardi, P. M. Derlet, and R. Maaß. Intermittent microplasticity in the presence of a
complex microstructure. Phys. Rev. Mater., 6:073602, Jul 2022. DOI https://doi.org/10.1103/
PhysRevMaterials.6.073602.


================================================================================
PAGE 252
================================================================================

Part III
Classical Machine Learning


================================================================================
PAGE 253
================================================================================

11
Introduction and General Concepts
of Machine Learning and Data Science
Errors using inadequate data are much less than those using no
data at all.
Charles Babbage (1791–1871)
English mathematician, engineer, and inventor
11.1 The Definition(s) of Machine Learning
The notion of machine learning (ML) is often used in a somewhat mystifying
manner, and often no difference is made between the notion of ML and artificial
intelligence (AI); ML often is synonym with a computer code that is able to perform
in a super-human fashion and that is able to make intelligent decisions based
on some data which, e.g., the code or machine might even have collected itself.
Clearly, neither is any of this possible today nor a definition that is helpful for our
understanding how machines learn and why this might be useful for data analysis
and data mining in science and engineering.
When we take a look at the different types of algorithms and the vast ranges of
applications that are relevant to the natural sciences or engineering, it becomes clear
that a systematic approach is needed, starting at the very beginning. What better
approach could there be than simply starting with a definition of ML?
As it turns out, writing down such a definition is not completely trivial because
there is not the generally agreed-upon definition of what ML is. It is, however, quite
illustrative to see some of the commonly quoted definitions:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 235
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_11


================================================================================
PAGE 254
================================================================================

236 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Some Definitions and Descriptions of Machine Learning (cid:2)
The field of study that gives computers the ability to learn without being explicitly
programmed.
ARTHUR SAMUEL (1959) coined this quote—originally in a slightly different
version in [6]
A computer program is said to learn from experience E with respect to some class
of task T and performance measure P, if its performance at tasks in T , as measured
by P, improves with experience E.
TOM M. MITCHELL (1997) in the book “Machine Learning” [2]
The purpose of machine learning is finding patterns through the use of computer
algorithms. Searching for patterns in data is a fundamental one and has a long and
successful history.
CHRISTOPHER B. BISHOP (2011) in the book “Pattern Recognition and Machine
Learning” [1]
Machine Learning is about extracting knowledge from data. It is closely related to
statistics and optimization but is very focused on prediction.
ANDREAS C. MÜLLER (2017) in “Introduction to Machine Learning with
Python” [3]
These are four quite different definitions. ARTHUR SAMUEL coined the first
definition—originally in a slightly different version in the context of playing
checkers against a computer [6]. His definition emphasizes that the computer
can learn, i.e., become better at something. Mathematically speaking this involves
measuring the performance, e.g., by a mathematical function. In the next chapter,
the cost function is introduced for that purpose, which might already be known
from mathematical optimization theory. The fact that performance gain happens
without being explicitly programmed for the task at hand signifies that learning is
data-driven and not only algorithmically determined, e.g., by writing down a list of
highly specific instructions.


================================================================================
PAGE 255
================================================================================

11.1 TheDefinition(s)ofMachineLearning 237
TOM. T. MITCHELL’s well-known definition takes this one step further by
formalizing the previous definition. Additionally, as a new aspect, the notion of
performance is introduced. However, this definition might need a bit of explaining:
experience E is related to processing some given (training) data; the occurrence
of a specific task T indicates that the learning process is tailored to a particular
problem, e.g., the prediction of temperature based on humidity and the number of
the calendar week. The performance P quantifies how well the program performs,
which is related to either an error or a performance measure, as above. For example,
this could be the mean value of all deviations between a “correct” value and the
value predicted by the algorithm. Machine learning is then the process, when the
performance for a particular problem is getting better as more and more input data
are provided, which is what “experience E” means. This definition is also closely
oriented to how humans learn. For example, when we were still young, we were
shown numerous images of cats and dogs. The more images we were shown, the
better we were able to differentiate between dogs and cats—without explicitly being
taught what makes a dog a dog.
At first glance the definition by CHRISTOPHER BISHOP focuses mainly on data-
mining aspects: finding patterns, i.e., m structure in data. One might think that while
this is an important task for ML, it neglects other, equally important tasks, namely,
training of the ML model (the “learning” process) and prediction. However, being
able to automatically detect “patterns”, i.e., relations between seemingly unrelated
data, is already a really big challenge: when the Babylonians looked at the sky and
could identify the paths of the planets, they detected a pattern in the motion. An
algorithm, which is able to perform such a task independently, clearly would seem
quite advanced. A “pattern” can also mean different aspects of a cat’s anatomy,
which define how to identify a cat. Thus, “pattern” is a very broad notion that fits
well with many different machine learning problems, while the training process is
still of no great importance here.
The definition by ANDREAS C. MÜLLER is written from a perspective of
information science: recall the DIKW pyramid, Sect.2.3, which is a visualization
of the concept, in which the value of the (raw) data is increased by turning it into
knowledge, e.g., by extracting structural information about the data. The definition
also includes that ML is intrinsically a multidisciplinary topic as it draws heavily on
statistics and mathematical optimization. However, the aspect of making predictions
is much more important than in (descriptive) statistics focusing on the description of
the characteristics of data distributions. This definition also includes the second key
aspect of ML in addition to making predictions, i.e., making inferences: based on
data processing, the goal is to find out what the underlying relations and the “inner
workings” are that created the data. Such an “underlying relation” could by, e.g., for
the case gravitational force, a quadratic time-position relation.
In this book we will adopt the following definition, which captures a number of
aspects from the above definitions:


================================================================================
PAGE 256
================================================================================

238 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Definition 11.1 (Machine Learning). Machine learning is the process in
which...
1. a computer program learns to find and extract patterns in given data;
2. the program makes predictions on new but somewhat related data, based
on what was learned in step 1;
3. the prediction accuracy in step 2 increases, the more data the program is
shown in step 1.
The detection of patterns within data refers to the identification of structure
in data. Such a structure is, e.g., to identify that all data roughly follow a linear
trend, which could be represented by a straight line segment. The “essence” of what
the machine learned consists of the two points that govern the line segment (or
the inclination and intercept). This is extremely condensed data compared to the
possibly hundreds or thousands of original data points. Finding such a line is what
a machine learning regression algorithm does.
Another example would be to detect regions of points that belong to the same
“class,” for example, in a microscopy image, we might want to differentiate between
two phases such as austenit and martensit. The ML algorithm is supposed to
automatically find these contiguous regions of pixels and decide if they belong to
the first phase or to the second one. In one of the simplest approaches, the geometry
of the regions could be given by two rectangles. Hence, in our example, we describe
each of the two phases by a geometrical shape, defined by only two points. This
is again a very strong reduction of the original dataset (the image pixel). This is
done by machine learning classification algorithms: they assign labels (i.e., different
categories, cf. Sect.6.1) to input data and thereby classify them.
In both cases, the resulting model, i.e., the regression line or the class labels, can
help to understand aspects about the underlying physical phenomena that created the
data. From understanding results something that exists independently of the dataset:
knowledge.
The aspect 2. in this definition tells us that ML is a tool for making predictions:
if the program learned from a few data points how a stress-strain curve should look
like (again, that would be the pattern), it would be able to predict the stress value for
a new strain value that was not in the dataset.


================================================================================
PAGE 257
================================================================================

11.2 HowandWhatDoMachinesLearn? 239
At the same time, 3. implies that learning is the process of ingesting training
data, during which the performance of a specific task improves. For example, adding
more and more stress-strain data pairs should cover more and more regions of the
stress-strain curve so that predictions eventually become very reliable.
11.2 How and What Do Machines Learn?
The question “How and what do machines learn?” has, at least from a bird’s eye
view, already been answered in the previous section.
The remaining question is still: how does a program represent the relation
between output and input? There are several ways to do this, and all of them strongly
depend on the kind of output that an algorithm is supposed to provide.
11.2.1 The Machine Learning Model
The basic structure of a ML model is shown in Fig.11.1. A machine learning model
takes input data, does something to it, and produces output data. During the training
process, some details of the model (the model parameters) are adjusted such that
the output data matches the expectations (e.g., some already known output data).
Once we decide that the model is well-trained, we can use it to make predictions,
i.e., provide new input data and obtain the corresponding output data.
This is also the reason for introducing the simple equation y = f(x)in Sect.3.3
.
as the fundamental equation for this text. As we always operate with several data
records, our preferred notation is
Y =h(X) (11.1)
.
for a single input and output variable. Also, we prefer to call the model function as h,
because in the next chapter, this will represent a ML “hypothesis.” For vector-values
input and output variables, the above equation reads
Y=h(X) ⇔ (Y , ..., Y )=h(X , ..., X ) . (11.2)
. 1 k 1 n
Obviously, a lot of details can be hidden inside the model h. Sometimes it is even
impossible to determine how the model works. Such a model is called a black box
model : a model that takes some input and somehow turns it into output, while the
internal “mechanisms” of this data processing are entirely unknown.


================================================================================
PAGE 258
================================================================================

240 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.1 A machine learning model is, from a bird’s eye view, very simple: it takes input data,
does something to it, and produced output data. During the training process, the model is adjusted
such that the output data matches the expectations (e.g., already known output data)
Clearly, if there is a black box model then there is also a white box model. And
indeed, a white box model is a completely transparent model, of which the entire
internal mechanisms are visible and could be, e.g., mathematically written down. A
simple example would be a mathematical function h that maps any variable x to a
point on a line given by the inclination m and the intercept y :
. 0
h: X ׀→Y =m·X+y . (11.3)
. 0
The two parameter m and y are the ones that need to be determined during the
. 0
training process. Often, this is indicated by writing the function as h(X; m,y ),as
. 0
introduced in Sect.3.1.
In the next subsection, one of the simplest possible learning algorithms is intro-
duced as an example of a “model” that does not explicitly rely on a mathematical
but rather on an algorithmic formulation.
11.2.2 A First Learning Algorithm: Instance-Based Learning
Assume the simple strategy that a program learns by simply “memorizing” the
training data. To make predictions for a new instance, the memorized data is
searched for the training instance that is most similar to the new instance, and this
is used as the predicted datum. This procedure is called instance-based learning
(or sometimes also memory-based learning). An example for a nearest-neighbor
approach to regression is shown in the table in Example 11.1.


================================================================================
PAGE 259
================================================================================

11.2 HowandWhatDoMachinesLearn? 241
Example 11.1 (Instance-Based Learning of Stress-Strain Data) (cid:2)
A tensile test was performed on a specimen strain [−] stress [MPa]
.
during which the axial force and
displacement were measured (cf. dataset 0.0001 −1.84
.
MDS-1 (tensile test)). Uniaxial strain and
0.0004 10.93
stress data were computed as shown in the
0.0009 20.24
table—these are the training instances. This
0.0012 34.73
is an example of only one input feature, i.e.,
the strain, and one output, i.e., the stress. 0.0015 43.78
Assume that we want to make predictions 0.0016 42.75
for a new input value, e.g., a strain value of 0.0021 43.42
ε = 0.0042. The nearest input value in the
. 0.0028 46.16
training dataset is the record
(ε = 0.0040;σ =46.36), and hence, the 0.0033 50.41
.
prediction of this instance-based learner is 0.0040 46.36
. σ =46.36. 0.0048 50.37
0.0050 49.51
0.0058 49.88
0.0064 43.97
0.0070 48.08
0.0077 52.05
0.0085 51.36
0.0089 52.63
0.0094 53.18
0.0102 50.61
Quantifying the meaning of most similar is straightforward, if all input values
have the same unit. In such a case, the use of common distance metrics works well.
It is then straightforward to compute the distance d between the new input data
xˆ and the input data of all training instances, e.g., as the Euclidean norm (which
.
in 1D is just the norm of the difference between the training input variable and
the given input variable). If the input data consists of n variables or features and
x = [x ,...,x ] is the input data for a specific training instance, the Euclidean
. 1 n
norm for the new input data is


================================================================================
PAGE 260
================================================================================

242 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
(cid:2)
d = (x −xˆ)2+···+(x −xˆ)2. (11.4)
. 1 1 n 1
Computing these distances of the new input data to all training instances and finding
the minimum value gives the best fitting data point that is then returned as the
prediction for the new point. This methodology looks for the nearest neighbor.
Thelinktothe k-nearest neighbors method (cid:2)
An extension of this method is the k-nearest neighbors (kNN) classification
algorithm, which uses the k closest neighboring points to determine the
predicted value. For example, in case ofk =3neighboring, the result could be
.
taken as the average value of these three points. This method will be discussed
in more detail in Chap.14.
The strategy of computing distances, however, is not always straightforward,
as not all features (input variables) have the same relevance; they, therefore,
might require some weighting. Furthermore, computing the Euclidean distance
only works if all input data have the same units. For mixed units, a feasible
approach is to perform dimensionless scaling and then, based on the dimensionless
numbers, compute the distance between the new and the given input data. The
scaling, however, necessarily includes a degree of arbitrariness. Knowledge of the
physical nature of the data might give a good hint for determining reasonable
scaling/weighting values; generally, this is a key problem in instance-based learning.
A characteristic of instance-based learning is that during the learning process,
training data is only stored. Effectively there is no training process of the model,
and no further processing of the data is done at this point. This is the reason
why instance-based learning is also called lazy learning: only when it comes to
making predictions, computations (e.g., evaluation of the distance metric) need to
be performed, which can be a costly and time-consuming process. An advantage of
lazy learning is that it is very easy to adjust the model to new training data: they are
simply added to the already stored training data. However, data storage itself can be
challenging if the datasets become larger.
Comparing this to the example of the function that represented data as a
straight line in the previous subsection, a drawback of instance-based learning
becomes visible: there is no explicit function as a representation of the underlying
knowledge. The knowledge consists simply of the agglomerated data, while no
further abstraction or complexity reduction of the data was performed.


================================================================================
PAGE 261
================================================================================

11.3 IntroductionoftheGeneralMachineLearningWorkflow 243
11.3 Introduction of the General Machine Learning Workflow
We have now obtained some first impressions of how ML models work, and we have
also learned a number of important notions. We will now broaden the picture and
take a look at the whole ML workflow as shown in Fig.11.2.
Step I is to obtain data, e.g., from experiments, simulations, or various online
resources. This step ends when we are able to somehow read and import the data
into the data analysis program, e.g., such that everything is ready for using Python
or numpy with the data.
Subsequently, the preprocessing of the data in step II begins. This may consist
of a number of chained manual or automatic processing steps. The preprocessing
is supposed to remove any problems that may exist in the dataset (such as missing
data, duplicates, etc.), but also to mathematically transform the data (e.g., by scaling
or through dimensionality reduction) into the dataset that can then be directly used
for the ML training process.
In step III, the type of ML model can be selected to be used for the subsequent
training of the ML model. When choosing an appropriate model, it is important to
consider, for example, whether the data distribution consists of continuous data for
which a trend is to be learned, or whether it is categorical data that is to be divided
into different classes.
As a prerequisite for learning in step IV, the (numerical) parameters of the
model implementation and the model parameters must be selected. Parameters that
are closely related to a model are also called hyperparameters. Then, the training
process can be performed, which sometimes takes less than a second, but can also
take several hours or even days (e.g., for large datasets with images with high
resolution).
The resulting trained model has to be tested in various ways in step V, such as
cross validation (see Sect.16.2) to ensure that the model, in fact, learned generic
features and can be used for new data as well. This is a particularly important step
as this is often the only guard against an unsuccessful training process. If the “error
analysis” (cf. Sect.11.7 for an overview over error measures and how to compare
datasets) reveals problems, then we should go back and repeat the training process
I. Data II. Pre- III. IV. V. ”Error VI.
Collection processing Modelling Learning analysis” Results
possibly from cleaning of Choice of ML Training the Testing the ML Visualization
a number of the data and algorithms; ML model model with analysis,
different preparing of e.g., super- ↓ new test data; prediction;
sources and the data (e.g., vised, unsu- finding the cross-valid- use of further
in different scaling or pervised; parameters that ation, check ML model
formats reduction of also: func- fit ”best” to the for overfitting ↓ knowledge
dimensionality) tional form data
Fig. 11.2 The machine learning workflow consists of six main steps, starting with the data
collection and ending at the exploitation of the results


================================================================================
PAGE 262
================================================================================

244 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
of step IV, e.g., with more data or with different training parameters. Or we even
might have to go back to step III and choose a different model.
Finally, in step VI we can use the results to make predictions or detect new
patterns in the dataset or use the trained model to infer something about the
underlying physics. All of this is ultimately served to increase our knowledge of
the physics problem.
The following sections will discuss some of these steps in more detail.
11.4 Data Collection
Data collection can be a simple process, such as when an experiment has been per-
formed, the data has been recorded and documented, and then only the appropriate
files need to be imported.
However, in such a case data collection can also become very complex when the
experiment needs to be tailored to the requirements of the ML method to be used.
A typical bottleneck is not only the quality of the dataset (extremely noisy data can
make ML training difficult), but often also the quantity, since not all experimental or
simulation methods provide sufficient data. In these cases, one has to fuse datasets,
e.g., from different experiments, possibly even performed by different people and
on different machines. Very often this also requires additional work, e.g., for reading
files in different formats or to understand the structure of the files in the first place.
Where did the data come from? (cid:2)
Even if an obtained dataset is well documented and ready to be used, it still
might be helpful to recall how the data was acquired, e.g., during an exper-
iment: At the beginning of this process, there is the physical phenomenon
or property to be measured (e.g., temperature or force). The sensor (e.g., a
thermistor or strain gauge) then samples this data in a continuous or time-
discrete manner. In the next step, this is typically converted into an electrical
signal, which might then get further processed by hardware or software (e.g.,
the signal might be filtered or amplified). In the final step, the signal is then
converted into a digital representation, which is typically the point of first
contact, when it comes to, e.g., statistical analysis of data.
Another way of collecting data is to search (e.g., in online resources) for training
datasets created by someone else. Then, the data usually needs to be downloaded or
queried, e.g., from a web page, web server, or repository. The data format can then
be converted into a format that can be used directly by the data analysis software
involved.
Thus, data typically used in statistical analysis or machines already have a long
history of various types of conversions and processing; it is (almost) never the “raw”


================================================================================
PAGE 263
================================================================================

11.5 DataPreprocessing 245
data that we work with. It is also clear that any such dataset may contain a number
of errors or variations.
11.5 Data Preprocessing
The goal of data preprocessing or data preparation is to transform the collected
data into something that can be used directly for statistical analysis or ML. This is
particularly important if the data was not created for the purpose of data mining or
statistical analysis. We assume that the data has already been obtained and is now
contained in a computer program in the form of a data structure (e.g., an array).
Preparing the data is a task that can sometimes take a lot of time. A number of tasks
are performed during this stage.
11.5.1 Data Cleaning
Data cleaning removes all unwanted artifacts, inconsistencies, and errors from the
dataset so that the data is then ready for further processing or analysis. This can take
a lot of time as it sometimes requires manual processing and simple browsing of the
data. However, the benefit of such a well-prepared dataset for statistical analysis
or ML is significant, as the quality and reliability of the results usually depend
heavily on the quality of the input data. In computer science, this was condensed
in the notion “garbage in, garbage out” (GIGO), or in other words, one cannot
expect meaningful results if the input data is of poor quality. What exactly “bad
quality” means depends on the particular case and/or analysis method and has not
been globally defined. These are the key steps in data cleaning, some of which
can be performed before or together with the exploratory data analysis (EDA) (cf.
Chap.9):
• Repair structural problems: This refers to problems due to different spelling
(e.g., variables might be written in British and American English), typos, or
wrong formatting (e.g., using a comma instead of a decimal point or using
a wrong date/time format). The first column in Fig.11.3a shows such an
inconsistency in the time format used. Structural problems can be caused by the
person, who is working with/creating the data, or be caused by errors during
transferring or recording of the data. Some of these problems already might have
been resolved, when file formats and metadata were validated.
• Remove duplicate data: Duplicate data could be due to data accidentally obtained
during data collection, e.g., when during an experiment several readings were
noted and one/some of them were noted twice. Another such situation can occur
when during recording of a video due to a wrong sampling rate a slightly varying
number of duplicates (repeated frames) are created. Lines 5 and 6 in Fig.11.3a
are an example for duplicates (and contain additional structural problems as
well).


================================================================================
PAGE 264
================================================================================

246 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.3 Data preparation exemplified for a file containing column data. (a) raw file content (line
numbers are not part of the file content); (b) cleaned up and “interpreted” data; (c) visualization of
thedatapoints.Thedatain(a) has a number of problems starting with the unclear column names,
various deliminators between columns (semicolon and space) duplicate data, and inconsistent
number formats. The processed data in (b) additionally contains knowledge of the units of the
measurement. Visualizing the data during EDA as in (c) helps to ensure that the data is in a
consistent state
• Remove irrelevant observation: This step refers to data or measurements that
have been included in a dataset but are not needed. For performance reasons (file
reading too slow) or data handling reasons (dataset too large) during reading,
such data should be excluded. Or they need to be removed so that they are not
automatically included in the data analysis. This might concern data that, for
example, had been recorded before the actual experiment started. For practical
purposes, removing irrelevant observations can make a lot of sense, e.g., when
we want to combine data from different sources. By including only the relevant
observations, it becomes easier and less error prone to merge such datasets into
one file or array.
11.5.2 Outlier Detection
Outliers are data points that strongly deviate from what is expected from the dataset.
Outliers can be related to a defective measurement device, any other disturbance
during recording the dataset or writing the file, or due to human errors. However,
they also might be the signature of rare events—which might be worth investigating.
Generally, outliers can be sorted into three classes: (i) Point outlier or global
outlier are those that are far away from all other data in the dataset. (ii) Contextual
or conditional outlier are found within the dataset, and their values significantly
deviate from all other points in the same context. For example, they may appear as
noise. (iii) Collective outliers are encountered, when an entire subset of the dataset
collectively behaves differently from the rest. Figure 11.4 shows a visualization of
the three types of outliers using the dataset MDS-1 (tensile test).


================================================================================
PAGE 265
================================================================================

11.5 DataPreprocessing 247
Fig. 11.4 The three main types of outliers (shown in orange) for stress-strain data with uncertain-
ties. Refer to dataset MDS-1 (tensile test) for further details of the data
Outlier detection can be a very difficult task. Statistical methods can help to
find outliers, e.g., by assuming that the data follows a certain kind of distribution;
everything outside that distribution then is an outlier. Even ML methods are used
(e.g., cluster analysis methods, which will be introduced later). During the data
preparation stage, unwanted outliers should be removed carefully as it is not always
clear, if they are indeed outliers or part of the dataset. As for data cleaning, also the
removal of unwanted outliers benefits from and can be done during EDA.
11.5.3 Data Transformation
Data transformation is a topic one could easily dedicate several chapters to. It can
be approached purely from the point of view of the data, not caring about the
underlying problem. While this is perfectly fine, data transformation also offers
the possibility to take some of our domain knowledge into account: for example,
knowing that the data in a certain dataset is supposed to be normally distributed,
because this is what the laws of physics postulate, might serve as a suggestion for
how to scale the data properly.
The main motivation for data transformation is that many analysis algorithms or
methods for data comparison work better when the respective datasets are directly
comparable, when all values fall into a certain range, or when the investigations
are performed in a certain (possibly different) coordinate system (e.g., a double
logarithmic plot easily reveals if data follows a power law). The most commonly
used methods for data transformation are introduced below.
Shifting
Shifting the data has the effect that all data are moved collectively to a new location.
For example, for the dataset D . = {(x i ,y i )} i=1...N , which consists of N points
in two-dimensional space, a shift by Δx along the x-axis can be mathematically
.
written as


================================================================================
PAGE 266
================================================================================

248 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.5 Data shifting moves the data “as a whole.” Aligning all peaks at .x =0makes it easier
to, e.g., inspect if the shapes of the curves or distributions are similar
Fig. 11.6 Data shifting in two dimensions: this plot shows the positions of the data points, i.e.,
the pairs. {(xi,yi)}—or the distribution of two discrete random variables x and y. Thus, each point
either has a value.c i attached or the points are be part of a “distribution”
. {(x i ,y i )} i=1...N ׀→ {(x i +Δx,y i )} i=1...N . (11.5)
The shape of the distribution is preserved. Shifting is useful when, e.g., time-
dependent measurements are to be compared, and each recording starts at a different
point of time. Shifting each dataset, so that unique or clearly visible features of all
datasets align, makes it possible to compare them more easily. Figure 11.5 shows
an example of two datasets that were shifted, so that their maxima are located at
the same x-coordinate. Similarly, bivariate or multivariate distributions can also be
shifted, either along only one direction or simultaneously along multiple axes. An
example of a bivariate distribution is shown in Fig.11.6.
Data shifting typically refers to applying an offset to the input variables only.
However, adding a constant value to the output variable might also be reasonable:
if the dataset consists of temperature values T((x,y)sampled at discrete positions
.
{(x ,y )}, shifting the data in space by (Δx,Δy) as well as adding an offset to
. i i .
the temperature (e.g., converting from degrees Celsius to Kelvin) can both be
reasonable.


================================================================================
PAGE 267
================================================================================

11.5 DataPreprocessing 249
Words of advice (cid:3)
Fordatathatarep arto fa p robability distribution function, shifting the probability
value is not reasonable, while shifting the values of the random variables might be.
Then, the expected value is shifted accordingly, but, e.g., the standard deviation stays
the same.
Scaling
Whenever the data of two variables have very different magnitudes, a number of
numerical problems may arise (e.g., numerical rounding errors when very large and
very small numbers are added). Additionally, as with data shifting, also scaling is
a useful preparation step for comparing functions or distributions. In Sect.11.7,we
introduce measures for quantitative comparison of the differences of datasets. Some
of these measures such as the EUCLIDean distance are sensitive w.r.t. the absolute
magnitude of the data. Figure 11.7 shows an example where the data was scaled
along the first coordinate.
Non-dimensionalization is a type of scaling based on physical quantities and
removes the physical units, which can help to simplify equations and to reveal their
structure, e.g., in terms of the speed of light or in terms of internal “length scales”
(see, e.g., [8]).
Scaling typically implies a linear mapping of the data or values to a “more
suitable” range, i.e.,
X ׀→λX, (11.6)
.
where λ is a constant. This constant can be chosen, e.g., as the dispersion of a
.
distribution, and thereby also acts as a means to remove the physical units. Linear
˜
scaling of a random variable X to Xhas the following implications for the mean μ
. .
and the standard deviation σ:
.
. X ˜ =λX, μ X ˜ =λμ X , σ X ˜ =λσ X . (11.7)
Fig. 11.7 Scaling of the x-coordinates of the data points. Here, the data in the right panel are
transformed, so that the maximum is approximately at 1, while in the left panel the maximum is at
.2·106


================================================================================
PAGE 268
================================================================================

250 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.8 Standardization of data distributions: left are three different distributions that are
standardized and scaled, so that each of them has a total area of 1, shown in the right panel
Nonlinear scaling is often used to transform a nonlinear problem into a linear one.
In the context of ML, we will use such a scaling in Sect.13.1.3.
Standardization
Quite naturally, also combinations of scaling and shifting can be used. Standardiza-
tion is such a combination. It changes the data, so that the distribution’s standard
deviation is 1 and the expectation value is 0. Mathematically, standardization is
given by the following transformation:
X−μ
X ׀→ X . (11.8)
.
σ
X
A benefit of this transformation is that the shapes of different datasets become
directly comparable because they are now defined on the same “scale,” given by
mean and standard deviation. Data that originally was not normally distributed
is still not normally distributed after the transformation. This is demonstrated in
Fig.11.8.
11.5.4 Other Transformations
A number of other transformation types exist, some of which are not well defined.
Among those is the commonly used notion of normalization which can have
different meanings:
• either or both of linear scaling and shifting (e.g., so that all values are ∈[0,1])
.
• plain standardization
• “converting” data such that it is approximately normally distributed (and not just
normalized!), e.g., using the so-called Box-Cox power transform; see below
• scaling so that the area below a distribution is 1


================================================================================
PAGE 269
================================================================================

11.5 DataPreprocessing 251
Words of advice (cid:3)
Be careful and try to understand what is really meant when someone says “normal-
ization” in the context of data transformation as this notion is not unambiguously
defined.
Further transformations and commonly used notions are:
• centering or mean centering: The goal of data centering is to ensure that the
dataset has a mean value of zero after scaling. This can be achieved by subtracting
the mean of the data
X ׀→X−μ , (11.9)
. X
which can be generalized to multiple dimensions in the obvious way. We
encountered this scaling in one of the previous chapters in the context of the
covariance matrix.
• range scaling denotes a combination of scaling and shifting.
• min/max scaling: A combination of scaling and shifting is also denoted as
min/max scaler. It is defined as the following map:
X−min(X)
X ׀→ , (11.10)
. max(X)−min(X)
which is, e.g., sometimes used during preparation of image training data for deep
learning methods.
• quantile transformer and (Box-Cox) power transforms: These two transfor-
mations transform data into distributions that are similar to Gauss function
distributions. Often, this requires changing the shape of the distribution.
A different class of data transformation methods are those that go beyond simple
mathematical operations, for example, the methods of principal component analysis
(PCA), which will be introduced in detail in Sect.15.2. This is a method that
maximizes the variance of data along a direction and thereby is able to reduce the
number of variables by simultaneously optimizing the information loss. It involves
rotation and translation of the data. Another ML specific transformation is the one-
hot encoding where class labels are turned into lists of zeros with only a single
one. For example, assume that a dataset consists of triangles, circles, and squares.
Then, we could assign the following values: circle = [0, 1, 0], square = [0, 0, 1],
and triangle = [1, 0, 0]. This is often for classification tasks in neural networks; see
Chap.17.


================================================================================
PAGE 270
================================================================================

252 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
11.6 A Taxonomy of Machine Learning Models
The third step in the Machine Learning Workflow,Fig.11.2, is concerned with the
choice of the ML model used for the data analysis. In this part, we restrict ourselves
to “classical” ML models, which are those that were derived in the spirit of statistics
and mathematics. This separates them mainly from deep learning (DL) (and related)
models, which are rather inspired by the information processing and interaction of
biological neurons. The latter models are investigated separately in Part IV of this
book.
Choosing the most suitable ML model for a particular data analysis or ML
experiment can pose a challenge due to the large number of different available
models. For instance, the Python library scikit-learn already contained at the time
of writing this book more than 100 different ML models [4]. Therefore, in the
following we will introduce the fundamental types of ML ranging from regression
and classification through dimensionality reduction and clustering. An overview is
given in Fig.11.9. In the remainder of the book, most of the ML methods shown will
be introduced.
Taking a look at Fig.11.9, we see that the first categorization is whether learning
takes place in a supervised, unsupervised, or reinforcement learning manner.
Supervised learning denotes a training process that relies on both the data of the
feature variable, x ,aswellasonlabels or target variables, y .
. i . i
Fig. 11.9 Taxonomy of some of the most important machine learning model types


================================================================================
PAGE 271
================================================================================

11.6 ATaxonomyofMachineLearningModels 253
The pointsP . = (x i ,y i ) are the training examples, presented to the ML model
based on which the model is supposed to learn. An important algorithmic aspect is
a feedback loop in the form of an error measure that measures the performance of
the model. Essentially, the ML model “tries” (using an error minimization strategy)
a number of different model parameters, makes predictions ypred for the given
. i
feature values x , and compares them with the given labels y . The labels serve as
. i . i
the supervision, as they tell the algorithm, if a particular prediction was accurate or
not. Regression (Sect.11.6.1) and classification (Sect.11.6.2) are the most famous
problem types considered in supervised learning; we have already briefly discussed
them above.
Unsupervised learning is a typical data mining and data exploration strategy.
It does not require supervision, i.e., no labels y are given, only the data of the
. i
feature variables are used as input. How or what could a model learn only from the
input variables? The goal of such methods often is to detect patterns in a dataset and
to understand the structure within potentially high-dimensional data. This can be
achieved by, e.g., clustering methods, which group similar points together. Methods
for dimensionality reduction are another type of unsupervised learning methods.
These methods aim at reducing the possibly thousands of dimensions of a dataset
to a significantly smaller number of features. A brief overview is given below in
Sect.11.6.4
A third class of learning methods is reinforcement learning. Such models are
based on “agents” that interact with an environment during trial and error. Their
actions are “reinforced” through the concept of “rewards” and “punishments.” The
agent’s goal is to maximize the number of rewards. This model strategy is important
for robotics, but also for playing strategy games (e.g., winning the game “Go”
was a great accomplishment [7]). Generally, reinforcement learning can be divided
into model-based learning and model-free learning, while the former is particularly
efficient if a physical model can be used. At the same time, this also might constrain
the model. To date, there are only a few applications of reinforcement learning in
physics and materials science, which is why they will not be discussed in more detail
here. An introduction is, e.g., given by Russell and Norvig [5].
11.6.1 Overview of Regression
In ML the goals of regression are twofold and exhibit a (at least at first glance) small
but important distinction to what is done in statistics:
• The first goal is to predict numbers for a given numerical input, based on existing
data, e.g., from measurements or computations (the so-called training data). This
is a data-driven approach to approximate a (typically unknown) mathematical
function and use it to make predictions.
• The second goal is inference, which typically implies that we want to find out
something about the underlying relation between input and output data, e.g., in
form of finding the most suitable mathematical formulation.


================================================================================
PAGE 272
================================================================================

254 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.10 Regression requires training data (left) and a function h. The coefficients of h
minimize the deviations between the data and .h(x)(middle panel) allowing to make predictions
with the trained model h: for an arbitrary, new input value.x ∗an output.y ∗is predicted (right)
Figure 11.10 illustrates the concept of ML regression. Given is training data, pairs
of features X and output values y. Next, we decide that the model h is a quadratic
function. Why? Because we, for example, already might know from the underlying
physics that the data should roughly obey such a relation, or because often, in the
spirit of Occam’s razor (Sect.1.2.5), a simple model is the most useful one, to start
with. But at the end of the day, this is simply our decision.
The model parameters or weights of the ML model are the coefficients of h.They
will be determined through the training process. The trained model (the hypothesis)
consists of the functional form (the quadratic function) and the three parameters.
This can now be used for making predictions, e.g., of the output value at position
x ∗ , which is obtained by evaluating the function y ∗ = h(x ∗ ). The trained model
. .
can also be used for inference, e.g., analyze where the function’s slope is zero or
interpret the function parameters in terms of the underlying physics problem.
But is this all not just plain curve fitting? Where exactly is the “machine
learning?” In ML, curve fitting is an important technique, but the ML aspects are
contained in the focus on making predictions or inference and not just on curve
fitting. Also, ML regression comes with a large toolbox of methods and concepts,
e.g., for figuring out if the training dataset is sufficiently large or for ensuring that
our models have indeed notable predictive capability.
11.6.2 Overview of Classification
In ML classification, the goal is to predict different categories of data. The input, i.e.,
the features, consists of numerical data, while the output denotes the class or label
of a data point. Thus, classification is another supervised learning method. Once the
ML model is trained, it is able to predict, for a given input X, what the corresponding
class Y would be. Data of different classes are separated by the decision boundary;
finding the decision boundaries is the purpose of training. Figure 11.11 shows an
example of two features and a target variable that represents three different classes,


================================================================================
PAGE 273
================================================================================

11.6 ATaxonomyofMachineLearningModels 255
Fig. 11.11 Supervised classification assigns a label to each point. Here, the label is a color and
reveals whether a point belongs to class #1, #2, or #3. For the colored points in the left panel, the
label is already known (the training data). From those, the decision boundary (dashed lines in the
middle panel) is obtained. The algorithm can now predict the class number for new data, e.g., the
gray points with the question marks in the left panel. This results in the labels symbolized by the
color of the markers in the right panel
indicated by the three different colors. Classification tasks are probably the type of
ML problems that we are most familiar with: already since our early childhood, we
sorted the world of animals into cats, dogs, bunnies, or mice. The features used
in classification are usually interpretable quantities. For example, in the case of
bending of a cantilever beam, one feature might be temperature; the other one might
be the deflection of the beam; the label is then “beam has cracks” or “beam is intact.”
Although making predictions is probably the most important aspect for classifi-
cation tasks, the aspect of inference is also of some relevance (e.g., “where do the
decision boundaries meet?”).
11.6.3 Overview of Clustering
Clustering of data is an unsupervised ML method. The learner needs to accomplish
the task of deciding, which of the given data points belong into the same group.
Figure 11.12 sketches the result of such a cluster analysis. Looking for the
nearest neighbors is one commonly used approach. However, it is not always
straightforward to decide which metric to use for measuring distances in the feature
space. A different approach is used in density models, which search for those regions
in the feature space that would result in the densest connected regions.
Typically, these methods are not able to automatically identify the number of
clusters, and we have to make an informed decision, e.g., based on our physical
insight into a specific problem. Such a subdivision of a dataset into different clusters
can also be used to provide labels for a subsequent supervised learning methods.
Another application is the detection of outliers.


================================================================================
PAGE 274
================================================================================

256 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.12 A clustering ML model took the coordinates of the points as input and, based on that,
groups points together (here into three different clusters) indicated by the three different colors.
Unlike in classification in Fig.11.11, the algorithm does not have any “examples” for the three
different clusters
11.6.4 Overview of Dimensionality Reduction
Dimensionality reduction denotes a process, during which high-dimensional data
(i.e., datasets that have a large number of features) is reduced to low- or at least
lower-dimensional data. The main objective of these methods is to preserve as much
of the original information as possible, or at least, to reduce the original dataset to
the information of interest. For a simplistic example of a dimensionality reduction,
let us imagine we are looking at the silhouette of an object projected by a torch on
a wall. We can see enough to make some conclusions regarding some geometrical
aspects that we are interested in. But we clearly also lose information regarding the
three-dimensional structure of the object.
High-dimensional data can be problematic for a number of reasons. One reason
is the curse of dimensionality , cf. Sect.2.4, which states that for a comparable “data
density” while increasing the dimensionality of the feature space, the amount of
data has to be increased more than linearly. As this affects also, e.g., supervised
learning methods, dimensionality reduction can often be found in the form of a
preprocessing step of a dataset. Another application domain is data visualization:
visualizing more than two or three dimensions is practically impossible. Therefore,
reducing a feature space to two or three dimensions, as some of the algorithms do, is
a very useful approach to allow visualization of complex datasets. Last but not least,
some datasets are simply too big and storage becomes an issue, while dimensionality
reduction might be able to help.
It is important to understand how these ML algorithms work and in which way
they alter the data, because after a dimensionality reduction, some information is
always removed; and we have to ensure that this is not the relevant information.


================================================================================
PAGE 275
================================================================================

11.7 ErrorMeasuresforNumericalData 257
Table 11.1 Names for input
X or X1, X2, ... Y or Y1,Y2, ...
and output variables used in
input variable output variable
ML. The boldface names are
the ones commonly used in feature variable target variable
this text feature label
independent variable dependent variable
explanatory variable response variable
predictor variable predicted variable
regressor or covariate regressand
11.6.5 The Zoo of Names for Input and Output Variables
We conclude this section by saying a few words about the representation of data for
machine learning problems. The main formal aspects have already been introduced
in Sect.3.3, while in this chapter we have so far focused on the huge variety of
different methods. Some of these methods have their roots in statistics; others have
grown from completely different disciplines. Consequently, this also brings along a
large variety of different names for variables. Most of the time, we will use feature
in this text, when referring to an input variable, while in some instances we call the
output variables also target variable. In particular in the context of classification, we
use label when referring to the output variable; in the literature label is also used for
the output of regression problems.
To be able to easily read the ML and data science literature, we summarize the
names that are most commonly found in Table 11.1. In this text, we avoid the
notion of independent variables, because this might suggest that these variables are
statistically independent of each other. Most of the time, input variables are in fact
not statistically independent in ML. A notion that seems to be missing is that of the
attribute. Usually, the attribute denotes the input variable; however, sometimes there
are input and output attributes, which is why we avoid it altogether. Typically, the
names for the input and output variables come as pairs, as highlighted in Table 11.1.
It is indeed a zoo of names ...
11.7 Error Measures for Numerical Data
Quantifying the agreement between individual observations and datasets is key to
quantifying the performance of a ML model. There are two fundamentally different
classes of measures: The first one considers numerical data and compares the values
of pairs of observations and then “somehow” sums these contributions up. This is
the subject of the remainder of this section. The second type measures the similarity
between sets of items, which is important for classification problems and will be
introduced in Sect.11.8. Both measures have in common that ultimately the error
(or agreement) is quantified by a single number.


================================================================================
PAGE 276
================================================================================

258 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Fig. 11.13 Measuring data
is affected by errors. The
measurements are an estimate
for the “true” data, and the
deviation between true and
measured data is the error.∈ i
11.7.1 Introduction to the Problem
In the following, we assume that a “true” function exists, e.g., a physical law from
which we are able to obtain “true data” or the data (x ,y ) for i = 1[,n] that
. i i .
are located on the line in Fig.11.13. This is our reference dataset. In the context
of statistics, the set of all possible points located on the line corresponds to the
population, and the points on the line represent the expected values. However, if the
purpose is not to obtain an estimate for the population, then the reference data does
not necessarily have to be obtained from a true function, and it can be any dataset.
A second dataset are measured data (x˜ ,y˜ ), which are affected by deviations or
. i i
disturbances ∈ from the reference data:
. i
∈ =y −y˜ . (11.11)
. i i i
The deviations are called residuals or errors, depending on the context, as we will
see in the next chapter. For now, we will call ∈ an error.
. i
In science and engineering, there are two types of errors; see Fig.11.14: precision
is the degree by which repeated measurement give the same result. If the distribution
of measured values is broader, then the scatter in the data will be larger; thus,
repeated measurements will be further away from each other. Accuracy , on the
other hand, tells us how close some measurements are, on average, w.r.t. the true or
reference value. Systematic errors often impact accuracy, e.g., if for all temperature
measurements we read the thermometer from an acute angle, so that our reading
◦ ◦
is always 0.2 C too high, the accuracy is this constant offset of 0.2 C. Any
. .
measurement can be accurate, but is not automatically precise and vice versa. The
goal is to have measurements (or in the context of ML: predictions) that are both
accurate as well as precise. In the context of ML, we will encounter this two types
of errors again. Then, they are called bias instead of accuracy and variability or
variance instead of precision.


================================================================================
PAGE 277
================================================================================

11.7 ErrorMeasuresforNumericalData 259
Fig. 11.14 Visualization of
the two types of errors, i.e.,
accuracy and precision, based
on the example of yield stress
measurements
11.7.2 Mean Absolute Error
The most straightforward error type is the mean absolute error (MAE) which in the
context of ML and numerical optimization is also called L1 loss. It is the mean of all
n absolute errors:
(cid:3)n (cid:3)n
1 1
MAE= |y −y˜ |= |∈ | . (11.12)
. i i i
n n
i=1 i=1
The MAE is a simple error measure that ranges from 0 to . +∞ and averages the
vertical distance between points.
The error is in units of the investigated variable and a scale-dependent measure,
which is an advantage compared to other measures introduced below. Therefore,
one has an intuitive understanding of the magnitude of a particular MAE value (if it
is “large” or “small”).
This type of measure is used for time-series analysis. However, in general MAE
is inferior to the mean squared error (MSE), which is introduced next. One of the
reasons is that the MAE is not differentiable at 0. Furthermore, each measurement
contributes without weighting and proportionally to the respective deviation ∈ . If
. i
the norm is removed, we end up with the mean error.
In Python, the MAE can be easily computed for the one-dimensional numpy
arrays y_ref and y by MEA = np.mean(np.abs(y_ref - y)) , where np as always
is an alias for the numpy package.
11.7.3 Mean Squared Error
The MSE is sometimes also called mean squared deviation (MSD) or, in the context
of ML and numerical optimization, also L2 loss. It is one of the most commonly used
error measures and defined as the average of the squared deviations:


================================================================================
PAGE 278
================================================================================

260 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
(cid:3)n (cid:3)n
1 1
MSE= (y −y˜ )2 = ∈2 . (11.13)
. n i i n i
i=1 i=1
Due to the square, the MSE is always larger or equal to zero, and it is symmetric with
regard to the sign of the deviation ∈ . A lower value implies a smaller deviation. Its
. i
values are in units of the squared variable of interest, which can make the intuitive
interpretation more difficult, as compared to the MAE.
Due to the square, the MSE is a second moment of the error about zero, and as
such, it quantifies both the precision (or variance) as well as the accuracy (or bias).
As another convenient property, outliers have a disproportionately large influence on
the value of the MSE. Or, in other words, if large errors are particularly undesirable
in an algorithm that involves the MSE, then the square effectively penalizes such
values. Finally, it will turn out that the square is also very useful in the context of
error minimization because when a first derivative or a first gradient needs to be
calculated in order to find the minimum, the resulting equations using the MSE are
much simpler than any other error measure. The Python implementation is not too
different from the implementation of the MAE: MSE = np.mean((y_ref - y) ** 2) .
A very useful property is the following decomposition
.MSE = Var( X ˆ )+ Bias(X, X ˆ ) , (11.14)
ˆ
where X is the measured variable and X is the true or reference variable. As a
.
consequence of this relation, it follows that if the bias is zero, the MSE and variance
are identical.
11.7.4 Root Mean Square Error
The standard deviation behaves to the variance as the root mean squared error
(RMSE) does to the MSE:
√
=
.RMSE MSE
(cid:4) (cid:4)
(cid:5) (cid:5)
(cid:5) (cid:3)n (cid:5) (cid:3)n
=(cid:6)1
(y −y˜ )2
=(cid:6)1
∈2 . (11.15)
n i i n i
i=1 i=1
Similar to the MSE,alsotheRMSE is always greater than or equal to zero, and a lower
value implies a smaller deviation. As for the MSE, outliers have a disproportionate
influence. The main difference to the MSE is that the RMSE is measured in units
of the variable of interest. Nonetheless, comparing two such error values with one
another is difficult due to the nonlinearity that stems from the squared term.
The MAE can be used as an upper and lower bound for the RMSE, and it can be
shown that:


================================================================================
PAGE 279
================================================================================

11.7 ErrorMeasuresforNumericalData 261
• .MAE ≤ RMSE holds
• if all errors have √the same magnitude then it is even .MAE = RMSE
• .RMSE ≤ MAE × n, where n is the number of records.
11.7.5 Coefficient of Variation
To be able to compare RMSE values, it is possible to perform a normalization, e.g.,
either with the minimum/maximum range of the variable, with the interquartile
range, or with the mean value of the variable. In the latter case, the resulting measure
is called coefficient of variation (CV):
RMSE
.CV = . (11.16)
X
Words of advice (cid:3)
Note that the standard deviation divided by the mean of a variable X, .σX/μX is
also called coefficient of variation. Furthermore, cross validation,tobeintroducedin
Chap.16, is also often abbreviated as CV. To avoid ambiguities, one can also write the
coefficient in Eq.(11.16) more explicitly as.CVRMSEor as.CV(RMSE).
11.7.6 Root Mean Squared Log Error
As a last and somewhat more exotic error measure, we briefly introduce the root
mean squared logarithmic error (MSLE) which is defined as follows:
(cid:4)
(cid:5)
(cid:5) (cid:3)n (cid:7) (cid:8)
.MSLE
=(cid:6)1
log(y i + 1)− log(yˆ i + 1) 2 . (11.17)
n
i=1
While the RMSE has the character of an absolute error and scales like the variable,
the MSLE has the character of a relative error. The logarithmic scaling of the values
strongly reduces the influence of extreme values. However, variables need to be
scaled, so that they lie in the definition range of the log function. As an example
.
consider the following two cases in which two values, y and yˆ, are given that differ
.
by 10%:
• . y =100,yˆ =90 → MSLE = 0.0153, .RMSE = 10
• . y =10,000,yˆ =9000 → MSLE = 0.0153, .RMSE =1000


================================================================================
PAGE 280
================================================================================

262 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
While the RMSE is scaled according to the scale of the variable, the MSLE as a
relative measure remains unchanged because it is insensitive w.r.t. the scale of the
error. The MSLE is not symmetric, which shows in the different error values for
under- and overestimation: assume that y = 1000. Then for a lower value of yˆ =
. .
600 it is .MSLE = 0.510 and .RMSE = 400 while for . yˆ = 1400 it is .MSLE = 0.33
and .RMSE = 400. Therefore, there is a larger penalty if y is underestimated than if
it were overestimated. In many applications this is reasonable, as there might be a
given limit value that shall not be exceeded, e.g., a radiation dose, while it is entirely
harmless to stay below the limit value.
11.8 Similarity Measures for Classification Problems
In the following, we first introduce several measures that are used for pixel-
wise comparison in image data before we then introduce the “cross-entropy loss
function” that is used as an “error measure” in classification problems. A materials
science example, based on which all measures are compared, is given in Sect.11.8.6.
11.8.1 From True Positives to False Negatives
Binary classifications problems require a decision whether a data point belongs to
one class or to the other one. For example, deciding if a specimen is defective or not
is a binary classification problem; “not defective” could be represented by zero and
“defective” by one. This type of problem is also encountered in binary segmentation
tasks, in which the ML model has to decide if a pixel belongs to a specific object or
not. In such cases the following four measures are important:
• The true positives (TP) is the number of all data points that have positive labels
(i.e., they have the value 1) for both the predicted target variable and the true
target variable.
• A true negative (TN) is just the opposite: if both the true value and the predicted
value are 0, this adds to the number of true negatives.
• The false positives (FP) is a prediction of 1, while the true value is supposed to
be 0.
• A false negative (FN), on the other hand, is a data point that is predicted as 0,
while the true value is 1.
As an example, we take a look at the following two arrays:
In [1]: import numpy as np
Y_pred = np.array([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0], dtype=int)
Y_true = np.array([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0], dtype=int)


================================================================================
PAGE 281
================================================================================

11.8 SimilarityMeasuresforClassificationProblems 263
Fig. 11.15 Confusion matrix
for a binary classification.
The gray shades indicate the
number of TN, TP, etc. If all
data are correctly predicted,
there should be only values
along the diagonal from top
left to bottom right
As we are going to compare the arrays element-by-element, there is one important
aspect to consider: comparison between numbers using the equal sign, such as
Y_pred == 1 , should be done only for integer numbers: hence the dtype=int above.
In [2]: TP = np.sum((Y_pred == 1) & (Y_true == 1))
TN = np.sum((Y_pred == 0) & (Y_true == 0))
FP = np.sum((Y_pred == 1) & (Y_true == 0))
FN = np.sum((Y_pred == 0) & (Y_true == 1))
print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')
Out [2]: TP: 4, FP: 1, TN: 5, FN: 2
The comparison Y_pred == 1 results in an array that contains True and False .
The operator \& performs an element-wise logical . AND which again results in
an array of True and False . Summation of the elements of such arrays implicitly
converts False to 0 and True to 1, effectively counting the number of True values.
All four quantities can be computed in this way.
For floating point numbers, this is not a well-defined operation. Figure 11.15
visualizes the four numbers in form of a matrix which is commonly called confusion
matrix. A confusion matrix can also be used to visualize the performance for multi-
class classification problems; the different class labels define the horizontal and
vertical axes.
11.8.2 The Jaccard Similarity or Intersection Over Union
Jaccard Similarity
Assume that for two sets of data, A and B, we want to quantify how similar these
sets are. The JACCARD similarity (or the JACCARD similarity coefficient) between A
and B is given as the number of elements that are contained both in A and B relative
to the total number of different elements (i.e., the elements that are either in A or in
B). Without duplicates, these are sets, cf. Sect.3.2.1). The JACCARD similarity J is
given by


================================================================================
PAGE 282
================================================================================

264 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
|A∩B|
J(A,B)= , (11.18)
. |A∪B|
where the number of elements of a set A is given by |A|. As a direct consequence,
.
two identical sets have J(A,B)=1due to the equality A∩B =A∪B. The other
. .
extreme is that two sets do not have any element in common. Then it isJ(A,B)=0.
.
For all other cases it is 0 <J(A,B)<1.
.
The Jaccard similarity works well as long as the two sets have approximately the
same size, |A|≈|B|. If one of them has significantly more elements than the other,
.
the denominator of Eq.(11.18) is always larger than the intersection of the two sets
and J will be small.
Intersection Over Union
In the context of ML, the JACCARD similarity coefficient is mostly encountered in
the context of computer vision problems, in particular for object detection and image
segmentation tasks. It is then encountered as intersection over union (IoU).
Object Detection and Image Segmentation (cid:2)
Object detection typically seeks bounding boxes, rectangles that fully enclose
an object, while image segmentation separates the pixels of an image into,
e.g., “object” (black) and “background” (white). More details are given in
Part IV.
For measuring how well a ML algorithm predicts a particular object in an image,
we compare the number of pixels belonging to the predicted object, A, to those of
the “true” object, B which are assumed to be known (the “ground-truth” given by
the labels). Mathematically, this is nearly identical to Eq.(11.18), but for IoU we
usually do not refer to sets. Instead, it is often directly phrased in terms of number
of pixels (even though this is not strictly necessary).
Definition 11.2 (Intersection over Union). The coefficient of intersection
over union (IoU) is given for two objects of an image, A and B by
numberofpixelsbelongingtoobjectsAandB
IoU(A,B)= , (11.19)
.
numberofpixelsoftheunionofbothobjects
Visually, it can be interpreted as shown in Fig.11.16, where, for didactical
reasons, we show also objects of different sizes. As only the relative number of


================================================================================
PAGE 283
================================================================================

11.8 SimilarityMeasuresforClassificationProblems 265
Fig. 11.16 Visualization of different cases of the IoU. The intersection are those items that the
two sets have in common (indicated by the blue filled area); the union is the contour of all items.
Dividing the former number by the latter number gives the IoU value, which is represented as the
fraction of the blue shaded area w.r.t. the total area of both circles. In an image, the areas would be
the sets of pixels
pixels or the areas are considered, the measure is scale-invariant, i.e., scaling both
directions of an image with the same factor does not change the IoUvalue.
.
Often, it is convenient to use an alternative equation for computing the IoU,
.
which is based on counting true positives (TP), false positives (FP), and false
. .
negatives (FN); refer to Sect.11.8.1 for a general introduction to these quantities.
.
In the context of images, a true positive is, e.g., a pixel that is correctly predicted to
belong to an object; a false positive is a pixel that is predicted to belong to an object,
but in fact is part of the background or of another object; a false negative is a pixel
that was predicted not to belong to the object of interest, but in fact does belong to
that object. The IoUcoefficient can then be expressed as
.
TP
IoU= . (11.20)
. TP+FP+FN
As the number of true negatives does not show up in this equation, the formulation
is not symmetric. This implies that unlike in Definition 11.2, interchanging the true
object and the detected object would yield different results.
11.8.3 The Dice Coefficient
The Dice coefficient is another measure for comparing geometrical areas of, e.g.,
images. It is defined as
Definition 11.3 (Dice Coefficient). Given are the areas of two objects A and
B. For these, the Dice coefficient is given by
2|A∩B|
Dice= (11.21)
. |A|+|B|


================================================================================
PAGE 284
================================================================================

266 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
The Dice coefficient is also called F1 score. In the definition, the denominator
gives the sum of the total area of A and B, which is different from the intersection
of the areas of the two objects (the latter is smaller or equal than A+B). As for
.
the JACCARD metric, the range of possible values is
.
[0,1]with 1 denoting a perfect
match. The Dice coefficient can be written as well in terms of TP, FP, and FN:
. . .
2TP
Dice= . (11.22)
. 2TP+FP+FN
As for IoU, the number of true negatives does not occur, and the equation is not
.
symmetric. This implies that unlike in one of the above definitions of intersection
over union, the true object and the detected one cannot be interchanged without
changing the result.
11.8.4 Precision and Recall
As two last measures for classification, we introduce the precision and recall. The
precision is the number of TPdivided by the total number of elements that belong
.
to the positive class:
TP
Precision= (11.23)
. TP+FP
Precision indicates how precise the ML model was. This is a useful measure if for a
particular problem, false positives are more important than false negatives, or if we
cannot calculate FNor FP.
. .
Recall (also called sensitivity), on the other hand, is the number ofTPdivided by
.
the total number of elements that belong to the positive class:
TP
Recall= (11.24)
. TP+FN
This number tells us how many of the true positives the model was in fact able to
predict as TPs.
.
In both cases, there is an “asymmetry” between FP and FN. This could be of
. .
relevance, e.g., when wrongly predicting that a sample is fractured has different
consequences than wrongly predicting that a fractured sample is not defective.
11.8.5 Categorical Cross-Entropy
We briefly introduce an entirely different concept and formulation that is commonly
used for classification and that also can be used for multi-class tasks. In typical
classification tasks, one has to compare distributions of pixels, which, e.g., might
be the label for a specific object. To do this based on the distributions of values,


================================================================================
PAGE 285
================================================================================

11.8 SimilarityMeasuresforClassificationProblems 267
the cross-entropy can be used. Assume that the ypred are the predicted class labels,
.
while the ytrue denote the ground truth. The categorical cross-entropy “loss”L (in
. .
the next chapter we will introduce the “loss” as a kind of error) is given for a multi-
class problem of N classes as
(cid:3)N
L=− ytrue log(ypred ) (11.25)
. i i
i
where i is the number of classes. In case of a binary classification, this reduces to
L=−ytrue log(ypred)+(1−ytrue)log(1−ypred). (11.26)
.
Note that the y are considered as distributions (here distributions of pixel values).
11.8.6 Example: Binary Segmentation of a TEM Image
In this example, we investigate a TEM image of a dislocation. The goal is to create
a “mask” that captures only the pixels that are representing the dislocation. The
TEM image is shown in the left panel of Fig.11.17. The dislocation consists of
222 pixels out of a total of 4096 pixels contained in the image. The “true mask”
shows the reference mask that was created by a domain expert: by looking at the
image, those pixel that belong to the dislocation were manually colored in black
(this is what is meant we someone speaks about “hand-labeling”). Note that there is
no unique solution, and two different persons most likely will create two different
masks. Mask A, B, and C show three different predictions for the dislocation. They
were artificially constructed, but, in general, would be the output of a DL model.
Mask A and B show a dislocation line that is fragmented; mask B and C contain
additionally a “v-shaped” artifact. For these three masks we obtain the metric scores
shown in Table 11.2.
image true mask mask A mask B mask C
Fig. 11.17 TEM image of a dislocation (the dark curve) for which the “mask,” i.e., those pixel
that belong to the dislocation, is wanted. Shown is the “true” mask, that serves as the reference,
together with three different predictions


================================================================================
PAGE 286
================================================================================

268 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
Table 11.2 Resulting metric scores for the predictions shown in Fig.11.17
Prediction IoU Dice Precision Recall TP FP TN FN
Mask A 0.68 0.81 1.00 0.68 151 0 3874 71
Mask B 0.88 0.94 0.88 1.00 222 28 3846 0
Mask C 0.67 0.80 0.85 0.75 168 28 3846 54
In summary, the following can be observed:
• Mask A does not contain anyFP, which implies that each pixel that was identified
.
as a “dislocation pixel” is indeed part of the dislocation. In such a case, one can
expect the precision to be 1.0 as found for this case. However, we can see that
not all dislocation pixels were correctly identified, as a significant section of the
dislocation is missing. This does not reflect correctly in the precision, but we can
observe this error in the recall which is only 0.68. The corresponding ML model
is said to have a higher bias or a tendency to under-predict. Therefore, despite
perfect precision, the model’s overall performance is not ideal due to the lower
recall.
• In contrast, the predicted mask B exhibits a different bias. It contains all
dislocation pixels but additionally also a part of the background (showing as
FP). We observe that now the recall is 1.0, which implies that the model has
.
successfully identified all true dislocation pixel. However, this comes at the
expense of precision, which is reduced to 0.88 due to the model predicting parts
of the background as dislocation, which increases the number of false positives.
By comparison, we find that the Dice score has improved significantly and is
above 0.94 now. This can be interpreted such that the model has now quite a
balanced performance in terms of precision and recall. However, the high Recall
score indicates that the model tends to over-predict.
• The last prediction, mask C, contains both false negatives and false positives. It
is therefore a combination of the two previous predictions.
The results show both under-as well as over-prediction, which results in the
(approximate) balance between Recall and Precision. Despite this, both the IoU
and Dice scores suggest comparable performance as compared to mask A, which
is clearly not the case.
In particular the last case underlines the limitations of using only a single metric
for model evaluation. It’s critical to take into account both types or errors (false
positives as well as false negatives) to faithfully represent the model performance.
Things to Remember About Similarity Measures for (Binary) Classification(cid:3)
In summary, we can see that even though a number of metrics are available, each
provides different information. Very often, it is not feasible to rely on only one of
them for judging the quality of a prediction. Using all, or at least some, of them
together gives a reliable estimate of the quality of the predictions.


================================================================================
PAGE 287
================================================================================

11.9 Exercises 269
11.9 Exercises
11.1. Is it possible that the mean squared error is zero? What would that imply?
11.2. Given are the following true and predicted data:
ytrue 3.2 4.4 6.7 11.0 14.6 18.7
. ypred 2.8 4.3 6.8 11.5 15.0 16.7
(a) Compute the mean absolute error (MAE), the mean squared error (MSE), the
root mean squared error (RMSE), and the root mean squared logarithmic error
(MSLE).
(b) If the fourth predicted value changes from 11.5 to 21.5, how does that affect the
different error measures?
11.3. In a research project, a large dataset of SEM images is available. The SEM
images show metal surfaces, and we are interested in analyzing and finding defects
in them. It is time-consuming and labor-intensive to manually annotate and highlight
defects in each image. Which class of machine learning techniques (supervised or
unsupervised) would be most appropriate to analyze and identify different types of
defects (and existence or lack thereof) in these SEM images?
11.4. In a research project, a large dataset of TEM images for different nanopar-
ticles is available. The nanoparticles come in different sizes and are located in
different regions in the image. A domain expert has annotated the image to
mark the pixels which encompass each nanoparticle to separate them from the
background. The goal is to use the dataset and the annotation to train a model that
learns to automatically identify and locate nanoparticles in new datasets. Which
class of machine learning techniques (supervised or unsupervised) would be most
appropriate to analyze and identify nanoparticles in each TEM image?
11.5. In a dataset of ceramic materials, the recorded properties for each sample
include processing parameters and material parameters such as temperature, pres-
sure, time, and the corresponding hardness value. A researcher wants to find out the
relationship between the processing conditions and the resulting hardness value. Is
this problem better suited to be treated as a regression or a classification problem?
Explain why.
11.6. In the research problem in Exercise 11.3, imagine that we have the resources
to annotate each SEM image and specify the type of defect (i.e., cracks or pores)
that is present. We then want to use machine learning to train a model that learns
this and can be used to identify defect types in new datasets. What kind of problem
is this treated as, classification or regression?


================================================================================
PAGE 288
================================================================================

270 11 IntroductionandGeneralConceptsofMachineLearningandDataScience
References
1. C. B. Bishop. Pattern Recognition and Machine Learning. Springer, 2011. ISBN 978-0-387-
31073-2. URL https://www.springer.com/gp/book/9780387310732.
2. T. M. Mitchell. Machine Learning. McGraw-Hill, 1997. ISBN 0070428077.
3. A. Müller. Introduction to Machine Learning with Python. O’Reilly, 2017. ISBN 978-1-449-
36941-5.
4. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
5. S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, USA,
3rd edition, 2009. ISBN 0136042597.
6. A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of
Research and Development, 3(3):210–229, 1959. DOI https://doi.org/10.1147/rd.33.0210.
7. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-
brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.
Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–
489, Jan. 2016. DOI https://doi.org/10.1038/nature16961.
8. M. Zaiser and S. Sandfeld. Scaling properties of dislocation simulations in the similitude regime.
Modelling and Simulation in Materials Science and Engineering, 22(6):065012, Aug 2014. DOI
https://doi.org/10.1088/0965-0393/22/6/065012.


================================================================================
PAGE 289
================================================================================

A First Approach to Machine Learning with 12
Linear Regression
If you don’t know where you are going, any road will get you
there.
Lewis Carroll (1832–1898)
English author and mathematician
12.1 The Roots of Regression Analysis
12.1.1 A Historical Perspective
Regression analysis is probably the flavor of machine learning (ML) analysis that is
most closely and most obviously related to the field of statistics. Regression origi-
nates from statistics and is a “statistical toolbox” with the purpose of estimating the
relationship between one or more independent variables—the inputs or features—
and one or more dependent variables, the outputs. It is instructive to take a look at
the history of regression analysis.
The origins of statistical regression date back to the late nineteenth century and
Francis Galton (1822–1911), a truly multidisciplinary scholar who did research in
the field of statistics, geography, biology, sociology, and meteorology, to name but a
few. Among others he investigated the diameters of sweet peas Galton [6]. His goal
was to understand which and how these characteristics were inherited from one
generation to the other. He designed a statistical experiment, and for the analysis he
introduced the linear regression technique where he approximated averaged data by
a line and used this for drawing conclusions. A summary of the experiment and the
data analysis is given in Example 12.1.
Another important step toward today’s regression analysis was taken by
KARL PEARSON (1857–1936) who also strongly influenced developments in
physics, theology, and psychology. He was one of the pioneers of statistics and
is known in particular for his correlation coefficient. In 1901 he introduced the
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 271
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_12


================================================================================
PAGE 290
================================================================================

272 12 AFirstApproachtoMachineLearningwithLinearRegression
“regression line” that he obtained from a least square error estimate [5]. Ever since
then, regression analysis became what we know it today (including the analysis of
regression coefficients and the relation to the p-value) and turned into an important
tool for statisticians.
It should be noted that apart from being excellent scientists and in particular
pioneering statisticians, both GALTON and PEARSON also believed in “eugenics”
and the concept of “race hygiene” in Germany under Hitler [4], which casts a slight
shadow of the history of statistics.
Example 12.1 (The Origins of Linear Regression: Galton’s Pea Experi-
ment) (cid:2)
The British anthropologist and meteorologist Francis Galton, a cousin of
the famous Charles Darwin, studied a vast range of topics (see, e.g., [2])
but became particularly well-known for his work on inheritance. In 1875
he investigated how genetic variations from mother plants become manifest
in daughter plants by an experiment: he gave 700 sweet pea seeds to each
of seven friends who had to plant them, grow, and finally harvest a new
generation of peas. Sweet peas are rather easy to grow and self-fertilize which
results in quite a controlled experiment as there is no influence from two
parents to be considered (Fig.12.1).
To analyze his data Galton used a “scatter plot” of the sizes of the daughter
peas versus those of the mother peas. The data is shown in Fig.12.2 where
markers of different sizes indicate how many times a particular combination
of parent/daughter seed size was encountered (however, instead of markers
Galton simply wrote the frequency as a number).
He then computed for each of the seven parent seed sizes the median value1
of all corresponding daughter seed sizes and drew a straight line through
these points—the first regression line was born! The slope is with≈0.3much
.
below 1 from which follows that the diameter of the daughters are closer to
the median (or the mean) than the parent diameters were. For a slope of 1,
parent and daughter diameters are the same. A horizontal line indicates is no
correlation, i.e., the daughter does not inherit the size from the parent.
Galton coined the notion “regression toward mediocrity” which today is in
statistics known as “regression to the mean.”
1Galton, probably for practical reasons, did not use the mean values which would have been
directly related to a least-square fit.
(continued)


================================================================================
PAGE 291
================================================================================

12.1 TheRootsofRegressionAnalysis 273
Fig. 12.1 Sweet peas (or
English garden peas) were
used by F. Galton for his
experiment on inheritance
properties. The image shows
peas in a pod. (©
User:Atomicbre/Wikimedia
Commons/CC-BY-SA-3.0)
Fig. 12.2 Size of the
daughter seeds vs. size of the
parent seeds as observed by
F. Galton
12.1.2 Statistics Versus Machine Learning
Eventually, statisticians’ regression analysis fused into today’s machine learning
regression—or, depending on the point of view, the field of ML borrowed some
of these statistician tools. There is no clear point of time that one could mark as
the birthday of machine learning regression. The history of regression makes it
sometimes difficult to decide if we are talking about the statisticians’ regression
or the machine learning regression and even more so as the methods look nearly
identical on a first glimpse.
Depending on the context, regression can have two slightly different meanings or
objectives, which can make it difficult to see where and what the ML part is. These
are the two different approaches which are explained using linear regression (i.e.,
finding the best fitting line):


================================================================================
PAGE 292
================================================================================

274 12 AFirstApproachtoMachineLearningwithLinearRegression
(cid:129) Linear regression in the spirit of statistical analysis: This is tantamount to
fitting a line to given data (a procedure that also can be generalized to higher
dimensions). “Fitting a line to data” translates into finding the line that fits
best to the data. Putting this into a more mathematical language it implies
to either analytically or numerically minimize some error.2 There, the error
typically consists of the sum of all squared deviations between the given data
and the line, Sect.11.7. Why to s quare the values? It turns out that for almost
all minimization strategies, this is a surprisingly good choice that simplifies the
respective formulations—even more than just using the sum of the absolute
values would do (also see Sect.11.7). This fitting procedure is called least
square (LSQ) approach. The overall goal is to obtain an as small as possible error
between line and given data. As opposed to the ML regression, this approach does
not require any training or testing data.
(cid:129) Linear regression in the spirit of machine learning: This approach has a
different goal while at the same time still using some methods from statistics.
Now, the goal is to make predictions, and the goal is not to obtain the best fit
to given data. Predicting implies that the method can give a result for new data
that has not been involved in the fitting processes. For this, the dataset is split
into a training and a testing dataset (and sometimes also into a validation dataset
as discussed later in Sect.16.2). Then, the objective is to fit the line only to the
training data. So far, we do not know how the model performs with new data
which is why the error will be calculate with the testing data. The purpose of this
ML regression model (or of any ML model) is to obtain a best performance on the
test data.
Unfortunately, the differences are not always made clear in the (online and
offline) literature and might leave the ML novice a bit confused. This becomes even
worse when someone claims that splitting the data into training and testing sets is
what they have always done to evaluate the quality of the fit. Was this then line
fitting or machine learning? You can find this out by asking what the purpose was:
if the purpose was to “prove” that there is a linear relationship between variables
with a certain degree of significance and, e.g., obtain the inclination of the line
and possibly call this “Young’s modulus,” then this was data fitting together with
hypothesis-testing, which clearly belongs into the toolbox of statistics. If the goal
was to create a “model” that represents generic aspects of some dataset and that is
used to make predictions for new data that was not already used for training the
model, then ML is at work.
For example, in GALTON’s experiment, the result of the linear regression was
the slope of the line which was used for comparing it to a “reference slope” of 1,
from which then a conclusion about the correlation of the size of the daughter to
2 Also, the human eye is surprisingly good at finding a good fit of a line to given data—at least as
long as the data is not too “unusual.” In any case, looking at a fitted line and the data is certainly a
good “sanity check”!


================================================================================
PAGE 293
================================================================================

12.2 GeneralConceptsandImportantTerminology 275
the size of the parents could be drawn. If the purpose of the line fitting procedure
would have been to predict the size of new daughter seeds based on a given size
of a parent seed (e.g., someone gave me a pea seed and asks how big the child
seed will become), then we need a model (e.g., a mathematical function) that takes
a parent as input and returns a daughter size as output. Here, the model would be
y =mX+y where the input (or “feature”) is X, the output (or “target”) is y, and
. 0
the model parameter are the slope m and the intercepty . Training the model results
. 0
in fixing these two parameters (here, m=0.34and y =10.1). Making predictions
. . 0
ˆ
means to take a new value of X (note the hat indicates that this is unknown data)
.
and evaluate the model yˆ = h(X ˆ ). The particular, parameterized function h is just
.
an approximation, i.e., our “hypothesis,” as it will be introduced below. The true,
“nature-given” function f is still unknown.
In the context of ML regression, there are a few different notions and concepts
that are not always clearly separated. In the next section, we will define those and
explain their interrelation.
12.2 General Concepts and Important Terminology
We will now go into more detail and introduce all aspects and tasks that are required
for ML regression. In ML there is a whole zoo of different notions and community-
dependent vocabulary. Therefore, we will also in this section introduce all used
notions such that misunderstandings can be avoided or at least easily be spotted.
As already briefly touched upon in the previous section, there are three, general
tasks during supervised regression which are shown in more details in Fig.12.3.
There, the first step is the choice of a specific model class which determines the form
of the mathematical function (e.g., a polynomial of . 4thorder) of the ML model. The
not yet determined parameterw are the model parameter, which we also sometimes
. i
write more explicitly as
h(X ,...,X ; w ,...,w ) or h (X ,...,X ) (12.1)
. 1 N 1 n w1,...,wn 1 N
There, the X are the N variables for which we assume that they play a role in
. i
describing the data. The choice of the model is important as it determines, e.g., its
complexity and the type of behavior that can be represented. Nonetheless, it is a
decision that we (as the modeler) have to make. As often, a good starting point is to
use an as simple as possible model that still captures most of the aspects of interest
of a dataset—another situation which reminds us of Occam’s razor from Sect.1.2.5.
The second step is to find the parameters of the model function h (e.g., the
coefficientsw of the polynomial of choice) such that the function approximates the
. i
given training data well. This is typically an iterative process where the parameters
are adjusted and the resulting training error is monitored simultaneously. The
training error can be simply the sum or squared sum of all deviations between the
target value and the predicted value by h (with the current set of parameters w )
. i


================================================================================
PAGE 294
================================================================================

276 12 AFirstApproachtoMachineLearningwithLinearRegression
Fig. 12.3 Regression consists of three parts: (1) Selecting a suitable model, e.g., a polynomial
function; (2) training the model, i.e., finding a suitable set of parameters .w1,...,wm (e.g., the
coefficients of the polynomial) based on a given dataset consisting of input features and the output
values for each data point; (3) making inference or predicting with the model h where for each
input data the output value will be predicted
(cid:2)N
1
trainingerror= (y −h(x ))2 , (12.2)
. i i
N
i=1
which we will discuss in more details below.
Once a set of model parameters was found that minimize the error between
training data and curve, the model is trained and can be used either for making
predictions (“what is the y value for a giving input X ,X ,...?”) or for inference,
. 1 2
i.e., analyzing the general input-output relationship.
In the following, the most important notions and concepts for ML regression
analysis are introduced. This is one of the fundamental sections that should not
be skipped as the introduced concepts are also of relevance to later chapters.
As regression has its roots in statistical modeling and “parameter fitting,” this
sometimes leads to some confusion concerning the relation between regression,
inter- and extrapolation and “parameter fitting.” Figure 12.4 visually shows the
difference and relation between them, and the next subsections will discuss these
in detail.


================================================================================
PAGE 295
================================================================================

12.2 GeneralConceptsandImportantTerminology 277
Fig. 12.4 Concepts and notions encountered in the context of machine learning regression: (a)
interpolation between given data points with a polynomial; the curve goes exactly through these
points. For any arbitrary point .x∗ ∈ [xA,xB ], one can get an interpolated value .y ∗.(b)usesthe
obtained polynomial to extrapolate the curve and thereby to reach points outside the range of the
given sample dataset. The dashed line in (c) visualized a different concept than the ones shown in
the previous panels. It represents the “true” function, e.g., a physics law, while the markers show
measurements of that function which are affected by noise and inaccuracies. The plot shown in (d)
shows a regression line h (in blue) that was obtained from the training data, here consisting of.7 0%
ofthedatashownin(c). The estimated function h minimizes the deviations to the training data and
serves as an approximation of f
12.2.1 Interpolation
Interpolation is a type of function approximation where a number of data points are
given, and we seek a function that passes exactly through all of the given points;
see Fig.12.4a. These points are also called “support points.” Having to enforce that
the function passes exactly through all points is a strong restriction and only makes
sense if the support points are not affected by noise.
If a polynomial is used as interpolant and N points are given, then the degree of
the polynomial is N −1. As an example, consider two points (N = 2): they define
. .


================================================================================
PAGE 296
================================================================================

278 12 AFirstApproachtoMachineLearningwithLinearRegression
a straight line, i.e., a linear function y = ax1+bwhich has the polynomial degree
.
of 1. Other commonly used interpolation functions are linear functions (as a special
case of a general polynomial) or (piecewise) cubic splines. numpy offers an easy to
use implementation based on the Polynomial class:
Note that there, poly is a Polynomial object that contains already the coefficients
determined during the “fitting” procedure. It also has a number of other useful
properties and methods (take a look at the online numpy documentation [1] for
further details).
12.2.2 Extrapolation
Obtaining values beyond the range of known observations is called extrapolation
and is another kind of function approximation, cf. Fig.12.4b. For given support
points, one can, e.g., construct a polynomial function and use it to predict data
outside the range of the given support points. This is, however, affected by uncer-
tainties, called Extrapolation error, that are becoming larger the further away from
the last support point the extrapolation is. In time series prediction, extrapolation
is an important method where already data up to the present is available. Based on
that, a future value is to be predicted. For extrapolation, the Python code would be
the same as above; only the values of x_interp would need to be changed.
12.2.3 True Functions and Noise
Often, in statistical ML we assume at least implicitly that for a dataset under con-
sideration, there exists an underlying true function h from which the data has been
generated, possibly in conjunction with some statistical sampling procedure. Hence,
one of the goals of an investigation would be to understand some characteristics of
the true function through statistical inference; see Sect.12.2.7. Such a true function
f would be the mathematical representation of, e.g., a physical law which, however,
in general is unknown. Almost always, f is additionally additively superimposed
with random noise (cid:2) which, e.g., could stem from measurement errors or other
.
uncertainties and can be written as


================================================================================
PAGE 297
================================================================================

12.2 GeneralConceptsandImportantTerminology 279
Y = f(X)+(cid:2) (12.3)
.
for the simplest case of a scalar function f , as also shown in Fig.12.4c. If f returns
a vector valued result, then each of the N components j is superimposed with an
. o
individual noise term, (cid:2) :
. j
Y =f (X , ...,X )+(cid:2) with 1≤j ≤N . (12.4)
. j j 1 n j o
Evaluating the function f at X together with the noise would then ultimately result
in the data that we use, e.g., for training a ML model. The trained model can then
help to understand aspects of the true function (see the explanations of inference in
Sect.12.2.7). In Fig.12.4c the true function is indicated as a dashed line and also
shown in Fig.12.4d.
12.2.4 The Training Process and the Machine Learning Model
In the context of ML regression, training of a ML model means to identify a function
h(x) that fits best to the data under consideration (we chose the letter h because
.
below this will be introduced as a hypothesis). This requires that already a functional
form for h has been chosen which defines the general mathematical structure of the
model function. For example, one could choose as function class (which later will
be introduced as hypothesis class) all polynomials of the shape
h(x)=w +w x+w x2+···+w xn . (12.5)
. 0 1 2 n
Training then determines the set of parameters {w ,w ,...,w }such that h(x)fits
. 0 1 n .
best to the training data. To differentiate between the general functional form with
unknown parameters and a function with already determined parameters, one may
also write for the latter h(x; w ,w ,...,w )which is pronounced as h evaluated
. 0 1 n
at x for the given parameters w ,....
. o
For a visualization see Fig.12.4c where the solid line shows the trained model
which approximates the given training data. However, the goal is that the trained
model not only approximates the given training data but that it captures the
relevant aspects of the underlying true function such that the trained model h is
a reasonable estimate of the true function f. The training error results from the
vertical “deviations” of h and the noisy data; typically, this error is computed as the
mean squared error (MSE) (cf. Sect.11.7.3).
For training such a model, only the line of Python code poly = Polynomial.
fit(...) from the code used for interpolation in Sect.12.2.1 would be required


================================================================================
PAGE 298
================================================================================

280 12 AFirstApproachtoMachineLearningwithLinearRegression
where the four output numbers are the coefficients of the polynomial. For example,
the “4.” corresponds to the constant term of the polynomial, and the corresponding
.
polynomial is 4−3.375x+x2+6.375x3.
.
12.2.5 Hypothesis/Hypothesis Space and the Parameter Space
We now introduce notions that originally stem from the rather theoretical field of
inductive logic programming and which are regularly encountered in the literature,
sometimes without being well-defined, though: the hypothesis,theh ypothesis space,
and the parameter space (see, e.g., [3])
A hypothesis in the context of ML denotes something that is nearly identical to a
trained model. To see the difference, let us first take a look at the definition of the
hypothesis space:
Definition 12.1 (Hypothesis Space) The hypothesis space Hconsists of all
.
functions h that map an element x from the dataset to a target value or label,
. i
y . The functional form or mathematical structure of these functions is fixed,
. i
and two different hypotheses, taken from the hypothesis space, differ only in
the particular choice of parameters.
Usually, for the hypothesis space, the letter His used. An example for His a cubic
. .
polynomial with parameters w ,...,w . There the hypothesis space consists of
. 0 3
cubic polynomials that differ only in the particular numerical values of the four
parameters w . Closely related is the parameter space which is defined based on
. i
elements of the hypothesis space:
Definition 12.2 (Parameter Space or Weight Space) The set of all param-
eter combinations used for the functions contained in the hypothesis space are
spanning the parameter space which is also sometimes called weight space.
In the next section, we will see how the parameter space can be used as a way to
visualize aspects of the hypothesis space.
How does this relate to machine learning? The ML algorithm chooses elements
of the hypothesis space and determines the most suitable hypothesis—the trained
model. However, the notions of a model and a hypothesis have slightly different
connotations: “model” often implies the concrete implementation, while “hypothe-
sis” implies that we have an idea about an underlying (usually) mathematical form
of the input-output relation that we express through the hypothesis h. In particular,
when we are talking about “using a hypothesis h to approximate the true function


================================================================================
PAGE 299
================================================================================

12.2 GeneralConceptsandImportantTerminology 281
f ,” then the word hypothesis suggests that it can be tested and that there even might
be a connection to an underlying theory.
12.2.6 Predictions
A sometimes misunderstood or confusing notion is that of prediction. In the context
of mathematical function approximation, prediction is often used synonymously
with extrapolation. In Fig.12.4b the dashed lines show such “predictions” of how
the function would continue outside the region defined by the support points.
However, in the context of ML the notion of prediction is used differently: there,
it means to use the trained model (the estimated function h) for computing the
dependent variable based on given input data that has not been used during the
training process. Typically, this implies that the values of x used for prediction
is roughly in the same range as the training data and the emphasis is not on
extrapolation. As this is a crucial point, we highlight it here again.
Words of advice (cid:2)
In the context of ML you will often encounter phrases such as “the performance of a
model is estimated using new data” or “we make predictions for previously unseen
data.” This doesnot mean that with a ML regression model, we can ask for a prediction
for any input data.
If the regression model has been trained with features in a certain range of input
values, e.g., from within.xA,...,xB , then typically we still don’t know much about the
behavior of the data outside that range, and we cannot (or only in a very limited way)
make predictions for completely new data that are outside.xA,...,xB .
12.2.7 Inference
The notion of inference is not always used consistently across different communi-
ties. For example, in the deep learning community, inference is sometimes used as a
synonym for making predictions because “inferring a value” is mostly interpreted as
“predicting a value.” However, in this text, we take a perspective that is influenced
by the statisticians view:
Definition 12.3 (Inference) Inference is understood as a concept quite dis-
tinct from prediction. While the goal of prediction is to use a trained model
to estimate the value of an output variable for a given input, inference has its
origins in the field of statistics and refers to trying to understand (aspects of)
the relation between the feature (i.e., the input of a model) and the target (i.e.,
the output) of a model.


================================================================================
PAGE 300
================================================================================

282 12 AFirstApproachtoMachineLearningwithLinearRegression
Typical questions that are answered during making inference are questions that
elucidate structural or mathematical aspects of the model, e.g., how does the output
change when the whole input or a specific feature changes in a specific way? Is
there a linear relationship or is it nonlinear? Which are the relevant features? As the
model represents relevant aspects of the physical reality, inference often also helps
to get insights into aspects of the underlying physics. It is thus quite different from
prediction, which boils down to evaluating a function for a given input value.
Most of the regression models used in statistical ML are suitable for making infer-
ence because there, at least roughly speaking, the model essentially is tantamount to
a mathematical function which can be easily interpreted in terms of mathematical
aspects but also in terms of physical implications.
12.2.8 Under-and Overfitting
Underfitting and overfitting are two notions that you will encounter frequently in
ML because they are directly related to the quality of a ML model. Underfitting takes
place if the fitting function that was used as approximation is too stiff and is not able
to adapt sufficiently to the data. If the fit function mostly captures the noise of the
data and not so much the general trend of the data, as a very flexible curve would
do, then overfitting occurs. The concept of under- and overfitting is visualized in
the left and right panel of Fig.12.5. A good approximation in the sense of a good
ML model is a model that is able to ignore the irrelevant fluctuations or noise and
in which the relevant behavior of the underlying physics becomes visible, as shown
in the middle panel of Fig.12.5. What is relevant and what is not can be difficult to
judge, but below we will at least learn some techniques that help us to determine an
optimal degree of line stiffness.
Fig. 12.5 The concepts of underfitting and overfitting: The line in the left panel is too stiff and
does not sufficiently capture the relevant details, it underfits the data. The right panel shows
overfitting where the too flexible line meanders through every data point and also irrelevant details
of the particular data distribution are reflected line. The middle panel shows an approximation that
is neither too stiff nor too flexible


================================================================================
PAGE 301
================================================================================

12.2 GeneralConceptsandImportantTerminology 283
12.2.9 Notations for Input and Output Variables
In this chapter, we adhere to the notations and conventions already introduced in
Chap.3. Here we only briefly recall these notations and will put them into the
context of ML regression.
(cid:129) Input variables (e.g., the x in h(x)) are often also called independent variables,
.
features,orjustv ariables, all of which will be used in this text. In the regression
literature, they are sometimes also referred to as predictors or explanatory
variables.
(cid:129) If there is only one input variable, we denote it by a capital X. If there are multiple
features, we use X for the i-th feature.
. i
(cid:129) Typically, a dataset consists of several data records or samples with several
features. In that case X is the name of the i-th feature and denotes a column
. i
vector where each entry corresponds to a sample of the input variable i. Even
though that this is in fact a column taken from a matrix of input data and vectors
in general are indicated by boldface letter, in this case, the character of the
variable corresponds to a scalar value, and we denote the i-th feature by X with
. i
X =[X ,...,X ]T . (12.6)
. i 1i mi
where m is the number of rows.
(cid:129) The output variable of a ML model is Y, i.e., the numerical value that we seek
to predict. It is also called label or target. Similar to the input variable, it may
consist of several components in which case we use indices, e.g., Y .
. i
(cid:129) A single data record i consisting of values for all features is written as lowercase
x , similarly y for the record i from the outputs.
. i . i
(cid:129) In this chapter we typically have a single(cid:3) or multiple (e.g(cid:4)., n) features and only
a single output. Then the pair (x ;y )or x ,...,x ;y are the “coordinates”
. i i . i1 in j
of the j-th data point for the feature number i. Each of such pairs is called an
instance or an example.
(cid:129) The training datasetD trainis then the set of all N training instances
.
Dtrain ={(x ,y )}N , (12.7)
. i i i=1
the testing datasetD testis defined analogously.
.
12.2.10 Training and Testing: Datasets and Errors Types
One of the important concepts in ML is to split the whole dataset into a training
dataset and a testing dataset (and sometimes even a validation dataset as introduced
in Sect.16.2 which is required for cross validation). The train-test split is sometimes
also called holdout method. It divides the whole dataset into two disjunctive subsets,


================================================================================
PAGE 302
================================================================================

284 12 AFirstApproachtoMachineLearningwithLinearRegression
the training and testing dataset, where each subset was obtained from statistical
sampling of the whole dataset (for other testing strategies, please refer to Sect.16.2).
The purpose of the training dataset is to train the ML model.
The trained model can be used to make predictions for new data, including the
testing data. Such “new data” would consist of only the features, and the prediction
would give the target values. The purpose of the testing dataset is to enable an
estimate of how good the predictive capabilities (i.e., predicting target values) of
the model are and how well the model “generalizes,” i.e., how well it works with
new data. It is important that this estimation is done with data that has not been
used during the training process because otherwise the model might have already
“memorized” the data. Figure 12.6 shows the train-test split along with the training
and testing errors for each point. For computing the overall training error3 of all N
data points, one could use the MSE of the output of a particular . h(x)and the training
data such that
(cid:2)N (cid:5) (cid:6)
1 2
trainingerrorE = h(xtrain)−ytrain (12.8)
. N i i
i=1
where . h(xt i rain) uses the trained ML model h to predict the output for the given
training inputXtrain. The target valuesytrainare the output used for training. During
. . i
the training process, the training error . E (typically the MSE as also in the equation
above) is minimized, e.g., by iteratively adjusting the model parameters. The testing
error of all M testing data points is in analogy to Eq.(12.8) given by
(cid:2)M (cid:3) (cid:4)
1
testingerrorE = h(xtest)−ytest 2 (12.9)
. M i i
i=1
Here, h has already all parameters fixed from the training and is only evaluated at
the given data xtest, which we above introduced as making predictions.Below,we
. i
will discuss in details how different levels of training and testing error values can be
interpreted.
To observe the influence of overfitting on the training and testing performance,
we use the same dataset as in Fig.12.6, but this time use a very flexible model,
as shown in Fig.12.7. Even though the flexible curve seems to fit perfectly well
to the training data (middle panel of Fig.12.7), the curve necessarily will exhibit
larger deviations from the testing data (right panel of Fig.12.7); it is said that the
model does not generalize well because it is too specialized toward a particular
dataset. This often also implies that the model is fairly complicated (in the figure,
a polynomial of degree 13 was chosen as compared to a quadratic function in
Fig.12.6). We can also compute the resulting training and testing MSE values: for
3 In Sect.12.3.3, we will see that there are different types of “errors,” but for now we use error just
as a synonym for any sort of deviation.


================================================================================
PAGE 303
================================================================================

12.2 GeneralConceptsandImportantTerminology 285
Fig. 12.6 The left panel shows the full dataset that has been split into a training and a testing set.
Only the training dataset was used to fit a polynomial function, resulting in the trained model h.
In the middle panel, the dashed lines indicate the deviations for each point that altogether results
in the training error. To evaluate how this model performs with new, previously unseen data, the
testing error was evaluated in the right panel, consisting of the distances of the testing data to the
model response using h
Fig. 12.7 The same dataset as in Fig.12.6 is used together with a much more flexible polynomial
that fits the training data quite nicely but introduces larger errors for the testing data
the balanced model, we have .MSEtrain = 1.1 and .MSEtest = 2.7 both of which have
the same order of magnitude, while for the flexible model we obtain .MSEtrain = 0.1
and .MSEtest =7098 which are off by nearly five orders of magnitude!
Therefore, usually the best model is the one that is as simple as possible and
which at the same time also shows a good generalization behavior. If the model is
too flexible it does not generalize well to the testing data. And what happens if the
model is too simple? Then both training and testing errors will be large. We will
investigate this behavior more systematically in the sections below.
Typically, the training dataset is larger than the testing dataset, e.g., 70% or 80%
.
of the dataset are chosen for training; the remainder is used for testing. A simple
Python implementation of splitting a dataset is given in Python Listing 12.1. This
function takes the feature and target matrix as input together with the fraction of
the data to be used as training data. A fourth function parameter allows to specify
a seed for the random number generator (line 10): specifying an integer number
ensures that the random shuffling in line 15 is reproducible. Without the seed, new
(pseudo) random numbers are generated for every call of this function. In line 14,


================================================================================
PAGE 304
================================================================================

286 12 AFirstApproachtoMachineLearningwithLinearRegression
Python Listing 12.1 Python function for performing the train-test split. Note that contrary to all
best practices in programming, we omitted docstrings, comments, type hints, and “sanity checks”
to ensure that the code stays compact
the implementation uses numpy.arange to create an array with numbers ranging
from 0 up to (and not including) number of samples. Each of these numbers will be
an index to an element in the arrays X and y . By randomly “shuffling” the indices
in line 16 and 17, we can therefore obtain random samples from both X and Y such
that each element of X still corresponds to the same element of y . Last but not least
in lines 19–22 we use the first n_training rows as training data and the remainder
as testing data. It follows a short example that shows how to use the function. There,
we assume that the function has already been imported and is available. We start by
defining a dataset consisting of one-dimensional array for X and Y:
Subsequently, the function for performing the train-test split is called. If the function
is called with always the same integer number for the seed, then the random
number generator always results in the same sequence of random numbers. As a
consequence, the train-test split will always result in the same datasets which is
useful, e.g., for debugging code:


================================================================================
PAGE 305
================================================================================

12.3 SimpleLinearRegression 287
And finally, we print the different datasets:
This code can be used as a starting point for some further experiments such as
plotting the training and testing data, varying the number of data records or the
percentage of training and testing data, and observing how representative the testing
dataset is for the training dataset.
Things to Remember about concepts and terminology (cid:3)
(cid:129) Beware of the distinction between interpolation and extrapolation, predic-
tion and inference, and functions, models, and hypotheses.
(cid:129) Also beware that some notions might be used in other communities slightly
differently or with slightly different connotations.
(cid:129) Using training and testing data is essential for machine learning and allows to
compute training and testing errors that help to differentiate between “good”
and “bad” models.
(cid:129) The best model is one that is as simple as possible and that still has a good
generalization performance w.r.t. to new data, such as testing data.
12.3 Simple Linear Regression
In this section we start by introducing the simplest type of regression models: the
problem under consideration is called simple linear regression if it has only one
scalar input variable and also returns a scalar variable as output. If the problem has
more than one independent variables, it is called multiple linear regression. The
attribute linear indicates that the model is a linear function of the form
h(x; w ,w )=w x+w (12.10)
. 0 1 1 0
There, w and w are the parameters to be learned from the data, also called
. 1 . 0
coefficients. As this equation represents the equation for a line, w is also called
. 0
intercept (i.e., the intercept of the line with the y-axis), and w is the slope of the
. 1
line.


================================================================================
PAGE 306
================================================================================

288 12 AFirstApproachtoMachineLearningwithLinearRegression
Fig. 12.8 Linear regression
with a single feature variable
and a single target variable:
for given data, we seek the
line that fits best to the data
The Link to Neural Networks (cid:3)
In the context of neural networks (cf. Chap.17), w would typically be called
. 0
bias b.
Linear regression essentially fits a line to a set of given data points. Thereby, the
whole dataset is approximated by a simple line, cf. Fig.12.8, but not necessarily a
straight line, as discussed in the following section. Such a line is useful for showing
trends in datasets and for emphasizing, e.g., the position of outliers. In the context
of ML, however, we are rather interested in using the function of the line for making
predictions.
12.3.1 The Linearity in Linear Regression
One might ask if a linear model isn’t extremely limited in terms of the phenomena
that can be represented. Luckily, linearity is not related to the shape of the curve.
In a linear model, only the parameters have to occur in a linear form; however,
variable(s) may occur as arbitrary terms. Unfortunately, due to historical reasons,
the literature is full of figures and plots showing straight lines which suggest that
this is where the attribute “linear” stems from. Hence, the differentiation between
nonlinear and linear models and their meaning might be somewhat counterintuitive
because in ML, unlike in common mathematical functions, the (non)-linearity is an
attribute of the parameters and not the variables. To make this a bit more clear, here
are some brief examples:
This has the consequence that a linear model is not only able to predict linear
relationships between features and the target variable (e.g., a straight line or
a plane in one-dimensional or two-dimensional feature space, respectively) but
also nonlinear behavior—at least as long as the weights are just simple, linear


================================================================================
PAGE 307
================================================================================

12.3 SimpleLinearRegression 289
(cid:129) h(x) = w1x + w0 ...isalinearmodelbecause each wi
occurs only as a factor.
(cid:129) h(x) = w1x2 + w0 ...isstillalinearmodelasbothparameters(i.e.,
the “coefficients”) occur as linear terms,
regardless of the x2 term.
(cid:129) h(x) = (w1)2x + w2 ...isa nonlinear model because w1 occurs as
quadratic term.
(cid:129) h(x) = w0 cos (w1)x + w0 ...alsois nonlinear as the coefficient of x is a
product of w0 and the already nonlinear
function cos(w1).
coefficients. Section 13.1 will go into more details concerning nonlinear model
behavior.
12.3.2 Hypothesis Space and Parameter Space
After the model type has been determined, each combination of parameters results
in a particular hypothesis about the true function. Here, we assume that our design
choice led to the linear equation Eq.(12.10). Then the hypothesis space H (cf.
.
Definition 12.1) contains all linear functions h(x) with particular values of w and
. . 0
w :
. 1
H={h(x) = w x +w |w ,w ∈ R} . (12.11)
. 1 0 0 1
The set of all parameter combinations(w ,w ) spans the parameter space which
. 0 1
is sometimes also called weight space. As the functional form of all contained
hypotheses does not change, the parameter space can be used as an alternative to
the hypothesis space which can be more easily visualized than the hypothesis space
itself. A demonstration is shown in Example 12.2.
Example 12.2 (Linear Regression: Data Space and Weight Space) (cid:2)
Given is a training dataset which is also shown by the markers in the instance
space (left panel):
X =[−1.06, −0.83, −0.08,0.04,0.58,1.33,1.81,2.26]
.
Y =[−0.70,−1.37,−0.28, −1.57,0.48,−0.67, 0.69,0.87].
.
Additionally, a number of lines with different parameters are used to
approximate the underlying true function of the dataset. The four lines are
(continued)


================================================================================
PAGE 308
================================================================================

290 12 A First Approach to Machine Learning with Linear Regression
shown in different gray colors. For each of the four functions, there is a
corresponding point in the parameter (or weight) space (right panel) where
each point represents a particular hypothesis (Fig.12.9).
Fig. 12.9 Instance Space and Weight Space. Left: The markers are training data; the lines
represent different approximations to the training data. Right: Each point in the weight
space is a particular hypothesis, and the marker colors correspond to the line
colors in the data space
The weight space is also a way to connect to physical knowledge that we already
may possess about the investigated situations, e.g., there may be physical constraints
restricting the range of the parameters which then also can be directly visualized in
the parameter space. In Example 12.2 we additionally introduced the notion of the
instance space which is sometimes also called data space: it is the space in which
the whole dataset “lives.” With these definitions of different spaces, we are now able
to precisely express what kind of quantity and relation we are referring to. Before
we turn to estimating which hypothesis is most suitable to represent the data, we
briefly summarize the new concepts and notions:


================================================================================
PAGE 309
================================================================================

12.3 Simple Linear Regression 291
ThingstoRemember about simple linear regression and hypothesis space(cid:3)
(cid:129) a hypothesis is a particular model function where the weights are determined
and fixed;
(cid:129) the hypothesis space often contains infinitely many functions and is therefore
usually not suitable for practical purposes;
(cid:129) the concept of parameter/weight space is useful for compact visualization of
the hypotheses and can also visualize physical constraints of the parameters.
(cid:129) despite the name, linear regression models can model nonlinear relations,
the linearity concerns only the weights;
12.3.3 Estimating Deviations: Errors and Residuals
We now turn to the following questions: Given a specific hypothesis, how can we
determine the “goodness” of the fit to the data? Or How can we determine the
weights such that they minimize an error?
To answer that, we first of all need to clarify the concept of errors: What our
common sense has called “error” can, in the context of statistics, be further specified,
as there exist two closely related error concepts. The statistical error is a measure
for the whole population and not just the sample (e.g., our training dataset). In
practical situations, this quantity usually cannot be computed, because we don’t
have exact knowledge about the whole population. The Residual error, on the other
hand, is based on the sample data only and thereby serves as an estimate for the (non-
observable) statistical error. This difference becomes particularly relevant when we
use a trained model as approximation for the true behavior. Coming back to the
concrete, simple regression problem, we can define the residual as follows:
Definition 12.4 (Residual or Residual Error) The residual r of a data
. i
record i is defined as the difference between the actual and the predicted target
value
r = y −h(x ; w ,w ), (12.12)
. i i i 0 1
or for general numbers of weights and features
r = y −h(x ; w). (12.13)
. i i i
The prediction is done based on a hypothesis function h, and y is a given
. i
target value (which can be easily generalized to multiple target values).


================================================================================
PAGE 310
================================================================================

292 12 A First Approach to Machine Learning with Linear Regression
We observe that the residual does not involve the true function f. While at least
a statistician would only denote the difference between f(x ) andy as error,inthe
. i . i
ML community this distinction is only rarely made as the true function is generally
unknown. Note that this residual formulation takes only the distance of the target
values into account, which is here the vertical distance from the training point to the
line.
About the “distances” used in residuals (cid:3)
There are other approaches than using the vertical distance, e.g., in Sect.15.2
we discuss the method of principal component analysis that on a first glimpse
has some similarity to regression analysis but that uses an Euclidean shortest
distance as measure as for the quality of a fit approximation.
12.3.4 The Loss Function and the Loss
The loss function, similar to the residual error, characterizes how good or bad
the agreement is between a prediction and the given target value. We define it as
follows:
Definition 12.5 (Loss Function and Loss) The loss functionL is a function
.
acting on each record of the dataset which characterizes the amount of
agreement that was lost. It results in one number, the loss valueL ≥0, where
.
a value of zero indicates perfect agreement between predicted value and given
value (the ground truth).
Here are some examples for commonly used loss functions:
Labs(wD| train) =|r |=|y −h(x ; w)| ...absolute loss
. i i i
Lsqr(wD| train) =(r )2 = (y − h(x ; w))2 ...squared/quadratic loss
i i i
Lhinge(wD| train) = max (0,1− y h(x ; w)) ...hinge loss .
i i
For regression problems, the squared loss is the most commonly used form, while
the hinge loss occurs in classification problems using support vector machines. It is
listed here for the purposes of showing that the loss function can but doesn’t have to
be based on the residual.
Choosing a suitable function is important as it has to be “compatible” with
the goal of the training and the problem at hand. For the ML regression problems


================================================================================
PAGE 311
================================================================================

12.3 Simple Linear Regression 293
considered in this chapter, however, using the quadratic loss function works well.
Section 11.7 has an overview and a discussion of different types errors and
deviations.
12.3.5 The Cost Function
The cost function (represented by a calligraphic “J”) quantifies how good (or rather
how bad) the whole set of target values agree with all corresponding predicted
values. Here is a definition.
Definition 12.6 (Cost Function) The cost function (or error function) J
.
maps all given data points to a single, real number J ≥ 0. It quantifies the
.
“badness” of the overall agreement of a particular model approximation or
hypothesis, where larger numbers indicate worse agreement. Typically, it is
the mean value of all N individual loss values of a dataset:
. s
1
(cid:2)Ns
J (wD| train) = L(w|x ,y ) (12.14)
. i i
N
s i=1
Unless stated differently, throughout this chapter, we assume thatL is the
.
quadratic loss such that
.
J is obtained as the MSE.
The goal of the training is to minimize the cost function through the best choice
of parameters w . Using a loss functionL that is always ≥ 0 is advantageous, as
. i . .
then loss contributions for different records do not cancel each other out. Using a
squared loss as compared to the absolute loss has the additional benefit that the
function’s minimum is differentiable, resulting in a mathematical form that later on
becomes much simpler than those from essentially any other formulations.4 Note
that this formulation is nothing but the training MSE. Using such a squared loss
function for finding best fitting parameters lent itself to the name of the well-known
least square least square (LSQ) method.
4 Hint: For finding the minimum, we have to take the derivative. Then, the the squared residuals
simply becomes just the residual—a very simple function.


================================================================================
PAGE 312
================================================================================

294 12 A First Approach to Machine Learning with Linear Regression
Words of advice (cid:2)
In the deep learning community, the two notions loss and cost are not always used
as we do it here, and the “training loss” in fact denotes the average training loss of
a batch, i.e., the cost. Sometimes the loss function also denotes what we introduced
above as residual; the cost function however always denotes the function that maps
all given data points to a single real number that represents the “badness” of the
current approximation or assumption.
Last but not least, there is another related notion that of the objective function
which stems from mathematical optimization theory and is the most general term
for any function that can be optimized. It can be the same as the cost function, but it
sometimes also contains an added regularization term for numerical reasons. In this
text, we will mostly avoid to use the notion “objective function.”
We are now ready to return to the problem of simple linear regression. To find
weights for which the regression line fits best to the data, the cost function should
be minimized for the given training data and with respect to the weights
J (w ,w |D train)→MIN , (12.15)
. 0 1
in which the notation already indicates that the weights are now considered as the
“variables” and the training data are given and fixed. Using the average of the
squared residuals is identical to computing the mean squared error (MSE), and the
training cost function takes the following form:
(cid:2)N (cid:2)N
1 1
J (w ,w |D train) = (y − h(x ; w ,w ))2 = (y − (w x +w ))2 ,
. 0 1 i i 1 0 i 1 i 0
N N
i=1 i=1
(12.16)
where the training dataset consists of the N data points (x ,y ) with 1 ≤ i ≤ N.
. i i .
Figure 12.10 shows how changing the inclination w of a line changes the error
. 1
value. In particular the right panel shows that the cost function has a distinct
minimum.
In analogy to the mean training loss, we can also obtain testing cost function.
However, now the weights are considered as fixed and are taken as the ones that
resulted from the training. Then, the testing cost function is defined as
(cid:2)N (cid:3) (cid:4)
1
J (w ,w ,D test) = ytest − h(xtest ,w ,w ) 2 . (12.17)
. 0 1 N i i 1 0
i=1
To minimize the training error J(w ,w | D ), a number of different
. 0 1 train
approaches can be used. A “brute force” approach is the so-called grid search
method where systematically all relevant parameters are varied, e.g., by subdividing
each parameter range into n values, until the minimum is found. In all generality,


================================================================================
PAGE 313
================================================================================

12.3 Simple Linear Regression 295
Fig. 12.10 Lines with different inclinations.w 1 (top left) and the cost function for the ideal, fixed
value .w 0 and varied w1. The cost values of the straight lines are indicated by the four markers
for a method with k weights, this results in nk combinations (see Sect.5.1.6)—a
.
number that very quickly cannot be handled anymore.
A general approach that is more efficient than a grid search and also applicable
for many other ML problems is to perform a numerical minimization where the
values of the parameters are determined iteratively. Mathematically speaking, a
necessary condition for finding a function minimum is that the first derivative is
zero. The first derivative of Eq.(12.17) is a derivative w.r.t. the weights
∂J (w ,w |Dtrain) ∂J (w ,w |D train)
0 1 = 0 and 0 1 = 0 . (12.18)
.
∂w ∂w
0 1
Insertion of Eq.(12.16) and replacing the hypothesis h with ypred gives
.
∂J(w ,w |D train) 2 (cid:2)N (cid:5) (cid:6) 2 (cid:2)N (cid:5) (cid:6)
0 1 = y − (w x + w ) (−1) =− y − ypred
. ∂w N i (cid:7) 1 i(cid:8) (cid:9) 0(cid:10) N i i .
0 i=1 =h(xi ; w1,w0) i=1
(12.19)
∂J(w ,w |Dtrain) 2 (cid:2)N (cid:5) (cid:6) 2 (cid:2)N (cid:5) (cid:6)
0 1 = y − (w x +w ) (−x )=− y −ypred x
∂w N i (cid:7) 1 (cid:8)i(cid:9) 0(cid:10) i N i i i
1 i=1 =h(xi ;w1,w0) i=1
(12.20)
These expression can then be used with a numerical method to iteratively determine
the values of the weights for which these two equations are zero. This will be
introduced in Sect.12.3.7.
For the particular linear problem considered here, it is also possible to obtain
the result using a direct mathematical derivation which is computationally fast
and also useful for validating the numerical approach. Therefore, in the following


================================================================================
PAGE 314
================================================================================

296 12 A First Approach to Machine Learning with Linear Regression
subsections, we start with a closed form derivation for minimization of the cost
function before the numerical approach will be explored.
12.3.6 Cost Function Minimization: Analytical Derivation
To find the parametersw andw that minimize the cost functionJ(w ,w |D ),
. 0 . 1 . 0 1 train
we take the first derivative with respect to the first and the second parameter and look
for the parameter values that result in zero. For findingw , this equation then reads:
. 0
∂J (w ,w |D )
0 1 train = 0 . (12.21)
.
∂w
0
Insertion of the quadratic loss function from Eq.(12.16), we obtain
(cid:2)N
1
2 (y − (w x +w )) (−1) = 0 , (12.22)
. i 1 i 0
N
i=1
(cid:11)where the summation is over all items of the training dataset. Realizing that
N x /N is the mean value of the x and therefore replacing it with the
. i=1 i . i
abbreviation x¯, we get
.
−y¯ +w x¯ + w = 0 (12.23)
. 1 0 .
⇔ w =−w x¯ +y¯, (12.24)
0 1
where onlyw is unknown. Therefore, we again compute the first derivative, but this
. 1
time with respect to w , set the resulting term to zero, and also insert the previous
. 1
result:
∂L(w ,w |D )
0 1 train =0 (12.25)
. .
∂w
1
(cid:2)
1
⇒ (y − (w x + w )) x =0 (12.26)
i 1 i 0 i .
N
(cid:2) (cid:2) (cid:2)
⇔ y x − w (x )2 − w x =0 (12.27)
i i 1 i 0 i .
(cid:2) (cid:2)
⇔ −w (x )2 − (−w x¯ +y¯)N x¯ =− x y (12.28)
1 i 1 i i.
(cid:5)(cid:2) (cid:6) (cid:2)
⇔ −w (x )2 −N x¯2 =− x y +x¯y¯N (12.29)
1 i i i .
(cid:11)
x y −x¯y¯N
⇔ w = (cid:11) i i (12.30)
1 (x )2 −N x¯2
i


================================================================================
PAGE 315
================================================================================

12.3 Simple Linear Regression 297
(cid:11)
where we used that x = N x¯ with N the number training data. These results for
. i
w andw can now be used to specify the hypothesis Eq.(12.10) that minimizes the
. 0 . 1
MSE error for the particular training datasetD . train . . h(x) can now be easily used with
the fixed weights for making predictions (note that the weights don’t show up as
argument of h anymore!), e.g., for a new and arbitrary value x
∗
the corresponding
.
output is h(x ∗ ) = w x ∗+ w .
. 1 0
We conclude this section by mentioning that it is possible to derive a general
closed form solution which is valid for all linear regression problems—regardless of
the number of features or the type of hypothesis. This requires a more complicated
derivations and linear algebra which will be presented in Sect.13.2. The numerical
approach in the following section is an approach that is applicable not only for linear
models but is a prerequisite for essentially all, more complex machine learning
problems.
Things to Remember (Closed form solution for linear regression) (cid:3)
The trained model is given by the function.h(x) = w1x+w0 where the two weights
that minimize the cost can be obtained as closed form solution:
(cid:11)N
xiyi −x¯y¯N
.w 1 = i (cid:11)N =1 and w0 =−w1x¯ +y¯ .
(xi)2 − N x¯2
i=1
There. x¯ and.y ¯ indicate the mean values of all x and y values of the training dataset,
and N is the number of training examples.
12.3.7 CostFunctionMinimization:TheMethodofGradientDescent
Here, we focus only on simple linear regression and one of the most simplistic
numerical minimization methods. Finding the minimum of a function is a ubiquitous
task in math, science, and engineering. Most of the time, however, it is not possible
to derive a closed form solution, which is where numerical minimization strategies
come into play. One of these numerical methods is the gradient descent method.
Even though it is a method that does not perform sufficiently well for any real
problem, it can be well used to explain the general methodology and how numerical
minimization strategies can be used in the ML context.
To keep the notation as accessible as possible, the gradient descent method is not
introduced in a general n-dimensional space but rather already formulated for the
specific regression problem, i.e., in terms of the weights and the cost function. In the
subsequent subsection, we then apply the method to the problem of linear regression
with one feature.


================================================================================
PAGE 316
================================================================================

298 12 A First Approach to Machine Learning with Linear Regression
Problem Formulation
Let us start with the general problem formulation: the problem at hand is a
minimization problem where we seek to find the minimum of a function.5 There
is a whole mathematical branch dedicated to such minimization problems, where
often the type of problem is further restricted to convex function minimization . As
an important property, this implies that the function has no more than one minimum
and no maximum, which is quite convenient for mathematical reasons. A quadratic
function (as shown in Fig.12.10) is convex; the function shown in Fig.12.11 on
the left is non-convex. A key element in mathematical optimization is the so-called
cost function or objective function, shown in the left panel of Fig.12.11, written as
J (w ,w ). This is the function to be minimized which represents the deviation
. 0 1
between a desired property and an actual property. For our problem this is the
sum of the (squared) distances of the training data from the line for a given set
of weights. The objective is to find the optimal set of weights such that this residual
error is minimized. If the mean loss were zero, we would have reached a perfect
fit. However, in reality the error usually cannot become zero, and we are satisfied if
it reaches some minimum value. Last but not least, most problems are non-convex,
and therefore the method of steepest descent is only able to find a local minimum
and not the global minimum (i.e., the overall minimum of the whole considered
domain). There exist other methods that are at least partially able to resolve this
problem that will be introduced when they are required in the next chapters.
Fig. 12.11 Error as a function of the weight space (left) with trajectories of the steepest descent
method. The different initial positions are the large markers labeled “A”, “B”, and “C”. The
minimization procedures result in weights that locally minimize the cost function, but only one
of them ends up in the global minimum. The right panel gives an idea about the “speed” with
which the error landscape is traversed. It also compares the MSE magnitude reached at the end of
the iterations. As step size a value of .η = 0.01 was used. The iterations were stopped when the
change between two subsequent MSE values was below.10−7. The small round markers are plotted
every 20 steps
5 In case that we are looking for a maximum, we still can use the whole “minimization machinery,”
and only the sign of the cost function needs to be changed.


================================================================================
PAGE 317
================================================================================

12.3 Simple Linear Regression 299
Motivation of the Gradient Descent Method
Taking a look at a hypothetical two-dimensional error landscape J (w ,w ) in the
. 1 2
left panel of Fig.12.11 we already can intuitively develop a straightforward idea for
finding the minimum: starting from some point in the weight space, we move always
perpendicular to the contour lines of equal error values, i.e., into the direction that
most effectively reduces the error—in other words, the negative gradient of the cost
function, ∇J (w ,w ; Dtrain). A “refresher” and a brief introduction into calculus
. 0 1
including the gradient operator ∇ can be found in Appendix A.1).
.
This approach is rather appealing as it can be comprehended intuitively. For
example, an often used hypothetical situation used as explanation is the following:
Assume that you are hiking the mountains and that you got lost. All you know is that
the cabin is at the bottom of the valley. As it has also gotten dark you can’t see very
far. But you can see (or rather feel) how steep the soil under your feet is. If you’re
always going into the direction downhill of the steepest slope, then eventually you
will find a point from which you can’t get further down and you hopefully found the
cabin.
Algorithmic Formulation and Numerical Aspects
The method of gradient descent (or: steepest descent) can be stated more formally
as shown in Algorithm 12.1.
During the initialization in 2. a start position in weight space needs to be
chosen. Furthermore, because gradient descent is an iterative numerical method,
a few numerical parameter need to be defined: (i) a parameter for checking for
convergence, TOLERANCE; (ii) a parameter for deciding how many steps we
would be willing to try at maximum before we entirely give up, MAX_STEPS;
and (iii) the learning rate η, i.e., a factor that scales the size of the steps taken.
.
Finding a new position in weight space is done incrementally based on updating
the old position by an increment (in 4.). For obtaining the increment (in 3.), the
gradient of the cost function w.r.t. the weights is required (cf. Eq.(12.19) and
Eq.(12.20)) which gives the direction of the steepest slope. This, however, is the
direction of the steepest ascend. Thus, in 4. the “-” sign is required to get the
direction of the steepest descent. Additionally, the scaling factor η > 0 which for
.
ML problems is called learning rate determines for how far we proceed along this
direction. The update in 4. will be done as long as there is a significant change
in the weights during the update steps or until the maximum number of allowed
steps are reached. How big a significant change is, is determined by the constant
TOLERANCE. Here, this is an absolute value, e.g., a value of TOLERANCE=10
−7
.
could be reasonable. What exactly a “reasonable value” is depends on the problem,
but as a rule of thumb at least the lower bound is given by the machine epsilon, which
is roughly speaking the value difference that can just be represented by the particular
number representation in the computer. For double precision numbers (i.e., a C++
double or a Python float), this is ≈10 −7. In practical situations this value is often
.
too small and may result in a huge number of iteration steps. For further details on
numerical computations, the reader is referred to the vast body of literature, e.g.,


================================================================================
PAGE 318
================================================================================

300 12 A First Approach to Machine Learning with Linear Regression
Algorithm 12.1: The Method of Steepest Descent for Simple Linear Regression (cid:4)
(cid:12)(cid:5) (cid:6)(cid:13)
1. Given are N training data records: D . train = x i t rain ,ytrain i i=1...N
2. Initialization of position in weight space and numerical parameters:
(cid:129) choose start values for both weights .w(0) =[w
0
(0 ) ,w
1
( 0 ) ]T
(cid:129) choose the step size parameter (“learning rate”) .η
(cid:129) choose tolerance for convergence test TOLERANCE
(cid:129) choose maximum number of steps s MAX_STEPS
3. Compute gradient at current position
(cid:129) The predicted value of y for any value of x is .ypred = w0 + w1x
(cid:129) The gradient needs to be computed with respect to the weights (cid:14) .w0 and .w (cid:15)1
where . ∇ w J = ∂ ∂ w J 0 , ∂ ∂ w J 1 T
∂J 2
(cid:2)(cid:5) (cid:6)
.
=− ytrain
i
− ypred
i
∂w0 N
i
∂J 2
(cid:2)(cid:5) (cid:6)
=− ytrain − ypred xtrain
∂w1 N i i i
i
4. Update the weights for the new step.( s + 1)
(cid:5) (cid:6)
.w (s+1) ← w(s) + η −∇ w J (w(s) )
5. Check convergence
(cid:129) IF . (cid:12) w(s+1) − w(s)(cid:12)< TOLERANCE
THEN: convergence reached. →EXIT
(cid:129) IF number of steps.i >M AX_STEPS
THEN: no convergence reached. → EXIT
ELSE: new iteration step, GOTO 4.
the book on numerical recipes and scientific computing by Press et al. [7] is highly
recommended. As a last aspect concerning the convergence behavior, we note that
monitoring the cost function can sometimes be a good alternative for monitoring all
weights individually. In this case the convergence test is obviously done only for the
value of the cost function.
Determining a suitable value for the learning rate can be tricky: on the one hand
we seek an as-large-as-possible learning rate as this implies a smaller number of


================================================================================
PAGE 319
================================================================================

12.3 Simple Linear Regression 301
Fig. 12.12 The steepest
descent method with too large
step sizes (.η = 0.5 for the
leftmost starting point “A”,
.η = 0.25 for “B”, and
.η = 0.9 for point “C”). None
of the trajectories converges
to one of the two minima.
Each of the small round
markers indicates one step
steps and thereby a faster learning. In particular when the calculation of the gradient
is computationally complicated and therefore takes a lot of time, this becomes
relevant. On the other hand, the larger the learning rate, the larger also the errors
can become because the step taken might be too large to capture changes in the
error landscape. This can be seen in Fig.12.12 where we start with the same initial
positions but this time with a much larger learning rate. Trajectory A seems to get
close to the minimum but then is trapped in an infinite “back-and-forth” between
the left of the minimum and right of it. This is also a typical situation where the
maximum number of steps are required as termination criterion; without this, our
code would never stop. Trajectory B suffers from accumulating errors and, as a
consequence, takes a wrong turn left and moves forever into the wrong direction,
again, a case for the maximum number of iterations. The (attempted) minimization
C is similar to A but keeps on jumping around wildly.
Mathematically speaking, the reason for large, accumulating errors is the fact
that the chosen update scheme for the weights is an explicit scheme: the gradient
is calculated based on the information of step s and is then used to predict the new
position in the weight space, i.e., at steps+1. However, the gradient at this new point
.
can have a very different value than the one used to calculate the new position. The
discrepancy between the gradients at the original and the updated position gives rise
to an error; the steeper the gradient, the larger the error. Ultimately, the learning rate
needs to be chosen according to the region with the steepest gradient. This results
in unnecessarily small steps in all other regions (e.g., compare the middle section of
line B in Fig.12.11) which makes the method computationally inefficient and which
is one of the reasons why the gradient descent method is not used in more complex
application cases.
If the cost function to be minimized is a convex function, then the choice of
any start point for the steepest descent iterations works and eventually results in
finding the global minimum. However, if the error landscape is non-convex as the
one shown in Fig.12.12, then the choice of reasonable initial values for the weights
is crucial: the algorithm might find two very different minima even if the two initial
points (A and B) are very close by.


================================================================================
PAGE 320
================================================================================

302 12 A First Approach to Machine Learning with Linear Regression
There are a number of numerical aspects that we did not touch upon here, e.g.,
adaptive determination of the learning rate or the problem that the computation of
the gradient can be expensive if the number of training data is large. Such more
advanced topics are considered in later chapters.
12.4 Computational Aspects of Vectorization
As a preparation for the complete numerical implementation in the next section,
we still need to briefly explain some computational aspects of the vectorized
implementation. There we have to differentiate between two different approaches.
12.4.1 The First Type of Vectorization: Partial Vectorization
In the following Python implementation, we use a technique called vectorization
where instead of writing for loops over vector/array elements, a mathematical
operation is performed simultaneously for all elements. For example, the vectorized
sum of two arrays a and b can be simply written as
and we don’t have to iterate over all elements of the matrices a and b :
Obviously, the first and vectorized version is much shorter. Additionally, the
vectorization by writing c = a + b creates code that looks very similar to the
mathematical formulation which makes it intuitively accessible and also easier to


================================================================================
PAGE 321
================================================================================

12.4 Computational Aspects of Vectorization 303
debug. Furthermore—and this is an at least equally important reason for using
vectorization—this code brings a significant speed up for the execution of the
program as compared to the use of for loops. For time-intensive computations, a
factor in between 2 and even more than 100 can be reached without having to bother
with any parallelization of the code. The computational time becomes relevant for
many ML problems, and in particular, if there are many data records involved, if the
functions to be evaluated are getting more complex, or if the algorithms need many
iterations. Python (or more specifically numpy) as well as MATLAB® both offer
vectorization as part of their language. This, however, often requires that some parts
of the code needs to be adapted (e.g., for loops over individual elements are to be
removed). The next subsection shows the derivation and Python implementation for
partial vectorization of the simple linear regression example that is still quite fairly
easy to understand.
As an example, consider the computation of the predicted label ypred = w +
. i 0
w x for all records i which we would implement as
1 i
There, the equation is evaluated for all entries of x at the same time such that
y_pred has as many entries as x .
12.4.2 A Second Type of Vectorization: Full Vectorization
There is, however, another approach that results in what we call a full vectorization:
rewriting the above equations exclusively in terms of linear algebra operations
is another way of using vectorization that additionally can be generalized to
problems with arbitrary numbers of features and weights. Now, the weights would
be represented by a weights vector, w = [w w ]T, and we use a scalar product for
. 0 1
each element i,
(cid:16) (cid:17)
w
ypred =[1 x ]· 0 = x˜ ·w . (12.31)
. i (cid:7)(cid:8)(cid:9)i(cid:10) w
1
=:x˜
The implementation needs to be such that again the scalar product is applied simul-
taneously to all records of the dataset which requires some more preparations as well
as formulations that are different from the original mathematical formulations. For
instance, the vector x˜ would become a matrix with a first column of ones. As this
.
requires some more careful work when formulating the expressions for computing
the derivatives of the loss (is the matrix transposed or not? are the weights multiplied
from the left or the right? etc.) and might not be of interest to all readers, the
derivation and implementation is shown in the separate section, Sect.13.2.


================================================================================
PAGE 322
================================================================================

304 12 A First Approach to Machine Learning with Linear Regression
12.5 A Worked Example of Simple Linear Regression
We will now return to the problem of univariate linear regression and step by step
introduce how Algorithm 12.1 can be transformed into Python code. Different types
of visualizations are shown and the results from this regression analysis are then
discussed and interpreted such that this section can be used as a “template” for
further studies.
12.5.1 Python Implementation
For computing the gradient we need the two individual derivatives of the cost
function w.r.t. the two weights. Luckily, for the problem under consideration, this
can easily be done, and in fact we already computed them above in Eqs.(12.19)
and (12.20), so that we only summarize them here
∂J (w ,w ; D ) 2 (cid:2)N (cid:5) (cid:6)
0 1 train =− y − ypred (12.32)
. ∂w N i i .
0 i=1
∂J (w ,w ; D ) 2 (cid:2)N (cid:5)(cid:5) (cid:6) (cid:6)
0 1 train =− y − ypred x . (12.33)
∂w N i i i
1 i=1
where y is the target value of the i-th training data record and ypred = w x +w
. i . i 1 i 0
is the predicted value for the two weights. We can now easily transfer the above
algorithm into Python code. Quite unsurprisingly, we start by importing the numpy
package. Then the training data is defined by randomly sampling the X_train values
from within an interval given by xlim . The array y_train is obtained as a linear
function, superimposed with some noise (through the variable eps_noise ).
This is one of the simplest way of creating a training dataset. As a next step, we can
now define a function that returns the two components of the gradient of the cost
function, Eqs.(12.32) and (12.33):


================================================================================
PAGE 323
================================================================================

12.5 A Worked Example of Simple Linear Regression 305
There, the numpy.sum returns the sum all elements o(cid:11)f the given one-dimensional
array. We also could have simply implemented . (1/N) (...) as numpy.mean(...)
but wanted to stay as close to the mathematical formulation as possible. The
statement residual * X is an element-wise multiplication of all entries of the two
variables. The above function then returns the two components of ∇J as a numpy
.
array. Similarly, we also implement a function for computing the MSE:
This, however, is only used for showing the final results. As a last preparation we
now have to define a few numerical parameter and the initial guess for the values of
the two weight:
Finding a good step_size often requires a bit of trial-and-error and also depends
on the magnitude of the involved values; if all values are roughly in the range of
unity then a reasonable value for the “stopping criterion” (the tolerance)is
.
1 0
−7
but larger values might work as well (as is here the case). Taking a look at the
convergence behavior of the weights or the value of the cost function helps to
determine this parameter and is always recommended. We can now perform the
update scheme where a maximum of maximum_number_of_steps update steps are
allowed in the for loop:


================================================================================
PAGE 324
================================================================================

306 12 A First Approach to Machine Learning with Linear Regression
Inside the for loop, we first compute the weight increment (recall that delta_w
is an array with two values) and update the weights to their new values. Then
we check if the length of the weight increment in weight space is smaller than
the tolerance ( np.linalg.norm is just the Euclidean norm). In case the weight
increment is negligible, convergence has been reached, and the break statement
exits the for loop, skipping the else statement. If the for loop is finished without
encountering the break statement (in other words, convergence has not yet been
reached), it will continue with the else clause and print the error message. Last but
not least, we summarize some of the most important results:
In [6]: print(f'w0 = {weights[0]:.3f}, w1 = {w eights[1]:.3f}')
print(f'{step + 1} steps taken')
print(f'final cost J: {cost(weights, X_train, y_train):.4f}')
w0 = -0.857, w1 = 0.512
99 steps taken
final cost J: 0.1794
We have now implemented a full training procedure for the regression problem
that even could be relatively easily generalized to more than one features and
multiple outputs. Next, we will now analyze the results and the performance of
the training process itself.
12.5.2 Visualization and Discussion of Training Results
Often, one of the most accessible approaches toward analyzing the results is
visualization of some aspects of the data. Obviously, a whole zoo of different
visualization types exists. All of them can be useful, depending on the context and
goals—this is our choice and requires a bit of experience.
One of the interesting investigations is to observe the evolution of chosen
quantities during the training procedure. This is helpful, e.g., for understanding
the convergence behavior of the training process. Two of such plots are shown in
Fig.12.13.The first plot shows the training data together with regression lines at
different “stages” of the training process. The respective weight values are shown in
the legend, and the colors of the lines corresponds to different stages (light = initial
weights, darkest = final weights). Visually, we can already see that the darkest line
fits well to the training data. This is an important verification and helps to see if the
Python implementation yields reasonable results at all. For producing this plot, for
each training step, we record the current values of the two weights, e.g., by


================================================================================
PAGE 325
================================================================================

12.5 A Worked Example of Simple Linear Regression 307
Fig. 12.13 Training data and lines for a few chosen weights from the training process (left panel).
The error distribution (“loss landscape”) is shown in the right panel, and the light-red line is a
“trajectory” for the chosen initial weights where each small red marker represents one training
step. The colors of the markers correspond to the line colors in the left panel
where we used a Python list for appending new weights, resulting in a list of
arrays. In the end, this is then converted to a numpy array. Using a list is faster and
more convenient to type than appending data to a numpy array (but this is also a
matter of personal preference). Then, the values of the first weight for all steps could
then be accessed by all_weights[:, 0] . Also the cost value of all iteration steps
could be stored in this way. To produce a plot of regression lines, we can plot a line
using y = w + w x for each weight pair and some given x values.
. 0 1
The second type of plot is a visualization of the weights space on top of a contour
plot of the cost values as a function of the two weights,L (w ,w ).
. 0 1
Words of advice (cid:2)
Thetypeofplotshowingthecost. J as a function of two weights is often referred to as
thelosslandscape, in particular in the deep learning community. Strictly speaking, this
not consistent with the terminology used here in statistical ML.Thel ossL . is related
to a value for each individual data record, while the shown cost is a resulting value
obtained from all data records.
The cost values are shown in different gray values. To create this plot, one has
to evaluate the cost function for each “pixel” of the image, i.e., by creating a two-
dimensional array of cost values. This is superimposed with the evolution of the
current weights as obtained for the previous plot. It is shown as the light-colored
line. Each of the tiny marker indicates one of the individual update step; the five
larger, colored markers correspond to the colored lines in the previous plot. We
observe that during the minimization steps, changes in the weight space are initially
larger and are decreasing as the minimum is approached. The reason for this is that
the update increment is scaled with the gradient value. Additionally, we can nicely
see that the trajectory seems to be perpendicular to the iso-lines of the weight space.
This is not surprising since the direction of the update is determined by gradient


================================================================================
PAGE 326
================================================================================

308 12 A First Approach to Machine Learning with Linear Regression
Fig. 12.14 Decrease of the cost (training MSE) during the training plotted with a logarithmic
y-axis (left panel). Variation of the two weights during the training (right panel)
of the cost function. However, if for the learning rate we would have chosen an
excessively large value, then this behavior would change and errors become larger.
Thus, this type of plot also helps to judge the appropriateness of the learning rate.
While this already gives a good idea about the convergence of the training
process, we can do more to see how some of the important variables and parameters
change during the training iteration: plotting the cost J versus the step number in
.
the left panel of Fig.12.14 shows that the cost decreases very quickly from about
15 to ≈1 within the first 40 steps. In the figure we used a logarithmic scaling of the
.
cost axis which reveals more details within the value range of small cost values.
The other plot in Fig.12.14 reveals how the values of the two weights change
as a function of the number of training steps. After 40 steps, w nearly reached its
. 1
converged value, while w does not converge as quickly. However, this behavior
. 0
strongly depends on the chose initial values of the weights and on the two-
dimensional structure of the cost function. Again, the colors of the five markers
are chosen to be consistent with the colors in all panels of Figs.12.13 and 12.14.
This should give already quite a broad picture of the behavior of the system.
Nonetheless, there even is more that one could do, e.g., we could create a line
plot showing how the required number of training steps to reach a certain accuracy
depends on the chosen step size, the true function could be shown (cf. Fig.12.15)
and compared to the regression line, the “loss landscape” could be plotted as a
surface in three dimensions for (possibly) better visualizing the shape, we can
compare the results to the analytical solution from the previous section, or we
could investigate how the convergence behavior depends on the initial guess. We
encourage the reader to try out such analysis and visualization as an exercise.


================================================================================
PAGE 327
================================================================================

12.5 A Worked Example of Simple Linear Regression 309
12.5.3 Validation of the Trained Model
There is still one important part missing which is also crucial within the concept of
machine learning: the validation of the trained model based on new data that has not
been used for the training. For this, the initial dataset was split into a training and
a testing dataset, e.g., using the strategy explained above in Sect.12.2.10 and there
in particular in Python Listing 12.1. The training and testing data along with the
corresponding cost as a function of iteration steps is shown in Fig.12.15. Note that
the weights are adjusted only during the training. Evaluating the cost function with
the testing dataD test and with the values of the weights w (i) and w (i) at iteration
. . 0 . 1
step i gives the testing MSE
.
testing MSE(i) ≡J (w
0
(i ) ,w
1
( i ) |Dtest) (12.34)
which is plotted in the right panel of the figure. In this case, the obtained testing
MSE is always larger than the training MSE and after . ≈40 steps doesn’t change
much even though the weights are still changing slightly. It is even possible that
the value of the testing MSE is below the training MSE. This depends on how
statistically representative (or comparable) the training and the testing datasets are.
This is clearly an important aspect in ML as it has a direct impact on how reliable the
training is. Luckily, for investigating this aspect of training and testing data, there
are methods available, called cross validation which will be introduced in Sect.16.2.
Fig. 12.15 Training and testing data (left) and the training and testing MSE during training. The
regression line uses the weights identified during the gradient descent and is based on the training
data only


================================================================================
PAGE 328
================================================================================

310 12 A First Approach to Machine Learning with Linear Regression
12.6 Multiple Linear Regression Models
We have already made a big step forward toward understanding ML regression:
many important aspects that build the foundation for generalizing simple linear
regression to more complex ML models have been introduced in the previous sec-
tions. This section considers multiple linear regression models where the “multiple”
refers to the fact that such models are able to consider multiple features as inputs.
The following section starts by formulating the hypothesis for such a case.
12.6.1 Hypothesis Formulation
A generalization of the simple regression model is a model that can handle
more than one input variable, e.g., the features X ,...,X . Such a model is
. 1 n
called multiple linear regression model, for which the hypothesis space consists
of hypotheses of the following form:
h(X ,...,X ; w ,...,w ) =w + w X +···+ w X . (12.35)
. 1 n 0 n 0 1 1 n n
The algorithm for the gradient descent method for multiple linear regression is
a straightforward generalization of the one shown in Algorithm 12.1. It is mostly
identical to that of simple linear regression except for the number of weights and
the resulting number of derivatives ∇ J as will be shown in the subsequent two
. w
sections.
12.6.2 Special Case: Multiple Linear Regression with Two Features
The case of two featuresX andX , implies that three weights are required:w ,w ,
. 1 . 2 . 0 1
and w . The derivation for the gradient of the loss function can be done in analogy
. 2
to Eq.(12.19) for the derivative w.r.t. w and in analogy to Eq.(12.20) for the
. 0
derivatives w.r.t. w and w , so that we only summarize the most important steps in
. 1 . 2
the following. For two features, the derivatives of the cost function read:
(cid:2)N (cid:5) (cid:6)
∂ 2
J(w ,w ,w ; Dtrain) =− y −ypred (12.36)
. ∂w 0 1 2 N i i .
0 i=1
(cid:2)N (cid:5)(cid:5) (cid:6) (cid:6)
∂ 2
J(w ,w ,w ; Dtrain) =− y − ypred x (12.37)
∂w 0 1 2 N i i i1 .
1 i=1
(cid:2)N (cid:5)(cid:5) (cid:6) (cid:6)
∂ 2
J(w ,w ,w ; Dtrain) =− y − ypred x . (12.38)
∂w 0 1 2 N i i i2
2 i=1


================================================================================
PAGE 329
================================================================================

12.6 Multiple Linear Regression Models 311
There, x is the i-th record of the first feature, X , and x is the i-th record of the
. i1 . 1 . i2
second feature, X (see the introduction to the notation for datasets in Sect.3.2.4).
. 2
We assume that each of the input features and output data are one-dimensional
numpy arrays, which could look like this (note that only the first few elements
are shown):
In [1]: import numpy as np
X1 = np.array([-1.889, -1.842, -1.741, -1.66, -1.593, -1.527, -1.451, ...])
X2 = np.array([ ...])
y = np.array([-1.555, -1.994, -1.574, -2.310, -1.295, -2.177, -1.523, ...])
Then, the gradient of the lost function can be computed using the following
Python function:
In [2]: def gradient_of_cost(weights, X1, X2, y):
y_pred = weights[0] + weights[1] * X1 + weights[2] * X2
residual = y.ravel() - y_pred
dL_dw0 = 2- * np.mean(residual)
dL_dw1 = 2- * np.mean(residual * X1)
dL_dw2 = 2- * np.mean(residual * X2)
return np.array([dL_dw0, dL_dw1, dL_dw2])
With similar definitions of the numerical parameters as before, the gradient descent
iteration also stays unchanged except for the changed function arguments of
gradient_of_cost :
In [3]: maximum_number_of_steps = 100000
step_size = 0.02
tolerance = 1e-4
weights = np.array([0.05, 0.2, 0.])
for step in range(maximum_number_of_steps):
dL_dw = gradient_of_cost(weights, X1, X2, y)
delta_w = step_size * (-dL_dw)
weights += delta_w
if np.linalg.norm(delta_w) < tolerance:
break
else:
print("ERROR: maximum number of iterations exceeded")
Figure 12.16 shows an example with two inputs and one target variable which
results in the shown regression plane. For more features the plane becomes a hyper
plane and cannot be plotted anymore.
Similar to simple linear regression, the important next step also for multiple
linear regression would be to investigate how the model with respect to testing data
performance. This is done in full analogy to what already was shown in Sect.12.5.3
and will therefore be left as an exercise.


================================================================================
PAGE 330
================================================================================

312 12 A First Approach to Machine Learning with Linear Regression
Fig. 12.16 Multiple linear regression for two features identifies the plane that fits best to the data.
Both the true function that was used to create the dataset and the fitted plane are shown
12.6.3 Generalization to an Arbitrary Number of Features
A generalization to an arbitrary number of features is not too complicated: for k
features, the derivatives of the loss function can be calculated in analogy to the
derivative w.r.t. w or w from above, i.e., for ∂J /∂w the only difference to
. 1 . 2 . k
Eq.(12.38) is a variable x instead of x at the end of the summation. In that case
. ik . i2
it is useful to rather operate with the feature matrix X which contains the features in
.
columns. This approach is additionally also computationally favorable but requires
some more linear algebra thinking. We will derive and demonstrate this in Sect.13.2.
Algorithm 12.2: Steepest Descent for General Linear Regression (cid:4)
1. Given are N training data records:
(cid:12)(cid:5) (cid:6)(cid:13)
D . train = x i tr 1 a in ,...,x i tr n a in ; y i train i=1...N
which consists of the n features.X1,...,Xn.


================================================================================
PAGE 331
================================================================================

12.7 Exercises 313
2. Initialization of position and parameters:
(cid:129) choose start values for all weights .w(0) =[w
0
(0 ) ,...,wn ( 0 ) ]T
(cid:129) choose the step size parameter (“learning rate”) .η
(cid:129) choose tolerance for convergence test TOLERANCE
(cid:129) choose maximum number of steps MAX_STEPS
3. Compute gradient at current position
...withrespecttotheweights.w i,
(cid:14) (cid:15)
either analytically or numerically . ∇ w J = (cid:5)∂ ∂ w J 0 ,..., ∂ ∂ w J n (cid:6) T
4. Update the weights .w (i+1) ← w(i) + η −∇ w J (w(i))
5. Check convergence
(cid:129) IF . (cid:12) w(i+1) − w(i)(cid:12)< TOLERANCE
THEN: convergence reached. → EXIT
(cid:129) IF number of steps.i >M AX_STEPS
THEN: no convergence reached. → EXIT
ELSE: new iteration step, GOTO 4.
The general steepest descent algorithm is shown in Algorithm 12.2.
12.7 Exercises
12.1
(a) How is the “training error” of a trained, linear regression model computed?
(b) How is the “testing error” of a trained, linear regression model computed?
(c) What is a machine learning model?
12.2
1. Consider the train-test split as shown in Python Listing 12.1. Why is the line with
rng.shuffle(...) needed? Show an example for the variable indices and after
shuffling.
2. Create an example dataset and plot the training and testing data in two different
colors. Compare plot of the data with and without shuffling.
3. Try a few different values as seed for the random number generator until you find
an “extreme” case where, e.g., many points of the training or the testing data are
close to each other. What does that imply for training and for prediction?
Hint: the dataset should not be too big, e.g., 20-30 points are a good value.


================================================================================
PAGE 332
================================================================================

314 12 A First Approach to Machine Learning with Linear Regression
12.3 Given are the support points of a polynomial
{(1, 3.5), (3, 5.1), (4, 3.5), (7, 8)}.
.
(a) Take a look at the documentation of numpy’s Polynomial class. How can you
access the fitted coefficients of the polynomial?
Even though Polynomial is not a plain function but in fact a class the usage
is still quite intuitive and does not require deeper knowledge about classes and
object oriented programming.
(b) Use numpy’s Polynomial to fit a polynomial with degree of 3 to the
data. Which are the interpolated values y for the x values of x =
interp interp
(2, 2.5, 3, 3.5, 4)? Write down the analytical function of the polynomial
including the determined parameters.
(c) Show the support points and the resulting polynomials of degrees 0 ... 3 in
a plot. Why does only the polynomial of degree 3 pass through all support
points? And how are the polynomials with the other degrees computed (hint:
take another look at the numpy documentation).
Optional information As Polynomial is a class, it therefore may contain, e.g.,
variables or even functions “under the hood.” Take a look at how the function fit
is called (it is Polynomial.fit(...) and returns a new Polynomial object where
the parameters are already defined). After this, we can use this object to do some
further things, such as to use it for interpolation, to compute derivatives, or to find
the roots (i.e., the points x for which it is f(x ) = 0). numpy has a number of
r r
other interpolation methods all of which have the same structure. So, it’s definitely
worth learning a bit about how to use the Polynomial class!
12.4 As a continuation of Example 12.2, the given training data should now be used
to determine the best fit of a line and thereby to determine the parameters w and
0
w .
1
(a) Manually compute the mean values x¯ and y¯ and the two sum-terms required for
computing w . Which is the resulting value of w ?
1 1
(b) Which is the value for w ?
0
(c) Implement the solution as Python program.
(d) Plot the training data as points and predictions as line.
(e) Compute the value of the training MSE.
(f) How does the MSE change if the slope is doubled or if the offset of the line is
increased by 0.5? Plot the resulting lines.
12.5 The dataset and the numerical parameters used in the example Sect.12.5 is
such that the training works very well. (a) Investigate and compare the training
behavior for two datasets where the first is taken from the example and a second
dataset that results in a different training behavior (e.g., in terms of the MSE or


================================================================================
PAGE 333
================================================================================

References 315
how w and w vary). (b) Vary the learning rate and discuss how this influence the
0 1
training behavior.
12.6 Section 13.1.1 discusses polynomial regression and hyperparameter optimza-
tion. Here, the task is to use numpy’s function fitting functionality for investigating
and reproducing the shown example and to compare it to the numerical solution
using the steepest descent method.
12.7 Create a multilinear dataset that has two features and a single target variable.
For this, randomly sample points that are located on a two-dimensional plane and
superimpose them with random noise for the feature. Then, try to “re-identify” the
plane using a multiple linear regression model. Compute the training and the testing
MSE.
References
1. Numpy. URL https://numpy.org/.
2. Website dedicated to Galton’s life and work, 2022. URL https://galton.org/.
3. H. Blockeel. Hypothesis Space, pages 511–513. Springer US, Boston, MA, 2010. ISBN
978-0-387-30164-8. DOI https://doi.org/10.1007/978-0-387-30164-8_373.
4. L. N. G. Filon, G. U. Yule, H. Westergaard, M. Greenwood, and K. Pearson. Speeches delivered
at a dinner held in university college, london in honour of professor karl pearson 23 april 1934.
URL https://archive.org/details/filon-et-al-1934-speeches-delivered-at-a-dinner/.
5. F.R.S. Karl Pearson, Liii. on lines and planes of closest fit to systems of points in space.
The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):
559–572, 1901. DOI https://doi.org/10.1080/14786440109462720.
6. F. Galton. Natural Inheritance. New York: Macmillan and Company, 5th edition, 1894.
7. W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes 3rd
Edition: The Art of Scientific Computing. Cambridge University Press, 3 edition, 2007. ISBN
0521880688.


================================================================================
PAGE 334
================================================================================

Advanced Methods and Topics of Regression 13
The purpose of computing is insight, not numbers.
Richard Hamming (1915–1998)
American mathematician
13.1 Nonlinear Model Behavior with Linear Regression
While fitting straight lines to data is certainly useful, the world’s most interesting
phenomena are nonlinear—which holds in particular for physics, materials science,
and engineering. Sometimes, linearization (e.g., by using a Taylor expansion) is a
way of reformulating and simplifying a physical problem such that we are only left
with linear behavior, but this is clearly limited and can come with a substantial error.
Before we start to introduce nonlinear models, let us start by defining what a
linear function is.
Definition 13.1 (Linear function) In a linear function all variables only
occur as powers of 1; no “mixed” terms involving more than one variable are
allowed. Mathematically, two conditions need to be satisfied for a function f
to be a linear function:
• Additivity: f(x+y)= f(x)+ f(y)
.
• Homogeneity: f(αx)=αf(x) ,
.
where x and y are the variables and αis a constant. If any of these conditions
.
is not fulfilled, then the function is a nonlinear function.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 317
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_13


================================================================================
PAGE 335
================================================================================

318 13 AdvancedMethodsandTopicsofRegression
Already in Sect.12.3.1 we briefly talked about the “linearity in linear regression,”
but the previous section might have created the impression that linear regression
nonetheless is just about straight lines or planes, at most. We will now introduce
how linear regression can be used to learn and predict nonlinear behavior between
input features and the output.
13.1.1 Polynomial Regression
When the dataset is the result of an underlying, nonlinear behavior, polynomial
regression is one of the methods that is able to produce a nonlinear model. The
hypothesis class in case of only one input variable is given by
(cid:2)k
h(x; w ,...,w )=w +w x+w x2+···+w xk = w xi . (13.1)
. 0 k 0 1 2 k i
i=0
To see why this is considered a linear function in the sense of Definition 13.1, we
realize that the attribute “linear” in the context of a machine learning (ML) model
refers to the weights: the w are considered the variables, and the (powers of) x
. i
are considered the constants. And they are constant indeed, as x represents, the
input data of a given dataset. Note that Eq.(13.1) contains the class of simple linear
regression models as a special case for settingn=1. The nonlinear behavior enters
.
through the various powers of x, but it is a linear equations with respect to the
coefficients w .
. i
Other than that, solving a polynomial regression problem is not fundamentally
different from what we have already done for the case of multiple linear regression.
In fact, by introducing the new features in form of powers of the original feature,
X ←X, X ←X2, ··· , X ←Xk , (13.2)
. 1 2 k
we obtain a multiple regression problem of the same structure as already introduced
in Sect.12.6:
h(X ,...,X ; w ,...,w )=w +w X +w X +···+w X . (13.3)
. 1 k 0 k 0 1 1 2 2 k k
Then, the training of this model follows the same steps as in the multiple regression
problem, and no changes to the numerical implementation are required. Predictions
are to be made usually using the original feature X and Eq.(13.1) or (less
commonly) the transformed features X together with Eq.(13.3).
. i
The degree of the polynomial, k,isahyperparameter (cf. Sect.11.3) that needs
to be fixed— either based on knowledge about the underlying physical mechanisms
(e.g., if we are investigating the motion of a body due to the gravitational force,
then we know that the distance is a quadratic function of time and the polynomial
regression model should at least contain the constant, linear, and quadratic terms),


================================================================================
PAGE 336
================================================================================

13.1 NonlinearModelBehaviorwithLinearRegression 319
or they can be found by performing a hyperparameter study, e.g., by investigating
how the training and testing loss changes as demonstrated in the following example.
Hyperparameter Optimization for Polynomial Regression
Figure 13.1 shows polynomials of different degrees that were fitted to a given
training dataset. To evaluate the predictive quality of the trained models, we also
evaluate the fitted functions based on new data, the testing dataset. Training and
testing data were both obtained by statistical sampling from a larger dataset. Plotting
the polynomials that were fitted to the training dataset on top of the training and
testing dataset shows a good fit of at least the polynomials of degree one and four to
both datasets.
As a quantitative analysis, we also recorded the training and testing cost values
which are obtained as the mean squared error (MSE). We observe a typical behavior:
initially, both losses are high and the model underfits (also see Sect.12.2.8).
Fig. 13.1 Polynomial regression: the left panel in the top row shows the model functions of
polynomials with different degrees trained on the shown training data. The right panel in the top
row shows the same regression lines but now together with test data. The bottom panel shows
the training and testing loss as a function of the polynomial “flexibility,” i.e., for polynomials of
increasing degrees. The lines are only shown to guide the eye; data exists only at integer polynomial
degrees


================================================================================
PAGE 337
================================================================================

320 13 AdvancedMethodsandTopicsofRegression
Then, both losses rapidly decrease, and the testing error reaches a minimum at a
polynomial degree of 3 or 4 (the true function that was used to create the dataset
had in fact a polynomial degree of 3). While the training loss decreases further
with an increasingly complex model (i.e., polynomials with more terms), the testing
loss already starts to increase. In other words, even though a more flexible model
represents more and more features of the training dataset, it is strongly specialized
toward the training dataset, and it does not generalize well to new data—the model
overfits.
The Bias-Variance Tradeoff (cid:2)
The left region in Fig.13.1 has high training and test errors; it is the region
of high bias due to wrong model assumptions, resulting in underfitting. The
region right of the minimum is dominated by a high sensitivity w.r.t. small
fluctuations in the data, showing in a high testing cost and a low training
cost. This error type is called variance. The b ias-variance tradeoff denotes
the property of a model to reduce the variance by increasing the bias and vice
versa. The goal is to minimize both errors.
Hyperparameter optimization is a strategy for determining aspects of the hypoth-
esis spaceH, i.e., for finding the most suitable model parameters. Here, the objective
.
would be to find the degree k of the polynomials that result in a trained model
which also generalizes well in view of new data. While this can be done manually
for models that contain one or two parameters, things will quickly become very
complicated once more parameters need to be taken into consideration. Recall that
the number of permutations with repetitions of k items with n choices is nk (c.f.
.
Sect.5.1.6). For example, five parameters each of which can take, say, 40 different
values, this results already in 405 = 102,400,000 possible combinations which
.
makes such a grid search nearly impossible, and smarter approaches should be
chosen (e.g., based on a random sampling combined with an optimization strategy).
Limitations
Polynomial regression is usually restricted to polynomials with degree of less than
approximately ten. The reason for this is that constructing higher-order polynomials,
i.e., finding the respective weights, is often a numerically ill-conditioned problem.
This means that in cases of high polynomial degrees, we cannot find the exact
solution anymore—not even approximately. This difficulty is growing with a larger
number of data records. Sometimes, feature scaling might help, e.g., by scaling
all values to the range of (−1,1) or by subtracting the mean value of a feature.
.
However, this will only slightly reduce problems in general. Using orthogonal
polynomials helps to circumvent some of the problems (cf. Chapter 7 of [5] and
references therein); however, often when fitting of polygons fails, it is advisable to
rather use alternative methods, as, for example, those introduced below.


================================================================================
PAGE 338
================================================================================

13.1 NonlinearModelBehaviorwithLinearRegression 321
Polynomial regressions looks quite promising in the sense that we can simply
increase the model complexity (i.e., the polynomial degree) to be able to represent
very complex data. However, any additional polynomial degree effectively implies
that an additional feature together with an additional parameter is added. This
may result in insufficient data, as already discussed in the context of the curse of
dimensionality in Sect.2.4 which ultimately results in overfitting. Overfitting is a
general problem with polynomial regression, even for one-dimensional datasets.
Polynomial regression is a non-local method which is a side effect of the
construction method: changing the position of one data point can affect the whole
curve. Trying to avoid this behavior leads to the method of regression splines,
presented in Sect.13.5.3.
Using the method of steepest descent for finding the best weights is generally
not an advisable strategy as in particular for multiple features, and in the case of
polynomial regression, the convergence behavior is very bad. As mentioned before
in case of linear regression, there exists a closed form solution (derived in Chap.13)
that is much faster; however, this cannot be easily generalized to other classes of
regression methods. Numerical minimization methods with a higher performance
exist; however, we will not explore them in detail. Regardless which numerical
method is used, the overall solution strategy for the ML problem always stays the
same.
Things to Remember Linear Models for Polynomial Regression (cid:2)
• Linear models can even capture nonlinear behavior between input and
output variables.
• A model is called linear because it is linear in theweights(and it doesn’t matter
how the features occur).
• Determining the degree of a polynomial can be difficult and is done either
based on knowledge about the data generation process or based on a
hyperparameter optimization.
• Usually polynomials with degrees of up to at maximum 6 or 7 are used; as an
upper limit they should be below half the number of records.
• Too stiff polynomials can cause extreme over- or undershooting— visual
inspection of the regression curves is always recommended (cf. the three
polynomials in Fig. 13.1)!
• Don’t use a model to “extrapolate” far beyond the range of the dataset.
13.1.2 Other Types of Hypothesis Functions
Everything that has been said about polynomial regression can also be easily
transferred to other mathematical structures of the hypothesis class. Typically,
the terms in which the feature occurs are linearly independent. Coming back to
the example of polynomials: polynomials of up to a degree of n form the basis


================================================================================
PAGE 339
================================================================================

322 13 AdvancedMethodsandTopicsofRegression
for a vector space and are linearly independent, i.e., w(cid:3)e cannot express eithe(cid:4)r
of the xk as a linear combination of all other terms xi |i =1...n, i /=k .
. .
Linear independence might seem like a good criterion for formulating increasingly
complex hypothesis functions. However, linear independence is not a requirement,
and additionally it does not imply that these terms are not correlated. In fact, for
polynomial regression, the terms may be highly correlated, e.g., if x is large thenx3
.
will be even larger, etc. Finding features that are not or only weakly correlated can
be a criterion for the choice of a suitable hypothesis function, as we also will briefly
discuss in the context of multicolinearity in Sect.13.3.5.
Further Parametric Formulations
When mathematical functions are used as hypothesis functions, they are fully
determined by a set of parameters, giving rise to parametric models . As a side
effect, the choice of such functions can also be motivated by knowledge about
underlying laws in the field of physics, materials science, and engineering, such that
these model functions have some relation to physical aspects. However, in general,
there is no restriction as to which function can be used. Here are a few mathematical
functions that are often used in ML models:
• Trigonometric functions: For example, if the dataset is known to result from
phenomena with periodicity in the time domain, e.g., h(t) = sin(w +w t)+
. 0 1
sin(w +w t);
2 3
• Exponential or logarithmic functions: e.g., h(x)=w +w log(w x+w );
. 0 1 2 3
• Combinations of all the above functions, possibly also with polynomials, etc.
These hypothesis functions have the convenient feature that the trained model can
be directly used for inference: the identified parameters of the hypothesis functions
often have a specific physical meaning or we can at least directly explain aspects
of the shapes of the functions. In general this is one of the biggest advantages of
parametric formulations, as compared to non-parametric approaches.
13.1.3 Linearization of Nonlinear Hypotheses
We conclude this section by a short discussion of a well-known trick that is also
useful for tackling situations where weights occur in nonlinear terms: transformation
of the features and target variables. As an example, consider the prediction y =
.
h(x;w ,w )
0 1
y =w exp(w x) with w ,w ∈R. (13.4)
. 0 1 0 1
This is an equation which is nonlinear in the parameter w . It can be linearized by
. 1
taking the logarithm on both sides


================================================================================
PAGE 340
================================================================================

13.2 GeneralizedFormulationsandVectorizationforMultipleLinear... 323
ln(y)=ln(w )+ w x (13.5)
.(cid:5)(cid:6)(cid:7)(cid:8) (cid:5) (cid:6)(cid:7)0(cid:8) (cid:5)(cid:6)(cid:7)1(cid:8) .
y∗ w∗ w∗
0 1
⇔ y ∗ = w ∗+ w ∗ x . (13.6)
0 1
This is now a linear equation in both weights. Subsequently, we can now use a
simple linear regression model for obtaining the weights. One can also visually
confirm that a prediction would be in fact linear. This might be trivial for this case;
however, if a larger number of features are involved and the linearized equation is
more complex, this still can be useful.
For making predictions or inference, we need to transform the weights back into
the original representation. For this, the inverse function needs to be applied to the
weights, which in this example is the exponential function. We obtain
w ←exp(w ∗ ) and w ←w ∗ . (13.7)
. 0 0 1 1
Comparing this results that one would have obtained from nonlinear “curve fitting,”
e.g., using a least square (LSQ) approach, we find that the obtained parameter are
slightly different. The reason for this is that any coordinate transformation also
changes the metric of the space, which also has an influence on how distances,
because they are also affected by the transformation.
Another example is the nonlinear relation
.
y = w
0
xw1 where for linearization a
logarithm function needs to be applied which results in log(y) = w ∗+w ∗ log(x).
. 0 1
After finding the two weights, they are back-transformed by . w 0 = 10w 0 ∗ and . w 1 =
10w 1 ∗ .
We summarize the classical linearization approach in an algorithmic box.
Algorithm 13.1: Linearization of nonlinear hypothesis functions (cid:3)
1. Identify the function.g(·)that is responsible for the nonlinear weight term(s).
2. Apply the inverse function .g
−1(·)
to both sides of the equation such that the
nonlinearity is “neutralized.”
3. Perform a linear regression training with the linearized equation
4. Transform the obtained weights back by evaluating.g(·)on them.
13.2 Generalized Formulations and Vectorization for Multiple
Linear Regression
For linear regression problems, there is an alternative solution approach to the
one introduced in the previous chapter. It is exclusively based on vector and
matrix operations from linear algebra such as scalar products or matrix-vector
multiplications. As an additional benefit, this method can easily be written in a


================================================================================
PAGE 341
================================================================================

324 13 AdvancedMethodsandTopicsofRegression
generalized form for including multiple features. It also lends itself to a very
efficient, vectorized implementations in Python—both for a closed form solution
and for numerical solution strategies.
The following derivation require vector and matrix operations which might be
slightly more difficult to follow than the original formulation used in the previous
chapter. To make the derivations more accessible, we sometimes use both index
notation and compact vector-matrix notation.
13.2.1 Hypothesis and Extended Feature Matrix
We start by rewriting the mathematical expression of the hypothesis in a form so
that it applies simultaneously to all m data records
⎡ ⎤
1
h(X ,...,X ; w ,...,w )=w ⎢ ⎣ . . ⎥ ⎦+w X +···w X (13.8)
. 1 n 0 n 0 . 1 1 n n
1
where the X are a column vectors representing the i-the column of the feature
. i
matrixX. The feature matrix has as many rows as there are data records. Recall that
.
h will return a scalar value for each of the m records which is why h is not printed
in a bold face (as opposed to a function that returns a vector valued quantity, e.g., a
force vector).
The formulation in Eq.(13.8) appears, with small variations, in a number of
different ML models. To simplify the derivations of the gradient of the loss, we
prepend a column of “1”s to the feature matrix X. For this extended feature matrix
.
, we will use the blackboard character X. Again, each row contains a data record.
.
Explicitly writing the extended feature matrix gives
⎡ ⎤ ⎡ ⎤
| | | 1 x 1,1 ··· x 1,n
. X=⎣ 1X 1 ···X n ⎦= ⎢ ⎣ . . . . . . ... . . . ⎥ ⎦ . (13.9)
| | | 1 x ···x
m,1 m,n
Notation of the Extended Feature Matrix (cid:2)
By now the reader is probably well aware that there are many different
quantities and variables in machine learning, in particular at the intersection
between mathematical formulation and numerical implementation. To more
easily memorize the meaning of X one can remember the extra column of
.
ones and associate this with the extra diagonal line in the symbol of extended
feature matrix, X, as compared to the regular feature matrix X.
. .
(continued)


================================================================================
PAGE 342
================================================================================

13.2 GeneralizedFormulationsandVectorizationforMultipleLinear... 325
Sometimes one also encounters the notion design matrix which, however,
sometimes refers to X and sometimes to X. We avoid the use of that notion
. .
entirely in this book.
We can now rewrite the hypothesis in compact matrix-vector notation. For this,
recall that we assume vectors to be column vectors such that in a matrix-vector
multiplication the matrix is left of the vector, a row matrix (see Appendix A.1.2 for
further details).
h(X; w)=Xw =Ypred , (13.10)
.
where the product of the matrix with the vector again yields a vector which contains
all records of the predicted variable Y. Insertion into the residual equation gives
R(w;Dtrain)=Y −Ypred =Y −Xw , (13.11)
.
where R(w;Dtrain) is the column vector containing the residuals of all training
.
records. Following the same argument as why the target variable Y is not written
boldface, we also use a capital, non-boldface letter R for the residual. The residual
for the record i is then written as r .
. i
13.2.2 Derivation of Cost Function and Cost Gradient
The next and slightly more complex task is to formulate the computation of the
training cost in terms of operations from linear algebra. To be as explicit as possible,
we write the following equations both in index notation and in compact vector-
matrix notation.
Taking the cost J as the mean of the squared residuals, we obtain in index and
.
in compact vector notation
1 (cid:2)N 1 ‖R‖2
J(w;Dtrain)= r2 = RT·R = , (13.12)
. N i N N
i=1
where the sum of all squared elements of R is simply the scalar product of R with
itself1 and ‖•‖ denotes the Euclidean norm of •. Upon insertion of the residual
. .
term, we get in index and compact notation
1 Recall that, . J(w0,...,wn ;Dtrain) would be pronounced as “The resulting cost . J for given
training data is a function of the weights ....”


================================================================================
PAGE 343
================================================================================

326 13 AdvancedMethodsandTopicsofRegression
(cid:15) (cid:16)(cid:15) (cid:16)
(cid:2) (cid:2) (cid:2)
1
J(w;Dtrain)= y − X w y − X w (13.13)
. i in n i im m .
N
i n m
1
= (y − Xw)T ·(y − Xw) . (13.14)
N
For minimization of the loss with, e.g., the gradient descent method, the gradient of
J with respect to the weightswis needed. This requires to calculate the derivatives
. .
of a scalar function (cf. Appendix A.1.9). We first realize that ∂J/∂wshould again
.
result in a column vector, containing the derivatives w.r.t. the different weights w .
. i
Starting from the index notation of J in Eq.(13.13) and applying the product rule,
.
the important aspect is to consider about which indices the “contraction,” i.e., the
summation about double indices, should be performed:
(cid:15) (cid:16)
∂J(w;Dtrain) 2 (cid:2) (cid:2)
=− y − X w X . (13.15)
. i ik k ij
∂w N
j
i k
From there we can now switch to the compact vector-matrix notation
∂J(w;Dtrain) 2 2
=− XT(Y −Xw)=− XTR (13.16)
.
∂w N N
where the transposition of Xis required because the index j should be the index of
.
the items of the resulting column vector, ∂J/∂w .
. j
Alternatively, we also could have started directly from Eq.(13.12). There, the
steps are (i) to use the product rule, (ii) realizing that neither Y nor Xdepend on w,
. .
(iii) that∂ wis the identity matrixI, as well as that (iv) the identity(Xw)T =wTXT
. w . .
holds
(cid:17) (cid:18) (cid:19)
∂J(w;Dtrain) 1 ∂Y ∂w
= (Y −Xw)T· −X +···
.
∂w N (cid:5)∂(cid:6)w(cid:7)(cid:8) (cid:5)∂(cid:6)w(cid:7)(cid:8)
=0 =I
(cid:18) (cid:19) (cid:20)
∂YT ∂wT
··· − XT ·(Y −Xw) (13.17)
.
(cid:5)∂(cid:6)w(cid:7)(cid:8) (cid:5)∂(cid:6)w(cid:7)(cid:8)
=0 =I
2
=− XT (Y − Xw)
N
2
=− XT R , (13.18)
N
which is the same result as in Eq.(13.16). These derivations are probably more
detailed than what one would normally find in such a text; however, we wanted to
ensure that it is comprehensible even for those who do not have a strong background


================================================================================
PAGE 344
================================================================================

13.2 GeneralizedFormulationsandVectorizationforMultipleLinear... 327
in linear algebra. This formulations is also computationally efficient as it only relies
on a single matrix-vector product. In analogy to Sect.12.3.7 and in particular to the
steps shown in Sect.4, we can now use the method of steepest descent or any other
suitable numerical minimization technique to obtain the weights, for which the cost
is minimized. Additionally, in Sect.13.2.3 we also derive a closed form solution.
13.2.3 Closed Form Solution
Even a closed form solution for computing the weights w for which the cost J is
. .
minimized can be obtained. For this, point of departure is Eq.(13.18) together with
requiring that ∂J/∂w =0
.
∂J(w;Dtrain)
=0 (13.19)
. .
∂w
⇒ XT (Y −Xw) =0 (13.20)
.
⇔ XT Y = XT Xw (13.21)
.
(cid:21) (cid:22)
⇔ w = XT X
−1
XT Y , (13.22)
where we assumed for inverting (XTX) −1 that the product XTX results in a non-
. .
singular matrix. Any matrix of the form XTX is called normal matrix or Gram
.
matrix. For inversion of this term, one typically has to resort to iterative numerical
solution methods which, however, for larger matrices can become numerically
problematic. This closed form solution is in the statistics literature also called
ordinary least squares (OLS). An example for a Python implementation of these
equations is given in Example 13.1.
Example 13.1 Vectorized Linear Regression in Python (cid:4)
Below is an example implementation for a dataset with four records for the
case of simple linear regression. We start by creating the feature X , the target
Y , as well as vector of weights. Since each of them is a column vector, we use
reshape to convert it from a 1D array to a 2D array which has one column
(the second parameter) and as many rows as necessary (indicated by the “-1)”.
(continued)


================================================================================
PAGE 345
================================================================================

328 13 AdvancedMethodsandTopicsofRegression
In [1]: import numpy as np
# feature matrix with 1 feature
X = np.array([1.1, 2.2, 3.0, 3.9]).reshape(-1, 1)
# target vector (1 column)
Y = np.array([1.3, 2.4, 2.9, 3.6]).reshape(-1, 1)
# (initial) weights in 1 column
weights = np.array([-0.5, 0.2]).reshape(-1, 1)
# number of data records
n_records = X.shape[0]
The extended feature matrix X is created by prepending a column of “1”s
.
from the left to X which can be achieved by “horizontally stacking” the
.
column vector and the matrix:
In [2]: # stack arrays horizontally
IX = np.hstack((np.ones_like(X), X))
∗
Next, the closed form solution for the weights, w , for which the loss
.
function reaches a minimum is implemented. There, the numpy function inv
from the linalg module was used for inverting . XTX.
In [3]: exact_weights = np.linalg.inv(IX.T @ IX) @ IX.T @ Y
Y_pred = IX @ exact_weights
Y_pred.reshape(-1) # reshape only for getting compact output
Out [3]: array([1.37635294, 2.26670588, 2.91423529, 3.64270588])
As a small test we also made a prediction for the training data, Ypred = Xw.
.
The symbol @ denotes the numpy matrix product, cf. Appendix A.2.4.The
shown output is quite close to Y which shows that the implementation seems
to work well.
13.2.4 Summary
In this section we have derived an efficient formulation for the class of all
multiple linear regression problems. Specifically, this required the introduction
of the “extended feature matrix” X and the consistent use of vector and matrix
.
operations. With this, even a closed form solution for finding the weights that
minimize the cost could be derived. An even more general bus also more abstract
implementation in terms of basis functions will be introduced below.
We now briefly summarize this section and the most important equations.


================================================================================
PAGE 346
================================================================================

13.3 GeneralizedFormulationofLinearRegressionwithBasisFunctions 329
Things to Remember Multiple Linear Regression (cid:2)
• Matrix-vector notation results in concise and in computationally efficient
expressions
• Formulations are generic enough to still apply, e.g., in polynomial regression.
• The most important equations are:
•ResidualError: R(w;Dtrain)=Y −Xw
. •CostandGradientofCost: J = 1 RT·R; ∂J =− 2 XTR
N ∂w N
.X is the “extended feature matrix” with the extra column of 1s, and N
are the number of records. With these equations, a numerical minimization
algorithm can be used to obtain the best weights.
• A
(cid:23)
clos
(cid:24)
ed form solution that minimizes the loss is given by .w =
−1
XTX XTY
Note that R and Y are both column vectors with as many entries as there are data
records. R and Y follow both the notation of the data matrix, Sect. 3.3.3.
13.3 Generalized Formulation of Linear Regression with Basis
Functions
In Sect.13.1.1 we saw that the problem of polynomial regression can be refor-
mulated, by a suitable transformation of the terms X0,X1,...Xk, in terms of the
.
multi-variable regression problem. In the following we will generalize this approach
further using the concept of basis functions.
13.3.1 Basis Functions
Linear regression requires that weights occur only as linear coefficients, and, e.g.,
no squares of a weight or products of different weights are allowed. However, the
terms to which the weights are coefficients can take arbitrary shapes. One example
that we already encountered are polynomials. Subsequently, these are again used to
motivate the introduction of basis functions.
The Example of Polynomials
Mathematically, a polynomial can be constructed from a number of monomials
which is defined as follows.


================================================================================
PAGE 347
================================================================================

330 13 AdvancedMethodsandTopicsofRegression
Definition 13.2 (Monomial) A monomial is a polynomial expression that
consists only of a single term in which products of powers of one or more
variables occur. Examples are x2, xy2, xn,or1.
. . .
The set of all monomials forms a basis for polynomials , e.g., it is directly clear
that any polynomial of the form
p(x)=w +w x1+···+w xn (13.23)
. 0 1 n
can be created by summing monomials, weighted by coefficients w ,
. i
(cid:2)n (cid:25) (cid:26)
. p(x)= w i xi = x0 x1 x2 ... xn ·w , (13.24)
i=0
where w is a column vector which contains the weights. The monomial terms are
.
called basis functions of the polynomial and are often represented by the Greek φ.
.
For a single variable x, the basis functions are given by
φ (x)=x0 =1, φ (x)=x1 =x , ··· , φ (x)=xn . (13.25)
. 0 1 n
A visualization of the first monomial basis functions and a polynomial is shown in
Fig.13.2. The polynomial p(x)can then be written as the scalar product of a vector
.
of basis functions φand the vector of weights:
.
p(x)=φT(x)·w with φ =[φ (x),...,φ (x)]T. (13.26)
. 0 n
An example in the right panel of Fig.13.2 is given where the first five monomialsφ
. i
are superimposed for given coefficients.
13.3.2 Hypothesis with Arbitrary Basis Functions
We now switch back from mathematical notation to “ML notation” with feature
X and target Y each of which contain n records. Generalizing the example of the
polynomial hypothesis to arbitrary basis functions is straightforward, and we can
assume that in all generality, a hypothesis h(X)has the following form:
.
(cid:2)n
h(X)=φT(X)·w = w φ (X) (13.27)
. i i
i=1


================================================================================
PAGE 348
================================================================================

13.3 GeneralizedFormulationofLinearRegressionwithBasisFunctions 331
Fig. 13.2 Left panel: The first five monomial basis functions in the range of .x ∈0[,1].
Monomials are linearly independent, i.e., we cannot construct a .φ k by linear superposition of any
other monomials of degrees.p,q,r,...with.p,q,r,.../=k. This is directly visible for at least the
first four monomials. The right panel shows a particular polynomial constructed from the first five
monomial basis functions
where φ is a column vector consisting of the basis functions φ ,...,φ , each of
. . 0 n
which maps an element of the feature X to a scalar value. For a single record x of
. i
the feature X, this is written as
⎡ ⎤
w
. h(x i )= (cid:25) φ 1 (x i ) φ 2 (x i ) ··· φ n (x i ) (cid:26) · ⎢ ⎣ . . . 1 ⎥ ⎦ (13.28)
w
n
Which are then the features in this model? We again use the same trick as in
Eq.(13.2) and map each basis function φ (X)to an individual feature, ξ :
. i . i
φ (X)׀→ξ , φ (X)׀→ξ , ..., φ (X)׀→ξ . (13.29)
. 1 1 2 2 n n
For a single data record x of the feature X, the hypothesis reads
. i
⎡ ⎤
w
. h(x i ; w)= (cid:25) φ 1 (x i ) φ 2 (x i ) ··· φ n (x i ) (cid:26) · ⎢ ⎣ . . . 1 ⎥ ⎦ . (13.30)
w
n
⎡ ⎤
w
⇒ h(ξ; w) = (cid:25) ξ 1 ξ 2 ··· ξ n (cid:26) · ⎢ ⎣ . . . 1 ⎥ ⎦ (13.31)
w
n
This is now the point of departure for a linear regression problem where both
weights and features occur only in linear terms, the hypothesis, and in fact each


================================================================================
PAGE 349
================================================================================

332 13 AdvancedMethodsandTopicsofRegression
of the basis functions is linear in the ξ , while any nonlinearity is hidden in the
. i
mapping of the basis functions. In other words, we started from an one-dimensional
input which can occur in form of an arbitrary functional, increase the dimension
of the problem to a higher-dimensional problem with n features, and thereby turn
it into a canonical problem that is linear in both the weights and the feature. This
allows to perform all subsequent steps in great generality.
13.3.3 Extended Feature Matrix with Arbitrary Basis Functions
The above “linearized” hypothesis can now be written for the whole dataset in
matrix-vector notation for the prediction Ypred
.
⎡ ⎤ ⎡ ⎤⎡ ⎤
ypred ξ ... ξ w
1 11 1n 1
⎢ ⎥ ⎢ ⎥⎢ ⎥
. ⎢ ⎢ ⎣ ypr . .
.
ed 2 ⎥ ⎥ ⎦ = ⎢ ⎢ ⎣ ξ 2 . .
.
1 . . . .. . ξ 2 . .
.
n⎥ ⎥ ⎦ ⎢ ⎢ ⎣ w . .
.
2⎥ ⎥ ⎦ ⇒ Ypred =Xw , (13.32)
ypred ξ ... ξ w
(cid:5) (cid:6)(cid:7)n (cid:8) (cid:5) m1 (cid:6)(cid:7) mn (cid:8)(cid:5) (cid:6)(cid:7)n (cid:8)
Ypred =:X w
where each entry of the target vector and each row of the matrix corresponds to one
data record. This equation looks identical to what we would expect for a multiple
linear regression problem. There,Xis again the extended feature matrix. Below we
.
will show that from this formulation we can very easily derive all so far considered
models of multiple or polynomial regression as special cases.
13.3.4 Derivation of Cost Function, Cost Gradient, and the Closed
Form Solution
As in the case of multiple linear regression, we define the residual error of the i-th
data record (cf. Eq.(13.12))as
r =y −ypred ⇒ R =Y −X·w. (13.33)
. i i i
If the training cost is again based on a ordinary least squares (OLS) formalism,
then we obtain the same formulation as in Eq.(13.12) for the cost function
J(w ,...,w ;Dtrain). The derivations of the gradient of the cost functions w.r.t.
. 1 n
the weights shown in Eqs.(13.16) and (13.18) were done for the problem of multiple
linear regression. However, since only a generic, extended feature matrix X was
.
assumed, the derivations did not require any further assumptions. Because we
managed to transform the problem of linear regression with arbitrary basis function
into the same mathematical structure as used for those derivations, the mathematical
form of the results are the same as for the multiple linear regression. Last but not
least, taking a look at the derivation of the closed form solution in Sect.13.2.3 we see


================================================================================
PAGE 350
================================================================================

13.3 GeneralizedFormulationofLinearRegressionwithBasisFunctions 333
that also this derivation can exactly be transferred to our current problem. Therefore,
we only summarize our results.
Things to Remember Important Formulations Using Basis Functions (cid:2)
With basis functions one can use the same formalism as for multiple linear regres-
sion:
• MappingofBasisFunctions: φi(x)׀→ξi
• ExtendedFeatureMatrix, e.g.: Xk: =[ξk1, ..., ξkn ] (forrowk)
• Hypothesis: h(X; w)=X·w
• ResidualError: R=Y −h(X; w)
1
. • CostFunction: J(w;Dtrain)= RT·R
N
∂ 2
• GradientofCostFunction: J(w;Dtrain)=− XTr
∂w N
• ClosedFormSolution:
(cid:23) (cid:24)
−1
(OLSestimator) w= XTX XTY
13.3.5 Numerical Aspects and Discussion
Closed Form Solution vs. Gradient Descent
Clearly, this is a very convenient result as it allows to use only one regression
framework that is able to cover all different types of linear regression. Also, the
availability of a closed form solution for all linear regression problems is very
helpful. Why then bother with computing the gradient of the loss function and
numerical minimization of the loss at all? There are two reasons: First of all, the
closed form solution disguises the fact that in many real life situations, e.g., when
handling large datasets with many features, the inversion of XTX also requires
.
iterative numerical solvers. Even if the matrixXis not a dense matrix and therefore
.
can be stored as a sparse matrix (see, e.g., the scipy.sparse module), the inverse of
the product might turn out to be a dense matrix and therefore might require a huge
amount of memory. Gradient descent (or other more efficient methods) requires
more or less only to store the original, sparse matrix, the target vector and the
weights. The second reason why one might not want to use the closed form solution
is that linear regression is about the only method for which such a solution exists. All
other methods have to resort to iterative optimization methods. Therefore, it might
be desirable to have a common “platform” (the numerical optimizer) with which
all methods can nicely interface. However, as long as you only want to quickly and


================================================================================
PAGE 351
================================================================================

334 13 AdvancedMethodsandTopicsofRegression
Fig. 13.3 The problem of collinearity: Dark marker are obtained from a linear equation with two
features that were superposition of with random noise. Dashed lines are predictions obtained from
the closed form solution for the weights. The light-colored markers are the projection into feature
space in which collinearity between features can be easily observed. The left dataset contains
significant noise, but the predicted line fits well. The right dataset has barely visible noise, but due
to the collinearity between.X 1and.X 2, the error during matrix inversion of the estimator (and thus
the prediction) is large. Removing the noise entirely would result in a singular matrix
with a few lines of code solve a linear regression problem, the closed form solution
could be a good starting point.
Multicollinearity
As a last mathematical aspect, we want to address the problem of collinearity
(or equivalently multicollinearity): if two of the features exhibit a very strong,
linear relationship, one variable can be linearly predicted by the other variable;
compare Fig.13.3. This introduces redundancy in the dataset which has the effect
that the extended feature variable has not full rank (cf. Appendix A.2.15) and
therefore cannot be inverted. As a consequence, neither the closed form solution
(OLS estimator of the weights) nor the numerical minimization strategy are able to
determine the weights. In reality, two variables are usually never perfectly collinear.
Nonetheless, approaching this situations already deteriorates the condition number
of X (which states how sensitive a (linear) function is w.r.t. small changes in the
.
input), and therefore, the inversion can at least be highly inaccurate.
13.4 Formulation of Chosen Cases in Terms of Basis Functions
In this section the general formalism for linear regression using basis functions will
be used to derive some of the most common linear regression models.


================================================================================
PAGE 352
================================================================================

13.4 FormulationofChosenCasesinTermsofBasisFunctions 335
13.4.1 Simple Linear Regression
The following hypothesis governs a simple linear regression problem:
h(w; X)=w +w X , (13.34)
. 0 1
which has only one feature, X. The training dataset consists of one column of input
data, x , and the corresponding target data y . The solution of this problem results
. i . i
in the line that fits best to the given data (in a OLS sense). The basis functions can
be identified as the following linear functions:
φ (X)=X0 ׀→ξ and φ (X)=X1 ׀→ξ . (13.35)
. 0 0 1 1
After the “linearization” the basis functionsφ andφ are considered as the two new
. 0 . 1
variables, ξ and ξ . With those we arrive at the canonical form of the hypothesis,
. 0 . 1
written for one data record:
(cid:17) (cid:20)
(cid:25) (cid:26)
w
. h(ξ; w)= ξ 0 ξ 1 · 0 . (13.36)
w
1
This can now be written for n data records such that the extended feature matrix
X has n rows and two columns. All relations concerning the cost function, its
.
derivatives, and the closed form solution still hold.
13.4.2 Multiple Linear Regression
The following hypothesis governs a multiple linear regression problem:
h(w; X ,...,X )=w +w X +···+w X (13.37)
. 1 n 0 1 1 n n
The solution of this problem results in the best fitting hyperplane for the given data.
The basis functions can be identified as the linear functions:
φ (x , ..., x )=1׀→ξ ,
. 0 1 n 0
φ (x , ..., x )=x ׀→ξ ,
1 1 n 1 1
φ (x , ..., x )=x ׀→ξ ,
2 1 n 2 2
.
.
.
φ (x , ..., x )=x ׀→ξ , (13.38)
n 1 n n n
Considering the basis function φ as the i-th feature X , we arrive at the canonical
. i . i
form, written for one data record


================================================================================
PAGE 353
================================================================================

336 13 AdvancedMethodsandTopicsofRegression
⎡ ⎤
w
. h(ξ; w)= (cid:25) ξ 0 ξ 1 ··· ξ n (cid:26) · ⎢ ⎣ . . . 0 ⎥ ⎦ , (13.39)
w
n
and all relations concerning the cost function, its derivatives, and the closed form
solution apply.
13.4.3 Polynomial Regression
The most commonly used set of basis function for polynomial regression are
monomials as introduced in Sect.13.3.1 and in particular in Eq.(13.25) which we
repeated here for completeness:
φ (x)=1׀→ξ , φ (x)=x1 =x ׀→ξ , ··· , φ (x)=xn ׀→ξ .
. 0 0 1 1 n n
(13.40)
After the “linearization” the basis functions φ through φ are considered as the
. 0 . n
(n+1)new variables, ξ ,...,ξ . With those we arrive at the canonical form of the
. . 0 n
hypothesis, written for one data record:
⎡ ⎤
w
. h(ξ; w)= (cid:25) ξ 0 ··· ξ n (cid:26) · ⎢ ⎣ . . . 0 ⎥ ⎦ . (13.41)
w
n
This can now be written for m data records such that the extended feature matrix
X has m rows and (n + 1) columns. All relations concerning the cost function,
. .
its derivatives, and the closed form solution still hold. It is also directly clear from
the above equations that simple linear regression is a special case of polynomial
regression. We will now write out some of the governing equations for polynomial
regression.
Feature Matrix
According to Sect.13.3.3 the extended feature matrix X(which is sometimes also
.
called design matrix in the statistics literature) has the following form:
⎡ ⎤ ⎡ ⎤
φ (x ) φ (x ) ··· φ (x ) 1 x x2 ··· xn
0 0 1 0 n 0 0 0 0
. X= ⎢ ⎢ ⎢ ⎣ φ 0 ( . . x 1 ) φ 1 ( . . x 1 ) ··· φ n ( . . x 1 ) ⎥ ⎥ ⎥ ⎦ = ⎢ ⎢ ⎢ ⎣ 1 . . x . . 1 x 1 2 · . . ·· x 1 n⎥ ⎥ ⎥ ⎦ (13.42)
. . . . . .
φ (x ) φ (x ) ··· φ (x ) 1 x x2 ··· xn
0 m 1 m n m m m m


================================================================================
PAGE 354
================================================================================

13.4 FormulationofChosenCasesinTermsofBasisFunctions 337
This type of matrix is known as Vandermonde matrix where each entry in row j and
j−1
column j is given by x . The first column of ones is the motivation for choosing
. i
the blackboard typeface for X, there the additional diagonal bar represents the
.
column of ones. This matrix can now be used for evaluating the closed from solution
or for computing cost gradients for a numerical minimization. An example is shown
in Example 13.2. Typically, predictions are made using the original feature X and
Eq.(13.1); in principle, also the transformed features X together with Eq.(13.3)
. i
could be used, but this is less common and also makes inference more difficult.
Words of advice (cid:3)
Writing the columns of the extended feature matrix as various powers of the features
can introduce numerical instabilities, cf. Sect. 13.4.5. In such cases, using orthogonal
polynomials [1]suchasCHEBYSHEV polynomials might be beneficial.
Further Commonly Used Expressions
For the sake of completeness, we now derive and show the most commonly used
expressions in index notation which could also be obtained without the concept of
basis functions.
The gradient of J w.r.t. the weight w is obtained from Eq.(13.12) as
. . j
∂ 1 (cid:2)n ∂ 2 (cid:2)n ∂h(x ;w)
J(w;Dtrain)= r2 =− r i . (13.43)
. ∂w N ∂w i N i ∂w
j i=1 j i=1 j
With the definition of the hypothesis for polynomial regression, Eq.(13.1), we can
explicitly state the formulation of the individual derivatives of the hypothesis
∂ h(x ;w)=x0 =1, ∂ h(x ;w)=x1 , ..., ∂ h(x ;w)=x j ,
. w0 i i w1 i i wj i i
(13.44)
Upon insertion of the derivatives into Eq.(13.43), we obtain
(cid:2)n
∂ 2
J(w;Dtrain)=− r x j . (13.45)
. ∂w N i i
j i=1
For the special case of only two weights, we recover the formulation of simple linear
regression:
(cid:2)n (cid:2)n
∂ 2 ∂ 2
J(w;Dtrain)=− r and J(w;Dtrain)=− r x .
. i i i
∂w N ∂w N
0 i=1 1 i=1
(13.46)


================================================================================
PAGE 355
================================================================================

338 13 AdvancedMethodsandTopicsofRegression
Example
An example implementation of polynomial regression together with a numerical
minimization strategy using the steepest descent method can be found in Exam-
ple 13.2. There, it also can be seen that the computations are very compact and the
Python code is almost identical to the mathematical formulations. This is one of the
benefits of the vectorized approach that exclusively uses vector and matrix products.
This also can make code maintenance and debugging easier—at the price that one
has to have some control about the matrix operations in numpy.
Example 13.2 Vectorized Polynomial Regression in Python (cid:4)
Creating the data set, cf. Example 13.1, e.g., for the use of reshape .
In [1]: import numpy as np
X = np.array([1.1, 1.4, 2.1, 3.1, 3.7, 4.2]).reshape(-1, 1)
y = np.array([1.2, 1.5, 1.7, 2.1, 2.3, 2.9]).reshape(-1, 1)
n_records = X.shape[0] # number of rows = number of data records
Next, initialize the “extended feature matrix” as a Vandermonde matrix.
numpy.vander requires the first argument to be a 1D array, hence the
flatten() function. Setting increasing=True defines that the “1s” form the
first column and not the first row.
In [2]: polynomial_degree = 3
IX = np.vander(X.flatten(), N=polynomial_degree + 1, increasing=True)
Next, we define some numerical parameters as well as the initial values for the
weights vector and do the numerical minimization in analogy to the previous
chapter.
In [3]: learning_rate = 0.0005
max_steps = 10000000
tolerance = 5e-7
weights = np.array([0.2, 0.2, -0.2, 0.2]).reshape(-1,1)
In [4]: for step in range(max_steps):
residual = y - IX @ weights
grad_of_loss = -2 / n_records * IX.T @ residual
weights_increment = l-earning_rate * grad_of_loss
weights += weights_increment
if np.linalg.norm(weights_increment) < tolerance:
break
else:
print("max. number of steps reached w/o converging")
Finally, we give a bit of output:
(continued)


================================================================================
PAGE 356
================================================================================

13.4 FormulationofChosenCasesinTermsofBasisFunctions 339
Fig. 13.4 Visualization of
the results from
Example 13.2. Shown are the
predictions for weights
obtained from the steepest
descent method and from the
closed form solution which
serves as a reference
In [5]: print("|weight increment| =", np.linalg.norm(weights_increment))
print("final loss value =", float(residual.T @ residual / n_records))
print("number of steps taken =", step)
print("computed weights :", weights.flatten())
print("closed form weights :", (np.linalg.inv(IX.T @ IX) @ IX.T @ y).flatten())
|weight increment| = 4.999996546878193e-07
final loss value = 0.002882401789420955
number of steps taken = 1620612
computed weights : [-0.19861264 1.96952451 -0.70613782 0.09787801]
closed form weights : [-0.53150882 2.42437848 -0.88765607 0.11996224]
A visualization of the results from the example are shown in Fig.13.4. There,
the lines were plotted from the predictions with the two weight vectors w. This
.
was done in a two-step process: First, an array of x-values was created, e.g., by
x = np.linspace(0.5, 5) . Then, an extended feature matrix . X was obtained for
these values by numpy.vander(x, N=polynomial_degree + 1, increasing=True) .
Finally, the new prediction with the given weights is ypred = IX @ weights . There,
it can be seen that even though the weights from the numerical minimization do
not exactly match the analytical reference solution, the resulting prediction is fairly
good. It also can be seen that the number of steps required for finding the weights
is very large, which shows again that the gradient descent method is, for practical
purposes, not a good choice.
13.4.4 Multiple Polynomial Regression
Polynomial regression with several features is a powerful method for representing
multidimensional, nonlinear behavior. We assume that n features X are given and
. j
that each features occurs in a polynomial without mixed terms (i.e., there are no


================================================================================
PAGE 357
================================================================================

340 13 AdvancedMethodsandTopicsofRegression
monomials that contain both X and X with j /= k). Furthermore, we assume
. j . k .
that the polynomial is of degree s −1, which implies that including the 0-th order
.
monomial, there are in total s monomials. Again, using basis functions turns out to
be a very convenient approach since we just have to follow the same “recipe” as
before. We can sort and define the basis functions of degree≥1as follows (the zero
.
degree functions will be included only once):
φ (X)=X ׀→ξ , ··· , (Xφ)=X ׀→ξ
. 11 1 1 1n n n
φ 21 (X)=X 1 2 ׀→ξ n+1 , ··· , 2n (Xφ)=X n 2 ׀→ξ 2n
φ 31 (X)=X 1 3 ׀→ξ 2n+1 , ··· , 3n (Xφ)=X n 3 ׀→ξ 3n
. .
. .
. .
φ s1 (X)=X 1 s−1 ׀→ξ (s−1)n+1 , ··· , sn (Xφ)=X n s−1 ׀→ξ s·n (13.47)
For brevity, we used X as function arguments of φ which implies all features as
. .
arguments, e.g., φ (X , ..., X ). Since after the “linearization” the ξ are now the
. 1 1 n . i
names of the transformed variable, we numbered them consecutively from 1 ton·s.
.
With those we arrive at the canonical form of the hypothesis, written for one data
record and including the “1” for the zero-th degree:
⎡ ⎤
w
0
⎢ ⎥
(cid:25) (cid:26) ⎢w 1 ⎥
. h(ξ; w)= 1 ξ 1 ··· ξ n·s ·⎢ ⎣ . . ⎥ ⎦ . (13.48)
.
w s·n
This could now be written for all m data records and summarized in the extended
feature matrix X. Thus, X has m rows and n·s +1 columns where each column
. . .
(except for the first column with ones) contains one of the terms from Eq.(13.47).
All relations concerning the cost function, its derivatives, and the closed form
solution still hold, e.g., with this formulation the prediction is still obtained from
Ypred =Xwwhere wis the weight vector with n·s entries.
. . .
Two-Dimensional Example
As an example we show the extended feature matrix for two features and with
polynomials up to a cubic term. For clarity, we directly express the elements of
the extended feature matrix in terms of the original features X and X
. 1 . 2


================================================================================
PAGE 358
================================================================================

13.4 FormulationofChosenCasesinTermsofBasisFunctions 341
⎡ ⎤
⎡ | | | | | | | ⎤ ⎢ 1 x 11 x 12 x 1 2 1 x 1 2 2 x 1 3 1 x 1 3 2⎥
. X =⎣ 1 X 1 X 2 X 1 2 X 2 2 X 1 3 X 2 3⎦≡ ⎢ ⎢ ⎢ ⎣ . . 1 x 2 . . 1 x 2 . . 2 x 2 . . 2 1 x 2 . . 2 2 x 2 . . 3 1 x 2 . . 3 2 ⎥ ⎥ ⎥ ⎦
| | | | | | | . . . . . . .
1x x x2 x2 x3 x3
m1 m2 m1 m2 m1 m2
(13.49)
where X and X are column vectors and each entry corresponds to one of the m
. 1 . 2
record. The vector of weights whas then n×s+1=2×3+1=7entries.
. .
13.4.5 Discussion of Polynomial Regression
We are now able to perform linear regression with polynomial basis functions and
for arbitrary numbers of features. This is clearly a powerful method; however, there
are also certain aspects and limitations that should be taken into account. First of
all, polynomial regression is a great method when inference is important. It is very
easy to analyze the approximating function mathematically, and even derivatives
can easily be obtained for arbitrary points (i.e., feature values). In many cases the
regression curve has a clear relationship to an underlying physical phenomenon
(e.g., the grain size distribution during normal or abnormal grain growth) which we
can easily analyze in terms of details of the polynomial.
However, polynomials don’t work well if the data represents a periodic phe-
nomenon. In this case, regression, e.g., with Fourier basis functions consisting of
sinand cosfunctions, might be a better choice.
. .
Using polynomials of high orders is often problematic: above we derive a closed
formulation for least-squares polynomial regression and a general numerical form.
These (and essentially almost all other formulations) are always based on the
extended feature matrix which contains the features to the power of up to the degree
of the polynomial as columns. For a single feature, this results in a Vandermonde
matrix. High powers of the input values may result in a system that is numerically
very difficult to solve because if in the extended feature matrix the data is taken
to the power of, e.g., 30, small changes of the feature can cause large differences.
Numerically, this is very difficult as the system of equations might become (close
to) singular. This problem also persists for the closed form solutions as it require to
invert the extended feature matrix. This is why in almost all practical situations,
polynomial degrees of more than ≈ 15 should be avoided. Instead, piecewise
.
polynomials, as introduced below, might be a good alternative.
Last but not least, there is another problem with polynomial regression that is
related to inference aspects. Often, we are interested in derivatives, e.g., in case
that we want to find out about a growth rate or the plastic strain rate. An aspect
of polynomials is that their functional form gets simpler upon differentiating, e.g.,
the polynomial degree is reduced. This, however, is quite opposite to how things
in nature typically are: integration smooths out fluctuations while differentiation


================================================================================
PAGE 359
================================================================================

342 13 AdvancedMethodsandTopicsofRegression
increases the complexity. In this respect, polynomials might not the most suitable
candidates in situations where maybe even several derivatives are required.
13.5 Semi-and Non-parametric Regression
The “common” regression is typically associated with parametric regression where
the goal is to identify a function that fits best to given data. Non-parametric
methods do not assume a particular mathematical form such as a polynomial, and all
components of a model are directly derived from the training data without explicitly
having to define a mathematical hypothesis class.
13.5.1 Problem Formulation
Parametric regression methods require a mathematical function h, given by the
hypothesis space, which approximates the generally unknown true function f (see
Sect.12.2 for an introduction and definition of these notions). Then, the fundamental
model for parametric regression is in all generality given by
Y =h (X ,...,X ; w ,...,w )+∈ ⇔ Y=h(X; w)+∈ . (13.50)
. i i 1 n 0 m i
where Y denotes the i-th output variable Y and ∈ is an error term.
. i . i
Non-parametric Methods
Non-parametric methods do not assume a particular mathematical form such as a
polynomial, and all components of a model are directly derived from the training
data. Thus, there is no hypothesis about a functional relationship involved. The
unknown model is treated like a “black box” and is denoted in the following by
m(or m in the scalar case) which allows to formally write
.
Y =m (X ,...,X )+∈ ⇔ Y=m(X)+∈ . (13.51)
. i i 1 n i
m is a placeholder for an arbitrary algorithm or a method that takes an input and
.
turns it into an output. Note that in this notation m does not depend on further
.
parameter or weights. Internally,mmay require parameters, but they are not exposed
.
and cannot be controlled or used for, e.g., writing down a mathematical function.
Both equations were written for a the general case of multivariate input and
vector-values output. In case of a scalar output Y, the hypothesis reduces to h and the
model to m. Unlike the parametric model, m can usually not be easily interpreted,
making inference and function analysis more difficult.
How does the non-parametric model component look like? There is a range of
different classes of approaches. Many of them assume that the approximation will
only be determined by the data in the direct vicinity of the point under consideration.


================================================================================
PAGE 360
================================================================================

13.5 Semi-andNon-parametricRegression 343
This leads to a local averaging of the neighborhood which effectively smooths the
data.
In almost all practical situations, some assumptions need to be made, e.g.,
concerning the mathematical form of the smoothing. Why is this then still called
a non-parametric method? The most important aspect is that there is no closed form
hypothesis function. Furthermore,m(X)may include some numerical or algorithmic
.
components as well.
Semi-parametric Methods
There is also a type of ML regression in between the parametric and the non-
parametric approach which is called semi-parametric method. Assume that we
observe some simple general trend or that we know about an underlying physical
relation but the overall dataset shows some additional behavior. This can then
be represented by a parametric component that is responsible for the behavior,
described through a more or less simple mathematical function. Additionally, this
is superimposed with a non-parametric model contribution, cf. Example 13.3 for a
materials science example.
Example 13.3 Nonlinear elastic deformation (cid:4)
An example is a dataset that exhibits overall a linear increasing behavior
but which is superimposed with complicated, nonlinear contributions. A
nonlinear elastic material could exhibit such a behavior. For example, the
stress-strain response of a hyperelastic material consists of a linear elastic
component and deviations thereof. In a phenomenological approach, the
linear elastic contribution can be represented as having a parametric model,
while the nonlinear contributions can be governed by a non-parametric model.
The resulting model can be written in compact matrix-vector notation as
Y=h(X; w)+m(X)+∈ . (13.52)
.
Examples for non-parametric models are spline regression models which fit a
piecewise smooth curve consisting of segments of polynomials or kernel regression
methods both of which will be discussed below.
13.5.2 Regression with Piecewise Polynomials
Polynomial regression (Sect.13.1.1) suffers, among others, from the fact that it is
a non-local method where changing the position of one data point more or less
strongly affects regions of the curve that are further away. This can be avoided by
using a number of polynomials (or, as shown in the next subsection, splines) each of


================================================================================
PAGE 361
================================================================================

344 13 AdvancedMethodsandTopicsofRegression
Fig. 13.5 Visualization of different piecewise polynomials. The true function from which the
data points (dark marker) are obtained is shown as dark line; the intervals within the piecewise
approximation is obtained are indicated by the thin dashed lines which also denote the support
points.ξ i of the polynomials
which is fitted to a contiguous subset of the data. The points defining the boundaries
of these regions are called knots.
Four examples showing a piecewise step function, piecewise linear, and piece-
wise quartic function are shown in Fig.13.5. A p iecewise polynomial function is
obtained by only considering the training datasetD within the interval given by
. i
the left and right knots, . ξ i−1 and . ξ i (see Fig.13.5). The sub-datasets are “non-
overlapping” such that for the corresponding datasets of any two intervals i and
j ,itisD . i ∩D j = (cid:27) ∅. Furthermore, the datasets of all intervals together contain the
total set of data, D =D. Each interval i is associated with its own polynomial
. i i
model, trained withD .
. i
Each individual polynomial has a finite number of monomials, and thus also the
number of parameters (i.e., coefficients of the monomials) is finite; additionally,
the whole function approximation consists of a finite number of polynomials, also
implying that the total number of parameters increases in proportion to the number
of knots. Nonetheless, piecewise regression polynomials or splines are considered
non-parametric methods, because the number of parameters can be quite large. Even
more importantly, the parameter relation is not directly suitable for inferences, at


================================================================================
PAGE 362
================================================================================

13.5 Semi-andNon-parametricRegression 345
least not in the same sense as the methods introduced in Chap.12 where we could
analyze the function and, e.g., its derivatives.
The piecewise constant polynomial is a polynomial of zero-th order and is also
called piecewise step function . Effectively, it gives the mean of all data inside the
respective interval.
The piecewise linear, quadratic, and quartic functions are polynomials of first,
second, and fourth degree. The best fitting parameters of the polynomials of all inter-
vals can be obtained as a least square estimate of the particular hypothesis function
h, as introduced for polynomial regression in Sect.13.1.1. As a consequence, the
piecewise constant functions is the mean of all y ∈D, while the piecewise linear
. i
polynomial is the regression line for the data in the respective interval. A Python
implementation is shown in Python Listing 13.1. An example usage is as follows.
1 from numpy import linspace
2 from numpy.polynomial import Polynomial
3
4 def piecewise_fit(x_data, y_data, knots_x, degree, n_interpol_points=10):
5 """Divides data into intervals and fits a polynomial to each ofthem
6
7 :param x-data, y-data: data points (numpy arrays)
8 :param knots-x: list ofthe horizontal positions ofthe support points
9 (= (n-knots-1) intervals)
10 :param degree: polynomial degree
11 :param n-interpol-points: number of equally spaced points in each interval
12 :returns: lists with the approximated data for each interval
13 """
14 piecewise_data = []
15 for x0, x1 in zip(knots_x[:-1], knots_x[1:]):
16 idx = (x_data >= x0) * (x_data < x1)
17 poly = Polynomial.fit(x=x_data[idx], y=y_data[idx], deg=degree)
18 interp_x = linspace(x0, x1, n_interpol_points)
19 piecewise_data.append((interp_x, poly(interp_x)))
20
21 return piecewise_data
Python Listing 13.1 A Python function that returns piecewise polynomial approximations at
equidistant points. The main difference to regression codes in the previous section is that we use
idx to “filter out” all points that are not in the current interval. The resulting data for each interval
are stored as a list of tuple of the x and y coordinates
As always, we assume that the training data is already available. It has the structure
In [1]: from numpy import array
X = array([-0.92, 1.69, -0.96, 1.89, 0.14, ...], dtype=float)
y = array([-0.49, -1.55, -0.37, -1.24, 0.13, ...], dtype=float)
The three dots ... are not valid Python code—replace them by your own data.
Then, we do the piecewise training. For this, we use four knots, and in each of these
three intervals we evaluated the trained model at 10 equidistant points (note that we
need at least 4×2data records such that in each interval there are at least 2 points
.
which is required by a linear function):


================================================================================
PAGE 363
================================================================================

346 13 AdvancedMethodsandTopicsofRegression
In [2]: piecewise_data = piecewise_fit(x_data=X, y_data=y,
knots_x=[-1.8, -0.2, 1.3, 2.2],
degree=1, n_interpol_points=10)
piecewise_data
Out [3]: [(array([-1.8, -1.1, -0.4, ...]), array([ 1.5, 0.0, -1.4, ...])),
(array([-0.5, 0.2, 0.8, ...]), array([-1.7, -1.5, -1.2, ...])),
(array([ 0.8, 1.5, 2.2, ...]), array([-0.4, -1.3, -2.1, ...]))]
From the output it becomes clear that piecewise_data is a Python list containing
tuples of the x and y data (the left and the right arrays). The list has a length of
3 as for each interval we have one tuple of interpolated x and y data. This makes it
easy to plot the approximations for each interval:
In [3]: import matplotlib.pyplot as plt
plt.plot(X, y, 'o', c='C0') # show the training data
for interp_x, interp_y in piecewise_data:
plt.plot(interp_x, interp_y, c='C3', lw=3) # show the learned function
While the implementation is not complicated, the question arises, how useful
are predictions where the hypothesis function is discontinuous, and the function
even shows large “jumps” at the support points. The answer is, “it depends”: if the
underlying “true” function follows a bilinear form, then a piecewise linear function
might be a suitable hypothesis, and discontinuities even might turn out to be only
mild. However, another approach are splines where the segments are mathematically
“glued” together.
13.5.3 Piecewise Regression Splines
In all cases where smooth or at least continuous curves are required, a remedy for
the discontinuities at the knots is to “glue” the segments together such that a smooth
curve is obtained. This results in the method of regression splines—one of the most
important non-parametric methods for regression [2].
Usually, the support points ξ are then called control points, and it is mathemati-
. i
cally enforced that the function and the first derivative are continuous. For details of
the formulation, please refer to the literature on numerical function approximation,
e.g., [3] or [4]. Natural splines additionally ensure that the second derivative at
the boundary points are zero. Among others, this helps to avoid larger under- or
overshooting at the ends, which is one of the problems of linear regression with
polynomials of higher degrees. This is known as RUNGE’s phenomenon observed
by CARL DAVID RUNGE in 1901 who found that higher-order polynomials do not
necessarily increase the interpolation accuracy. A key aspect of piecewise regression
splines is that for each interval a cubic, quartic or higher-order spline is created.
These individual splines are in a OLS sense the best fit to the given points, while at
the same time the continuity of the splines and their first (sometimes even higher)
derivative across the intervals are enforced.


================================================================================
PAGE 364
================================================================================

13.5 Semi-andNon-parametricRegression 347
Fig. 13.6 Comparison of four different regression approximations: cubic polynomial (top left),
piecewise cubic polynomial (top right). Piecewise splines of 3rd and 4th order (bottom left and
right)
Figure 13.6 shows a comparison of four different OLS approximations to a dataset
that was obtained from a polynomial of fifth order, superimposed with random
noise. The cubic polynomial is not a bad approximation (depending on the dataset
extreme oscillations can occur) but is not able to capture the downward trend toward
the right of the dataset. The piecewise polynomial approximation exhibits strong
discontinuities at the knots. Both approximations are rather sensitive w.r.t. changes
in the dataset or in the case of the cubic polynomial, also w.r.t. changes of the knot
positions. The two piecewise spline approximations both perform very well and are
much more robust than the other two methods.
The Python package SciPy [6] provides an easy to use implementation in form of
the SciPy function LSQUnivariateSpline which can be imported (besides the usual
packages) as follows:
In [1]: import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import LSQUnivariateSpline


================================================================================
PAGE 365
================================================================================

348 13 AdvancedMethodsandTopicsofRegression
Next, we create a dataset by superimposing a polynomial of fifth order with random
noise:
In [2]: x = np.linspace(-1.5, 2.2, 100)
y_true = -2.3 * x + x**2 + 1.2 * x**3 - 0.4 * x**4 - 0.1 * x**5
y = y_true + 1.5 * (np.random.rand(x.size) - 0.5)
The ScyiPy function requires the training data, the inner knots, and the degree of the
polynomial. The outer knots are automatically taken as the minimum and maximum
of the dataset.
In [3]: inner_support_points_x = [-0.5, 1.1]
spl = LSQUnivariateSpline(x, y, inner_support_points_x, k=3)
spl is an object that contains the information of the fitted spline and that can act as
a function, e.g., we can evaluate it at the positions x :
In [4]: plt.plot(x, y_true, ls='--', label='true function')
plt.plot(x, y, lw=0, marker='o', label='noisy training data')
plt.plot(x, spl(x), label='prediction')
plt.legend()
The resulting plot is similar to the one for piecewise cubic regression splines in
Fig.13.6.
This is also a useful example for conducting some further experiments. Among
others, it could be seen that the position of the knots place a very important role.
HowtoChoosetheI nterval Size
There are a number of options for determining the most suitable interval size and
the number of intervals. The first approach just splits the domain into a number of
equally sized intervals. As long as the data density is approximately constant, that
generally works well. In case that data is distributed very heterogeneously, a better
approach is to split the data into quantiles such that each interval contains the same
number of data. Last but not least, looking at the data and using our intuition or
knowledge about the underlying problem also can help to determine the number
and position of the knots.
13.5.4 Kernel Regression with Gaussian Basis Functions
Kernel regression is an approach where all points in a certain neighborhood around
a particular position x˜ are averaged. The kernel function acts as a weighting
.
where typically points that are further away from x˜ get a smaller weight value
.
assigned during averaging. Kernel functions with compact support (i.e., roughly
speaking, from a certain point on their value is zero) act as a particular kind of
efficient averaging window for computing moving averages. This operation results


================================================================================
PAGE 366
================================================================================

13.5 Semi-andNon-parametricRegression 349
in smoothing of the data which is also the reason why the process is called kernel
smoothing.
∗
Assume a one-dimensional kernel function K(x ,x ) that is centered at one of
. i
∗ ∗
the training features x . Then the prediction of the smoothed target value y at x is
. i . .
obtained from
(cid:28)n
K(x ∗ ,x )·y
i i
y ∗ = i=1 . (13.53)
. (cid:28)n
K(x∗,x )
i
i=1
From this equation, it is directly visible that the kernel function acts as a weighted
∗
average of the data around x : if the kernel function is just a constant, then y is the
. i .
mean of all data; if it has a constant value in a region around zero and has zero value
everywhere else, then this acts as a moving average.
A common choice for a kernel is a Gaussian-like function given by the following
equation:
(cid:18) (cid:19)
(x−μ)2
K(x,μ)=exp − , (13.54)
.
α
where μ is the position of the center and α governs the spread of the distribution
. .
in analogy to the standard deviation. Any factor of the exponential function can
be removed because (i) in Eq.(13.53) it would cancel out and (ii) in the context
of regression with basis functions, the weights act as such pre-factors, as we will
see below. As the Kernel contains the Euclidean distance between two points, this
particular function is also called radial basis function (RBF)—the Gaussian in two
dimensions is radial symmetric.
How can these kernel functions be used within the regression framework of
basis functions? Starting from Sect.13.3.2, we observe that the kernel functions
are already basis functions. We position the radial basis function (RBF)at
.
μ
j
, and
the target valuey of each data pointP(i)is then weighted according to its position,
. i .
i.e., the feature value
.
x
i
. This is done for all n radial basis function (RBF)
φ (x )= K(x; μ ,α ), ... , φ (x )= K(x; μ ,α ), (13.55)
. 1 i i 1 1 n i i n n
where we slightly changed the notation of K for clarity. The hypothesis for some
weights wis then
.
⎡ ⎤
w
0
⎢ ⎥
. h(x i ; w)=[1 φ 1 (x i ) ···φ n (x i )]·⎢ ⎣· w · 1 · ⎥ ⎦ , (13.56)
w
n


================================================================================
PAGE 367
================================================================================

350 13 AdvancedMethodsandTopicsofRegression
and thus we arrived at a formulation that can be used with the linear regression
framework as introduced before.
Words of advice (cid:3)
Note that both parameters .α and .μi need to be chosen and fixed before the
regression is performed; they are neither determined through the training, nor are
they properties of the distribution of feature data used for training.
To understand how everything works together, we present a minimal Python
implementation. For reuse, we define two functions, RBF and a function for
assembling the feature matrix with the RBFs, feature_matrix . Both are shown in
Python Listing 13.2. For using these function, we start by importing numpy and
Python Listing 13.2 Kernel regression with radial basis functions: the Python function for the
RBF and for creating the feature matrix (to be used with the code snippets in the text)
have to make sure that the two functions are available (e.g., by importing them from
a file or by pasting them here):
In [1]: import numpy as np
# either import the functions RBF and feature_matrix
# or insert their code from the listing (see text) here
We generate a dataset from a polynomial and superimpose some random noise:
In [2]: X = np.linspace(-1.5, 2.2, 80)
Y_true = -2.3 * X + X**2 + 1.2 * X**3 - 0.4 * X**4 - 0.1 * X**5
Y = Y_true + 1.5 * (np.random.rand(X.size) - 0.5)


================================================================================
PAGE 368
================================================================================

13.5 Semi-andNon-parametricRegression 351
Next, we define the number of kernel functions to be used, their position, and their
shape:
In [3]: n_rbf = 4 # number of RBFs
mu_rbf = np.linspace(-1.8, 2, n_rbf) # position of RBFs
alpha = 1 # shape of all RBFs
With this, we can now call the function that populates the feature matrix with the
data evaluated by the RBFs and compute the weights using the closed form solution
for the ordinary least squares approach:
In [4]: IX = feature_matrix(X, mu_rbf, alpha)
w = np.linalg.inv(IX.T @ IX) @ IX.T @ Y
For new data, X , we can create a new feature matrix keeping all parameters aside
from X constant and use the weights to make a prediction.
In [5]: Y_pred = feature_matrix(X, mu_rbf, alpha) @ w
Finally, we can visualize the training data and the predictions
In [6]: plt.plot(X, Y_true, label='true function')
plt.plot(X, Y, lw=0, marker='o', label='training data')
plt.plot(X, Y_pred, label='prediction')
plt.legend()
which gives a similar result to that shown in one of the panels of Fig.13.7.Inthis
figure we observe that the method is relatively robust w.r.t. the number of RBFs (in
the figure we compare 4 RBFs to 10 RBFs), and we also see that the predictions
match the data fairly well for a large range of values α. Furthermore, we observe
.
that for a very small value of α (i.e., a small standard deviation), the individual
.
GAUSSians become directly visible (upper left panel of Fig.13.7). This would
become more pronounced and result in overfitting if the number of kernels would
be increased. Only if the kernel functions strongly overlap (bottom left panel) then
the performance of the method breaks down.
This simple example also demonstrates how versatile and powerful the frame-
work of linear regression with basis functions is. Adding the concept of “kernel
smoothing” additionally augments the range of applications.
The shape factor of the distributions, α , is also called the bandwidth and
. i
influences the smoothing. In this regression approach, the α usually have the same
. i
values. This parameter could be obtained from an parameter optimization where the
objective functions can be chosen as the MSE of a testing dataset. There are also
approaches that determine different values for the α , e.g., based on the density of
. i
data which is useful in case that the resulting approximation has “holes,”, i.e. regions
that are not properly covered by the RBFs. The position of the kernel functions can
be determined similar as what was sketched for the piecewise spline approximation.


================================================================================
PAGE 369
================================================================================

352 13 AdvancedMethodsandTopicsofRegression
Fig. 13.7 Kernel regression with radial basis functions. The dashed lines indicated the positions
of the kernel functions. See the text for more details
Kernel Density Estimation (cid:2)
Kernel density estimation (KDE) is based on kernel smoothing and is a way
to estimate probability functions. It is contained as a standard tool in many
statistics, machine learning, and data visualization toolboxes.


================================================================================
PAGE 370
================================================================================

13.5 Semi-andNon-parametricRegression 353
Fig. 13.8 Visualization of Fourier basis terms (left panel) and their superposition with randomly
chosen parameters as a function of time (right panel)
13.5.5 Kernel Regression with FOURIER Basis Functions
An effective approach toward data that contains inherent periodicity is the use
of Fourier basis functions . There, the angular functions sin and cos are used
. .
in analogy to a Fourier series because calculus tells us that almost any function
can be approximated by a Fourier series. Obviously, the more Fourier terms are
involved the better the accuracy. The Fourier basis consists of terms such as
sin(ωt),cos(ωt),sin(2ωt),cos(2ωt), and so on. There, ωis the angular frequency,
. .
and in general also a phase shift ϕmight be added. The Fourier basis then takes the
.
form
φ (t)=1 and φ (t)=cos(ω t +ϕ ) fork ≥1 (13.57)
. 0 k k k
The first basis function is a constant offset. As this is an often encountered situation
for periodic phenomena (e.g., the yearly temperature in Europe fluctuates around the
◦
mean value of roughly 10 C), it was included here as well. Otherwise, the dataset
.
would have required appropriate preprocessing. A visualization of a few of the first
basis functions is shown in Fig.13.8 which also shows how the superposition of
some terms can look like. All subsequent steps would then be done in full analogy to
what was introduced in more detail for the Gaussian basis functions in Sect.13.5.4.
13.5.6 Other Commonly Used Basis Functions
Fourier basis functions for periodic systems and monomial basis functions for non-
periodic models have been the first choice for curve fitting and regression problems
until the mid-twentieth century. Apart from their great numerical behavior, this is
also due to the fact that both methods are accessible for mathematical analysis and
therefore are based on a solid theoretical foundation.


================================================================================
PAGE 371
================================================================================

354 13 AdvancedMethodsandTopicsofRegression
While Fourier regression is particularly well suited for data that was obtained
from periodic sampling in the time domain, however, for irregularly spaced data,
wavelet regression is more suitable, which uses a related type of periodic basis
functions.
An alternative for periodic problems are sinusoidal functions of the shape
φ (ω)=sin(ω), φ (ω)=sin(2ω), φ (ω)=sin(3ω),etc.
. 1 . 2 . 3
13.6 Further Nonlinear Regression Models
While we mainly investigated linear regression methods in this chapter, nonetheless,
also the field of nonlinear regression is relevant and contains a number of important
and commonly used methods. Some of them we have already encountered above:
almost all semi-and non-parametric models are nonlinear models.
In what follows we very briefly introduce these methods. Even though it is
not possible to understand all mathematical and implementation details from these
descriptions, it should be sufficient, though, to be able to decide if this or that model
might be suitable for a particular problem.
13.6.1 Ridge-and LASSO Regression
Linear regression operates with a hypothesis of the form
h(X)=w +w X +···+w X +ε (13.58)
. 0 1 1 n n
where the weights were obtained during training, e.g., from minimizing the mean
sum of the quadratic loss
(cid:2)
1
J(w)= (y −h(x ))2 . (13.59)
. i i
N
i
In Sect.13.3.5 we already discussed the phenomenon of mulicolinearity which
may occur if two variables are strongly correlated and which can cause inaccurate
estimations of the weights and bad generalization to new data.
LASSO regression remedies this problem by introducing a penalty term, thereby
creating a new minimization problem of the form
(cid:2) (cid:2)n
1
J(w)= (y −h(x ))2+λ ‖w ‖. (13.60)
. i i j
N
i j=1
(cid:28)n
The term λ ‖w ‖ is denoted as shrinkage penalty and acts as a regularization.
. j
j=1
It has the effect that as this term is increased, the values of the weights are


================================================================================
PAGE 372
================================================================================

13.7 SummaryandConclusion 355
automatically decreased. Additionally, the variance of the model is decreased, and
therefore the model reaches a better test MSE.
Ridge regression is not too different and uses as shrinkage penalty a slightly
(cid:28)n
different term, λ w2. Effectively, this avoids that coefficients can become zero,
. j
j=1
while LASSO regression allows that some weights can become zero, which implies
that these can be removed from the model. Cross validation (Sect.16.2) typically
helps to determine the hyperparameter and to find out which of the two models
performs better in a particular situation.
13.6.2 K-Nearest Neighbors Regression
The k-nearest neighbors (kNN) algorithm is one of the few algorithms that can be
used both for supervised classifications and for regression. In a classification task,
the model searches for the k-nearest neighboring data points and then performs
a “vote” where the majority “wins” and thereby determines the class of the
unknown datapoint. In a regression approach, however, the neighboring data points
are typically averaged. kNN regression is easily able to predict highly nonlinear
relations. The underlying kNN algorithm is explained in more detail in Sect.14.4.
13.6.3 Support Vector Machine Regression
Support vector machines (SVMs) are another model type that can be used for both
supervised regression and for classification. The idea is, roughly speaking, that
the decision boundary in a classification problem can be envisioned just as well
as a regression curve. The use for classifications problems is more common, and
therefore we will discuss it in more detail in Sect.14.6.
13.6.4 Artificial Neural Networks
Artificial neural networks (ANNs) also belong to the class of nonlinear methods
that can be used for regression problems. They will be discussed in detail in later
chapters.
13.7 Summary and Conclusion
This chapter had a focus on extending and generalizing methods and models for
regression. There the focus was twofold: on the one hand, we formulated the
problems from Chap.12 in terms of compact matrix-vector notation and vectorized
all operations to achieve a satisfying efficiency. On the other hand, we generalized


================================================================================
PAGE 373
================================================================================

356 13 AdvancedMethodsandTopicsofRegression
the regression framework by introducing the concept of basis functions. This
is a “unifying framework” for parametric, semi-parametric, and non-parametric
regression and additionally allows to easily incorporate nonlinear model response.
Adding the concept of kernel functions increased the number of tools for nonlinear
modeling strongly.
13.8 Exercises
13.1 Think of a problem where the fact that derivatives in polynomials result in
simpler functions does not fit to the physical reality and discuss alternatives.
13.2 In Sect.13.5.2 an implementation for piecewise polynomials of arbitrary
degree was introduced. Implement the special case for piecewise constant polyno-
mials that does not require numpy’s Polynomial class.
13.3 RUNGE’s phenomenon, the extreme over- or undershooting behavior of a
higher-order polynomial at the boundaries was initially studied using RUNGE’s
function f(x) = 1/(1 + 25x2). Investigate this function for the polynomials of
degrees up to 5, and compare this with a least square spline approximation for
different numbers of knots x =[−1,...,+1].
i
13.4 Following Sect.14.4, implement the k-NN regression method, and apply it to
the MDS-1 (tensile test) dataset. Create a plot of the training, and test MSE and use
it to justify your choice of the optimal number of nearest neighbors.
13.5 Explain what parametric and non-parametric methods are and why they are
called like this.
13.6 Choose an appropriate ML method for learning the MDS-1 (tensile test)
dataset. The method should be as simple as possible and contain the most relevant
materials scientific phenomena. Which method do you choose? Implement the
method of your choice, and compare your solution to a solution using linear
regression with a polynomial of third degree.
13.7 In a dataset of ceramic materials, the recorded properties for each sample
include the processing conditions such as temperature, pressure, and time and
the corresponding hardness value. A researcher wants to find out the relationship
between the processing conditions and the resulting hardness value using regression
and regularization techniques. The researcher wants to know which of the process-
ing conditions affect the hardness value the most, i.e., they want to find out the
most relevant processing conditions that impact the hardness value and focus more
on those and find and discard the irrelevant features. What kind of regularization
should they use and why?


================================================================================
PAGE 374
================================================================================

References 357
13.8 In the previous question, the researcher realizes that there is a strong cor-
relation between the temperature and the pressure features. The presence of
strong correlations between the predictors makes the model unstable, resulting in
inaccurate estimations and leads to bad generalization to new data. To tackle this,
what kind of regularization should they use and how does it help?
References
1. G. Arfken. Mathematical Methods for Physicists. Elsevier, 1985. DOI https://doi.org/10.1016/
c2013-0-10310-8.
2. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer New
York, 2009. DOI https://doi.org/10.1007/978-0-387-84858-7.
3. Q. Kong, T. Siauw, and A. M. Bayen. Python Programming and Numerical Methods. Elsevier,
2021. ISBN 978-0-12-819549-9. DOI https://doi.org/10.1016/c2018-0-04165-1.
4. W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes 3rd
Edition: The Art of Scientific Computing. Cambridge University Press, 3 edition, 2007. ISBN
0521880688.
5. G. A. F. Seber and A. J. Lee. Linear Regression Analysis. Wiley, Jan. 2003. DOI https://doi.org/
10.1002/9780471722199.
6. P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski,
P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman,
N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I˙. Polat, Y. Feng,
E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero,
C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0
Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature
Methods, 17: 261–272, 2020. DOI https://doi.org/10.1038/s41592-019-0686-2.


================================================================================
PAGE 375
================================================================================

14
Supervised Classification
“All models are wrong, but some are useful.”
George Box (191–2013)
British statistician
14.1 Introduction to Supervised Classification
Supervised classification of data is a concept where an machine learning (ML)
model is trained to sort data into different groups, each of which corresponds to
a different category or class. An important property of categorical data is, that—as
opposed to typical numerical data—one cannot interpolate between categories. In
supervised classification, the classes are predefined, i.e., the training data consist of
a number of inputs, the features, and a single output, the class, which is typically
an integer number. This integer, however, can be mapped to text labels (e.g., in a
classification model for images, we could assign the value 0 to “car”, 1 to “tree,” 2
to “person,” etc.). The boundary between regions of data points of different classes
is called decision boundary, e.g., indicated by the line in Fig.14.1. For more than
two features that line becomes a (hyper) surface in multidimensional space.
The most simple ML methods are linear classifiers, which aim to split a dataset
into two parts using a straight line in two-dimensional feature space and or a
(hyper) plane in a higher-dimensional spaces. Thus, linear methods only work if
the data are linearly separable; however, by certain transformation methods (e.g.,
used in support vector machines, Sect.14.6), linearly non-separable data can be
made linearly separable. More complex methods are also able to find more complex
decision boundaries, e.g., such as the one shown in Fig.14.1.
In binary classification, data are sorted into exactly two disjunct groups each of
which corresponds to one class. It can happen that the binary classification algorithm
might misclassify some data: a data point of a class A that is wrongly identified as
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 359
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_14


================================================================================
PAGE 376
================================================================================

360 14 SupervisedClassification
Fig. 14.1 The decision
boundary in supervised
classification separates data
points that belong to different
classes (here represented by
markers with different
colors). Depending on the
ML model a decision
boundary might not sort all
data points into correct
classes, which results in
“false positives” and “false
negatives” together with a
typically less ragged or
twisted curve
Fig. 14.2 Supervised classification consists of three parts: 1. Selecting a suitable model, e.g., a
polynomial; 2. training the model, i.e., finding a suitable set of parameters (e.g., the coefficients of
the polynomial) based on a given dataset consisting of input features and the class label for each
data point; 3. making predictions with the model h where for each input data the class will be
predicted
not belonging to class A is, from the perspective of class A, called a false negative.A
false positive (from the perspective of class A) is a data point from class B that was
wrongly labeled as belonging to class A. Section 11.8.1 introduces these measures
in more detail. If more than two classes are present, the problem of sorting the data
points into one of these classes is called a multiclass classification.
The three, general steps during supervised classification are shown in Fig.14.2,
which is similar to what we encountered for the case of regression in Fig.12.3.
Again, as a first step, the choice of a specific model class is included. While this
is not always explicitely considered in the literature, it is an important step as the
performance of the trained model strongly depends on this decision (also refer to the
introduction in Chap.12). For instance, the choice of the model determines whether
the decision boundary can take the shape of a straight line or of a complex, curved


================================================================================
PAGE 377
================================================================================

14.2 Rule-BasedClassificationMethodsandDecisionTrees 361
line. Which one is more appropriate depends on the data and the purpose of the
classification, i.e., besides data science aspects, domain-specific aspects play a role.
The mathematical function or the algorithmic form of the model has a number
of parameters that need to be determined. As in regression, this is done in the
subsequent training process where each data record is given by the input matrix
Xand the corresponding target variable Y.
.
Words of advice (cid:2)
In classification, the output matrix consists of only a single target variable,
which gives the label of all features. Therefore, in classification, we almost
never use the output matrix Ybut instead only the target variable Y .Then
.
the dataset is written asD=(X; Y).
.
Additionally, the dataset needs to be split into a training and testing set. During
the training, the model parameters θ are then determined such that the labels of
. i
the training data can be reproduced as good as possible. Finally, the model can
be used to make predictions or inference, in analogy to Fig.12.3. In classification,
though, prediction is typically more important than inference because often the
decision boundary is not governed by a simple mathematical functions (e.g., a
polynomial) but rather by some numerical algorithm that does not allow to perform
straightforward “function analysis.”
In the following sections, a number of ML classification models and algorithms
are introduced in order of roughly increasing complexity. This is not a complete
introduction of all possible methods. We rather focus on a chosen methods, and
introduce them in details , such that the reader should be able to understand how the
respective methods work and what the consequences are for their applications.
14.2 Rule-Based Classification Methods and Decision Trees
An intuitively comprehensible type of classification methods are rule-based meth-
ods, which, in some aspects, are very close to rules that we express through
language. Another related method is decision trees. In the following, we explain
both and then discuss the relationship between them.
14.2.1 The Idea Behind Classification Rules
Rule-based classification methods rely on a composition of one or more IF-THEN
conditionals. Each such conditional consists of an antecedent (the precondition in
the IF part) and a resulting consequent of the rule. Here is an example for such a
rule:


================================================================================
PAGE 378
================================================================================

362 14 SupervisedClassification
IFweather is sunnyANDthereis noschoolTHENicecreamparlour ...
.
...has many customers.
(14.1)
The antecedent of the rule consists of two conditions (weather is sunny and there
is no school), which are combined by a logical AND. The consequent (ice cream
.
parlour has many customers—or not) is then the actual prediction of this simple
classification rule: it predicts whether or not there are many customers in the ice
parlour based on two inputs. The above rule can also be written more formally as
(weather = sunny) ∧ (school_day=no) ⇒ ...
.
...ice_cream_parlour_has_many_customers =True,
(14.2)
where the logical AND is represented by the wedge symbol (∧). This expression
. .
looks already quite formal and as if it is already in a shape, such that it could
be easily implemented in a computer program. However, we still have to define
and quantify some of the notions. For instance, what is weather? And when is it
considered sunny? Is it sunny if the sun shines in a particular moment or do we
require the past hour to have non-stop sunshine? And what if there are also a few
clouds? We probably should refine and define what is important about the weather.
As a first idea, we can use the average number of sunny hours per day, n , and
. sun
require them to be above a certain number of hours, N .
. sun
But how we can we detect if it’s a school day and not a Sunday or a vacation
day? Assume that the computer had access to, e.g., the traffic information of the
road toward the school. This could yield the information of how many cars drove
by in the morning. Let’s call this number n . On a school day there will be many
. cars
cars in the morning while on any other day there will be just a few. Again, using a
threshold valueN and comparing it to the actual number of cars help to make the
. cars
decision. Now the problem can be restated as
ice_cream_parlour_has_many_customers =(n ≥N )∧(n ≥N ).
. sun sun cars cars
(14.3)
So far so good, but where is the “learning” aspect here? For determining the
two threshold values, N and N , we can make an educated guess, or we
. sun . cars
could attempt to do a purely mathematical derivation, we could create a simulation
model—or we could do this in a data-driven manner and use training data to
determine them.
Figure 14.3 shows some records of a (artificial) dataset. Just by looking at the
data, we find that the threshold value N should be somewhere in between 2
. sun
and 5 hours, the threshold N will be in between 19 and 25 cars. A computer
. cars
program would use an automated strategy to find the threshold values—the program
learns the data. In this case, all training data were reduced to only two numbers, the


================================================================================
PAGE 379
================================================================================

14.2 Rule-BasedClassificationMethodsandDecisionTrees 363
Fig. 14.3 “Learning from
data” whether an ice cream
parlour should expect many
customers (only they know,
how many “many” are) based
on the two features (i)
number of sunshine hours and
(ii) traffic density at the
school as an indicator for
whether it’s a school day or
not. Based on the data in the
table, the threshold values
.Nsunand.Ncarswere
determined such that in the
diagram both classes are
completely separated. For
making predictions, these two
parameter are “fixed” and
whenever a point falls into the
orange-shaded region, the
result will be that many ice
cream customers are to be
expected
two threshold values. Clearly, these two values cannot tell us where exactly all the
training data points were located but it can tell us something about the more general
aspects of the dataset.
For this example, the learning strategy that resulted in Fig.14.3 was rather
simple: take the positive examples and enclose them in a rectangle with minimal
area; the lower left corner gives N and N . This is clearly not a great
. sun . cars
strategy as, e.g., it is very sensitive to outliers, but at least, it is very simple to
implement. More robust solutions are based on an iterative optimization algorithms
that minimize an error. However, for this, we need to define an error measure for
such classification problems, which will be done below.
The result of the learning strategy for learning the (single) class of “many
customers expected” can be seen in the plot in Fig.14.3: there, the two solid lines
separate the region of positive examples (the orange data points) for the class “many
customers expected” from the negative examples (the blue data points). The line
indicating the boundary of the class is called decision boundary.
Finally, we can now make predictions using the two threshold values: for a new
input data record x ∗ = (n ∗ ,n ∗ ) we evaluate Eq.(14.3) with the now fixed
. sun cars
parameters N and N and predict, if there are many customers to be expected
. sun . cars
today. Using more data records will (often/mostly/hopefully) increase the accuracy
of the prediction.
Alternatively, we can also use the equation to analyze the structure of the data,
e.g., by plotting those feature values that result in high customer numbers (that’s the


================================================================================
PAGE 380
================================================================================

364 14 SupervisedClassification
shaded area in Fig.14.3). Additionally, we directly observe that such kind of rules
typically result in straight decision boundaries (or combinations thereof).
Words of advice (cid:2)
This is a very simple example, and the “learning strategy” is even more
simplistic. However, it helps to already understand a few core aspects. To
makesurethatyouunderstandsomelimitationsoftheapproachthinkabout
what happens to the decision boundary when more positive or negative
data, or when a positive and negative one were swapped, by accident.
14.2.2 The Relation of Rule-Based Methods and Decision Trees
Any set of rules has a corresponding decision tree, and vice versa. However, even
simple rules can lead to complex decision trees. If the antecedent of a rule consists
only of conditions that are combined by logicalANDs, then a tree can be constructed
.
by “chaining” the conditions as shown in Fig.14.4. There, the consequent of the rule
is given by the leaf on the very bottom, which assigns the class label “yes” or “no.”
Creating a rule based on a tree (“rule extraction”) typically results in more
complex formulations than necessary. The reason is that the most intuitive approach
is to create one rule for each path from the root to each leave. Subsequently, all rules
are combined through a logical AND, resulting in the antecedent.
.
IF (n ≥ N ) ∧ (n ≥ N ) THEN ice_parlour_has_many_customers
. cars cars sun sun
=True
.
IF (n < N ) ∧ (n ≥ N ) THEN ice_parlour_has_many_customers
. cars cars sun sun
=False
.
IF (n < N ) ∧ (n ≥ N ) THEN ice_parlour_has_many_customers
. cars cars sun sun
=False
.
IF (n < N ) ∧ (n < N ) THEN ice_parlour_has_many_customers
. cars cars sun sun
=False.
.
Fig. 14.4 Decision tree for
the rule Eq.(14.3).Thereare
two conditions, each of which
has two results. Therefore,
there are altogether four
scenarios that only result in
“yes” if both conditions are
satisfied


================================================================================
PAGE 381
================================================================================

14.2 Rule-BasedClassificationMethodsandDecisionTrees 365
By analyzing the tree beforehand and realizing that the two conditions on the
second level concerning the number of cars are identical one could already optimize
the rules. However, trees can also be unsymmetrical regarding the decisions, which
makes “compressing” such rules sometimes difficult.
There are a number of further practical aspects that rather suggest the use of
rules. Among those are simple extensions of rules that can be quite powerful, e.g., a
default cases can be included using an ELSE statement
IF(x >N) ∧ (y >x+2) THEN classA ELSE classB. (14.4)
.
Visualizing this in a decision tree is significantly more complicated than reading
such a decision rule. Other extensions are exceptions, e.g.,
IF(x >0) ∧ (y >0) THEN classA ELSE classB
.
EXCEPT 3≤x ≤5 THEN classB (14.5)
which represents already a relatively complicated rule that is nonetheless still very
comprehensible. As an additional benefit, such rules can also easily be implemented
in a Python code. However, both classification rules and decision trees are important
tools, in particular because the way how decisions are made can be understood and
related to domain knowledge. Here, we only summarize a few of the most important
definitions, aspects, and notions for classification rules:
. Mutually exclusive: If two rules are covering two different data records, then they
are mutually exclusive. If more than two rules need to be considered for a single
data record, then they are said to be not mutually exclusive.
. Ordering and Voting: How can one decide which of the rules that are not
mutually exclusive determine the class? Either, rules need to have different
priorities and can be ordered such that the first rule that is “activated” is
responsible for the class prediction. An alternative is to perform a voting among
the rules for a each records’ class, possibly including some weighting. There,
the rules do not have to be ordered, which may have additional implementation
advantages.
. Non-exhaustive rules: If records of a dataset are not covered by the rules, then
the set of rules is said to be not exhaustive.
. Coverage: The notion of coverage of a rule-based classifier quantifies how many
records of a dataset are handled by a specific rule. Coverage is a numbern
. coverage
that gives the percentage of the records in a dataset that fulfill the antecedent
conditions. It is defined as
.
n
coverage
=N
r
/NP (14.6)
where
.
N
r
denotes those records that are covered by the rule and
.
NP is the total
number of records.


================================================================================
PAGE 382
================================================================================

366 14 SupervisedClassification
. Accuracy of a rule: The accuracy of a rule is defined as the number of correctly
classified records divided by the total number of records that are covered by the
rule under consideration.
Decision trees can be strongly generalized, and there exist many sophisticated
methods for creating, compressing, and optimizing them. This, however, is beyond
the scope of this text. For further information, the reader is referred to the large body
of literature (see, e.g., the introduction in [4]).
14.3 Notions and Concepts for Classification Problems
Above, we introduced classification trees and rule-based methods in an intuitive
manner as one way of defining an ML model. To systematically explore the structure
of general classification problems and various methods, we will now introduce
important notions, mathematical foundations, and concepts.
We assume that the reader is already familiar with the concept of splitting the
data into training and testing data and with the hypothesis. Both were introduced in
Sects.12.2.10 and 12.2.5, respectively.
14.3.1 Datasets, Models, and the Hypothesis Class
We assume a datasetDwith two given featuresX andX and known classification,
. . 1 . 2
i.e., the label Y. It consists of altogether
.
NP records (which are also called
examples),
D . ={(x i1 ,x i2 ;y i )} i=1...NP . (14.7)
A supervised learning algorithm should then, based on the training data, yield a
hypothesis h that is able to map the input data (X ,X )to the output classification,
. 1 2
Y. Clearly, there exist many different algorithms or mathematical functions that
might be suitable for this task (just think of all the possible rectangles enclosing the
positive examples in the plot of Fig.14.3). The learning algorithm is responsible for
finding the particular parameterization that is “most suitable” (we’ll discuss later
what this exactly implies).
The above introduced classification rule, Eq.(14.3), defines a whole class of
hypotheses each of which has the some structure but differs in the two parameters
N andN . The collection of all hypotheses, i.e., feasible versions with different
. sun . cars
parameterizations is then the hypothesis class.
We will now explain this based on a more complex example, dataset DS-1 (Iris
Flowers, Sect.4.7), which consists of the measured width and length of the petal
and the sepal of iris flowers and the corresponding name of the species. The goal of
classification is here to find the name of the flower species (the class label) based on
the measurements. An ML algorithm is typically operating with numbers. Therefore,


================================================================================
PAGE 383
================================================================================

14.3 NotionsandConceptsforClassificationProblems 367
the three flower names (class labels) were encoded as integers 0, 1, and 2, the so-
called class IDs (see the description of the dataset in Sect.4.7 and in particular the
table in Fig.4.7a). Here, we only focus on two input features, the sepal length and
width. In a first step, the goal is to train an ML model to identify the iris setosa based
on the sepal length and width; the other two features contained in the dataset petal
length and width, are not used.
A simple starting point for identifying the iris setosa species can be expressed as
the plain language rule: all data points contained in a particular region belong to
class with ID=0. Additionally, the region still needs to be mathematically described
by a parametric, geometrical shape, e.g., a rectangle. As we saw already above,
rectangles can be very conveniently defined by rules, e.g., all points inside a
rectangle, which is given by the lower left corner(x ,y )and the upper right corner
. 1 1
(x ,y ),aregivenby
. 2 2
is_inside_rectangle=(x ≤x ≤x ) ∧ (y ≤x ≤y ). (14.8)
. 1 2 1 2
However, so far, the four parameters x ,...,y are not determined and thus,
. 1 2
Eq.(14.8) constitutes the hypothesis classH. The classHconsists of infinitely many
. .
rules, each of which is defined by a different set of values (x ,y ,x ,y ).
. 0 0 1 1
The dashed box in Fig.14.5 visualizes a particular hypothesis (i.e., rule for a
particular parameter combination): if a data point is contained inside the rectangle
given by x = 0.0,y = 0.6,x = 0.8,y = 2.4, then it belongs to class 0 (iris
. 1 1 2 2
setosa). Here, the four sides of the rectangle define the decision boundary, a notion
that we will now define as:
Fig. 14.5 The plot is a visualization of the extract from the iris dataset Fig.4.7a where the only
used features are the width and length of petals, and the color code indicates the class ID (the name
of the species). The dashed box denotes a particular hypothesis for class 0, the solid rectangle
corresponds to the most specific hypothesis for class 0, and the light-blue-colored region left of the
dotted line highlights the most general hypothesis for class 0


================================================================================
PAGE 384
================================================================================

368 14 SupervisedClassification
Definition 14.1 (Decision Boundary). A decision boundary is a hyperplane
that separates two classes. For a dataset with two features shown in two-
dimensional space, this reduces to a line; in three-dimensional space, the line
becomes a plane.
A further notion in the context of classification is the notion of the most specific
hypothesis , which in our example is given by the smallest possible rectangle that
encloses all positive examples, i.e., data belonging to class 0, and no data belonging
to any other class. Taking a look at Fig.14.5, it is easy to imagine what the most
specific hypothesis for class 0 would look like (shown by the rectangle drawn with
the thin solid line). It is also clear that this hypothesis is unique in the sense that
there exists only one such rectangle fulfilling the requirements.
Another hypothesis type is that of the most general hypothesis , which for our
example is the rectangle, that has the maximum possible size, contains all positive
examples and no negative examples. This is a bit more difficult to visualize: for
class 0 (iris setosa) it is the region that extends infinitely into positive and negative
vertical direction, just misses to contain the data of class 1 on the right and stretches
infinitely into negative horizontal direction, as indicated by the shaded region in the
left half of the figure. The most general hypothesis needs to contain all data points
of the respective class.
Separating class 2 from class 3 is more complicated, and the simple structure
of the chosen hypothesis class, which contains all rectangular regions, is no longer
sufficient. Even an inclined line would not be able to separate class 1 and 2 such
that no negative examples occur. A line separating these would need to be curved
or have many kinks. Obtaining such a decision boundary requires more complex
algorithms, which we will derive in more details below.
We conclude this section by some thoughts on how to implement the search for
a simple, rectangular decision boundary: a straightforward strategy for separating
class 0 from the other two classes is to simply find the minimum and maximum of
the data records of label 0 as shown below. We start by importing the obligatory
modules.
In [1]: import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
Here, the dataset is obtained from the scikit-learn package. Next, we extract the
data to be used. For getting the two feature, we have to ensure that the columns
with index 3 and 2 are the correct ones, e.g., by checking the documentation of the
dataset:


================================================================================
PAGE 385
================================================================================

14.4 NearestNeighborClassifier 369
In [2]: DS = load_iris()
X1 = DS.data[:,3]
X2 = DS.data[:,2]
y = DS.target
Subsequently, for finding the minimum and maximum coordinates, we only want
to consider the data points with label value zero. This is achieved by creating the
mask —an array with Boolean values:
In [3]: mask = (y == 0)
x0 = np.min(X1[mask])
x1 = np.max(X1[mask])
y0 = np.min(X2[mask])
y1 = np.max(X2[mask])
Now, we can visualize the data: we plot the rectangle and all data records where
their color is chosen according to their class labels:
In [4]: fig, ax = plt.subplots()
ax.plot([x0, x1, x1, x0, x0], [y0, y0, y1, y1, y0])
ax.scatter(X1, X2, c=y)
This results then in a plot similar to Fig.14.5 where the rectangle from the last
Python cell corresponds to the one shown by the solid line in the figure.
14.4 Nearest Neighbor Classifier
The k-nearest neighbors (kNN) method is based on the experience that most datasets
describe continuous phenomena. The consequence is that if two points are close to
each other in the feature space, they will belong to the same class and therefore
have the same label: the closer the points, the more similar their class labels should
be. Hence, classifying a new data point comes down to finding the one (or several)
neighboring points in feature space, so that, based on their labels, the label of the
new data point can be determined.
In order to being able to quantify what the distance between points is, we
start now by introducing some mathematical distance measures. Then, the 1-
nearest neighbor classifier is introduced, which has an interesting algorithmic
correspondence to a method of computational geometry and then generalized the
concept toward using n nearest neighbors and multi-class classification.
14.4.1 Distance Measures
The most commonly used way of measuring distances is the EUCLIDean distance,
which is the measure that we also use in our daily life. It is defined as


================================================================================
PAGE 386
================================================================================

370 14 SupervisedClassification
Definition 14.2 (EUCLIDean distance). Given two points that are repre-
sented by the vectors x and x ∈ Rn, the Euclidean distance D between
. 1 . 2 . 2
the points is defined as
(cid:2)
D (x ,x )=||x −x || ∈R = (x −x )2+(y −y )2. (14.9)
. 2 1 2 2 1 1 2 1 2
The EUCLIDean distance can be easily generalized to arbitrary dimensions,
e.g., in n-dimensional space the distance of two points x = (x ,...,x )and
. 1 n
y =(y ,...,y )is
. 1 n
(cid:3)
(cid:4)
(cid:4)(cid:6)n
D (x,y)=(cid:5) (x −y )2. (14.10)
. n i i
i=1
This measure is symmetric, which implies that the distance between point A
and point B is the same as the distance between point B and A—as expected. The
EUCLIDean distance is the most commonly used measure for the kNN method. It
is not scale invariant: all features (i.e., dimensions) are treated equally, regardless
the units. This might make scaling or normalization necessary to avoid that small
relative changes in one feature lead to large differences in distance.
A generic distance measure is the MINKOWSKI distance, which is also called
p-norm. It is defined as
Definition 14.3. MINKOWSKI distance Given are two points, represented by
the vectors
.
x
1
and
.
x
2
∈Rn. Then, the MINKOWSKI distance is defined as
(cid:3)
(cid:4)
(cid:4)(cid:6)n
D(x,y)= (cid:5) p |x −y |p (14.11)
. i i
i=1
There, p is a parameter that defines the type of measure:
. p=2 results in the EUCLIDean distance
. p=1: gives the Manhatten distance, which adds up the absolute values of
the differences along each axis
. p= 0: recovers the HAMMING distance, which effectively counts the
number of features
. for p→∞the distance approaches max |x −y |, which acts as a logical
. . i i i
OR


================================================================================
PAGE 387
================================================================================

14.4 NearestNeighborClassifier 371
In the following, we will mainly use the EUCLEDean distance, but we suggest
to keep in mind that this might not be optimal as soon as variables have different
dimensions or orders of magnitudes.
14.4.2 The (First-or One-) Nearest Neighbor Method
The most intuitive approach is to search only for the nearest neighbor of a given
point and then to assign that point the label of the nearest point. This is shown for
two points in Fig.14.6.
More formally, we can state this approach as follows: Assume a given training
dataset consisting of features and labels. Then, for a new point P in feature space
xˆ with unknown label yˆ we search for the most similar point of the training data,
. .
x , and assign its label to the new point, yˆ := y . “Most similar” is defined here
. i . i
as having the smallest EUCLIDean distance. This can be written as the following
Python code:
In [1]: import numpy as np
# X contains the training data: 2 features in columns and 4 records
# in rows, y contains the 4 labels (0 or 1) of the training data.
X = np.array([[ 0.19, -1.63], [1.62, 3.02],
[-2.54, 0.61], [-1.29, -0.26]])
y = np.array([0, 0, 1, 1], dtype=int)
# Define the coordinates ("features") of a point for which the
# label is wanted:
x_new = np.array([2, 0])
where we defined the training data and the unknown point. Next, we compute the
Euclidean distance of P to all points in X:
Fig. 14.6 Visualization of
the nearest neighbor method:
the training data is given by
the round markers where the
two colors denote two
different labels. For the two
unlabeled points P1 and P2,
the closest point of the
training data determines the
label, i.e., P1 belongs to class
1 and P2 belongs to class 2.
Additionally the two nearest
neighbors are shown for a
point P3 (cf. the explanations
in Sect.14.4.3)


================================================================================
PAGE 388
================================================================================

372 14 SupervisedClassification
In [2]: dist = []
for x_train in X:
# each row of X contains two coordinates ("features")
dist.append(np.sqrt((x_train[0] - x_new[0]) ** 2 +
(x_train[1] - x_new[1]) ** 2))
Then, we can find the nearest point and print its label:
In [3]: # Find index of smallest element (=nearest) using
idx = np.argmin(dist)
# ... and show the predicted class label
y[idx]
Out [3]: 0
There, a particularly useful function is numpy ’s argmin function: it is applied to a
numpy array and returns the index of the element in the array that has the lowest
value.
Computation of Distance between Sets of Points (cid:3)
In general, computing the distance between two sets of points can quickly
become a computationally expensive procedure. In such cases, using the
efficient scipy function distance_matrix is recommended.
RelationtoVoronoitesselation There is a related mathematical problem that helps
to understand how the nearest neighbor algorithm is working under the hood: it is
concerned with the question how the areas around a set of given “seed points” can
be partitioned such that all points within each area are closer to the respective seed
point than to any other seed points. The solution can be found by creating a Voronoi
diagram or Voronoi tesselation. Figure 14.7 shows a Voronoi tesselation of the data
from Fig.14.6, where additionally each cell was colored according to the label of
the training data. More details on the Voronoi tesselation and related geometrical
concepts and algorithms can be found in [1,3].
Why do the boundaries between each cells consist of straight lines? This is due
to using the EUCLIDean metric as distance measure; using, e.g., the Manhatten
distance would results in nonlinear shapes. Voronoi diagrams can be used in
multidimensional space and, in the context of materials science, have been used
to generate three-dimensional grain structures as input for simulations.1
1 As most of the real grain microstructures result from a growth process, they obey often a
lognormal distribution function. Therefore, many algorithms for creating artificial grain structures
either postprocess the Voronoi structures or more recently use other algorithms.


================================================================================
PAGE 389
================================================================================

14.4 NearestNeighborClassifier 373
Fig. 14.7 Using a “Voronoi
tesselation” for partitioning
the plane into areas, where
each of them contains one of
the given training data points
(“seeds”, the shown markers).
The areas are chosen such
that the points belonging to
them are closer to the
respective seed than to any
other seed. Additionally, the
decision boundary is shown
as the polygon that separates
regions of different colors
(classes)
With view on ML, it is quite remarkable that this relatively simple algorithm is
able to generate a complex decision boundary, which takes the shape of a polygon.
However, the flexibility of the method comes with the drawback that it is prone to
overfitting.
14.4.3 The General k-Nearest Neighbor Method
The k-nearest neighbors (kNN) method is a straightforward extension of the above
introduced method. kNN considers not only the direct neighbor point of an unknown
point but rather a certain neighborhood. This is done by using the k neighbors where
the label of the unknown point is inferred from the labels of the k neighbor data
points (cf. P3 in Fig.14.6). For determining the unknown label, different strategies
exist. For example, voting assigns the label of the most frequent class among the
k points; if there is a tie, then a class could be assigned by random choice. This,
however, is the reason why often odd numbers for k are preferred.
Problems may occur in case of class imbalance, i.e., where one class has more
data than the other. Then, voting is always biased toward the class with the most
samples. One remedy is to weight the k points according to the inverse of the
distance between the trainig data and the to-be-classified point. This method is
known as weighted nearest neighbor classifier.
The hyperparameter k “regularizes” the classification as it reduces noise and
fluctuations and avoids a strong dependency on only a single point. While values
k > 1 do not smooth out the boundaries between classes, it still prevents that
.
the decision boundary gets too strongly adapted to the training data. It thereby is
useful for avoiding overfitting. Nonetheless, the decision boundary still will stay
non-smooth and will have a polygonal shape with a number of kinks. The geometry
is solely determined by the training data and by the used distance metric. Figure 14.8
shows the decision boundaries for four different values of k. There, it can be seen


================================================================================
PAGE 390
================================================================================

374 14 SupervisedClassification
Fig. 14.8 Decision boundary of the kNN method for different values of k .Thevalueof.k = 3
already removes “kinks” and protuberances that are quite specific for this particular training dataset
quite effectively
that for k = 1and k = 2the shape of the decision boundary is strongly influenced
. .
by “extreme points”, and it closely resembles details of the data structure. Fork =3,
.
the curve is not bending around individual instances any more.
Determining which is a suitable value for the hyperparameter k can be done
manually by trial and error (as a rule of thumb, using k = 3 ...5 often gives
.
good results but is also dependent on the data) if one does not perform a full
hyperparameter optimization study. If k is manually varied, it is often advisable
to record the prediction accuracy and take the value of k for which the training error
is minimal before it eventually increases again.
All kNN classifiers are so-called lazy learners because there is no model that is
parameterized during the training process (e.g., recall that in a rule-based model, the
free parameters are identified; in a neural network even millions of parameters, the
weights, are identified during training). Thus, the key information of the training
data is not encoded at training time. Instead, the whole training process of a kNN
classifier consists of simply storing all training data and searching for the most
similar instances at prediction time. This is also called instance-based learning (cf.
Sect.11.2.2) and has a few noteworthy side effects:
. the training time is negligible as no computation-intensive training takes place;
. as the number of training data records increase, predictions will become more
and more costly as larger and larger datasets need to be searched
. deploying the ML model effectively means to deploy all training instances, which
can result in huge datasets
. it is very easy to add new training data (simply store them), no retraining of the
model is needed


================================================================================
PAGE 391
================================================================================

14.4 NearestNeighborClassifier 375
kNN can also be used for regression problems. Then the voting is replaced by
averaging.
14.4.4 Multi-class Classification
Multil-class classification can be done in analogy to what has been introduced
above and does not pose any new methodological problems. Figure 14.9 shows
the decision boundary for a classification of three classes. Also shown is the
misclassification error (i.e., the total number of wrongly classified data) as a function
of the nearest neighbor parameter k. The error was computed both for the training
dataset and the validation dataset after splitting the whole dataset into 60% training
and 40% validation data. There it can be observed that a low training error (there
are even no misclassifications fork =0!) does not imply a low testing error. Rather,
.
the maximum of the testing error and the minimum of the training error are a clear
indicator of overfitting. With increasing k, the training error increases while the
testing error decreases to a minimum error of seven misclassifications.
Words of advice (cid:2)
Directly comparing the values of the absolute classification error for the
training and validation dataset is not reasonable as the whole dataset was
split into validation and training datasets of different sizes!
Without cross-validation, it is not easy to choose a suitable value of k;anyvalue
between 9 and 15 seems reasonable. For example, for k ≥ 9, we see a substantial
.
decrease in the testing error. k ≤ 10 means that each point is determined by a
.
Fig. 14.9 Decision boundary of the kNN method for data from three different classes for a
particular value of k along with the number of misclassified instances as a function of k for the
training and validation dataset


================================================================================
PAGE 392
================================================================================

376 14 SupervisedClassification
neighborhood of 10 other points. This already corresponds to a relatively large area
of the shown two-dimensional plane. Given that the spatial distribution of points
is roughly homogeneous, that means that each predicted point is associated with
a region that is, in this example, approximately a tenth of the shown area. If the
physical phenomena investigated by this method exhibit a wave length that is much
below that size, then one cannot expect to get a small error. Thus, choosing the
hyperparameter k = 7 or k = 9 might be a reasonable choice. Example 14.1
. .
presents an example, where the knowledge about the underlying problem and its
data also turns out to be helpful for determining hyperparameters.
Example 14.1 kNN and a “Microstructural Length Scale” (cid:2)
Imagine you investigate a lamellar microstructure as shown in Fig.14.10.You
also might have acquired information for the α +γ region about, e.g., the
. 2
chemical composition, represented then in form of two classes stating if a
point is part of the matrix or not. The chemical composition was measured
at the points shown in Fig.14.11 where the two colors denote the two classes
(i.e., different chemical compositions).
Taking a look at the left plot we observe that there the whole dataset is
associated with different length scales: The first one results from the random
sampling and is the mean distance between sampling points. The second
length scale is a microstructural length scale and is the spacing between two
“bands.”
Choosing a suitable value for the number of nearest neighbors, k, is
influenced by the microstructural length scale, e.g., a small value of k = 5
.
nicely identifies the lamellar structure, whereas a high value of k does not
classify the structural details properly—it even predicts most of the blue
instances as “orange class” and vice versa. In the left panel, the circle around
the red astericks shows the area within which the five nearest neighbors are
located, the middle panel shows the circle for the 47 nearest neighbors. The
small circle has the same diameter as one of the bands, whereas the large
circle stretches across three bands. This is the reason why k = 5 coincides
.
with the minimal training error while k = 47 coincides with the maximum
.
error—which is even higher than the baseline of 0.5!
In summary, this example shows that it can be quite important to relate
the hyperparameter k to the physical phenomena responsible for the dataset
– or that considering our physical background knowledge can help to avoid a
costly hyperparameter optimization.


================================================================================
PAGE 393
================================================================================

14.4 NearestNeighborClassifier 377
Fig. 14.10 Example of a
lamellar microstructure found
in a TiAl-based alloy that was
reinforced with Carbide
particles (taken from [2],
Creative Commons CC BY
4.0, https://creativecommons.
org/licenses/by/4.0)
Fig. 14.11 Classification of measurement points of a lamellar microstructure. Shown are the
training data and classification results of the kNN method using two different values of k ; the
right panel shows the relative error
14.4.5 Implementation
A Python code that predicts class labels based on given training data is shown in
Python Listing 14.1. It uses an efficient SciPy function for computing a matrix of the
distances of all possible combinations. Additionally, numpy functions are used for
sorting the instances according to distances and for the subsequent voting. Further
explanations are given by the comments in the listing. Note, that there is no training
function, as the kNN method is an instance-based learner.


================================================================================
PAGE 394
================================================================================

378 14 SupervisedClassification
1 from numpy import ndarray, empty, argsort, argmax, bincount
2 from scipy.spatial import distance_matrix
3
4 def predict(X_train: ndarray, y_train: ndarray, X: ndarray, k=1) -> ndarray:
5 """
6 Returns the k-nearest neighbors for all instances in `X` and given training data,
7 X and X_train are 2D numpy arrays, y_train is a 1D array.
8 """
9 # row i of dist contains the distances for vector X[i] to every
10 # other vector of the training dataset
11 dist = distance_matrix(X, X_train)
12
13 # Go through all rows of dist, i.e. the distance of vector X[i] to all training
14 # points: get the `k` closest points and check which class has the most entries
15 y = empty(X.shape[0])
16 for i, d in enumerate(dist):
17
18 # get the indices "that would sort the 1D array `d`" from nearest to farest
19 sorting_indices = argsort(d)
20 kn_y = y_train[sorting_indices][:k].ravel()
21
22 # `bincount` returns the number of occurrences of each value of the array --
23 # therefore, the position of the maximum IS the class with the maximum count
24 count = bincount(kn_y)
25 y[i] = argmax(count)
26
27 return y
Python Listing 14.1 An implementation of the kNN algorithm. In case that there is a tie during
voting (here implemented using the argmax function, which returns the index of the element with
the maximum value), then only the first class with the highest count contained in count will be
used
Things to Remember about the kNN classifier ... (cid:3)
. The special case of.k=1is closely related to the Voronoi diagram.
. Complexity: It is a rather simple to understand methods that can be easily
implemented
. Accuracy: It is a very accurate model that can predict complex decision
boundaries and depends on only few hyperparameter (the number of neigh-
bors k and the distance metric)
. Performance: quite slow predictions due to “lazy learning”; increasingly slow
with growing dataset, only small dependency on value of k
. Multiple classes: kNN can easily be used for two-class and multi-class classifi-
cation
. Memory: Requires large storage as all training data are stored
14.5 Gaussian Naive Bayes Model
The method called Gaussian Naive Bayes (GNB) is one of the well-established
classification methods that has been around for several decades. It is a probabilistic
model and belongs to the class of the so-called naive Bayes methods .


================================================================================
PAGE 395
================================================================================

14.5 GaussianNaiveBayesModel 379
14.5.1 The Class of Naive Bayes Models
The naive Bayes methods all have in common that they are based on BAYES’
theorem, which, for n features X and given a specific label Y reads:
. i
P(Y)P(X ,...,X |Y)
P(Y |X ,...,X )= 1 n . (14.12)
. 1 n
P(X ,...,X )
1 n
Words of advice (cid:2)
ThenaiveBayesmodelsareprobabilisticmodels,andthereforethe“features”
should be rather understood as random variables, and Eq.(14.12) should be
written more explicitly as
P(Y =y |X =x ,...,X =x )
. 1 1 n n
P(Y = y)P(X =x ,...,X =x |Y =y)
= 1 1 n n .
P(X =x ,...,X =x )
1 1 n n
There, it is more obvious that, e.g., X is the random variable and x is the
. 1 . 1
particular value. For different data records, the random variable stays the
same, and only the particular value changes. However, for brevity, we will
use the above short notations.
The key idea of the naive Bayes methods is to make the “naive” assumption, that
theX andX of every possible pair of input variables are conditionally independent
. i . j
for a given label Y, which translates to
. P(X i |Y,X 1 ,...,X i−1 ,X i+1 ,...,X n )=P(X i |Y) (14.13)
an which generalizes to the following equation:
(cid:7)n
P(X ,...,X |y)= P(X |Y). (14.14)
. 1 n i
i=1
Upon insertion of this identity into Eq.(14.12),itfollows:
(cid:8)
P(Y) n P(X |Y)
P(Y |X ,...,X )= i=1 i . (14.15)
. 1 n
P(X ,...,X )
1 n
Finally, upon realizing that P(X ,...,X )is constant we find that the probability
. 1 n
to obtain the label Y for the given features is proportional to the nominator of
Eq.14.12:


================================================================================
PAGE 396
================================================================================

380 14 SupervisedClassification
(cid:7)n
P(Y |X ,...,X )∝P(Y) P(X |Y). (14.16)
. 1 n i
i=1
Now, we have a probability for every possibly class label, given a set of feature
values. The most likely class label is obtained as the one that gives the maximum
probability,
(cid:9) (cid:10)
(cid:7)n
Ypred =argmax P(Y) P(X |Y) . (14.17)
. i
Y
i=1
The question left to be answered is: How canP(Y)andP(X |Y)be obtained? The
. . i
first probability is, for all naive Bayes models, straight forward: P(Y)is simply the
.
relative frequency of class Y w.r.t. the training datasetD train:
.
(cid:11)n
[[y =y]]
i
P(Y =y)= i=1 , (14.18)
.
n
where [[y = y]] is 1 only if y = y and 0 otherwise. Effectively, this gives the
. i . i
number of times that label y occurs in the training dataset.
For a naive Bayes model, the factor P(X | Y) can be estimated in a similar
. i
way, whereas for a Gaussian naive Bayes model the formulation is shown in the
following.
14.5.2 Maximum-Likelihood Estimation for the Gaussian Naive
Bayes Model
The particular formulation of P(X |Y)is, where the various naive Bayes methods
. i
differ; for the Gaussian Naive Bayes, the likelihood of every feature is assumed to
follow a (Gaussian) normal distribution (cf. Sect.10.7). To calculate the conditional
probability P(X | Y) it is important to realize that in this expression, Y denotes
. i
a particular class label, Y = y. Therefore, the dataset needs to be split into sub-
.
datasets, each of which contains only data records that have the same label. For
example., for label j, we write this as
(cid:12) (cid:13)
D . y=j = (x 1 ,...,x n ,y)|(x 1 ,...,x n ,y)∈Dtrain andy =j (14.19)
For example, for feature number i and label j,itis
(cid:14) (cid:15)
. P(X i |Y =y)=N (x i ,y)∈D y=j ; μ y ,σ y . (14.20)


================================================================================
PAGE 397
================================================================================

14.5 GaussianNaiveBayesModel 381
Note that the normal distribution is fitted to all data records that have the same class
j. This needs to be done for all features and for all class labels. Also note that the
conditional independence of the features implies that for each variable a univariate
normal distribution is fitted and not a multivariate distribution to all features at the
same time.
14.5.3 Algorithm and Python Implementation
As a preparation for a Python implementation, we summarize the above formu-
lations in the steps shown in Algorithm 14.1. In particular, we emphasize the
difference of the training and the prediction by using explicit superscripts.
Algorithm 14.1: Gaussian naive Bayes Classification (cid:4)
algorithm]algo:classification:GNB
Given is a training datasetD . train consisting of n features, m records and a single
target variable that contains the class labels in form of integer numbers.
1. Split the dataset into disjunct datasets,D . y tr = ai k n, such that the class label within each
of them are the same.
2. Findthemeanvaluesandthestandarddeviationsbyfittinganormaldistributions
to the training data—individually, for each class label and for each feature. For
example, for the records taken fromD . y tr = ai 1 n, this results in the mean values
.μ X1,y=1,...,μXn,y=1.
3. Compute.P(Y =y)for all class label. E.g., for label k this is
D| train|
.P (Y =k)=
D|
y
tr
=
ai
k
n|
where. | A|denotes the number of elements in set A.
4. Choose a new input record for which the label is to be predicted is
pred pred
.(X
1
,...,Xn ).
5. Compute the conditional probabilities.P(Xi | Y)for each feature, separately for
each datasetD . y p = re k d . E.g., for class number 1 this reads
(cid:16) (cid:17)
.P(X
1
pred|Y =1)=N X
1
pred; μX1,y=1,σX1,y=1
.
.
.
(cid:16) (cid:17)
P(Xn pred|Y =1)=N Xn pred; μXn,y=1,σXn,y=1 ;,


================================================================================
PAGE 398
================================================================================

382 14 SupervisedClassification
where the normal distributionsN . are evaluated at the records of the testing
dataset .X
i
pred (.i ∈[1,n]).
6. Compute the “score” for each possible class (the term inside the parentheses in
Eq.(14.17)), e.g., for class.Y =1:
(cid:16) (cid:17)
. SCORE(Y =1)= P(Y=1)· P(X 1 pred|Y =1)· ...· P(Xn pred|Y =1)
7. Compare the scores of the record, for which the label is to be predicted, for all
classes and choose the one with the highest score as the most likely one.
In principle, GNB is a simple method. However, the devil is in the detail, which
is here, mainly the split into sub-datasets and when to use training or the to-be-
predicted data. For example, we also realize that the training process is finished
after Step 2.Step3 is then a preparation for the following prediction. The advantage
of this detailed algorithm is that we can very easily transfer it now into Python code.
We write this for the special case of two classes and avoid for loops and
advanced vectorization in favor for readability.
In [1]: import numpy as np
from scipy.stats import norm # Gaussian distribution
from numpy.random import multivariate_normal
Here, norm is be required for computing the probability density function (PDF),
multivariate_norm for sampling from a bivariate Gaussian distribution for creating
training data:
In [2]: X_class0 = multivariate_normal(mean=(-1, 0), cov=[[1, 0], [0,2]], size=30)
X_class1 = multivariate_normal(mean=(2, 3), cov=[[1, 0], [0,1]], size=20)
Y_class0 = np.zeros_like(X_class0)
Y_class1 = np.ones_like(X_class1)
Next, we compute the mean values for all classes and for all features. As additional
preparation, we also already computed the marginal probabilities of both classes.
In [3]: means_class0 = [np.mean(X_class0[:,0]), np.mean(X_class0[:,1])]
means_class1 = [np.mean(X_class1[:,0]), np.mean(X_class1[:,1])]
stddev_class0 = [np.std(X_class0[:,0]), np.std(X_class0[:,1])]
stddev_class1 = [np.std(X_class1[:,0]), np.std(X_class1[:,1])]
P_Y_for_class_0 = np.size(Y_class0) / np.size(Y)
P_Y_for_class_1 = np.size(Y_class1) / np.size(Y)
Now, the model is trained, and we choose a feature vector for which we seek the
class label.


================================================================================
PAGE 399
================================================================================

14.5 GaussianNaiveBayesModel 383
In [4]: X_pred = np.array([2, 2])
X_pred is just a one-dimensional array (if the code should be vectorized, this needs
to be changed). Subsequently, we create numpy “Gaussian random variables” that
can be used to evaluate the PDF at the “location” X_pred . (cf. Sect.10.7 for more
details).
In [5]: # class 0
rv0 = norm(loc=means_class0[0], scale=stddev_class0[0]) # first feature
rv1 = norm(loc=means_class0[1], scale=stddev_class0[1]) # second feature
P_X1_given_class0 = rv0.pdf(X_pred[0])
P_X2_given_class0 = rv1.pdf(X_pred[1])
# class 1
rv0 = norm(loc=means_class1[0], scale=stddev_class1[0]) # first feature
rv1 = norm(loc=means_class1[1], scale=stddev_class1[1]) # second feature
P_X1_given_class1 = rv0.pdf(X_pred[0])
P_X2_given_class1 = rv1.pdf(X_pred[1])
Finally, we are able to compute the scores for the point w.r.t. both classes
In [6]: P_X_given_class0 = P_X1_given_class0 * P_X2_given_class0
P_X_given_class1 = P_X1_given_class1 * P_X2_given_class1
score_class0_given_X1_X2 = P_X_given_class0 * P_Y_for_class_0
score_class1_given_X1_X2 = P_X_given_class1 * P_Y_for_class_1
if score_class0_given_X1_X2 > score_class1_given_X1_X2:
Y_pred = 0
else:
Y_pred = 1
where Y_pred contains the predicted class label of X_pred .
As always, the decision boundary can be visualized by making predictions
at equidistant points in the two-dimensional feature space, which are then used
together with a contour plot, as shown in (Fig.14.12).
We conclude the GNB model with two short comments: To avoid small numbers
and numerical underflows for the values of the conditional probabilities (they tend
to be rather small), many implementations use logarithmic probabilities. If there are
Fig. 14.12 The Gaussian
naive Bayesian model:
decision boundary and
training dataset


================================================================================
PAGE 400
================================================================================

384 14 SupervisedClassification
not data at all for a specific class, then one might encounter a “division by zero”
error; therefore, in a production code, one should guard against this case.
Things to Remember about the Gaussian naive Bayes model (cid:3)
. The GNB model is able to learn nonlinear decision boundaries.
. Strictly speaking, the data need to be normally distributed; however, in in
practical situations, this is not strictly required.
. The implementation is relatively simple, and the method is fast because no
iterations for finding parameters are required.
. As long as the conditional independence is fulfilled, it is quite a reliable
method.
. By comparison with other methods, larger amount of data are required for
achieving good results.
14.6 Support Vector Machines
The main issue with kNN is the computational cost for comparing all n points to
all other points. With an support vector machine, this approach changes entirely.
The linear support vector machine (SVM) method is one of the older methods
and was created already in the 1960s by VLADIMIR VAPNIK. The extension is
the nonlinear SVM, which is a very powerful classification method. Both will be
introduced subsequently.
Support Vector Classification (cid:3)
Because SVMs is used both for regression and classification, sometimes a SVM
in the context of classification is in the literature also called support vector
classifier, abbreviated as SVC while the variant used for regression is called
support vector regressor, abbreviated as SVR.
14.6.1 Linear Support Vector Machine
Assume there are two classes of linearly separable data. There exists no unique
solution for choosing, e.g., the inclination of a straight line for separating the two
groups of points, compare the left panel of Fig.14.13. This is exactly, where SVMs
come into play: there, the idea is to maximize the distance from the line to the
nearest data points, making the problem of dividing linearly separable data by a
straight line uniquely solvable. This is shown in the right panel of Fig.14.13. Note,
that the solid line (the decision boundary) is located exactly centered between the


================================================================================
PAGE 401
================================================================================

14.6 SupportVectorMachines 385
Fig. 14.13 Linearly separable dataset: the left panel shows possible linear decision boundaries;
the right panel shows a decision boundary (solid black line) that maximizes the distance to the
nearest data (dashed lines). The “support vectors” are indicated as the three short (two blue and
one orange) lines, perpendicular to the decision boundary
two dashed lines. This concept can be generalized to higher-dimensional data. Then,
the linear decision boundary becomes a (hyper-)surface; other than that, nothing
changes. The short lines from the decision boundary to the three data points are the
support vectors and, by construction, have the same lengths. This distance is called
margin, and the hyperplane that maximizes the margin is called optimal hyperplane.
There is a whole group of methods that maximize the distance from data to the
decision boundary: these are called maximum margin estimators.
Hints on Numerical Implementation
The numerical algorithm is based on an optimization strategy: there, the goal is that
the distance to the nearest points of the two classes should both be d, and it should
be maximized. Typically, for classification problems, the hinge loss is used, which
is defined as
(cid:18)
0 ifytrain·ypred ≥1.
L(xtrain,ytrain)= (14.21)
.
1−ytrain·ypred otherwise.
where the two class labels take the values −1and +1. As a consequence, ytrain =
. . .
ypred implies a zero loss value while ytrain! = ypred results in a loss value of 2. The
.
cost function will consists of two terms: (i) the mean hinge lossL of all examples
.
and (ii) the constraint that maximizes the width of the data-free channel, which
leads to a constraint optimization problem. ypred depends on the inclination and
.
the intercept of the line in two dimensions, which are given by the parameters to
be optimized. In all generality, the free parameters define the hyperplane (or in the
two-dimensional case: the line). If the parameters are chosen correctly, then the
boundaries of the whole margin (the dashed lines of the shaded area in Fig.14.13)


================================================================================
PAGE 402
================================================================================

386 14 SupervisedClassification
touch the data only in a few number of points. Generally, if the dataset has n features,
then a total of n+1points necessarily touch the boundaries of the “channel”.
.
Taking the optimal hyperplane as a decision boundary is a robust approach: even
if new training data are added, it is expected, that this hyperplane is, on average,
the least affected as compared to any other hyperplane. Additionally, points that are
located within the convex hull of the data of any class have no effect on the decision
boundary and the support vectors. This also has the side effect that SVM is not very
sensitive w.r.t. outliers.
Unfortunately, it is also obvious that this method (similar to logistic regression)
only works for linearly separable datasets. There are approaches that allow a small
number of points in the data-free, separating region. Those act as a regularization,
as the original requirement is a bit relaxed, and leads to multi-objective optimization
strategies. However, the main characteristics of the dataset still have to be such that
the data are mainly linearly separable. Otherwise, one has to ressort to nonlinear
methods, such as the nonlinear SVMs.
14.6.2 Nonlinear Support Vector Machine
Data that are not linearly separable can be used for supervised classification with
the method of nonlinear support vector machines. The main idea is, that the dataset
is mapped to a higher-dimensional space, in which there are more possibilities to
find a suitable decision boundary. The left panel of Fig.14.14 demonstrates the idea
based on a one-dimensional dataset which is turned into a two-dimensional dataset
by assigning all blue markers theX -value of 0 and all orange markers theX value
. 2 . 2
of 1. In the right panel, we can see that then it becomes nearly trivial to separate the
two classes. In all generality, the functions that perform such a mapping are kernel
Fig. 14.14 The left panel shows a dataset with only one feature and with two different class labels,
indicated by the two colors, which are clearly not linearly separable. By using the class labels to
“extrude” the data and thereby to create a new feature, the two groups of data become linearly
separable, indicated by the dashed line


================================================================================
PAGE 403
================================================================================

14.6 SupportVectorMachines 387
Fig. 14.15 Support vector
classificationfor a dataset
with two features. The larger
circles around the data points
show the support vectors
functions, cf. Sect.13.5.4. There, each point would be replaced by, e.g., a GAUSSian
distribution, and weighted with the integer representing the class label. Then, each
of these GAUSSians is superimposed, which, depending on the shape parameter of
the distributions, results in a smooth function—the kernel projection—for which
different “levels” can be easily separated using a threshold.
However, in practical situations, where datasets can have many features, this
is a procedure that can become computationally very expensive. The solution is,
what is known as the kernel trick. There, the training on the projected data can be
done implicitly, without having to construct the higher-dimensional, superposition
of kernel functions. This makes nonlinear SVMs not only very effective for data
that are not linearly separable; the kernel trick is the reason, why this method is
computationally very efficient.
There are a number of kernel functions that can be used, such as polynomials or
radial basis functions. As the derivation and the implementation are more lengthy,
we will not detail it here and rather suggest one of the ML libraries, such as scikit-
learn, which has a very efficient implementation.
A demonstration of how the decision boundary looks using SVM for a dataset
with two feature is shown in Fig.14.15. For this, we used a radial basis function
(RBF) kernel, which requires only the standard deviation σ as a parameter and is
.
given by
(cid:19) (cid:20)
||X −X ||2
f(X ,X )=exp − 1 2 . (14.22)
. 1 2 2σ2
The RBF kernel is often used as a “default” for nonlinear problems Typical sizes of
datasets for SVM are usually “mid-sized” records, e.g., up to several thousands of


================================================================================
PAGE 404
================================================================================

388 14 SupervisedClassification
data records. If the datasets are becoming much larger, say, several millions of data
records, then it is usually advisable to first try to reduce the dataset by some other
means or to use ML methods that are more suitable for large datasets.
14.7 Exercises
14.1. Figure 14.3 is a very simple example, and the “learning strategy” is even
more simplistic. What happens to the decision boundary when more positive or
negative data are added? Think about a simple approach that also would consider the
negative examples and implement it in a Python code. Plot the decision boundary.
How sensitive w.r.t. to outliers (=data points with extreme values) is your strategy?
14.2. Use your kNN Python implementation to numerically show that increasing
the value of k does not significantly influence the computational cost for predictions.
14.3. Rewrite and optimize the code for the Gaussian naive Bayes method from
Sect.14.5.3. Proceed in the following two steps:
(a) In a first step, use for loops such that you are able to apply the code to
an arbitrary number of features. Also, predictions for an arbitrary number of
records should be possible
(b) Now, optimize the code for performance. For this, use vectorization to replace
some of the for loops. Measure the performance gain as compared to task (a)
References
1. F. Aurenhammer, R. Klein, and D.-T. Lee. Voronoi Diagrams and Delaunay Triangulations.
World Scientific, Aug. 2013. DOI https://doi.org/10.1142/8685.
2. J. Lapin, K. Kamyshnykova, and A. Klimova. Comparative study of microstructure and
mechanical properties of two tial-based alloys reinforced with carbide particles. Molecules,
25(15), 2020. ISSN 1420-3049. DOI https://doi.org/10.3390/molecules25153423. URL
https://www.mdpi.com/1420-3049/25/15/3423.
3. C. Tóth, J. Goodman, and J. O’Rourke. Handbook of Discrete and Computational Geometry.
Discrete mathematics and its applications. Chapman and Hall/CRC Press, 3rd edition, 2017.
ISBN 9781498711395. DOI https://doi.org/10.1201/9781315119601.
4. I.H.Witten,E.Frank,andM.A.Hall. Data Mining: Practical Machine Learning Tools and
Techniques. Morgan Kaufmann Series in Data Management Systems. Morgan Kaufmann, 3
edition, 2011. ISBN 978-0-12-374856-0. URL http://www.sciencedirect.com/science/book/
9780123748560.


================================================================================
PAGE 405
================================================================================

15
Unsupervised Learning
“Study hard what interests you the most in the most
undisciplined, irreverent and original manner possible.”
Richard Feynman (1918–1988)
American theoretical physicist
15.1 Introduction to Dimensionality Reduction
Twenty years ago, the need for dimensionality reduction was directly clear, as
computer memory and storage capacities were very limited and expensive resources.
Why should we still worry today about “a few dimensions more or less”?
15.1.1 Example
Let us start with a simple example to elucidate some aspects of the dimensionality
of data:
Example 15.1 Dimensionality of Image Data (cid:2)
Consider the dataset DS-2 (MNIST), which consists of small images with a
size of28×28pixel. Each of their pixel is either black or white and therefore
.
has the value 0 or 1. How many different images can be created? There
are
228×28
possible realizations. However, most of these images look mainly
.
random and only extremely few of them contain notable structure and patterns
that look like digits. Therefore, sampling from the high-dimensional space
(continued)
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 389
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_15


================================================================================
PAGE 406
================================================================================

390 15 UnsupervisedLearning
of all images is not reasonable, and we seek to construct lower-dimensional
spaces that ideally contain only those data points that we are interested in.
Example 15.1 can also be interpreted such that an machine learning (ML) method
that takes samples from the high-dimensional space would have a very hard time
to separate the noise from the signal. The curse of dimensionality, introduced in
Sect.2.4, suggests the same but in a slightly different embodiment: reducing the
dimensions of the feature space results in a denser data coverage, which is beneficial
for the ML training. These are reasons why often, methods for dimensionality
reduction are used to preprocess highdimensional data for a different ML method.
15.1.2 The extrinsic and intrinsic dimensionality
A similar problem is also known in the field of signal processing and information
science. There, commonly used notions are that of the extrinsic dimensionality
versus the intrinsic dimensionality [2]. The extrinsic dimensionality of a dataset
is very well defined as the given number of features or variables. The intrinsic
dimensionality, however, gives the number of variables that are needed to represent
the dataset after all redundancy has been removed. As such, the reduced set of
variables can be understood as a minimal representation of the data. This does
not imply that the minimal representation is exactly identical to the original
representation but it typically does imply that all of the “most relevant” aspects
of the dataset are recovered. An important aspect of dimensionality reduction is
to transform the original data into a new representation where (i) the intrinsic
dimensionality can be easily seen and (ii) where the intrinsic dimensionality is—
sometimes significantly—smaller than the extrinsic dimensionality.
15.1.3 Notations and Conventions
In the previous chapters, we always considered supervised learning methods where
we needed both an set of input and output data. Here, we only require input data,
and hence, we start with a brief overview over the used notations for this type of
dataset. Assume that the data of interest in the datasetD consist of m records (e.g.,
.
measurements) and that the data itself may not be just scalar data but might either
consist of vector-valued data or of data with several attributes (e.g., temperature,
displacement, and stiffness could be three such attributes),
D . ={x i } i=1,...,m withx i =[x i1 x i2 ... x in ]∈Rn . (15.1)


================================================================================
PAGE 407
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 391
There, each of the x is considered a row vector and has n features or attributes.
. i
Note, that this notation requires some care (and the use of the transposition
symbol) as we usually assume that vectors are column vectors. The dataset can be
represented by a data matrixX, which has n features in columns and m data records
.
in rows. Explicitly writing all components gives:
⎡ ⎤ ⎡ ⎤
—x — x x ··· x
1 11 12 1n
⎢ ⎢—x 2 — ⎥ ⎥ ⎢ ⎢x 21 x 22 ··· x 2n ⎥ ⎥
. X=⎢ ⎣ . . ⎥ ⎦ =⎢ ⎣ . . . . . . ⎥ ⎦ . (15.2)
. . . .
—x — x x ···x
m 21 22 mn
Often, we assume that the data for each feature are centered around the mean value,
which can be achieved by subtracting the mean of the j-th column, μ , from each
. j
of its elements x ,
. ij
x =x −μ . (15.3)
. ij ij j
Therefore, we denote by X the centered data matrix where its elements are the
.
x , the row vector x is the centered data record, and the column vector X is the
. ij . i . j
centered feature number j.
15.2 Principal Component Analysis: Theoretical Background
and Derivations
Principal component analysis (PCA) is a powerful method from the toolbox of
statistical data analysis; it is one of the most well-known and well-studied methods
used for dimensionality reduction of a dataset, i.e., for constructing a representation
of the data that has an intrinsic lower dimensionality and at the same time preserves
as much structure of the data as possible. The dataset itself consists of only the
featuresX , no labelsy as in supervised learning methods are used or required. The
. i . i
features are the coordinates of points in the feature space and can be represented in
a two-or three-dimensional plot as point clouds. The coordinate system that is used
for showing such data is arbitrary, and the Euclidean coordinate system is only one
possible choice. However, coordinate systems and coordinate transformations play
an important role in PCA, as we will see in the following.
The main idea of PCA is to find the best or most suitable coordinate system where
a reduced number of directions (or basis vectors) contain as much information
(or equivalently: as much structure of the data) as possible. Additionally, the
dimensionality of the data can be reduced by projecting the data along these
lower-dimensional, “optimal” directions, which are embedded into the original,
high-dimensional space. In the following sections, we will see that PCA can
answered many questions. Among those are the following:


================================================================================
PAGE 408
================================================================================

392 15 UnsupervisedLearning
• Are the input variables correlated such that they might contain redundant
information?
• How can variables be decorrelated?
• Is there an alternative representation of the data in terms of “simpler” and fewer
input variables?
• What does “simpler” mean in the context of PCA.
• Which (possibly new) variables are the most important ones for representing the
data such that the less important variables could be removed without loosing to
much of important information?
PCA is typically used for high-dimensional data. Examples of high dimensional
data are images that can have millions of pixel with each pixel being a single feature,
or datasets from molecular dynamics simulations containing the position of millions
of atoms. PCA helps to reduce the dimensions of datasets and thereby to enable better
generalization of (statistical) ML models. It helps to remove noise from images. And
it ultimately helps to use computational resources more efficient, e.g., in terms of
computational time or memory requirement.
PCA does not require any previous knowledge about the dataset, which can be
useful if we don’t have that knowledge. Obviously, this also might be a drawback
as it is not possible to include any information about the underlying physics of the
problem that we might possess. As an example think of a dataset that contains values
of potential and kinetic energy measured at many points within a closed system.
In PCA it is not possible to enforce the law of conservation of total energy (this
would be our “physical preknowledge”), and the lower-dimensional representation
will very likely result in a very different total energy.
PCA is closely related to singular value decomposition (SVD) and is formulated
in terms of eigenvectors, eigenvalues, using techniques and concepts from linear
algebra. It is a linear method for dimensionality reductions since it relies on linear
transformations of basis vectors. The mathematical details are given in Appendix A,
but the remainder of this section can be understood without comprehending all
mathematical details.
15.2.1 A Simple Motivation from Solid Mechanics
In continuum mechanics, a related application case is to transform a coordinate
system with respect to which a stress state is defined so that the coefficients of the
stress tensor are simplified to the so-called principal stresses. Those readers who
have an engineering or physics background most probably will have come across
either the graphical construction method using the so-called “Mohr’s circle” or
directly used eigenvalues and -vectors to find the principal stress components in
two-or three dimensions. Typically, the corresponding mathematical derivations of
the equations for computing principal stresses are based on formulating equilibrium
equations of forces: there, the vertical and horizontal force components are com-
pared to equivalent forces acting parallel and perpendicular to an inclined plane.


================================================================================
PAGE 409
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 393
Fig. 15.1 In two dimensions, a stress state is given by two normal and one shear component.
Principal stresses are obtained by rotating the reference coordinate system such that the shear
stress vanishes. The stress tensor then consists of only two components, the principal stresses—a
lossless “data compression” with a compression ratio of 3:2
Forces can be “converted” to stress components. Then, the inclination of that plane
is found by requiring that the resulting shear stress acting in the plane vanishes—
a minimization problem. From the normal and perpendicular forces then the two
principal stress components are directly obtained, cf. Fig.15.1.
In this case, the originally three components of the stress tensor are reduced
to two different stress variables that are able to still represent the same overall
stress state—this is roughly similar to what PCA is automatically doing for
general datasets: automatically finding directions (i.e. “descriptors”) that represent
features of the given dataset such that redundancy is removed and the data are
“compressed” through this dimensionality reduction. For the example of principal
stresses, the redundancy is the result of the Poisson effect through which normal
strain components (and through this also stress components) along the x and
y direction are coupled. Here, the set of new variables are prescribed and the
coordinate transformation is computed. PCA is able to automatically find a suitable
coordinate transformation (i.e., a new basis system). The variables w.r.t. these
principal components, however, might not be interpretable in terms of any direct,
physical meaning.
15.2.2 Further Examples of “Higher”-Dimensional Data Sets
The next applications and examples take a look at PCA from a point of view of data
science and explain some of the main aspects:
1. The Iris dataset: One of the famous datasets in the ML literature is the Iris
dataset. The dataset consists of 150 records each of which has four attributes
characterizing different kinds of flowers; for more information, refer to the
description od the DS-1 (Iris Flowers, Sect.4.7) dataset in Chap.14. The dataset
is summarized in the six scatter plots in Fig.15.2. PCA can be used to get rid
of noise and redundancy, e.g., from the figure we observe, that the petal length
and petal width are positively correlated and therefore already one of them might


================================================================================
PAGE 410
================================================================================

394 15 UnsupervisedLearning
Fig. 15.2 The six scatter plots visualize the data of the four-dimensional “Iris Dataset” where
the three different flowers are represented by the three colors. The right plot depicts the dataset
reduced to the first principal component. There, the “error bars” show the mean value (the dots)
and the standard deviation (lines)
carry sufficient information. The right plot of Fig.15.2 shows the data projected
onto the axis of the first principal component. While some information is lost, it
turns out that most of the important information, e.g., for a classification task is
still retained, and only one principal component is already able to separate the
three classes of different flowers fairly well. Only considering some or even only
one principal components is a way of data compression—here to a compressed
size of 25%the original dataset.
.
2. Discovery of New Materials: The power of PCA lies in reducing high-
dimensional data to only few dimensions. There, the dimensionality reduction
helps to escape the “curse of dimensionality” (Sect.2.4) since sampling
the reduced space typically requires significantly fewer sampling points.
In computational materials science and theoretical chemistry, this has been
exploited for the discovery of new materials with enhanced properties (see e.g.
[5,13]).
3. Image Compression: A different type of dimensionality reduction can be
observed in the context of image data: an image with, e.g., 32 × 32 = 1024
.
pixel can be represented as a vector with 1024 entries by placing the values of
all pixels on top of each other. This vector is thus defined in a 1024-dimensional
space. PCA helps to represent these vectors in an alternative space where we can
tune the accuracy (or, inversely, the level of compression) by including more
or less dimensions into the new representation. PCA allows to reduce images to
a few features, which then can be used for as input for other data mining or
prediction methods. Examples include characterization of microstructures from


================================================================================
PAGE 411
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 395
microscopy or simulations or the distribution of stress data from two-dimensional
finite element method (FEM) simulations as, e.g., in [19].
15.2.3 The Algorithm of Principal Component Analysis in Brief
We will now summarize the method of PCA in a way that requires only basic
knowledge of linear algebra and vector/matrix calculus. All proofs and further
discussions can be found after this section.
The main idea of PCA is to find a number of k new coordinate axes or basis
vectors that maximize the variance of the data. There, k is usually smaller than the
original dimension of the dataset. Why is it important to maximize the variance of
data in the new coordinate system? Imagine points on a single coordinate axis: the
smaller the variance of the data, the closer the points get to each other (on average),
which ultimately means that all points would coincide for a variance value of zero.
The variance is a measure for the variability of the data, and the more spread out
the data is the more “distinguishable” are details of the distribution and its structure.
Therefore, maximizing the variance can be interpreted as maximizing the amount
of contained information.
A two-dimensional example is shown in Fig.15.3. There, the first step is to center
the dataset such that the mean coincides with the origin of the coordinate system.
Subsequently, the first direction of a new coordinate axis for which the variance is
maximized is obtained. This is the first principal component or basis vector of the
principal space, which here is one-dimensional. The data are then projected onto
this direction. If the principal subspace consists of multiple components, then a new
direction that is orthogonal to the previously direction is computed. The direction
should not just be orthogonal to all previous axes, it also should again maximize the
remaining variance of the data. This procedure is then repeated for as many times
as there should be principal components (PCs).
In an additional postprocessing step, the projected data from the PC space can be
transformed back to the real space (e) and (f) in Fig.15.3, which also gives a first
hint about how PCA can remove noise.
It will turn out that the principal directions are nothing but the eigenvectors
of the data covariance matrix, which is a surprising and at the same time also
very convenient fact. Eigenvectors and -values are typically sorted according to
the magnitude of the eigenvalues such that the first eigenvalue corresponds to the
largest possible variance of the data along one direction—the direction of the first
eigenvector. The PCA algorithm can be summarized as shown in Algorithm 15.1.
15.2.4 Preparation: Orthogonal Projection of Data onto a Line
As a preparatory step for the derivation of the PCA method, we now derive the
mathematical description of projecting the dataD onto a line and investigate the
.
consequences on the variance of the projected data. We again assume that the dataset


================================================================================
PAGE 412
================================================================================

396 15 UnsupervisedLearning
Fig. 15.3 PCA for a two-dimensional dataset (a)where.X 1and.X 2are correlated: after centering
the original dataset (b), the first principal component is obtained ((c), vector shown in red). The data
is then projected onto this direction (d). As a “postprocessing” step, the data from the “principal
space” can be reconstructed in the real space (e) where the centering needs to be reverted (f)
is represented by the centered data matrix X with features in columns and each
.
row being a data record or sample. In this section, we are only concerned with
the mathematical derivations for a single point. We therefore use as a abbreviation
x :=x in this section, which denotes one of the rows of the centered data matrixX.
. i .
A key idea of principal component analysis is to project data points orthogonally
onto a suitable line or—by iterative construction—a plane, or a hyperplane that
typically has a lower dimension than the space from which the data was taken. E.g.,
ifD has two features, then the points are to be projected onto a line, given by a
.
column vectoru. Figure 15.4 visualizes and introduces the most important variables
.
and steps for the projection. The left panel shows that the process of projecting
consists of decomposing the position vectorx(which points to one of the data points
.
and starts at the origin) into a vector that is parallel to the line given byuand a vector
.
nthat is perpendicular to u. Here, uis a vector representing an line with arbitrary
. . .
inclination; determining its “most suitable” direction will be done in the section
below. The two vectors, pand n, can be easily computed using vector calculus (the
. .
detailed derivation is given in Appendix A.1.8)as


================================================================================
PAGE 413
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 397
Algorithm 15.1: Computation of the Principal Components (cid:3)
1. Ensure that the data matrix.X contains n features in columns, each of the m rows
is a record.
2. Ensurethat.X iscentered,e.g.,foreachfeature i wesubtractthemeanofallrecords
.E[Xi]from each entry of the column vectors.X i:
.Xi :=Xi −E[Xi]1 with 1 =[1,...,1]T (15.4)
Optionally, standardize the data for each feature: .Xi := Xi/si where .s i is the
standard deviation of feature i.
3. Compute the centered data covariance matrix.S from the centered data.X :
.S=
m−
1
1
X T X (15.5)
4. Compute the eigenvalues .λ i and the corresponding unit eigenvectors .u i of the
centered data covariance matrix.S .
5. Sort the eigenvalues and corresponding eigenvectors such that.λ
1
≥λ2 ≥ ...≥
λnwith all.λi ∈Rand.λi ≥0;..u 1 is then the first principal component, and.u i is
the i-th principal component.
6. Assuming that .U is the matrix of the first k eigenvectors. Then, the projected
coordinates of the data in the system of the k largest principal components are
(cid:8) (cid:9)
| |
∗
.X =XU with U= u1 ··· uk . (15.6)
| |
x·u
p =cu with c= (15.7)
. uT·u .
n = xT − p = xT − cu , (15.8)
where the dot “·” denotes the scalar product between two vectors. Recall, that x
. .
represents a row from the centered data matrix X and therefore is a row vector
.
while u is a column vector, as is the convention. The vector u in Eq.(15.7) can
. .
be understood as a basis vector for the one-dimensional space of a line. c can be
understood as the coordinate w.r.t. that basis vector.
Intuitive explanation: An intuitive analogy of the vector u and the scalar
.
c is our usual three-dimensional space with Cartesian coordinates where the
“coordinate axes” are given by the unit basis vector e = [1,0,0], e =
. 1 . 2
[0,1,0], ande =[0,0,1]. Each pointxis represented by three scalar values
. 3 .
c , c , and c ∈Rsuch that
. 1 . 2 . 3
(continued)


================================================================================
PAGE 414
================================================================================

398 15 UnsupervisedLearning
x =c e +c e +c e ∈R3 . (15.9)
. 1 1 2 2 3 3
The only difference with regard to the above “projection coordinate system”
is that u is not necessarily a vector with unit length (at least not here), and
.
that we are dealing with only one dimension, which requires only one basis
vector.
The right panel of Fig.15.4 shows all resulting, points projected onto the line that
was given byu. These points can be understood as an approximation for the original
.
data. Clearly, there is an error involved, which would get smaller the closer to the
line all points are located. At the same time, the larger the deviation from the line,
i.e., the variance along the perpendicular direction, the less appropriate it is to use
only one component.
Variance and Mean Squared Error as a Function of Line Orientation
So far, the lines’ angle given by u was arbitrarily assumed, but obviously there
.
are angles of inclination of the new axis that are “better” suited than others (again,
we still have to define what “better” means in this context). Figure 15.5a,b show
two examples of different orientations together with the corresponding orthogonal
projections of the dataset. We observe that the line in Fig.15.5a as compared to the
one in Fig.15.5b looks like a very good fit to the data in the sense of a line fitting
approach: the normal distance seems to be, on average, rather small. We can now
sample the whole space of different orientations of the projection axis and compute
two characteristics for the resulting configurations: the total variance as a function
of the rotated direction,Var (X)and the mean squared distance (i.e., the error mean
. u
Fig. 15.4 The left panel shows data points in two dimensions. One of the points at .x is projected
on a line, which is given by .u . The figure shows how the vector .x is decomposed into a two
components, .n and .p . The right panel visualizes a one-dimensional coordinate axis, spanned by
the vector.u together with all projected points of the dataset


================================================================================
PAGE 415
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 399
squared error (MSE)) of the data from the line. The total variance of the projected
data is defined as
1
(cid:10)Ns
1
(cid:10)Ns
Var (X)= ‖p ‖2 = c2 , (15.10)
. u N −1 i N −1 i
s i=1 s i=1
where‖p ‖is the Euclidean distance (see Appendix A.1.7) of the projected pointp
. i . i
from the origin in the one-dimensional coordinate system ofu, and the summations
.
run over all . N s samples. The MSE value in this rotated coordinate system is obtained
from the perpendicular distance ‖n ‖of the points to the line,
. i
1
(cid:10)Ns
MSE (X)= ‖n ‖2 . (15.11)
. u i
N
s i=1
The resulting plots are shown in Fig.15.5c where the length ofuwas kept constant,
.
and only its angle was changed. The lines show that MSE and the variance behave
opposite to each other: the minimum of . Var u coincides with a maximum .MSEu value
and vice versa. Taking a look at the above two equations, it can be seen that they have
the same structure. The pre-factors are slightly different but will become nearly the
same for larger number of samples, N .pand nare perpendicular to each other (cf.
. s . .
Eq.(15.8)) and therefore are, as a function of angle, offset by π/2, which explains
.
the shift of the maxima and minima in the plot. Thus, finding a rotated coordinate
axis that minimizes the MSE implies that the variance of the data is maximized.
Generalization and Matrix Notation
In a next step, we will now simplify some of the equations. At the same time, also the
formulations are generalized such that they can be applied to the whole data matrix
and that they can be easily implemented with Python and the numpy package. This
requires a bit more of vector or matrix calculus but has the benefit of very concise
and computationally efficient formulations.
We now assume without loss of generality that‖u‖=1and discuss how vectors
.
and matrices are transformed upon change of basis, i.e., change of the coordinate
system. In what follows, only the most important aspects are summarized.
Assume that the current system is a two-dimensional Euclidean coordinate
system. A record number i of the centered data matrix is given as the row vectorx .
. i
Another unit vectoruis given as a column vector, and we ask what the projection of
.
x onto uis.1 The assumption that uhas unit length simplifies the above equations,
. i . .
Eq.(15.7), such that the projected coordinate c of each point x reduces to
. i . i
1 As mentioned before, the differentiation between row and column vectors can be tiring and
would be unnecessary if tensor calculus is employed. Nonetheless, for most of the readers, tensor
calculus makes the formulations rather inaccessible, and the implementation with Python is not
always obvious. Hence, we have to put of with row and column vectors and plenty of “. T ” symbols.


================================================================================
PAGE 416
================================================================================

400 15 UnsupervisedLearning
Fig. 15.5 The top left and top right panels depict the shortest distances of two-dimensional data
to a line that is given by its angle w.r.t. the horizontal axis. The distances are shown as lines in
two colors, to highlight on which of the two sides of the line the point is located. The bottom
panel shows the projected variance and the mean square error (MSE) as a function of all angles.
The points at the ends of the two dotted lines correspond to the configuration shown in the two top
panels. (a) Optimal orientation. (b) Non-optimal orientation. (c) Variance and MSE in the projected
system
c =x ·u. (15.12)
. i i
Rewriting this equation for all samples results in the projected data matrix, which
∗
we abbreviate in the following as X :
.
(cid:11) (cid:12)
. X ∗ := c 1 c 2 ···c Ns T =X·u. (15.13)
Now it can be shown that the mean of each projected coordinate is zero for a
centered dataset:
(cid:13) (cid:14)
∗ 1
(cid:10)Ns
1
(cid:10)Ns
1
(cid:10)Ns
μ =X = c = (x ·u)= x ·u=0·u=0
. u i i i
N N N
s i=1 s i=1 s i=1
(15.14)


================================================================================
PAGE 417
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 401
where we used the distributivity of the scalar product of vectors and, in the last step,
the fact that the data are already centered with a mean value of zero for each feature,
including the feature number i.
Last but not least, we also reformulate the expression for the total variance,
Eq.(15.10), for the case that ‖u ‖ =1. Point of departure is again Eq.(15.7) where
.
we extend the expression for pfrom a single record x
. . i
p =(x ·u)u (15.15)
. i i
to the whole data matrix X:
.
⎡ ⎤
— p —
P= (cid:15) X·u (cid:16) uT with P= ⎢ ⎣ . . 1 ⎥ ⎦ . (15.16)
. .
—p —
m
This is already a bit more complicated. A good approach for understanding the
details of these vector and matrix operations is to look at the dimensions of the
involved variables: the product X·u results in a column vector with m entries (m
.
being the number of samples) where for each row i of Xthe dot product with uis
. .
computed. Multiplication with the row vector uT from the right is an outer product
.
and results in a matrix P that has the same shape as X. Insertion into Eq.(15.10)
. .
gives:
1
(cid:10)Ns
1
(cid:10)Ns
1
(cid:10)Ns
Var (X)= c2 = (x ·u)2 = (x ·u)(x ·u)
. u N −1 i N −1 i N −1 i i .
s i=1 s i=1 s i=1
(15.17)
1
(cid:10)Ns (cid:15) (cid:16)
= (x · u) uT · xT (15.18)
N − 1 i i
s i=1
The parentheses in the last term can now be removed, the two u’s taken out of the
.
sum, and the term x xT is identified as an outer product. udoes not depend on the
. i i .
record number i and therefore can be taken out of the summation. It follows
(cid:13) (cid:14)
1
(cid:10)Ns (cid:15) (cid:16) (cid:15) (cid:16)
Var (X)=uT· xT·x u=uT· Su , (15.19)
. u N −1 i i
s i=1
where we identified the term in the parentheses as the centered sample covariance
matrix S. However, the final result is just a scalar due to the two-fold multiplication
.
with the vectors u.
.
In summary, we have managed to reformulated all relevant equations in a form
that only consists of vector and matrix operations, no explicit summation has to be
performed anymore. The advantage does not only lie in the superior computational


================================================================================
PAGE 418
================================================================================

402 15 UnsupervisedLearning
performance of any code, it is also a formulation that can be easily extended to
the most general case of a multiple projection directions. In that case, we will use a
matrixUthat consists of a number of projection vectors and, e.g., Eq.(15.13) simply
.
becomes
(cid:8) (cid:9)
| |
∗
X :=XU with U= u ··· u , (15.20)
. 1 k
| |
where later on these
.
u
i
will be the first k principal components (PCs).
15.2.5 Maximizing the Variance along the First Principal Direction
We will now derive how to choose the new directions such as to maximize the
covariance. We choose a greedy maximization, which means that we don’t find all
directions simultaneously but that we rather pick the direction that gives the most
effective maximization first and then choose the subsequent direction such that the
remaining variance is also maximized. It can be shown that this solution is also
a solution for the global maximization problem. This approach is one of the more
accessible derivations among a number of different derivation approaches. A related
approach is to minimize the MSE, which is left for the reader as an exercise.
While we now obtained a formulation for the variance of the projected data as
function of the line vector u, we still don’t know how to choose uas to maximize
. .
the projected variance. Simply finding the maximum, e.g., by taking a derivative
of Eq.(15.19) and finding the u for which the derivative (or the gradient more
.
specifically) becomes zero is not sufficient as this always would result in u = 0
.
as a trivial solution. This, however, would violate that the length of u should be
.
constant—we even postulate without loss of generality a value of 1 (see the previous
section). If the length would not be fixed, then it would be very easy to change
the variance just by changing the length. Hence, this “side condition” is important,
leading to a constrained optimization problem:
The Constrained Optimization Problem of PCA:
Maximize the projected variance Var (X) by simultaneously ensuring the
. u
constraint that ‖u‖=1.
.
Solving such a class of problems can be done using the concept of Langrangian
multiplier. It is based on formulating a Lagrange function L which returns in a
.
scalar value.L consists of two terms, the term to be maximized (i.e., the variance
.
from Eq.(15.19)) and a term that contains the constraint, formulated such that it is
zero if the constraint is fulfilled and multiplied by a weighting factor—the so-called
Lagrangian multiplier λ. The Lagrange function reads:
.


================================================================================
PAGE 419
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 403
(cid:15) (cid:16) (cid:15) (cid:16)
L(u;λ)=uT· Su −λ uT·u−1 . (15.21)
.
Note, that u is the variable while λ is a parameter. The goal is now to maximize
. .
L(u), which is a different problem than maximizing the variance. For this we take
.
the first derivative ofL w.r.t. uand search for solution such that it becomes 0:
. . .
∂L(u;λ)
=0 (15.22)
.
∂u
(which we sometimes write in short as ∂L(u;λ)). Insertion of Eq.(15.21) gives
. u
the following expression
(cid:15) (cid:16)
∂L(u;λ) ∂ uSuT ∂(uT·u−1)
= −λ =2Su−2λu=0. (15.23)
.
∂u ∂u ∂u
There we used the fact that Sdoes not depend on uand therefore is a considered as
. .
constant during differentiation. Asuoccurs as a quadratic term it is∂ (u·u)=2u.
. . u
Dividing both sides by two and bringing the second term to the right-hand side of
the equation, we obtain the result as
Su=λu. (15.24)
.
This is the eigenvalue equation that we already encountered in the context of
visualizing the normal distribution. Further details can also be found in the appendix
in Appendix A.3.1. The fact that we arrived at this equation has the following
implications and consequences:
Implications and Consequences: There are two important results that
directly follow from the fact that we obtained an eigenvalue equation:
• The vector uis an eigenvector and
.
• The Lagrange parameter λis an eigenvalue of the (centered)
.
• covariance matrix S.
.
The fact that uhas unit length with u2 = 1helps to solve the equation for λby
. . .
multiplying both sides with uT:
.
Su=λu ⇔ uT·Su=λuT·u (15.25)
. .
⇒ λ = uT ·S u . (15.26)


================================================================================
PAGE 420
================================================================================

404 15 UnsupervisedLearning
Fig. 15.6 The original data
matrix.X has two features (top
panel), the dotted line shows
the direction of the first
principal axis as obtained
from the first eigenvector.u 1.
The bottom shows the
projected data.c=Xu1along
the first principal axis. Note
that the two horizontal axes
.X 1and c have a different
scaling due to the different
basis vectors
This is an important result as it allows us to conclude which is the direction of
maximum variance: it is the direction of the eigenvector that corresponds to the
largest eigenvalue of the data covariance matrix. The direction of an eigenvector
is also called a principal direction, and the direction corresponding to the largest
eigenvalue is called the first principal component .
Projecting each data point x onto the direction of the first principal component
. i
gives its coordinate in the coordinate system defined by the principal direction,c =
. i
x ·u. This is also known as the principal component score value. Already before
i
we stated the equation in compact notation that holds for all points of a dataset,
X ∗ = Xu. Now, the new finding is that u is not just an arbitrary vector but a very
. .
special one: the first eigenvector.
An example application where these steps are used for reducing a two-
dimensional dataset to one dimension is given in Example 15.2, a visualization
is shown in Fig.15.6 (also compare Fig.15.3). The reader is encouraged to use this
example as a starting point for own experiments, e.g., by using different distributions
or computing the variance or MSE of the projected data.
Example 15.2 Computing the First principal component with Python (cid:2)
Consider a dataset that consists of two-dimensional points, obtained from
sampling a bivariate normal distribution that is characterized by given mean
values and a covariance matrix:
(continued)


================================================================================
PAGE 421
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 405
In [1]: import numpy as np
from scipy.stats import multivariate_normal
true_mean = np.array([3.1, 0.9])
true_cov = np.array([[0.7, 0.3],
[0.3, 0.3]])
rv = multivariate_normal(mean=true_mean, cov=true_cov)
X = rv.rvs(size=150)
X = X - np.mean(X, axis=0)
sample_cov = np.cov(X, rowvar=False)
sample_cov
Out [1]: array([[0.6329477 , 0.27707288],
[0.27707288, 0.29295513]])
X contains two columns each of which has been centered. The output
shows the data covariance matrix that has slightly different values than the
population (“true”) covariance matrix. Each time this code is run, it will
create slightly different values due to the random sampling. From eig, we
obtain eigenvalues and -vectors and can compute the score values, i.e., the
coordinates w.r.t. the first PC:
In [2]: eigenvals, eigenvecs = np.linalg.eig(sample_cov)
if eigenvals[0] > eigenvals[1]:
u = eigenvecs[:, 0]
else:
u = eigenvecs[:, 1]
c = X @ u
Last but not least, we visualize the original data and the projected data
In [3]: import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=2, gridspec_kw={'height_ratios': [1, 0.1]})
ax[0].plot(X[:,0], X[:,1], '.')
ax[0].plot([-2, 2], np.array([-2, 2]) * u[1] / u[0])
ax[1].plot(c, np.zeros_like(c), '.')
which results in a plot similar to the one shown in Fig.15.6.
15.2.6 The Second Principal Direction
Principal component analysis is concerned with the k principal directions that
represent as much detail from the data as possible. So far, we found the first principal


================================================================================
PAGE 422
================================================================================

406 15 UnsupervisedLearning
direction. How can we find the next principal direction? The next direction should
be orthogonal to the previous direction, which is a requirement that is automatically
fulfilled for the eigenvectors as they are mutually orthogonal. The new direction is
supposed to maximize the variance along the new direction.
We will now denote the first, already known direction by u and the second, to
1
be determined direction by u . For now, we assume that u need not necessarily be
2 2
an eigenvector; it can be an arbitrary unit vector orthogonal to u satisfying
1
(cid:17)
uT ·u =0 and ‖u ‖= uT ·u =1. (15.27)
. 1 2 2 2 2
The main difference to the previous derivation is that the Lagrangian function now
consists of two constraint terms (that is, the two equations in Eq.(15.27)), one
ensuring the unit length and the other ensuring the orthogonality:
L(u ;λ ,a)=uT ·Su −a(uT ·u −0)−λ(uT ·u −1). (15.28)
. 2 2 2 2 2 1 2 2
Here, λ and a are just two Langrangian multiplier, and we do not need to assume
2
that one of them is an eigenvalue (even though this is exactly what will come
out below). The rest of the derivation is straightforward and follows along the
line of the derivation for the first principal component. We start by computing
∂ L(u ; λ ,a) = 0, which gives:
u2 2 2
2Su −2λ u −au =0. (15.29)
. 2 2 2 1
By multiplication with uT from the left we obtain
1
2uT ·Su −2λ uT ·u −auT ·u =0. (15.30)
. 1 2 2 1 2 1 1
We realize that transposing the first term can be undone by additionally exchanging
the position of the two vectors, u and u , and S is symmetric. u is by construction
2 1 2
orthogonal to u , which cancels the second term. The scalar product in the third
1
term is one because u is a unit vector with‖u ‖= 1. With this we obtain:
1 1
2uT ·Su −a =0 ⇔ a =2uT ·Su . (15.31)
. 2 1 2 1
With insertion of the identity from Eq.(15.24) for the first eigenvector and -value
we obtain
2uT ·Su =2λ uT ·u (15.32)
. 2 1 2 2 2.
⇔ 2S u = 2λ u . (15.33)
1 2 2
With this, we can now simplify Eq.(15.29), which brings us again to an eigenvalue
equation


================================================================================
PAGE 423
================================================================================

15.2 PrincipalComponentAnalysis:TheoreticalBackgroundandDerivations 407
Su =λ u . (15.34)
. 2 2 2
With the same argument as before, one can conclude that u is an eigenvector,
2
corresponding to the eigenvalue λ . u maximizes the (remaining) variance while
2 2
at the same time its direction is orthogonal to u . The vector u thus defines the
1 2
second principal component .
15.2.7 The k-th and the Best k Principal Directions
The k-th principal direction
Common applications of PCA or related methods in ML analysis and data mining
either treat the number of principal components as a hyperparameter or have as the
sole aim to determine the number of PCs. Thus we would like to be able not only to
obtain the first or first two principal directions but the first k principal directions.
The steps for obtaining the resulting equations for the k-th principal component
are a generalization of the steps for obtaining the second component. We assume
that the first k − 1 eigenvectors u and -values λ are already known. Again,
. . i . i
the Lagrangian function consists of three groups of terms: (i) the function to be
minimized, i.e., the variance projected onto the new direction u , (ii) a constraint
. k
for ensuring that the vector governing the k-th direction has unit length, and (iii) a
sum of constraints that enforce the mutual orthogonality between all directions:
(cid:10)k−1
L . (u k ;λ k ,a 1 ,...,a k−1 )=uT k ·Su k −λ k (uT k ·u k −1)− a i (uT k ·u i ).
i=1
(15.35)
As before, the variable of the Lagrange function, u , is unknown and might be
. k
arbitrary; the same holds for the parameter λ . Again, it will turn out that these are
. k
the k-th eigenvector and -value. The derivation for this follows the same steps as
before: Taking the derivative and solving the resulting equation ∂ L = 0 results
. uk
again in an eigenvalue equation from which follows that λ and u are in fact an
. k . k
eigenvalue of S and the corresponding eigenvector. Furthermore, the set of the k
.
eigenvalues ofShas a corresponding set of k eigenvectors each of which maximizes
.
the variance along its respective direction.
The Best k Principal Directions
Now, we are able to successively compute all n = rank(S) eigenvalues of the
.
covariance matrix. The maximum number of principal directions is also n, which
usually is also the number of features of the original data matrix. Without loss of
generality, the eigenvalues and the corresponding eigenvectors can now be sorted
such that
λ ≥λ ≥···λ ≥···≥λ whereallλ ≥0. (15.36)
. 1 2 k n i


================================================================================
PAGE 424
================================================================================

408 15 UnsupervisedLearning
Thus, we obtain the best k principal directions, which maximize the variance by
simply choosing the first k eigenvectors. All steps for computing the k principal
directions and the corresponding projection of the data are summarized in Algo-
rithm 15.1 on page 397.
Projecting the data onto these directions is an approximation of the original
dataset and therefore introduces an error—but we managed to systematically reduce
the dimension of the dataset from originally n to . k ≤ n, which is why PCA is a
method for dimensionality reduction. To estimate the quality of this dimensionality
reduction, we will now take a look at how the total variance of the projected data
changes as more and more principal directions are taken into account.
Total Variance of the Projected Data
To assess the overall variance of the projected data w.r.t. the principal directions,
we compute the total variance. To do so, there is an elegant way that is based on
the eigendecomposition of a square matrix A, which implies that any symmetric,
.
real-valued matrix
A∈Rn×ncan
be factorized as
.
A=UDUT , (15.37)
.
where D is a diagonal matrix consisting of the eigenvalues λ of A such that
. . i .
D=diag(λ ,...,λ ). Uis an orthogonal matrix which contains the n eigenvectors
. 1 n .
scaled to unit length as columns, and without loss of generality we assume that
the eigenvalues and the according eigenvectors are sorted such that λ has the
. 1
largest, and λ has the smallest value, compare Eq.(15.36). From U being an
. n .
orthogonal matrix it thus follows thatUUT =IwhereIis the identity matrix. Further
. .
details on orthogonal matrices and the diagonalization can be found in particular
in Appendix A.2.17 and A.3.2. Further information on eigendecomposition and
Singular Value Decomposition, which is a related method, can be found in
Appendix A.3.5
Above, we found out that the covariance data matrixSfulfills these prerequisites
.
for the eigendecomposition, and we can therefore obtain the corresponding diagonal
matrix as
D=UTSU, (15.38)
.
which consists of all sorted eigenvalues of S. The matrix U contains as columns
. .
all corresponding, mutually orthogonal eigenvectors of S. With this, any row vector
.
x (i.e., a specific record i of the dataset) can be represented in terms of the basis
. i
system formed by the eigenvectors. The transformation from the Euclidean system
to the eigensystem is simply done by the following linear transformation
x ∗ =x U. (15.39)
. i i


================================================================================
PAGE 425
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 409
Similarly, we can also apply this transformation to the whole data matrix X, as
.
already introduced in Eq.(15.20)
X ∗ =XU. (15.40)
.
where column i contains the scores for all data points w.r.t. principal component
number i and Uacts as a projection matrix. Using the definition of the covariance
.
matrix, we can now write down the formulation of the covariance matrix w.r.t. the
principal components:
∗
S =PTSP. (15.41)
.
Comparing this equation to Eq.(15.38) reveals that the data covariance matrix in
the principal system is simply the diagonal matrix which contains all eigenvalues!
Therefore, we can now very easily compute the total covariance of the projected
∗
data, Vartotas the trace of S :
. U .
(cid:18) (cid:19) (cid:10)n
∗
Vartot(X)=tr S = λ . (15.42)
. U i
i=1
If only the first k principal components are to be considered, then the total
covariance comprises only the sum of the first k eigenvalues,
(cid:10)k
Var (X,k)= λ , (15.43)
. U i
i=1
since the λ are sorted in decreasing order.
. i
15.3 Application Aspects and Examples of PCA
This section will go in detail through several numerical and practical aspects of
principal component analysis. This also helps to explain details of dimensionality
reduction that can be generalized to other ML methods. The great benefit of PCA,
however, is that it is rather simple in terms of its implementation.
15.3.1 Image Data and the Principal Component Analysis
So far, the governing equations of PCA were written for a number of features, i.e.,
vector-valued data or several individual scalar variables. However, a particularly
interesting type of “data” are images. How can we treat images in the PCA
framework as outlined in Algorithm 15.1? Grayscale images consist of an regular


================================================================================
PAGE 426
================================================================================

410 15 UnsupervisedLearning
Fig. 15.7 Preprocessing of image data for the use with PCA: the real image is represented as a
two-dimensional matrix of values (here: strongly down-sampled). The values are here represented
by different gray shades, and the gray values result from anti-aliasing of the real image. Then, the
“image vector” is obtained by concatenating all rows of the matrix into a single vector where each
gray value corresponds to a number, shown below the “image vector”
array of pixels each of which has a value in between 0 and 255 (or 0 and 1, depending
on the underlying convention) binary images only contain values of 0 or 1. Such
a two-dimensional array can be converted into a one-dimensional representation
by flattening the image into a vector: there, each row of pixel is appended to a
resulting vector as sketched in Fig.15.7. Thus, an image consisting of n rows and m
columns of pixel is mapped to a vector with n×melements. In case of a black and
.
white image with 256 by 256 pixels, such a vector would be an element of a huge,
65536-dimensional vector space. In this space, each image is a single point. The
combination of pixels that in fact results in a picture with interpretable, intelligible
shapes or patterns is very rare, by comparison. This can be demonstrated by picking
random constellations of pixels as shown in Fig.15.8. Even though these three
images are from different points of the high-dimensional space, none of them results
in meaningful patterns. This also implies that most of the non-random images are
very particular. For example, images of human faces have some common features,
which also show in the vectors of pixel values. Even images of very different
features will exhibit some similarities and therefore only will occupy a probably
quite small subspace of the high-dimensional space of all possible image vectors,
the images are embedded into the high-dimensional space. Principal component
analysis can help to group similar images together in the principal component space.
As an example, we turn now to one of the classical datasets, the MNIST dataset, DS-
2(MNIST).
The MNIST Dataset
The dataset DS-2 (MNIST) is one of the famous and classical datasets for ML,see
Sect.4.8 for further information. In the context of PCA, it has the advantage that
the content of the dataset consists in very clear geometrical shapes that are easily
accessible (we know very well how numbers look). Therefore, this dataset will help
us to more easily interpret our results.


================================================================================
PAGE 427
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 411
Fig. 15.8 The space of all possible pixel combinations mostly consists of random looking images.
Images with recognizable patterns and shapes are very rare, by comparison. For the three examples,
pixel values were drawn from a random uniform distribution of numbers . [ 0,1]
One of the objectives for analyzing the images of handwritten digits is to be able
to assign labels to them, i.e., to infer which number was written. Classical feature
engineering approaches try to identify characteristic features or typical structures,
e.g., a “1” consists always of a more or less vertical line, sometimes with a short
slash at the top (at least in many European countries). The handwritten 7 at least
also has the vertical line, but it also might come with the “European cross bar”.
Therefore, finding characteristics and quantifying them is in fact rather difficult, due
to the large variety of different writing styles. PCA does not perform a classification
of the images but does group similar images together, which can be useful, e.g., as
a “preprocessing” step for a ML classification. Projecting images onto the first two
principal components is particularly useful as this can be easily visualized.
Python Implementation and Results
We will now explain the steps required in Python to perform a PCA for the MNIST
dataset. We assume that the MNIST training dataset has been already read into the
variable X and the corresponding labels (i.e., the 10 digits) are contained in y.The
first index of X is the number of the image, the second and third indices are the row
and column of the pixel. y is an one-dimensional array. We start with a quick sanity
check:
In [1]: import numpy as np
# [acquire data for X and y] -- this is not shown here for brevity!
print("X.shape:", X.shape)
print("y.shape:", y.shape)
Out [1]: X.shape: (60000, 28, 28)
y.shape: (60000,)
The output shows that we have altogether 60,000 data records (i.e., images), each
of which is represented by a matrix of 28×28pixel. These are the training dataset,
.
the MNIST database additionally consists of an additional 10,000 images as testing
data. As a preparation, we convert each image into a one-dimensional vector, which
is the “canonical form”. Additionally, we center the data:


================================================================================
PAGE 428
================================================================================

412 15 UnsupervisedLearning
In [2]: X = X.reshape(-1, 28 * 28) # create image vector from 28x28 img.
mu = X.mean(axis=0) # mean for each column
X = X - mu # center each column of X
As an optional step, the features of the data matrix are standardized. As opposed
to centering of the data, this is optional and mostly useful in situations where the
centered features have very different ranges.
In [3]: sigma = X.std(axis=0) # standard deviation for each column
mask = np.isclose(sigma, 0) # ensure that all entries of sigma are != 0
sigma[mask] = 1e-12 # and set only those to 1e-12 which are ~ 0
X = X / sigma # standardize X (this is optional!)
Here, we ensured that none of the entries of the standard deviation sigma —a vector
with 28×28=784elements—is zero for avoiding “division by zero” errors. Now,
.
the data covariance matrix can be computed, considering that X contains features
as columns. Additionally, we obtain eigenvalues and eigenvectors:
In [4]: data_cov = np.cov(X, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eig(data_cov)
Both eigenvalues and corresponding –vectors are sorted such that the eigenvalues
are in descending order:
In [5]: # get the indices that would sort the eigenvalues
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:,idx] # these are column vectors!
There, the “ [::-1] ” reverses the order of the indices (similar to numpy.flip() )
that are returned by argsort . Now we can create the matrix of the two principal
components—the first two eigenvectors—and project the data onto the first two
principal components:
In [6]: U = eigenvectors[:, :2] # matrix of the first 2 eigenvectors
X_proj = X @ U # perform the change of basis
The symbol @ denotes the product of two matrices. U has as many rows as the total
number of pixel per image; here it has two columns. X_proj has as many rows as
samples and as many columns as there are principal components, such that the data
projected onto the first two principal components is now contained as the columns
of X_proj . Visualization of the data can now be done with a scatter plot:
In [7]: import matplotlib.pyplot as plt
plt.scatter(X_proj[:,0], X_proj[:,1], c=y, cmap='tab10', alpha=0.3)


================================================================================
PAGE 429
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 413
Fig. 15.9 All samples of the 10 digits contained in the MNIST training dataset projected into the
latent space. The images of .28×28pixels are projected onto the first two principal components.
No standardization of the centered data was done. A circled number denotes the projected digit
(e.g., the digit “0” is represented by the red marker), and the position of the number additionally
indicates the center of the respective distribution
There, we made use of the fact that matplotlib.scatter can take a one-dimensional
array as an argument for the color parameter c . This array contains the color values
to be assigned to each of the points. The resulting distribution is shown in Fig.15.9.
We observe that already two components are able to “cluster” the data for each digit.
Nonetheless, for many digits, there is still a large amount of overlap, which implies
that in this embedding there is no unique map from a projected data point to a digit.
Additionally, we also see that, e.g., the “1”s are much more localized than, e.g., the
“0”s, which gives an idea about the variability of the original data.
Discussion
How can the distribution of points for this two-dimensional embedding be inter-
preted in terms of the original image data? Figure 15.10 shows a dataset that was
reduced for ease of visualization: there, only the projected data for the digits 0
and 1, and 5 are shown, which—to some extent—occupy rather distinct regions
with relatively small overlap. One can now pick a point and take a look at the
corresponding input images, as shown in the figure. The distribution of “1”s is the
most localized one. Taking a look at the three examples in the column of insets at the
very right of Fig.15.10, it can be observed that here, a higher value of the second
component seems to imply a line inclined toward the left while a lower/negative
second component implies an inclination toward the right.
The small images on the top show examples taken from the centers of the three
distributions. The digits look upright, centered, and look relatively “average”. This
conclusion, however, has to be taken with a grain of salt as there is some variation
in the images of the points nearby.


================================================================================
PAGE 430
================================================================================

414 15 UnsupervisedLearning
Fig. 15.10 Latent representation: The points are the projected images for the digits 0, 1, and
5 from the MNIST dataset. The latent subspace consists of the first two principal components.
For chosen points of the dataset also the corresponding images (digits) are shown. The color of
their frame indicates to which group of points they belong. Also shown are the centers of the
distributions, indicated by the three circled numbers 0, 1, and 5. Note that the two axes do not have
the same scale
How about the “extreme” positions? The three leftmost images on the bottom
show data points that are far away from the mean. All of them contain some
imperfections, untypical shapes or are particularly thick or thin, which seems to
be an aspect of all extreme points. This can also be seen for the “1” in the bottom
row of digits.
Finally, we take a look at two points in the PC space that are almost at the same
location but belong to different digit classes. One of such examples are the points
for “5” and “1” at the bottom right. The “5” can almost be taken for a “1” and has
the same “average” orientation. Here, we might be tempted to conclude that nearby
points are also visually similar. However, this is not always the case, as can be seen
by comparing the “0” and “1” on the bottom and which are also located at nearly the
same location in the embedding. Nonetheless, they are entirely different in shape.
Two principal components are obviously not sufficient to learn the difference
between these two shapes. While it is certainly tempting to interpret the principal
components and the representation of the data, it is important not to rely too strongly
or too blindly on this and to remember that the principal components in all generality
do not represent variables that can be physically interpreted.2
2 There are cases, where principal components have been directly linked to features in images.
An example is the so-called eigenfaces [16]: there the first eigenface represents the average gray
value, the second eigenface accounts for the gray value contrast of face and hair, the third eigenface


================================================================================
PAGE 431
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 415
15.3.2 Accuracy of the Representation
This leads to the question if or how the projected representation changes or how
the accuracy is affected by increasing the numbers of principal components? Is it
possible to represent differences better by using more PCs? One way of visualizing
accuracy related aspects is a scree plot.
Scree Plot
Recall that one of the key ideas of PCA is to maximize the variance along each
principal component—which implies to maximize the total variance. We already
discussed that maximizing the variance is identical to maximizing the information
content or minimizing the mean squared error of the representation.
In Eq.(15.43) we found that the total variance of the data projected onto the first
k components is the sum of the first k, largest eigenvalues. This allows for an easy
way for visualizing and analyzing the influence of the components. A scree plot as
introduced by [6] is a line plot that shows the spectrum of the covariance matrix—or
in other words: the magnitude of the eigenvalues obtained from the PCA, as shown as
the solid line in the top panel of Fig.15.11. As usual, the eigenvalues are sorted from
largest to smallest, which are in total 28×28 = 784(the number of pixel of each
.
MNIST digit image) eigenvalues. Here we see, that the magnitude of the eigenvalues
quickly falls below 5 after the first 25 eigenvalues, only the first ≈150eigenvalues
.
are larger than 1, and the λ are zero for i >712. In the bottom panel of Fig.15.11,
. i .
a logarithmic scaling of the vertical axis was used, which in particular reveals more
details in the regime where the eigenvalues are large and strongly changing. The
scree plot helps to decide which are the components with the highest statistical
significance. In such a plot, the “ellbow” can be identified as the point of maximum
curvature after which changes in the eigenvalues happen considerably slower. This
approach should be used only as a rough estimate, though, as it is mathematically
not well defined.
Scree and the Scree Plot (cid:2)
Scree plot is an unusual name that originates from the word scree, which is
used to describe a mass of stone debris or broken rock fragments that has
typically agglomerated at the foot of a hill or mountain. Why is this name
chosen for the plot type? The largest eigenvalues are the top of the mountain
and the big rocks while all the scree and the small pebbles are then at the foot
of the mountain.
for lightning from the side, higher-order principal components represent features of the images of
increasing complexity that are more and more difficult to interpret.


================================================================================
PAGE 432
================================================================================

416 15 UnsupervisedLearning
Fig. 15.11 Eigenvalues for
all digits and the proportion
of the total variance that is
explained by the first i
components. The left panel
has a linear scaling of both
axes while the right plot has a
logarithmic scaling of the two
vertical axes and additionally
only shows the first 200
eigenvalues, revealing more
details of the eigenvalues
with larger magnitudes. The
solid line in the left panel is
what is typically called a
“scree plot”, the dashed line
shows the “amount of
variance” that was explained
by the first k components
An alternative representation is the cumulative sum of the eigenvectors as shown
in the second curve in the two figures. The cummulative sum is scaled such that
the maximum would be 100 percent because using all eigenvalues recovers the
unreduced information content of the original data. This type of plot is particularly
useful as the sum of the k first eigenvalues is the projected variance of the data in the
k-dimensional embedding, compare Eq.(15.43). As such, it gives us the proportion
of variance that is explained by the specific choice of k principal components.
(cid:20)k
λ
i
explainedVar(k)= Var u (k) = i=1 , (15.44)
. Vartot (cid:20)N
u
λ
j
j=1
where the total variance is given by Eq.(15.42), N is the total number of eigen-
values, and k is a given number of eigenvalues for which the coefficient should be
obtained. Using numpy this value can be computed as follows:
In [1]: from numpy import cumsum, sum, arange
cumsum_eigenvalues = 100 * cumsum(eigenvalues) / sum(eigenvalues)


================================================================================
PAGE 433
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 417
There, we assumed that the eigenvalues are sorted in descending order and directly
multiplied with 100 to convert into percent. It can be plotted by, e.g.,
In [2]: plt.plot(arange(1, cumsum_eigenvalues.size + 1), cumsum_eigenvalues)
where the arange expression ensures that the first eigenvalue starts with the index
number 1 and not 0. In the top panel of Fig.15.11, we can easily see that in
order to represent 80% of the data (in terms of variance) at least the first ≈ 150
. .
principal components need to be included, and for 90% of the variance explained
.
235 components would be required. Thus, with only 30% of the data we achieve
to represent nearly the same amount of information as contained in the original
dataset. Therefore, scree plots or related plots can help to determine the number of
components that have to be included into a data analysis—in other words, they are
a tool to determine the intrinsic dimensionality of a dataset.
15.3.3 Reconstruction
Clearly, referring to this or that percentage of “variance explained” is useful but
at the same time also somewhat abstract in that sense that it is not clear what the
error implies in terms of features of the images. To this end, we will now investigate
how data from the lower-dimensional embedding can be transformed back into the
original vector space of the 28×28dimensional images.
.
Point of departure is the projection of the (centered) data matrix onto the principal
components, which gives the coordinates in terms of the eigenvectors as derived in
Eq.(15.20):
∗
X =XU. (15.45)
.
In this equation, Uis the matrix of eigenvectors which consists of all the eigenvec-
.
tors in columns and is quadratic . X is as always the centered data matrix with N
. . f
in columns and N records. Because Uis even an orthogonal matrix it is U −1 =UT
. s . .
and we obtain
∗
X=X UT =XUUT (15.46)
.
where X is the reconstructed data, which are here identical to the original data
.
because U contains all eigenvectors. However, if only the first k eigenvectors are
.
∗
considered, we have to introduce a separate variable X in Eq.(15.46) for the
.
reconstructed data:
(cid:15) (cid:16)
.
X ∗ =XU[:,1:k] U[:,1:k] T , (15.47)


================================================================================
PAGE 434
================================================================================

418 15 UnsupervisedLearning
where
.
U[:,1:k] is a submatrix of
.
U that consists of only the first k columns, cf.
Sect.3.2.4. As a final postprocessing step, only the centering and possibly also the
standardization need to be undone. For the jth column, this reads
(cid:18) (cid:19)
X ∗ = X ∗ σ +μ 1, (15.48)
. j j j j
where σ and μ are the standard deviation and the expected value of the j-th
. j . j
∗
feature, respectively. Writing this out for each element x of X (for brevity we
. ij .
∗
skip the superscript ), this is
.
⎡ ⎤ ⎡ ⎤
x ··· x x σ +μ ··· x σ +μ
1,1 1,N 1,1 1 1 1,N N N
⎢ ⎣ . . . . ⎥ ⎦= ⎢ ⎣ . . . . ⎥ ⎦ (15.49)
. . . . .
x ··· x x σ +μ ··· x σ +μ
M,1 M,N M,1 1 1 M,N N N
∗
where X has M records in rows and N features in columns.
.
Hence, given a point in the space of principal components, we can now transform
it back into the original space. For the problem of the MNIST dataset, we can
thereby “reconstruct”, e.g., an image from its representation in the principal
component space as demonstrated in Fig.15.12. The first three columns correspond
to the three digits shown in Fig.15.10, which were chosen from the means of the
three distributions. Here we observe that already two components are sufficient to
roughly reconstruct the “0” and the “1” such that they are legible. The “5” is quite
blurry, has additional artifacts and therefore requires more components. Note that we
could also take a look at the weights, i.e., the eigenvectors reshaped into a 28×28
.
pixel array, which helps to understand which structures are most relevant. Using
150 components for the reconstruction already results in very accurate images. The
amount of data required for this representation is less than 20% of the original data.
Therefore, PCA serves as a very effective compression method for these images.
Denoising
So far, we have already seen that PCA is a method that is able to perform a multitude
of different tasks. We conclude this by a somewhat unusual application: denoising
of images.
Denoising can be done by first learning the PCA components of a set of images
with defects or noise, which serves to identify the basis for the transformation.
Then, choosing a number of PCs that is—usually significantly—smaller than the
total number of PCs, the data are reconstructed, and only the dominant characteristics
from the images are pertained. In this way, the noise will be removed as a side effect
of reducing the total variance.
As a demonstrator we again use the MNIST dataset. To set up the dataset, we
chose in each image 40% of the pixel and replace their values by a value randomly
sampled from the interval [0,255], which mimics image defects or other noise.
.
Then, the dataset is centered by subtracting the mean value of all samples for each


================================================================================
PAGE 435
================================================================================

15.3 ApplicationAspectsandExamplesofPCA 419
Fig. 15.12 Reconstruction of some MNIST digits using four different numbers of principal
components (PCs). The first three digits are those that are shown on top of Fig.15.10
Fig. 15.13 Using PCA for denoising of images: the model was trained only with “noisy” version
of the original MNIST training images. For further explanations, see the text
pixel i, . μ i . This is then used for a PCA to obtain the eigenvectors. We use the first
. k = 40 eigenvectors as a basis for the transformation of the noisy images into PC
space and transform the result back to the original space. “Un-centering” the data
is done by adding μ for each pixel. An example using the “5” from Fig.15.12 can
. i
be seen in Fig.15.13. The final thresholding step (pixel with values < 128 are set
.
to zero, all other pixel are set to 255) is only for visualization purposes but shows
already that the denoising can also be a very effective preparation for a binarization
of an image.
The choice of the number of eigenvectors k is important: if k is too low, then
the dominant structure is not represented properly, if it is too high, then more and
more fluctuations can be represented as well. For the MNIST images, a value of
k =25...50works relatively well, but there is also a certain dependency on details
.
of the image as will be investigated as an exercise.


================================================================================
PAGE 436
================================================================================

420 15 UnsupervisedLearning
15.4 Further Methods for Dimensionality Reduction
PCA is a useful method, and it is always surprising how powerful this method is
despite its simplicity. However, there are other methods as well some of which
are able to overcome problems of PCA. Here, we only introduce a very incomplete
selection. The first group of methods are methods related to PCA that are mainly
based on matrix decomposition. The second group consists of the methods of the
so-called ensemble methods.
15.4.1 The Relatives of Principal Component Analysis
For brevity, we will only cover two methods that are directly PCA-related and
skip the classical methods of independent component analysis as well as linear
discriminant analysis.
Randomized PCA
PCA becomes compute and memory intensive if the datasets become large (large
being hundreds of thousands of records and above). Randomized PCA [10] is a
method that uses random sampling in order to construct a reduced subspace into
which the data are then projected. Randomized PCA is still a linear method.
Kernel PCA
Another extension of PCA is the method of kernel PCA which was originally created
for classification problems that have a non-linear decision boundary. Similar to the
concept of support vector machines (SVMs), also here the kernel trick is utilized (cf.
Sect.14.6 for more information). Figure 15.14 shows the performance of the regular
PCA in comparison to kernel PCA for the case of a two-dimensional dataset and a
two-dimensional embedding. The dataset contains data of two categories that are
not linearly separable. We observe that PCA is not able to separate the two groups
of points as it is a linear method. In fact, the function between the original space
and the embedding is close to the identity. Kernel PCA, on the other hand, is very
well able to separate the data points because the transformation of the data using a
kernel function (e.g., a RBF kernel consisting of products of Gaussian functions or a
polynomial kernel, cf. Sect.13.5.4) is non-linear. Reconstruction, i.e., projection of
data back into the original feature space, however, is more complicated, due to the
use of kernels. By comparison, the regular PCA is typically superior in this regards.
15.4.2 Manifold Learning Methods
PCA and related methods are parametric methods, as the mapping can be explicitly
written in terms of a mathematical function. Unsupervised, non-parametric dimen-


================================================================================
PAGE 437
================================================================================

15.4 FurtherMethodsforDimensionalityReduction 421
Fig. 15.14 Comparison of the “vanilla” PCA with kernel PCA
sionality reduction is an alternative, and is generally associated with the so-called
manifold learning.
Among the most prominent examples are the isomap method [15], spectral
embedding [1], multidimensional scaling [4], t-distributed stochastic neighbor
embedding (t-SNE) [17], and Uniform Manifold Approximation and Projection
(UMAP) [11]. In particular t-SNE as well as UMAP are very useful for data
visualization and will be briefly explained in the following.
t-distributed Stochastic Neighbor Embedding (t-SNE)
t-SNE is related to multidimensional scaling by the fact that both methods are
based on pairwise distances of the data. Multidimensional scaling attempts to find
distances between points in the embedding such that they are as close as possible
to the original (high-dimensional) distances. However, in general it is not possible
to preserve the high-dimensional distances in a lower-dimensional space. t-SNE (or
in fact also the original SNE) takes an alternative approach and tries to preserve
the structure of the neighborhood of points the original space when transformed to
the embedding. This is achieved by introducing the measure of similarity of points
(the closer the points the higher the similarity). A kernel function is introduced in
the high-dimensional space, where the kernel size is adaptively chosen such that
always the same number of points are covered. This results in one of the important
hyperparameters, the perplexity parameter, which governs, roughly speaking, the
number of desired nearest neighbors for a central point inside a cluster. Larger
values of perplexity result in more nearest neighbors, which as a consequence might
not show smaller structure. Smaller values of perplexity hide more of the global
structure. Typical values are in the range of 5–50.
The low-dimensional similarity is defined based on the students-t distribution,
hence the “t” in t-SNE (the original SNE just used a Gaussian kernel). The resulting
loss function contains both similarity measures and can be numerically minimized
using a gradient descent method.


================================================================================
PAGE 438
================================================================================

422 15 UnsupervisedLearning
The Relation of t-SNE to Molecular Dynamics (cid:2)
Interestingly, it turns out that the structure of the loss function is similar to that
of a potential with repelling and attractive components. As a consequence, if
we consider each data point a particle, then the gradient descent optimization
implies to follow the trajectory of a multi-body simulation, as also done in
molecular dynamics simulations. The resulting equilibrium configuration of
the “data particles” in two dimension is the embedding.
The solution of this problem is not unique, and the minimization exhibits many
local minima. To avoid “getting stuck” with a configuration that is far from optimal
(e.g., in form of split clusters), a strategy called early exaggeration is used: for a
number of steps, all attractive forces are scaled by a value (e.g., a factor of 4 for 100
or 200 iterations). Subsequently, the system is switched to the regular, non-scaled
forces. Early exaggeration requires at least these two hyperparameters (a factor and
the number of steps). Last but not least, we briefly mention that initialization plays
an important role. For this, using a PCA-based initialization is a common method
and also usually the default in most software packages.
t-SNE plots are not entirely trivial to read [18]: (i) hyperparameters have a
strong influence on how the results look; (ii) the size of resulting clusters does
not have a meaning; (iii) distances between clusters are possibly not interpretable;
(iv) for interpreting plots in terms of topology, one might have to see several
plots with different perplexity values. Figure 15.15 shows two examples of how
t-SNE transforms different types of data from a two-dimensional space to a two-
dimensional embedding. The upper row shows that for small numbers of perplexity,
many small structures occur and there is no global structure. Clear clustering is
observed for a perplexity of 30 and 50. For 100, the solution is rather extreme: the
attractive forces between the particles of the “membrane” are in equilibrium with the
repelling forces between the “core” and the “membrane”. Here, early exaggeration
might help to separate the two group into to topologically simpler blobs. The bottom
row shows again that a perplexity of 2 mainly preserves local structure, resulting in
the circle. For increasing numbers of perplexity, more and more global structure is
considered. The gap in the line, e.g., at perplexity of 5, is the result of repelling
forces of points that are in the original space far away from each other.
Uniform Manifold Approximation and Projection (UMAP)
UMAP approaches the manifold learning from a slightly different angle and is based
on RIEMANNian geometry and algebraic topology [11] for preserving both local and
global structures in the data. In many situations UMAP performs computationally
better than t-SNE. Additionally, it is not restricted in terms of the embedding
dimensions. Thus, UMAP can be used in a reasonable way for two- or three-
dimensional visualization purposes, but it also can produce higher-dimensional data.
This makes UMAP a good choice when the dimension-reduced data is supposed to


================================================================================
PAGE 439
================================================================================

15.4 FurtherMethodsforDimensionalityReduction 423
Fig. 15.15 Top row: 2 times 75 points with different dispersion. Only looking at the plot with
perplexity 100 suggests that the orange points are a one-dimensional, circular structure, which is
not the case. Bottom row: Also here, using different values of perplexity helps to show different
types of information. (Diagrams taken from [18] with permission) (licensed under Creative
Commons Attribution CC-BY 2.0, see https://creativecommons.org/licenses/by/2.0/)
be used as input for any other ML methods. There are a number of hyperparameters
required for a UMAP-based dimensionality reduction. The four most important
hyperparameters are: (i) the number of neighbors, (ii) the minimum distance
between points, (iii) the number of components of the embedding (i.e., a value of
one produces data for a line plot, a value of two produces data for a two-dimensional
scatter plot), and (iv) the used metric. Figure 15.16 shows a comparison of t-SNE,
PCA und UMAP applied to the MNIST dataset. First of all, there are significant
differences in the computational time for performing the transformation on the
dataset with 70,0000 records. On the laptop with which this book was written, t-
SNE required 7.30min, UMAP took 1.30min, and PCA was nearly instantaneously
finished with a mere 2.5s. Running Kernel PCA on this dataset was not possible, due
to very extensive memory requirements. Here, UMAP is able to create more distinct
“clusters”. If the goal would be to use this data as input for a clustering method or
a supervised classification method, one would clearly choose UMAP. Nonetheless,
if data visualization and data exploration are the main purpose, then it is certainly
worth trying out both of them and additionally varying some of the main parameters.
UMAP/=t-SNE? (cid:2)
.
Even though the formulations in the respective publications of UMAP and
t-SNE are rather different, it turns out that there is an interesting correspon-
dences. Using t-SNE with early exaggeration for the MNIST data shown in
Fig.15.16 the embedding is nearly identical to what results from UMAP.
Further discussions and investigations of this topic can be found in [7, 8].


================================================================================
PAGE 440
================================================================================

424 15 UnsupervisedLearning
Fig. 15.16 PCA, t-SNE, and UMAP applied to the MNIST dataset. t-SNE was used with a
perplexity value of 30, for UMAP we chose 15 neighbors as main parameter. For all other
parameter the default values were used
15.5 Clustering
Cluster analysis is an ML technique for grouping similar objects into clusters.
Clustering is an unsupervised method, and therefore, the dataset consists of only
the N features,
D . ={X 1 ,...,X n } i=1...M , (15.50)
while the corresponding labelsY ,i =1...M are wanted for all M examples. How
. i .
to mathematically or algorithmically characterize and detect a cluster, this is the
point where the following methods all differ slightly. In the following, only a brief
overview of clustering methods is given where we only give more details for the
Gaussian mixture model. For a more detailed overview, see the review by [14] and
the references therein.
15.5.1 k-Means and k-Medians
The k-means clustering algorithm has the goal to create k partitions from a given
dataset, where hard boundaries delimit each cluster. This implies that there may be
no overlap between clusters. This can be a strong restriction, as we already saw
in the context of classification in Chap.14. In fact, there is a relationship between
the unsupervised k-means algorithm and the supervised k-nearest neighbors (kNN)
method (cf. Sect.14.4) as also the k-means method can be interpreted in terms of
a Voronoi tessellation. The “means” in k-means stems from the fact that in this
algorithm the centroids of Voronoi cells are determined from the mean values of
the cluster data. As hyperparameter, k-means requires the number of clusters. The
method k-medians is very similar to k-means; the difference is that it uses the
medians for determining the Voronoi cell centers and not the means. Both methods
have the restriction that the cells consist of polygons.


================================================================================
PAGE 441
================================================================================

15.5 Clustering 425
The algorithm consists of a iterative sequence of so-called “expectation steps”
and “maximization steps” (more information will be given in Sect.15.5.2 for the
Gaussian mixture model).
Algorithm 15.2: k-Means Clustering (cid:3)
1. Decide on the number of clusters, k, e.g., based on physical knowledge of the
underlying problem
2. Initialize the k centroids randomly
3. Expectation Step: Assign each data record to its neares centroid
4. Maximization Step: Compute the new mean of each cluster. This is then the
updated centroid.
5. GOTO 3 an repeat UNTIL convergence has been reached.
15.5.2 Gaussian Mixture Model
The Gaussian mixture model (GMM) [3, 12] is a probabilistic model, similar to
the Gaussian naive Bayes (GNB) model that was introduced in the context of
classification (Sect.14.5), but the main difference is here that the labels are to be
predicted.
Model Description
For the GMM it is assumed, that the probability density function (PDF) of a single
cluster takes the functional form of a normal distribution N. Multiple clusters
.
are governed by multiple (possibly multivariate) normal distributions, all of which
are additively superimposed. If a normal distribution of the cluster number j is
characterized by the list of parameters θ (usually the mean μ and the covariance
. j . j
matrix Σ ), we can write the weighted superposition as
. j
(cid:10)k (cid:15) (cid:16)
F (D|Ф )= αND|θ (15.51)
. k j j
j=1
where k is the total number of clusters, w(cid:20)hich we have to provide, and the
.
α
i
>
0 are the weights for each cluster with α = 1. In the following we adapt
. j j
the commonly used notion of the component, which is identical to a cluster. The
complete set of parameters for the Gaussian mixture model is then denoted by
Ф={α ,...,α ,θ ,...,θ }. (15.52)
. 1 k 1 k


================================================================================
PAGE 442
================================================================================

426 15 UnsupervisedLearning
For the GMM, “learning” refers to identifying the values of all model parameters
such that the superimposed Gaussian distributions are the best fit to the training
data.
Parameter Estimation from Maximum Likelihood
How can we obtain the parameters of the normal distributions? Recall, that during
classification with the GNB, we could simply compute the mean and the standard
deviation from the data that was already “sorted” according to the class labels. Here,
this is not possible, because there are no class labels given. Therefore, a different
approach needs to be taken. In GMM this is based on an error minimization, during
which the parameters of the normal distributions are optimized with the objective
that the resulting distribution F fits best to the data. The default approach for such
a probabilistic model is to obtain the optimal distribution F based on the maximum
likelihood method.
Maximum Likelihood Estimation
An important assumption is that all data points are independently distributed random
samples. Then, one obtains the probability p of a single data point x from the
. i
superimposed Gaussian distributions as:
(cid:10)k (cid:15) (cid:16)
p(x |θ)= αN x ; θ , (15.53)
. i j i j
j=1
from which the likelihood L of the entire dataset as the product of the individual
probabilities of all M data records x is obtained:
. i
(cid:21)M (cid:15) (cid:16)
L= p x |θ . (15.54)
. i j
i=1
L can been understood as the joint probability of the combined likelihood of
all the individual data points (note that L is not normalized, it can thus not be
interpreted as a probability density f). As in the context of the GNB, for numerical
purposes, we take the logarithm of the likelihood (see the comment at the end of
Sect.14.5):
⎛ ⎞
(cid:10)M (cid:10)k (cid:15) (cid:16)
lnL= ln ⎝ αN x ; θ ⎠ , (15.55)
. j i j
i=1 j=1
where we additionally made use of the fact thatlog(a·b)=log(a)+log(b).lnLis
. .
now a function of the data and the model, given through the set of model parameters,
i.e.,(α ,μ ,σ )for the one-dimensional case or(α ,μ ,Σ )for the general, multi-
. j j j . j j j
dimensional case.


================================================================================
PAGE 443
================================================================================

15.5 Clustering 427
Expectation–Maximization Algorithm
For obtaining the best fitting model parameters, a minimization problem is to
be solved. There, the partial derivatives of lnL with respect to each parameter
.
are required to be zero. To simplify the derivation, we only consider the case of
univariate normal distributions here; the multivariate case follows the same line of
argumentation but would require to deal with more complex derivatives of matrix
products and covariance matrices. In the one-dimensional case, the j-th distribution
is fully characterized by the mean μ and the standard deviation σ . Together
. j . j
with the third model parameter α , which governs the weighting of the individual
. j
distributions, we require that
∂lnL ∂lnL ∂lnL
=0, =0, and =0, (15.56)
.
∂μ ∂σ ∂α
j j j
which results in a system of (3k−1)nonlinear equations.
.
For solving this system of equations, the so-called expectation maximization
algorithm is employed, which is an iterative, numerical algorithm. In the expectation
step (commonly called “E step”), we assume, that the model parameters (μ ,σ )are
. k k
fixed for each of the k sub-model. Then,p(r |x )gives the probability that pointx
. i . i
belongs to cluster number r.,
α ·N(x ; μ ,σ )
p(r |x )= r i r r . (15.57)
. i (cid:20)k
α ·N(x ; μ ,σ )
j i j j
j=1
The maximization step (“M step”) starts with computing the derivative of the log-
likelihood w.r.t. μ (see Eq.(15.56))as
. j
(cid:8) (cid:15) (cid:16) (cid:9)
∂lnL =− (cid:10)M p(j |x ) ∂ lnσ + x i −μ j 2 =0, (15.58)
. ∂μ j i=1 i ∂μ j j 2σ j 2
and the other two partial derivatives are written analogously. By solving these equa-
tions, we obtain new estimators for the parameters of the j-th normal distribution,
(cid:20)M (cid:20)M (cid:15) (cid:16)
p(j |x )x p(j |x ) x −μ 2
i i i i j
μ = i=1 and σ2 = i=1 , (15.59)
. j (cid:20)N j (cid:20)N
p(j |x ) p(j |x )
i i
i=1 i=1
as well as for the weighting factor


================================================================================
PAGE 444
================================================================================

428 15 UnsupervisedLearning
(cid:10)M
1
α = p(j |x ) . (15.60)
. j i
N
i=1
Therein, M denotes the number of data records used for the fit. With these improved
estimates for the parameter, we can then return to Eq.(15.57). Once the parameter
values do not change anymore (up to a tolerance), we can then terminate the
iteration. Algorithm 15.3 summarizes the GMM algorithm for the case of univariate
Gaussian distributions. The generalization to a higher-dimensional feature space can
be done by extension.
Algorithm 15.3: Univariate Gaussian Mixture Model with k Components (cid:3)
We assume that the number of components k is known, guessed, or given.
I. Initialization
1. Randomly choose mean values .μ1,...,μk, e.g., by randomly sampling k values
without replacement from the dataset.
2. Define the variances.σ
j
2of each components j as the sample variance, e.g.,
(cid:10)M
1
.σ
j
2=
M
(xi −x)2
i=1
where.x ist hesamplemeanofD . .
3. Set all k weighting factors.αr =1/k.
II. The Expectation Step (E Step)
1. Compute the probability that point .x i belongs to cluster number r according to
Eq.(15.57).
III. The Maximization Step (M Step)
1. Calculate for each parameter and for all components a new estimator according
to Eq.(15.59).
2. GOTO 1 and repeat UNTIL convergence has been reached.
15.5.3 Model Selection with the BIC Criterium
Model selection is a process where from a set of candidate models the most suitable
is chosen. The candidate models are obtained by varying model hyperparameters.


================================================================================
PAGE 445
================================================================================

15.5 Clustering 429
For example, in the case of the GMM, the number of components k is such a
hyperparameter. For each value k, a candidate model is trained. Then, the optimal
model can be chosen using an appropriate selection criterion. The Bayesian
information criterion (BIC) is one of the commonly used selection criteria [9] and is
given by:
BIC=−2lnL+dlnM . (15.61)
.
where d denotes the total number of parameters of the model, M is the number of
data records, and L is the maximum likelihood achieved by the model. The first
term of the equation is the maximized likelihood of the model. An increase of the
values of L results in a reduction of theBICvalue. Therefore, the most likely model
.
is that which has the lowest BIC value. The second term represents a penalty for
.
the number of parameters, as the chances of overfitting grow with the number of
parameters.
OCCAM’s Razor and the Curse of Dimensionality (cid:2)
The penalty in proportion to the number of parameters can also be interpreted
in terms of OCCAM’s razor Sect.1.2.5 where it is stated that the simpler model
should be the one that is more likely to be the true one. Overfitting for higher-
dimensional parameter spaces is the consequence of the fact that the “data
coverage” is no longer sufficient.
The BIC is closely related to the Akaike information criterion (AIC). It reads
AIC=−2lnL+2d . (15.62)
.
The two terms have the same functionality as in case of the BIC; however, the penalty
term has no dependency on the number of data records.
In both cases, the goal is to minimize the BIC or AIC, respectively; the model that
gives such a minimum is considered the best suited model. An advantage of the BIC
is that, if one of the candidate models is the “true model” (i.e., one that was used
to generate the data, cf. Chap.12), then the probability that BIC will select the true
model increases with the size of the training dataset. The AIC, on the other hand, is
insensitive to the size of the training dataset.
15.5.4 Density-Based Methods: DBSCAN
Density-based methods do not have any restriction in terms of the shapes of the
clusters. A cluster is understood as a region in which the density of data point (i.e.,


================================================================================
PAGE 446
================================================================================

430 15 UnsupervisedLearning
number of points per area, volume or hyper-volumes) exceeds a given threshold.
One of the most well-known methods is DBSCAN.
The algorithm searches for regions with very high densities, while regions of
low density are related to either noise or outliers. The original algorithm requires
two parameters: the size of the neighborhood εand the minimum number of points
.
that indicate a “cluster region”. DBSCAN is very efficient and is suitable also for
large datasets. As an additional advantage, it does not require the user to specify the
number of clusters beforehand, as opposed to the k-means method.
15.6 Materials Science Examples
We will now turn to two examples: dimensionality reduction for time evolving
microstructures and clustering of indentation data.
15.6.1 Dimensionality Reduction for Microstructures from the ISING
Model
The first investigated dataset, MDS-2 (Ising Model), contains simulation data of
microstructures at different temperatures. What are the features that PCA would
use, and how do these patterns change over the course of the simulations? For
visualization purposes, only the first two principal components of the whole dataset
MDS-2 (Ising Model) are used. visualization of the temperature of the system as a
function of these two components results in the plots shown in Fig.15.17. There,
high temperatures correspond to strong randomness without any structure. We also
clearly see in the right panel that there is a significant difference between the data
with temperature below the CURIE temperature
.
T
C
≈ 2.26and the data above
.
T
C
.
Higher temperatures result in more noisy microstructure, which strongly localizes
the data in the principal space. As the typical “Ising microstructure” forms, the
variance increases, which shows in the large scatter of the data in the principal
space. Could we take an image of a microstructure and, based on that, predict
the corresponding temperature? Only very roughly. For example, for principal
components
.
(30,−30), it is clear that the temperature must be below the CURIE
temperature, but for other points in the principal space, this is not possible.
As a last step in the PCA analysis, we will take a look at a scree plot (cf.
Fig.15.18). We observe that after an initial phase of k ≈ 30 components, the
.
proportion of the variance explained rises up to approximately 70%. After this
“ellbow,” the curve’s inclination is much lower, and it takes another ≈ 130
.
components to reach 100%.
When we use t-SNE to visualize the structure of the dataset, we do not see
any particular structure in the data, see Fig.15.19. Also, varying the perplexity
parameter of the t-SNE algorithm does not produce any significant changes.


================================================================================
PAGE 447
================================================================================

15.6 MaterialsScienceExamples 431
50
25
0
−25
−50
−50 −25 0 25 50
component 1
2
t nenopmoc
4
3
2
1
erutarepmet
Fig. 15.17 Principal component analysis of microstructure data from MDS-2 (Ising Model).
Shown is the temperature as a function of the first two principal components. In the left panel,
the temperature is shown by the color of the marker, while in the right panel, the vertical axis is the
temperature
Fig. 15.18 Scree plot for the
data of the Ising model
Fig. 15.19 Visualization of the MDS-2 (Ising Model) using t-SNE with three different perplexity
values: 3, 12, and 40 (from left to right)


================================================================================
PAGE 448
================================================================================

432 15 UnsupervisedLearning
Fig. 15.20 Principal component analysis of microstructure data from MDS-3 (Cahn-Hilliard
Model). Shown is the temperature as a function of the first two principal components. The left
panel shows the temperature as indicated by the color, while in the right panel, temperature is
plotted on the vertical axis
15.6.2 Dimensionality Reduction for Microstructures from the
CAHN-HILLIARD Model
The dataset MDS-3 (Cahn-Hilliard Model) contains the data of phase microstructure
evolutions together with the resulting total energies. Altogether 20 simulations
are contained in this dataset. Taking a look at the first two components and the
corresponding energy in Fig.15.20 reveals a very interesting structure. The temporal
evolution of each individual simulation can be exactly traced in the latent space.
All simulations start at (0,0), which is reasonable as all phase microstructures (the
.
concentrations) of all simulations are initialized with uniform random distributions.
From these line-like structures, we observe that (i) in between a few time steps, the
microstructure does not change too much; (ii) each simulated microstructure seems
to have a very particular structure since many of the lines occupy distinct places in
the PCA space. Comparing these results to that of the MDS-2 (Ising Model) dataset
from above, we clearly see in the dataset the absence of any randomness within the
simulation model. Figure 15.21 shows the microstructure at three different stages of
four different simulation runs.
Visually, it is not obvious that each microstructure evolution is entirely different
or distinct. It is also not obvious that all microstructures seem to become increas-
ingly different, at least in terms of the principal components.
From this part of the analysis, we can conclude, that at least in some aspects the
dataset is statistically not representative because (a) most of the simulations do not
intersect in the latent space and (b) the simulation data only cover a very small part
of the latent space. Furthermore, each simulation is unique in that sense that at an
early stage, there is a bifurcation point after which two simulation datasets strongly
diverge and which determines much of the following microstructure evolution. Here,


================================================================================
PAGE 449
================================================================================

15.6 MaterialsScienceExamples 433
Fig. 15.21 Microstructure
evolution as obtained from
the CAHN-HILLIARD model
(MDS-3 (Cahn–Hilliard
Model)). Each column is a
different simulation. The top
row is a state close to the
beginning of the simulation,
themiddlerowisan
intermediate state, and the
bottom row shows the
microstructure at the point of
time when the simulation was
stopped
PCA might serve as a tool to monitor the data creation process and to help us judge
if the number of simulations is already sufficient.
15.6.3 Clustering of Indentation Data
As a last example, we investigate the dataset MDS-5 (Nanoindentation) and present
part of an analysis that has been published in [20, 21]. The first question was to
get a baseline for how well the GMM can be expected to work for the quality and
the amount of available data. For this purpose, we investigated two pure metals,
Cu and Cr for which there should be only one material property along with the
corresponding scatter. Figure 15.22 shows the PDF. Taking a look at the BIC,wesee
that the minimum value is reached for one component, indicated by the star, which is
in line with our expectation that for a pure material there should be no other (harder
or softer) “phases”.
Next, we turned to the composites and again analyzed the GMM for a number
of different components k , Fig.15.23. For these materials, we observe an entirely
different behavior. Taking a look at the Young’s moduli (the two left panels in the
figure), we can very well observe that the histogram fits to the superposition of three
Gaussian distributions. Taking a look at the BIC citerion for model selection, this is
supported by the minimum at k = 3. The three Gaussian distributions can then be
.
related to the properties of Cu, the properties of Cr, and the properties of inter-phase
region separating Cu-rich from Cr-rich regions.
Further details can be found in [20]. There, also a bivariate Gaussian mixture
model was applied, to leverage the combined features of hardness and Young’s
modulus.


================================================================================
PAGE 450
================================================================================

434 15 UnsupervisedLearning
Fig. 15.22 Probability density functions (left and middle column) and plot of BIC (right column)
of pure Cu and Cr. The BIC values are shown for both, Young’s modulus (left “y” axis) and
hardness (right “y” axis), where the star indicates the respective minimum. The top row shows the
data for pure Copper, the bottom row for pure Chromium
Fig. 15.23 1D GMM results of CuCr25 (top row) and CuCr60 (bottom row) at .1μm depth.
The left panels show a histogram of the .E i and the fitted individual Gaussian as well as their
superposition. The panels in the second column show histograms of the.H iand the (superimposed)
Gaussians. The two panels on the right show the BIC of H and E, where the star indicates the
respective minimum


================================================================================
PAGE 451
================================================================================

References 435
15.7 Exercises
15.1. Consider the derivation of the principal component formulations and in
particular Eq.(15.15). Why do we need p and c and can not just take the norm
i i
of p in order to obtain c ?
i i
15.2. Assume that a dataset consists of k features and the principal component
analysis that is performed on the dataset consists of k components. How much of
the variance of the original dataset can be explained by the k components? Explain.
15.3. Do the PCA analysis for the MDS-3 (Cahn-Hilliard Model) dataset as done in
Sect.15.6.2 and create a scree plot. Compare it to the scree plot of the Ising model
and discuss the differences.
15.4. Use the MDS-4 (Properties of Chem. Elements) dataset for a principal
component analysis and plot the first two components. Color the marker according
to whether they are metals or nonmetals. Repeat the same with the scikit-learn
implementation of t-SNE and vary the perplexity parameter. Which method would
you recommend as input for a cluster analysis?
15.5. Visualize the “weights” of the PCA of the DS-2 (MNIST) dataset. For this,
take the eigenvectors and reshape them into images of 28×28 pixels. Which are the
patterns or shapes that are the most important or most obvious ones?
15.6. This is a continuation of Exercise 15.5. Do the same study for the MDS-3
(Cahn–Hilliard Model) dataset and discuss the results in view of the results from
the DS-2 (MNIST) dataset.
15.7. PCA for the MDS-2 (Ising Model): do a PCA of only two components and
pick four points in the PC space that are located at extreme points (e.g. the maximum
of component 1 and a zero value of component 2). Their temperature should be
approximately the same. Compare the corresponding real space microstructure
images among each other and with an image that has a zero value for both
components.
15.8. Perform a visualization using the scikit-learn implementation of t-SNE for
the MDS-3 (Cahn-Hilliard Model) dataset
References
1. M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Comput., 15(6):1373–1396, jun 2003. ISSN 0899-7667. DOI https://
doi.org/10.1162/089976603321780317.


================================================================================
PAGE 452
================================================================================

436 15 UnsupervisedLearning
2. R. Bennett. The intrinsic dimensionality of signal collections. IEEE Transactions on
Information Theory, 15(5):517–525, 1969. DOI https://doi.org/10.1109/TIT.1969.1054365.
3. C. M. Bishop and N. M. Nasrabadi. Pattern recognition and machine learning, volume 4.
Springer, 2006.
4. I. Borg and P. J. F. Groenen. Modern Multidimensional Scaling. Springer New York, 2005.
DOI https://doi.org/10.1007/0-387-28981-x.
5. M. Boubchir, R. Boubchir, and H. Aourag. The Use of Principal Component Analysis for the
Prediction of Double Halide Perovskites A2BX6. Journal of Multiscale Modelling, 12(02),
2021. DOI: https://doi.org/10.1142/s1756973721500049.
6. R. B. Cattell. The scree test for the number of factors. Multivariate Behavioral Research,
1(2):245–276, 1966. DOI https://doi.org/10.1207/s15327906mbr0102_10. PMID: 26828106.
7. S. Damrich, J. N. Böhm, F. A. Hamprecht, and D. Kobak. From t-sne to UMAP with
contrastive learning, 2023.
8. A. Draganov, J. R. Jørgensen, K. S. Nellemann, D. Mottin, I. Assent, T. Berry, and C. Aslay.
Actup: Analyzing and consolidating tSNE and UMAP, 2023.
9. S. Gideon. Estimating the dimension of a model. The annals of statistics, 6(2):461, 1978.
10. N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):17–288,
2011. DOI https://doi.org/10.1137/090771806.
11. L. McInnes, J. Healy, and J. Melville. UMAP: Uniform manifold approximation and projection
for dimension reduction, 2020.
12. D. A. Reynolds and R. C. Rose. Robust text-independent speaker identification using gaussian
mixture speaker models. IEEE transactions on speech and audio processing, 3(1):72–83, 1995.
13. F. Saidi, S. Khetari, I. S. Yahia, H. Y. Zahran, T. Hidouri, and N. Ameur. The use of principal
component analysis (PCA) and partial least square (PLS) for designing new hard inverse
perovskites materials. Computational Condensed Matter, 31:e00667, June 2022. DOI https://
doi.org/10.1016/j.cocom.2022.e00667.
14. A. Saxena, M. Prasad, A. Gupta, N. Bharill, O. P. Patel, A. Tiwari, M. J. Er, W. Ding, and
C.-T. Lin. A review of clustering techniques and developments. Neurocomputing, 267:664–
681, 2017. ISSN 0925-2312. DOI https://doi.org/10.1016/j.neucom.2017.06.053.URLh ttps://
www.sciencedirect.com/science/article/pii/S0925231217311815.
15. J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290(5500):2319–2323, Dec. 2000. DOI https://doi.org/10.
1126/science.290.5500.2319.
16. M. Turk and A. Pentland. Eigenfaces for recognition. J. Cognitive Neuroscience, 3(1):71–86,
jan 1991. ISSN 0898-929X. DOI https://doi.org/10.1162/jocn.1991.3.1.71.
17. L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/vandermaaten08a.html.
18. M. Wattenberg, F. Viégas, and I. Johnson. How to use t-sne effectively. Distill, 2016. DOI
https://doi.org/10.23915/distill.00002. URLh ttp://distill.pub/2016/misread-tsne.
19. C. Yang, Y. Kim, S. Ryu, and G. X. Gu. Prediction of composite microstructure stress-strain
curves using convolutional neural networks. Materials and Design, 189:108509, Apr. 2020.
DOI https://doi.org/10.1016/j.matdes.2020.108509.
20. C. Zhang, C. Bos, S. Sandfeld, and R. Schwaiger. Unsupervised learning of nanoindentation
data to infer microstructural details of complex materials, 2023a.
21. C. Zhang, C. Bos, S. Sandfeld, and R. Schwaiger. Nanoindentation Datasets of Copper-
Chromium Composits, Sept. 2023b. DOI https://doi.org/10.5281/zenodo.8336072.


================================================================================
PAGE 453
================================================================================

Machine Learning Techniques 16
I don’t get wrapped up in technique and the like.
Fay Godwin (1931–2005)
British photographer
16.1 Feature Engineering and Feature Importance
How can we obtain create features for machine learning (ML) models and how can
we show that these are possibly better than other choices? The next two subsections
provide some fundamental knowledge about useful approaches and techniques.
16.1.1 Feature Engineering
Feature engineering is an ML technique that utilizes the existing variables to create
new variables. The new variables are not contained in the original dataset and
typically relate to the existing features in a nontrivial manner. Such a relationship
can be, e.g., a simple variable transformation, e.g., using the square or the logarithm
of an existing feature. In the context of feature engineering, this is called feature
transformation. The motivation for performing this transformation is very similar
the reason why variables are transformed in physics and in numerical maths: to
simplify equations and to increase the stability and robustness of a model
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 437
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_16


================================================================================
PAGE 454
================================================================================

438 16 MachineLearningTechniques
Words of advice (cid:2)
Often, feature engineering is understood as everything that is part of the prepro-
cessing (cf. Sect.11.5). This would include the treatment of outliers, data imputation,
as well as scaling and variable transformation. In this text, we only consider those
techniques as feature engineering that actually result in new features. Data cleaning
is notpartofthis.
A method related to feature transformation is categorical encoding, which is a
kind of a transformation. The main idea is to transform categorical features into
numerical features. The simplest way of doing this is to assign each different label
an integer number; this technique is also called label encoding. one-hot encoding
(OHE) is a method for turning different categories into a binary representation, cf.
Example 16.1. One of the advantages of this technique is that an ML model can
output different probabilities for each label, which reveals more information than
just a single label output (also see Sect.18.5 for a Python example).
Example 16.1 (One-Hot Encoding and Microstructure Classification) (cid:2)
One-hot encoding is commonly used for classification with deep learning (DL)
models. The idea is to convert different categorical class labels into a sequence
of ones and zeros. For example, imagine a multi-class instance segmentation
where a DL model detects different types microstructural features in a
micrograph, e.g., a grain boundary, a twin, a dislocation, a precipitate, and
the rest: background. The encoding is then
grainboundary → 1,[0,0,0]
.
twin → 0,[1,0,0]
dislocation → 0,[0,1,0]
precipitate → 0,[0,0,1]
background → 0,[0,0,0]
The output of a DL model would consist of four individual output “nodes,”
each of which represents the binary value zero and one.
Feature engineering is also a good opportunity to introduce domain knowledge
into an ML model, such that the feature represents physical variables or more
complex, mathematical terms. In the context of feature engineering, this is called
feature creation.


================================================================================
PAGE 455
================================================================================

16.1 FeatureEngineeringandFeatureImportance 439
16.1.2 Permutation Feature Importance
Permutation Feature Importance quantifies the importance of a specific feature in
the relationship between all input variables and the output. This technique was first
introduced in 2001 by L. BREIMANN [1].
The first step is training a model with the true features. In a second step, the
prediction error for the original model is compared with the prediction error after
randomly permuting the data of an input variables one at a time (see Example 16.2
for an example of the permutation process).
Example 16.2 (Permutation of Features) (cid:2)
How does “permutation of the features” work in practice?
A given feature matrix X has the data shown in the left table while a
.
permutation of the first feature is shown in the middle table, a possible
permutation of the second feature is shown on the right:
original features: perm. of 1st feature: perm. of 2nd feature:
X X X X X X
1 2 1 2 1 2
1 10 3 10 1 10
2 11 1 11 2 14
. . .
3 12 3 12 3 11
4 13 5 13 4 15
5 14 4 14 5 12
Assume that a model was trained with the original features, resulting in the
following hypothesis of a regression problem
f(X ,X )=0.5+X ·exp(X −1).
. 1 2 1 2
Then, this hypothesis is used for predictions based on the two permuted
feature matrices.
A feature is considered more “important” than another feature, if the prediction
using the permuted feature results in a higher error than for making prediction with
another permuted feature. The argument for a higher feature importance is that a
larger error indicated that the model “relies” more strongly for making predictions
on that feature than on any other feature. If the original error is given byEorigand the
.
error after permuting the data of feature i is given byEi, then the feature importance
.
FIi is defined as any of the two measures
.


================================================================================
PAGE 456
================================================================================

440 16 MachineLearningTechniques
Fig. 16.1 Feature
permutation importance for a
classification problem with
dataset MDS-4 (Properties of
Chem. Elements). The
whiskers in the box plot result
from the fact that for each
feature, a number of random
permutations were performed
Eorig
FIi = or FIi =Eorig−Ei . (16.1)
. Ei
This value is then computed for all features. To increase the reliability of that result,
several different random permutations of the same feature are used and averaged.
Typically, the permutation feature importance of all features is summarized by
sorting the feature importance according to magnitude. Figure 16.1 shows the
feature importance for the dataset “MDS-4 (Properties of Chem. Elements).” We
observe that the ionization energy is by far the most important feature. It is also
obvious that computing feature importance is also useful for feature selection, e.g.,
by dropping less important features. In this case, if we were to reduce the number
of features, the atomic radius would certainly be the first feature to remove.
Words of advice (cid:2)
Care needs to be taken in cases where features are strongly correlated. While each
individual feature might have a low feature importance, dropping all of them might
result in a model that behaves significantly worse than the original one.
16.2 Data Splitting, Cross-Validation, and Statistical
Resampling
In Sect.8.4, we introduced how through statistical sampling a set of examples,
the sample can be chosen from a population. Sampling implies very often that
an investigated dataset is only a subset of the real, possibly even infinitely large
population. Therefore, a good statistical sampling is one that creates a dataset
through which the relevant statistical properties of a population are reflected. We
have already discussed a number of sampling methods and systematic strategies for
this purpose and also had a look at the source for different types of sampling errors.
In the context of ML model validation, we encounter another situation where
sampling strategies are required: Training a model without validation may give
wrong results. In Sect.12.5.3, we saw that there can be big differences between the
training performance of a model and the behavior with respect to new, previously
unseen data. Remember, that in ML we are not interested how well the method works


================================================================================
PAGE 457
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 441
on the training data, we are interested in the accuracy of the predictions when the
method is applied to new data. This is the goal of cross-validation. It is useful for
identifying problems such as overfitting or selection bias (i.e., a systematic error
due to non-random sampling). However, before we introduce the cross-validation
methods, we first need to discuss the difference between training, testing, and
validation datasets.
16.2.1 The Difference Between the Testing and Validation Dataset
There is an important conceptual difference between the testing dataset and the
validation dataset. What we have learned so far was that to evaluate the performance
of a model, we have to use a subset of the whole dataset that has not been used for
the training. Typically, this is called the testing data. As long as the whole dataset is
only split into these two subsets, there is no confusion about which is which.
However, when a hyperparameter study is performed for, e.g., determining the
most suitable model by varying parameter of the hypothesis function, care needs
to be taken. In that case, the held-out dataset would be used for selecting the
most suitable candidate model, e.g., by comparing the performance on the held-out
dataset for each candidate model. We cannot use any of the data that were already
used during the model selection, in the final performance evaluation. In such cases,
three distinct datasets are required: the training dataset, the validation dataset, and
the testing dataset, which are defined as follows:
Definition 16.1 (Training Dataset). The training dataset consists of all data
records used for training a model.
Definition 16.2 (Validation Dataset). The validation dataset is a held-out
dataset that is entirely separate from the training dataset. It is used for
evaluating the performance of the different models during hyperparameter
tuning.
After validation of all candidate models, we will choose the best-performing model,
which is then called the final model. This is the point where the testing dataset comes
into play:


================================================================================
PAGE 458
================================================================================

442 16 MachineLearningTechniques
Definition 16.3 (Testing Dataset). The testing dataset is a dataset that is
entirely separate from the training and the validation dataset. Its purpose is
to be used for evaluating the final model, i.e., after the model selection was
finished.
In other words, if there is no hyperparameter optimization and there is just the model
that we are going to train, using the concept of splitting the data into training and
testing data is, in general, perfectly fine (which also explains why in such situations
the differentiation between validation and testing set is no strictly followed).
As soon as a model needs to be selected from a set of candidate models, then we
have to keep a validation and a testing dataset separated from the training data (and
from each other).
The following sections introduce a number of methods that are commonly used
for the purpose of cross-validation. Note, that most of these are explained for
regression problems, e.g., using the mean squared error (MSE) as error measure.
This can be easily transferred to classification problem by replacing the MSE with
a suitable classification measure such as the relative number of falsely classified
examples.
Words of advice (cid:2)
... on he notions of validation and testing: In the following subsections, we do not
consider a split into a training, a validation, and a testing dataset but only a training
and a testing dataset. Beware that in this case, the latter is often synonymously called
testing or validation set. Below, you will encounter both notions, which might lead to
some confusion.
16.2.2 The Holdout Method—Or the Train/Test Split
The holdout method is the simplest method for testing the performance of a model. It
is sometimes also simply called “train-test split” and is probably the most commonly
known method. The idea behind this method is as follows:
1. Partition the sample of data into two complementary subsets or partitions by
randomly assigning data points to the subsets. The two partitions are the training
and the validation set.
2. Perform the training (i.e., build the model) on the training set.


================================================================================
PAGE 459
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 443
Fig. 16.2 The holdout method (i.e. the “common train/test split”) splits the full dataset (sym-
bolized by the “data dots”) into disjunct training and validation datasets, each of which contains
random samples from the full dataset. The performance of the trained model is then estimated
based on the so far “unseen” validation data
Fig. 16.3 Two scenarios of splitting a dataset into 60% train data and 40% test data. Left: all data
points have the same probability to end up as train or test data. Right: test data are chosen with
a higher probability from the left region of the dataset. The true function is the function used to
create the data without noise. The dashed lines show a least square fit of a quadratic polynomial to
the train data, showing the prediction of this model
3. Validate the analysis (i.e., validate the model performance) on the other subset
(the validation or testing set1 ).
Usually, the training set is larger than the validation/testing set. Typically used
fractions of the total dataset size are 60%–80% of the dataset for training and 40%–
20% for testing/validation. The concept is shown in Fig.16.2.
The method requires only a single validation run and is therefore rather fast.
However, depending on the sample size or the type of sampling, the holdout method
can be very sensitive with respect to the particular choice of the partitioning of
the data, and for small datasets, this method is strongly influenced by the selection
bias. As an example, in Fig.16.3, the effect of a rather extreme selection bias on
the train/test split is shown. There, the noisy data are created by sampling the “true
function” given by an quadratic equation, which was superimposed with random
1 In particular for the holdout method, the terms validation and testing are often used interchange-
ably in the literature.


================================================================================
PAGE 460
================================================================================

444 16 MachineLearningTechniques
Fig. 16.4 100 random realizations of a 60% training/40% test data split in analogy to the example
shown in the left panel of Fig.16.3. Shown are the resulting fitted lines in blue, plotted on top of
each other (left panel); the dark line is the true function; the dashed line is the mean of all 100
experiments. The right panel shows the histogram of all MSE values
noise. The dashed line is obtained from a least square fit to the training dataset, and
the resulting quadratic function then represents the learned model. A statistically
“unbalanced” train/test split results in two sub-datasets that are statistically not
representative of the whole dataset. While the performance of the model for the
training data is good, it does not perform well for the test data, as can be seen by
the big deviation between fitted line and test data. As for the unbalanced training
dataset, almost no data for the left third of the plot are available, it is not surprising
that there the model performance is worse than for the balanced train/validation
data. This “selection bias” is a particular problem for small datasets or for those that
have a very heterogeneous distribution of data, e.g., with a strong clustering in some
regions.
Quantitative Analysis
Quantitative analysis of the performance can be done for such a regression problem
based on the MSE as error measure. For example, for the above example, the true
function is known, and based on that the difference between prediction and true
values can be obtained and used in the MSE. Figure 16.4 shows an ensemble of
100 different, random train/test splits (60%/40%) together with the corresponding
distribution of the MSE. There, it can be seen that for some realizations there is
a substantial error, and even the mean of all 100 realizations shows significant
deviations from the true curve in some regions. But most importantly, the variance
of the predicted data is quite large. Therefore, based on a single experiment, with
the holdout method, it is difficult to judge the quality of the prediction.


================================================================================
PAGE 461
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 445
Python Listing 16.1 An implementation of the train-test split
Things to Remember about the Holdout Method (or just: train/test split)(cid:3)
(cid:129) easy to implement
(cid:129) fast in terms of computation (only one pass needed)
(cid:129) can be strongly dependent on the random choice of the partitioning
(depending on structure or variance of the data and in particular for small
sample sizes)
(cid:129) size of the training dataset is significantly reduced. (cid:2) often, test error might
get overestimated
(cid:129) gives only a rough estimate of the true performance of a model
In Python Listing 16.1, a function for the train-test split is shown, which takes a
dataset in form of a one-dimensional X and y array


================================================================================
PAGE 462
================================================================================

446 16 MachineLearningTechniques
16.2.3 Leave-One-Out Cross-Validation
The biggest shortcomings of the holdout method are (i) a dependency on a selection
bias, (ii) the dependency on the structure of the dataset in case of nonuniform
sampling and the significant reduction of data records available for training. The
leave-one-out cross-validation (LOO CV) tries to remedy some of these problems
through an “extreme” partitioning strategy:
1. Keep only a single data point for validation and use all other n−1observations
.
for training the model.
2. Use the trained model for making a prediction for the single, left out data point
and compute the error for this point, e.g., the MSE for a single point (which is the
squared difference).
3. Perform the split of the dataset, followed by training, prediction, and error
calculation altogether n-times until all data records were once “left out” as
validation points.
4. If each experiment results in the MSE.i , then the resulting cross-validation error is
(cid:2)n
1
. CV= MSEi (16.2)
n
i=1
Figure 16.5 shows the first five of these experiments. As compared to the holdout
method, the LOO CV exhibits a strongly reduced bias because each experiment is
trained on virtually the entire dataset. Additionally, this measure does not contain
any random effects since all possible partitions are considered and reflected in the
average cross-validation estimate. While the holdout method gives a different value
whenever it is computed (due to the randomness of the train/test split, cf. Fig.16.4),
the LOO CV is a deterministic method that can only give as many different values
as there are records in the dataset. For least squares regression using polynomial
functions, there exists a formulation for the averaged CV that effectively has
the same computational cost as the holdout method [3]. However, this cannot be
generalized to arbitrary regression or classification methods; for those, there are still
n experiments to be performed, which is why the computational cost is increasing
in proportion to the number of observations.
Things to Remember about the leave-one-out cross-validation (cid:3)
(cid:129) relatively easy to implement
(cid:129) for n samples, n experiments need to be performed. (cid:2) high computational
cost
(cid:129) deterministic cross-validation error, no influence from random sampling


================================================================================
PAGE 463
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 447
Fig. 16.5 Shown are the first five experiments for the LOO CV strategy. During each experiment,
only one data point is used for validation, the rest is used for training, resulting in an as large as
possible training dataset
16.2.4 k-Fold Cross-Validation
k-fold cross-validation (k-fold CV) combines the advantages from both the holdout
method and leave-one-out cross-validation: it creates a sequence of different
training/validation partitions—the so-called folds and based on these, computes
an average cross-validation error. The validation partition contains, unlike in the
LOO CV method, more than one observation. Here is a brief summary of the strategy:
1. Split the whole dataset into k partitions by randomly assigning data points to
the partitions. Fold number i consists of partition i, used as validation set; all
remaining partitions are used together as training set.
2. Train the model for with training fold i and based on its predictions compute an
error measure for this fold i, e.g., the MSE.
3. Perform the training, prediction, and error estimate for all k folds.
4. If each experiment results in the MSE.i then the resulting cross-validation estimate
is
(cid:2)k
1
. CV= MSEi . (16.3)
k
i=1
A sketch of this method is shown in Fig.16.6 for five different partitions and folds.
The holdout method corresponds to the evaluation of a single fold. Obviously, in the
limit case of n observations and . k = nfolds, this approach is identical to LOO CV
introduced above. Comparing the two methods, it turns out that often the resulting
MSEs is very close. For practical purposes, . k = 3 ...10 are reasonable number of
folds where k =3is feasible for small dataset and k =10for larger ones. For very
. .
small dataset it is important to use a value of k that is a divider of the number of
observations such that all partitions have the same number of data points.
How to decide on whether we deal with a small or large dataset and the fold
sizes are representative? A first impression can be obtained by calculating summary


================================================================================
PAGE 464
================================================================================

448 16 MachineLearningTechniques
Fig. 16.6 This diagram shows the strategy for a 5-fold cross validation: The complete dataset
(symbolized by the “dots” in a row) is split into five equal amounts of data, the partitions.During
five separate experiments, each of these five partitions is used as validation dataset while the
remaining four datasets are used for training. The partitioning for each experiment stays the same
(note that in each experiment the position of the “data dots” is not changing)
statistics and observing (i) how pronounced the variance of the MSEs is and (ii) how
the MSE compares to that of the whole dataset.
As compared to the LOO CV, the computational cost is significantly lower.
Nonetheless, training a model or, to a lesser extent, making predictions on very
large datasets four or eight times might not always be feasible. In these cases, the
holdout method might be the only choice.
Things to Remember about the k-fold cross-validation (cid:3)
(cid:129) more complex to implement
(cid:129) for k folds, k experiments need to be performed. (cid:2) medium computational
cost, does not scale badly with the number of observations
(cid:129) non-deterministic cross-validation error as the observations are randomly
assigned to the folds, but it is a robust performance estimate
16.2.5 Repeated k-Fold Cross-Validation
The idea behind this method is to repeat a single k-fold CV and to analyze the
variation among the repetitions. In between each repetition, the data are randomly
shuffled so that for each repetition, different samples are contained in the folds.
Then, one of the two different cases can be expected: (i) The model are “well-
behaved” and stable such that for the same model parameters, small variations in
the training dataset still give the same prediction for the validation dataset. (ii) If


================================================================================
PAGE 465
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 449
Fig. 16.7 The stratified 5-fold cross validation is a method for handling classification problems
with class imbalance. Stratified corresponds to the fact that each fold contains samples with
identical class statistics. This is symbolized by the two different markers: each fold has the same
number of each marker, and the ratio between “triangle” examples and “circle” examples is the
same as in the entire dataset. Apart from this all of the above concerning the k-fold CV applies
a model has a high sensitivity with respect to small variations in the training data,
this also results in more pronounced variations of the predictions across different
repetitions. The aspect of model stability can be investigated through repeated
k-fold CV.
16.2.6 Stratified k-Fold Cross-Validation
So far, all discussed methods can be applied to both regression problems as well as to
classification problems (in the latter case, instead of the MSE suitable error measures
need to be employed, such as the relative number of misclassified observations).
A specific problem occurring only for classification problems is the problem
of class imbalance, i.e., strong variations of the number of examples per class
(for regression problems a similar phenomenon can occur if different regions have
a different density of data, e.g., see the distribution of energy for the “MDS-3
(Cahn-Hilliard Model)” dataset). Stratified sampling implies that each fold contains
approximately the same distribution of samples per class as contained in the total
dataset. This is an important aspect as the goal is to make predictions for the whole
population based on a set of samples. Figure 16.7 gives a visualization of the method


================================================================================
PAGE 466
================================================================================

450 16 MachineLearningTechniques
16.2.7 Bootstrap
So far, in all above approaches, it was important to split the dataset into disjunct
partitions. This is the most obvious difference to the bootstrap:2 For choosing data
points for the training dataset samples are randomly drawn with replacement from
the entire dataset. This may result in a bootstrapped dataset with a number of
identical data points, compare the example in Example 16.3.
Meaning of “bootstraps” (cid:3)
Originally, in the literature, the phrase by one’s own bootstraps denotes the
self-starting process that is able to proceed by itself and without any extra
help.
The bootstrap of the original datasetD can be performed a number of times, say
.
n-times, and each time one obtains a different datasetD ∗ . Each of the bootstrapped
. n
datasets can now be used to train a model, resulting altogether in an ensemble of
models. Within this ensemble, the average model predictions can now be obtained
from
(cid:2)n
1
(cid:4)y(cid:5)= y(xi) where xi denotesallx ∈D∗ (16.4)
. n i
i=1
Then, e.g., the so far not used samples can be used for validation purposes. The
training datasets are then overlapping subsets of the entire dataset.
Bootstrapping is a method originally used in statistics and belongs to the class of
resampling methods. It was first described in 1979 by Efron [2], a statistician from
Standford University, as an enhancement and generalization of the Jacknife method.
Both methods are also used for “regular” random sampling in statistics and there in
particular for estimating the median, bias, and variance of a dataset.3
2 In the ML community it is often just “bootstrap”, “bootstrapping”, or “the bootstrap” but
originally called “the bootstrap method”
3 A curious side note about the origin of the name “Bootstrap” for this method: at the end of [2], B.
Efron acknowledged discussions about the naming of his method: “[...] I also wish to thank the
many friends who suggested names more colorful than Bootstrap, including Swiss Army Knife,
Meat Axe, Swan-Dice, Jack-Rabbit and my personal favorite, the Shotgun, which, to paraphrase
Tukey, ‘can blow the head off any problem if the statistician can stand the resulting mess’.”


================================================================================
PAGE 467
================================================================================

16.2 DataSplitting,Cross-Validation,andStatisticalResampling 451
Example 16.3 (Bootstrapping a Dataset of Categorical Data.) (cid:2)
Assume a training dataset,D , which consists of 10 records. Each record is
.
governed of two real valued features, x and x , and a class label y as output
. 1 . 2
where y is one of the three marker symbols. The dataset is shown in the table
below; additionally, the figure shows a visualization of the dataset.
2
1 5
1
0 5
1 2 3
1
2
The bootstrapped datasetD ∗ is obtained by drawing 10 samples fromD
. .
with replacement, i.e., any record could be drawn multiple times. One could
now use, e.g., the numpy function random.randint(1, 11, 10) to draw 10
numbers from the half-open interval [1,...,11) which might give the to be
.
drawn number of sample n = {6,4,6,7,2,3,2,3,10,4}. There, the sample
.
number 6 occurs twice, sample number 7 occurs only once, sample number
8 was not drawn. This is shown in the following, already compacted table in
which the last column gives how many times a sample was drawn.
The scatter plot on the right indicates the number of drawings of the
same sample by the small superscripts, e.g., the triangle with the superscript
(cid:2)x2 denotes two “coinciding” data points. The new, bootstrapped dataset is
.
similar to the original one in some aspects but, e.g., misses some data points.
Therefore, it has slightly


================================================================================
PAGE 468
================================================================================

452 16 MachineLearningTechniques
16.3 Baseline Models
Sometimes, we need to get a quick and rough estimate of what “good performance”
actually means. A baseline or a baseline model is usually a very simplistic model or
estimation that serves as a rough orientation and that can be created without having
to invest a significant amount of time. This does not necessarily have to be a non-ML
model, it can also be any other reference model that is already available. However,
the emphasize is on simplicity and certainly excludes an involved training process,
let alone hyperparameter optimization. Typically, a good model should perform
better than the baseline model.
A good first step is, to take a look at the data, maybe perform a exploratory data
analysis (EDA), which is also useful for determining a performance metric.
Baseline Models for Classification
As an example, consider a classification problem with two classes each of which are
equally likely. The simplest strategy is then to use a classifier that randomly assigns
class label to new data. This will give the correct results in 50% of all cases, and
.
the base line accuracy would be
.
5 0%.AnyML that we train should perform better
than that; if this is not the case, it might be a hint that there is something wrong
with the implementation. This approach can also easily be generalized to multi-class
problems. If the dataset has a class imbalance, then one can use the most frequently
occurring class label and from that compute accuracies for the whole dataset.
Baseline Models for Regression
A very simple approach for obtaining a regression baseline model is to use some of
the information about the expeced value of the data distribution itself. E.g., using
the mean (or median) of all data records will give us at least a rough idea about a
range in which some of the data will be located. Another approach is to use any
constant value that is chosen, e.g., based on our domain knowledge of a problem.
An example is shown in Example 16.4.
Example 16.4 (Regression Baseline for the Tensile Test Dataset) (cid:2)
The goal is to create a regression model for the MDS-1 (Tensile Test) dataset
that is able to predict the stress–strain curve. We only focus on the high-
◦
temperature regime of 600 C and strains up to 0.007. A simple baseline
.
model is to take the mean or the median of all stress values as shown in the
figure.
(continued)


================================================================================
PAGE 469
================================================================================

References 453
As can be seen, the median has a high value since there are many examples
in the plastic regime while the mean is more balanced. The MSE value for the
median baseline is 59, for the mean baseline it is 46.
If we implement a simple linear regression model, then we get a MSE
value of 24—not great but better than the baseline, which also gives us some
confidence that our implementation is probably correct.
References
1. L. Breiman. Machine Learning, 45(1):5–32, 2001. DOI https://doi.org/10.1023/a:
1010933404324.
2. B. Efron. Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 7(1), Jan.
1979. DOI https://doi.org/10.1214/aos/1176344552.
3. G. James, D. Witten, T. Hastie, and R. Tibshirani. An Introduction to Statistical Learning.
Springer New York, 2013. DOI https://doi.org/10.1007/978-1-4614-7138-7.


================================================================================
PAGE 470
================================================================================

Part IV
Artificial Neural Networks and Deep Learning


================================================================================
PAGE 471
================================================================================

17
From the Perceptron to Artificial Neural
Networks
A single neuron in the brain is an incredibly complex machine
that even today we don’t understand. A single ‘neuron’ in a
neural network is an incredibly simple mathematical function
that captures a minuscule fraction of the complexity of a
biological neuron.
Andrew Ng (born 1976), American computer
scientist and technology entrepreneur
17.1 A First Model of a Neuron
The biological neuron is often used to explain the motivation behind the computa-
tional model(s) of neurons. While, in the early days of the computational neurons,
it was certainly true that they were designed to mimic biological neurons, hoping to
create an “artificial intelligence”. However, later on, with the emergence of more
powerful models this was changed, and it is no longer the goal to imitate the
biological neuron (unless we are diving into “neuromorphic computing”). In the
following, we trace the historic development artificial neural networks (ANNs) and
thereby understand how, why, and when these single neuron models became the
“launch pad” for today’s powerful and complex deep learning architectures.
17.1.1 The Biological Neuron
The name neuron was coined in the late nineteenth century by the anatomist HEIN-
RICH WILHELM WALDEYER [11], who already speculated 1881 that biological
neurons might be the basic building blocks of the nervous system. Generally,
neurons are nerve cells for processing and transmitting chemical and electrical
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 457
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_17


================================================================================
PAGE 472
================================================================================

458 17 FromthePerceptrontoArtificialNeuralNetworks
Fig. 17.1 Anatomy of a multipolar neuron: left is the presynaptic cell, right is the postsynaptic
cell. The input signal arrives at dendrites/dendritic branches, is processed in the soma/cell body,
which passes the signal through the axon to the synapses or other neuron’s dendrite (indicated by
the red arrows). (Image adapted from [2], licensed under CC BY 3.0 license1)
signals. Typically, a large number of neighboring neurons (even up to 200,000) are
interconnected with each other and thereby exchanging information.
A typical neuron consists of a cell body, the soma, whose purpose is to process
the received signals. The links between the neurons consist of dendrites (from
the Greek δένδρον, pronounced dendron = “tree”) and the axon (from the Greek
αξο˜ν, pronounced axon = “axis”): through the dendrites, the signal is received
.
from other neurons, and the axon transmits the output. The synapse is located at
the end of the axon and acts as connection point between two neurons; mostly, it
“translates” electrical signals into chemical signals, the so-called neurotransmitters.
A particular type with one axon and two or more dendrites/dendritic branches is
called multipolar neuron, see the sketch in Fig.17.1. Neurons are most commonly
encountered in the central nervous system and spinal cord of almost all animals.
Neurons react to electrical signals, or more precisely to the change of voltage
over time. If this change within a small time span is sufficiently large, then the
neuron “fires” by emitting an electrochemical pulse. This pulse is transmitted by the
axon, getting weaker with increasing distance, and eventually reaches the synaptic
connection where it causes an excitatory or inhibitory response. The particular
kind of response can be modified in a number of different ways. Hence, the
signal transfer can be adapted by certain types of stimuli—a process that is called
1https://creativecommons.org/licenses/by/3.0/deed.en


================================================================================
PAGE 473
================================================================================

17.1 AFirstModelofaNeuron 459
“synaptic plasticity”.2 It was only about 10 years after the neuron got its name from
H. W. WALDEYER that EUGENIO TANZI, an Italian neuropsychiatrist, hypothesized
that during repetitions of a learning process, a repeated neuronal activity was
initiated that reinforces the current synaptic connections and makes the crossing
of the signal easier [1]. Further information on the biological process from the point
of view of computer science can be found in, e.g., [6].
17.1.2 Introducing...the McCulloch-Pitts Model
Two persons are responsible for the McCulloch-Pitts (MCP) model, which is named
after them: WARREN. S. MCCULLOCH and WALTER PITTS. The MCP model [3]
was developed in 1943 and is also called Threshold Logic Unit (TLU) model. It was
the first computational model designed to resemble or imitate a biological neuron.
It’s main goal was to make a decision y ∈ 0{,1}, i.e., to answer a question with
.
“yes” or “no” based on some input x ∈ {0,1}. There, the values 0 and 1 typically
. i
correspond to the Boolean values False and True.
Example 17.1 (A decision that the MCP model could make) (cid:2)
A decision made by the MCP model might be answering the question “Is it
going to rain in the afternoon?”. The decision could be based on the input
information x : “The sun is shining right now” (“yes” or “no”); x :“Theair
. 1 . 2
humidity is above 70%” (“yes” or “no”). The output is then y = 1 (“yes, it
.
will rain in the afternoon”) or y =0(“no, it will not rain in the afternoon”).
.
In analogy to the example of the biological neuron in Fig.17.1, the flow of data
would take place from left to right: the input values x from the left (dendritic
. i
branches) are processed or aggregated by the cell body and transferred to the right
through the axon to the synapse, which then fires (or not).The MCP model is based on
the assumption that the time for the information transfer along the axon is negligible.
The aggregation of the input data x happens in the form of an (unweighted) sum
. i
while the fact that the synapse fires only above a certain threshold is modeled
through a nonlinear activation function ϕ. Mathematically, the aggregation of the
.
input happens in the aggregation or transfer function a(·),
.
(cid:2)n
a(x)= x , (17.1)
. j
j=1
2 This notion is the reason why an online search for the word “plasticity” not only shows materials
science results on plastic deformation of metals but also shows—an equally large number of—
seemingly unrelated search results from the field of neuro science.


================================================================================
PAGE 474
================================================================================

460 17 FromthePerceptrontoArtificialNeuralNetworks
yielding the net input of the unit. The binary output y is then obtained by applying
the activation function ϕto the aggregated data
.
y =ϕ(a(x)). (17.2)
.
In the case of the MCP model, the activation function is taken as a unit step function
with the threshold value θ,
.
(cid:3)
1 IF a ≥θ
ϕ(a)= (17.3)
.
0ELSE,
as depicted in Fig.17.2a. It is important that the comparison is ≥ and not just =
. .
as can be seen in the next section. A typical sketch of the MCP model is shown in
Fig.17.2b; Fig.17.2c shows the commonly used short version.
Words of advice (cid:2)
In a number of publications and online resources, the MCP model is not always
presented correctly: there, the input values are often individually weighted with
factors, which, according to the publication of McCulloch and Pitts from 1943 [3], is
not correct. In the original MCP model, the inputs are not individually weighted and
represent Boolean values.
Fig. 17.2 The MCP model takes the input.x i, sums them up in the transfer function, and “makes a
decision” with the activation function. For this model, the activation is the unit step function shown
in (a) and has a threshold .θ , which is also indicated in the detailed symbolic representation of the
model in (b). (c) show a compact representations of the model for three inputs


================================================================================
PAGE 475
================================================================================

17.1 AFirstModelofaNeuron 461
Table 17.1 Representation
Input Data Processing Output
of the logical AND by a MCP
model with two inputs and a .x 1 .x 2 .a(x) = x1 + x2 .a ≥ θ ? .y = ϕ(a) .x 1 ∧ x2
threshold value.θ =2.The 0 0 0 .0 ≥ 2 ? 0 0
two output columns show the 1 0 1 .1 ≥ 2 ? 0 0
model output as well as the 0 1 1 .1 ≥ 2 ? 0 0
true output.x 1 ∧x2 1 1 2 .2 ≥ 2 ? 1 1
17.1.3 Boolean Decision Problems
Many problems can be turned into a Boolean decision problem , i.e., decisions where
the input data and the outcome (i.e., the decision) can be reduced to statements with
“yes” or “no” answers. In other words, the output data are categorical data and
thereby solve binary classification problems as there are only two possible outcomes
(0 or 1), which, however, can be interpreted, e.g., as “the testing specimen shows
failure” or “the testing specimen is intact.”
In the MCP model, both the input values . x i and the output y can only take the
values 0 or 1, which can then be interpreted as the Boolean values True ≡ 1
.
and False ≡ 0. Therefore, the model has the potential to solve certain Boolean
.
decision problems, hence the alternative name “Threshold Logic Unit” (TLU).
We will now investigate if the MCP model would be able to represent the
fundamental Boolean operations AND, OR, and NOT, and thus if the model would
. . .
be able to make such a decision. A numerical, automated strategy for finding the
most suitable value of θ is a different task and will be addressed for subsequent,
.
more complex models below.
• Logical ANDoperation
.
This logical operator is represented using the ∧ symbol. For example, for two
.
input valuesx andx ,itisx ∧x . The result of the logical AND is True if and
. 1 . 2 . 1 2
only if all input values are True. For a neuron with two inputs, this is the case
if the threshold value is chosen as θ =2; for a logical unit with n inputs, θ must
. .
be set to n.
This can be easily seen in Table 17.1 by comparing the column “a =x +x ”
. 1 2
with the column “x ∧x ”: only when a changes from 1 to 2 the output should
. 1 2
change from 0 to 1 and thus . θ =2. Thus, the MCP model is able to represent the
logical ANDfunction.
.
• Logical ORoperation
.
This logical operator is represented using the ∨ symbol. For example, for two
.
input values x and x , the OR function is x ∨x . The result of the logical OR
. 1 . 2 . 1 2
is True if at least one of the input values is True. For a neuron with two and
more inputs, this is the case if the threshold value is chosen asθ =1as shown in
.
Table 17.2.


================================================================================
PAGE 476
================================================================================

462 17 FromthePerceptrontoArtificialNeuralNetworks
Table 17.2 Representation
Input Data Processing Output
of the logical OR by a MCP
model with two inputs and a .x 1 .x 2 .a(x) = x1 + x2 .a ≥ θ ? .y = ϕ(a) .x 1 ∨ x2
threshold value.θ =1.The 0 0 0 .0 ≥ 1 ? 0 0
two output columns show the 1 0 1 .1 ≥ 1 ? 1 1
model output as well as the 0 1 1 .1 ≥ 1 ? 1 1
true output.x 1 ∨x2 1 1 2 .2 ≥ 1 ? 1 1
Thus, the MCP model is able to represent the logical OR function.
• Logical NOToperation
.
This logical operator is represented using the ¬ symbol. For example, for an
.
input x, the NOT function is¬x. The result of the logical NOT function is True
.
if the input is False (and vica versa). The NOT function is the counterpart to
an inhibitory synapse.
For representing the NOT function, though, it would be necessary to weight
the input with −1 and adjusting threshold alone is not sufficient, which can be
.
shown as follows: Assume that x =0. It follows:
.
x =0 ⇒ y =¬x =1 (17.4)
.
As for a single input, the transfer function becomes the identity function,a(x)=
.
x, we are looking for the θ that fulfills the inequality
.
a(x)=x =0≥θ ⇒ θ ≤0. (17.5)
.
Assuming x = 1it is y = 0, and we are looking for values of θ that do not
. . .
satisfy the inequality, a =x ≥θ, such that y evaluates to 0 (False):
.
x (cid:2)θ ⇒ θ >1. (17.6)
.
As it is impossible to fulfill Eqs.17.5 and 17.6, the NOT operation cannot be
.
represented by the MCP model.
Another Boolean operator is the XOR operator, which is represented by the
.
symbol ⊕ or also by ∨˙. Its result is True if and only if x /= x , i.e., one input
. . . 1 2
is True while the other one is False. The XOR operator can be replaced by a
.
combination of AND and OR operations together with the NOT operator. As the
. . .
MCP model cannot represent the . NOT operator it is also not able to represent the
XORoperator. Thus, a more flexible model is required that, as it turns out, needs to
.
at least be able to “weight” the inputs with individual and adaptable factors.


================================================================================
PAGE 477
================================================================================

17.2 TheRosenblattPerceptron 463
Things to Remember about the McCulloch-Pitts (MCP)model: (cid:3)
• The MCP (or TLU) model is the simplest possible model motivated by the
biological neuron.
• The input data have the values 1 (True)or0 (False) and are aggregated
(summed up); ...
• ...if this value is larger than a threshold value, the output is 1 (or True);
otherwise, it is 0 (or False).
• The model can only make the Boolean decisions “.AND”and “.OR”while
operations “.NOT”and “.XOR”cannot be represented by the model.
17.2 The Rosenblatt Perceptron
After the pioneering development of the McCulloch-Pitts model, it took another
15 years until the perceptron as an enhancement of the MCP model was developed
by the psychologist Frank Rosenblatt: in 1957, he published the report “The
Perceptron: a perceiving and recognizing automaton,” whose aim he specified as
to “ [...] formulate a brain analogue useful in analysis ” 7[]. In this publication,
the word “perceptron” originally denoted an interconnected network structure and
not, as it is typically understood today, the individual unit. The first hardware
implementation was the Mark I perceptron, which was assembled in 1957 at Cornell
Aeronautical Laboratory in Bufallo, New York. It was a pure hardware realization of
the perceptron “network” where weights were represented as potentiometers, which
were adjusted by electro-motors. A short overview about the work of Rosenblatt can
be found in [5].
The name perceptron was derived from the fact that the inputs consisted of
20×20cadmium sulfide photo cells for “perceiving” a 400 pixel image; the output
.
consisted of signal lights that could be activated when a threshold—similar to
the MCP model—was exceeded. Activation was mutually exclusive such that two
signals would not be activated at the same time. Further details can be found in the
original publication [7].
In the following, we introduce one of such units and will subsequently denote
this as a perceptron; if more than one of these units are combined, then we will
denote this as a network of perceptrons.
17.2.1 The Mathematical Model of the Rosenblatt Perceptron
The first main difference in the MCP model is that the inputs . x are weighted with
individual values. The second difference is that the input values can take real values
while for the MCP they could only be Boolean, i.e., 0 or 1. Additionally, a bias
acting as offset for the weighted and aggregated inputs was introduced. With this


================================================================================
PAGE 478
================================================================================

464 17 FromthePerceptrontoArtificialNeuralNetworks
the transfer function takes the shape of a linear combination,
(cid:2)n
a(x)=b+ w x , (17.7)
. j j
j=1
where the w are variable weights and b is the bias. This equation can be
. j
reformulated if the bias is also interpreted as weight such that the new input and
weight vector looks
(cid:4) (cid:5) (cid:4) (cid:5)
. x˜ = 1,x 1 , ···x n T and w˜ = b,w 1 , ···w n T . (17.8)
With this, the transfer function can be rewritten using the scalar product between
two vectors, which is computationally efficient as already mentioned by Rosenblatt:
a(x)=w˜T·x˜. (17.9)
.
The activation function is again the unit step function with the threshold value θ (as
.
in Eq.17.3),
(cid:3)
1 IF a ≥θ
ϕ(a)= . (17.10)
.
0ELSE,
A schematic representation of a perceptron with three inputs is depicted in
Fig.17.3.
Note that b does not always occur in the above equation, as it can be “brought to
the other side” of Eq.17.10, and then it just contributes to the threshold (alternative,
the threshold can also be included in the bias). The mathematical model formulation
contains the original MCP model as a special case, as can be seen by setting . θ = 0
and all w =1:
. i
Fig. 17.3 Schematic of the
Rosenblatt perceptron with
three adjustable weights
.w1,w2and.w 3and an
adjustable bias b..θ is the
threshold parameter of the
activation function


================================================================================
PAGE 479
================================================================================

17.2 TheRosenblattPerceptron 465
⎡ ⎤
1
(cid:4) (cid:5) ⎢ ⎢x 1 ⎥ ⎥ (cid:2)n
. a(x)= 0,1, ···,1 ·⎢ ⎣ . . ⎥ ⎦ = x i . (17.11)
. i=1
x
n
This is the formulation for the transfer function of the MCP model; the activation
function is the same for both models.
Python Code for the Perceptron Model
The above can be easily implemented in Python, if we assume that the inputs
and weights are all numpy arrays. For this, the code of the perceptron model is
shown in Python Listing 17.1. The function predict makes use of the fact that the
PythonListing17.1 Implementation of the Rosenblatt perceptron for making a prediction. Here,
the prediction will be a value of either 0 or 1. However, it is also possible to use, e.g., . −1 and . + 1
(see Sect.17.2.5 for an explanation)
aggregation function can be computed as a scalar product between x and w.Inthe
. .
code this is done using Numpy’s function dot which takes two vectors (or rather:
1D Numpy array ’s) as arguments. The example usage is for two inputs; for more
inputs only the number of elements in the weights and input vectors need to be
adapted.
Example 17.2 (Prediction with the perceptron model for two inputs) (cid:2)
Assume a perceptron with two inputs and a threshold of θ =1. Furthermore,
.
given are the weights w = 1.5and w = 2together with the bias b = 0.5.
. 1 . 2 .
We can then make predictions, e.g., for the input vector x =[1,1]:
.
(continued)


================================================================================
PAGE 480
================================================================================

466 17 FromthePerceptrontoArtificialNeuralNetworks
x contains a single data record with two features. For several data records,
x has to be a two-dimensional array with features in columns and records in
rows.
17.2.2 Again: Boolean Decision Problems
As the MCP model is contained as a special case of the Rosenblatt perceptron, this
enhanced model is still able to represent the ORand ANDoperations. We will now
. .
take a look, if the NOTand XORoperators can be represented:
. .
Logical NOT Operation
Starting with the definition of the NOToperator
.
x =0 ⇒ y =¬x =1 (17.12)
. .
x = 1 ⇒ y =¬x =0 (17.13)
and the transfer function for a perceptron with a single input x
a(x)=b+wx (17.14)
.
we are looking for the θ that fulfills the inequality
.
a(x)=b+wx ≥θ (17.15)
.
Forx =0this inequality should be True, i.e., it isb≥θ. As we have two equations
. .
(or rather inequalities) and three unknown variables, θ, b and w, we need to fix at
.
least one of them, and we choose the threshold as θ = 0.Forx = 1the inequality
. .
Eq.17.15 should not be fulfilled and it reads
b+w (cid:2)0 (17.16)
. .
⇔ b + w <0 (17.17)
.
⇔ w <− b. (17.18)


================================================================================
PAGE 481
================================================================================

17.2 TheRosenblattPerceptron 467
Table 17.3 Representation of the logical NOT by a perceptron model with one input. The
parameters are chosen as.θ =0,.b =0 and.w =−1. The two output columns show the perceptron
output and true output
Input Data Processing Output
.x1 .a(x) = b +wx .a ≥θ ? .y = ϕ(a) . ¬ x
0 0 .0 ≥ 0 ? 1 1
1 . − 1 . −1 ≥0 ? 0 0
Fig. 17.4 Visualization of response of the perceptron for two different parameter sets. The blue
and red color shades show the value of the transfer function .a = b+x1w1 +x2w2, the dashed
line is the “decision boundary” that separates points with values below the threshold .θ from those
that have values above the threshold. The markers show four particular points; blue filled markers
denote those input values .x1,x2 for which the perceptron would be activated (.y = 1). (a) .b =
0,w1 =1.5,w2 =2,θ =1.(b).b =1,w1 =1,w2 =−2,θ =0.5
Choosing b = 0, it follows that the weight w should be negative, e.g., w = −1.
. .
With these parameters we get the response as summarized in Table 17.3.
Thus, the perceptron is able to represent the Boolean operation NOT—and
.
therefore potentially also to learn such a response (even though so far, it is not
clear how “learning” is done for such models).
Logical XOR Operation
The mathematical proof that the XOR operation is beyond the capability of
.
the perceptron is left as an exercise. Instead of a formal proof we will use a
geometrical interpretation in Sect.17.2.3 to explain why the XORoperation cannot
.
be represented.
17.2.3 Geometric Interpretation
The mathematical model of the perceptron for two inputs can be visualized and
interpreted based on geometrical relation. Figure 17.4 shows as a two-dimensional
contour plot the values of the transfer function a as a function of x and x for two
. 1 . 2
particular parameter sets b,w ,w , and θ.
. 1 2 .
If the inequality of the activation function is further restricted to an equality, i.e.,


================================================================================
PAGE 482
================================================================================

468 17 FromthePerceptrontoArtificialNeuralNetworks
Fig. 17.5 If the perceptron
would be able to represent the
logical.XORoperation, then
only the points shown as dark
markers would have output
values of .y=1as shown.
However, there exists no
straight line that could
completely separate the dark
from the light markers
(cid:3)
1 IF a =θ
ϕ(a)= (17.19)
.
0ELSE,
and we only consider the case a =θ then the set of equations can be simplified to
.
w x +w x +b=θ (17.20)
. 1 1 2 2 .
w θ − b
⇒ x =− 1 x + . (17.21)
2 1
w w
2 2
This is the equation for a line where the slope is−w /w and the offset or intercept
. 1 2
is (θ − b)/w . It is called decision boundary as it separates the regions where
. 2
the neuron fires (ϕ = 1) from those where it doesn’t fire (ϕ = 0) (see Chap.14
. .
where the notion of the decision boundary was introduced in detail). In analogy,
with three (or more) inputs, the separating line turns into a separating plane (or
decision hyperplane). The decision boundary is shown as dashed line in Fig.17.4.
The visualization of a(x ,x ) also reveals that a is a linear equation. This has
. 1 2
implications for the type of problems that can be represented by the perceptron
model: only linearly separable data can be learned and properly represented.
What does that imply for the XORproblem? To understand this, we take a look
.
at the left plot of Fig.17.4. There, only the marker at x = x = 0 has a value of
. 1 2
y = 0, all other points indicate a value of y = 1. In terms of logical operations
. .
this would represent theORoperation. From this we also can directly see why there
.
is some freedom in choosing the parameters as there exist a multitude of different
lines, all of which separate the three blue markers from the white marker. The XOR
.
operator has y =1if and only if there is exactly one of the input value that has the
.
value 1, i.e., only x = 1 and x = 0 and x = 0and x = 1 result in the output
. 1 . 2 . 1 . 2
y =1. Figure 17.5 visualizes this configuration.
.
If the perceptron would be able to represent (and eventually “learn”) the XOR
.
Boolean decision, then there must exist a straight line that separates the points with
y =1from those with y =0. As the figure shows, this is not possible.
. .


================================================================================
PAGE 483
================================================================================

17.2 TheRosenblattPerceptron 469
Even though, the perceptron is more powerful than the MCP model, it is still not
able to make an “either-or” decision, and it is limited to classification problems of
linearly separable data.
Historical Comments (cid:3)
In the original publication by ROSENBLATT [8] from 1958 most of the used
notions were named in analogy or motivated by the (human) eye and our
ability to detect visual patterns:
• What we today denote as input nodes was originally named sensory units
(S-points), e.g., that of a retina which ...
• ...can be excitatory or inhibitory depending on if they increase or reduce
the resulting signal—in our model this is represented by the sign of the
respective weight.
• The signals are transmitted to the association cells (A-units)—our com-
bined transfer/activation unit.
• The response units (R-points) are the output node(s), activated once the
threshold is overcome.
Already ROSENBLATT [7,8] used as variable for the threshold the Greek letter
θ, which is why this is still the most common choice.
.
17.2.4 Supervised Learning with the “Perceptron Learning Rule”
As opposed to the MCP neuron, the concept of the Rosenblatt perceptron includes
a feedback loop, where through the so-called perceptron learning rule the error is
back propagated, allowing the weights and bias to be automatically adjusted. The
“regular” evaluation of the perceptron, i.e., computing ypred = ϕ(x), is the f eed
.
forward step. Figure 17.6 shows the forward calculation direction and the backward
direction where the error is “propagated” from the right to the left.
Back-propagation is a crucial component of any deep learning (DL) network and
will then include the calculation of gradients, but we will come to that later. These
are two notions also used in the deep learning networks introduced in one of the
following chapters.


================================================================================
PAGE 484
================================================================================

470 17 FromthePerceptrontoArtificialNeuralNetworks
Fig. 17.6 The feed-forward
consists of evaluating the
transfer function and the
threshold for given weights,
bias, and input values. During
back propagation, the weights
are automatically adjusted
based on the output of the
perceptron
Binary Classification (cid:3)
The Rosenblatt perceptron and the MCP model are able to solve classification
problems: based on some input data, they can make a decision, and their
output data are of categorical type. It is a binary classification as there are
only two possible outcomes (0 or 1), which, however, can be interpreted, e.g.,
as “the sample shows failure” or “the sample is intact.”
The perceptron learning rule is a simple approach through which by comparing
the desired output with the current output the weights are iteratively changed by
small amounts and in proportion to that difference. How strong the influence of the
difference is on the updated values is controlled by a scalar factor, the learning rate,
which we denote in the following by the Greek letter η. If the learning rate η is
. .
0, the weights and bias should not be changed; equally, if the current output value
is already the desired output value, the weights and bias should not be changed.
Even though it will turn out that this learning rule is unsuitable for a robust learning
strategy, the Rosenblatt perceptron is the first attempt of supervised learning in the
context of early neuronal networks.
Perceptron Learning Rule
Assume that a number of training records in the form of the input features xtrain =
.
{xtrain,...,xtrain} and the response ytrain are given, and the perceptron should be
1 n .
trained to be able to make predictions. Then, for the training the algorithmic steps
shown in Algorithm 17.1 need to be performed.
Algorithm 17.1: Perceptron Learning Rule (for 1 Training Epoch) (cid:4)
1. Initialization:
a. initialize the weights w1,...,wn and bias b with zero
b. choose a suitable learning rate η ∈ (0, 1]
c. prepare a set of data records (e.g., rows from the data matrix)


================================================================================
PAGE 485
================================================================================

17.2 The Rosenblatt Perceptron 471
2. Feed-forward step: Choose a new data record and make a prediction for xtrain
with the current values of the weights and bias:
a. compute the weighted sum a(xtrain; w,b)
b. evaluate the activation function: ypred = ϕ(a;θ)
3. Backward propagation: use difference between predicted and true output to
compute the update increments for weights and bias
Δw = (ytrain − ypred) xtrain and Δb = ytrain − ypred (17.22)
Update the weights and bias to obtain the new values
w ← w + ηΔw and b ← b + ηΔb (17.23)
4. GO TO step 2 and repeat with a new record (xtrain,ytrain) until all data records
were used exactly once.
Epoch
Any full iteration, consisting of both forward steps and weight/bias updates for all
data records, is called an epoch. A complete training process usually consists of
several epochs where each new epoch starts with the updated weights and bias
values from the previous epoch. Typically, for a very small learning rate, more
epochs are required than for larger learning rates. Often, it is the goal to use a
learning rate that is as large as possible so that only few epochs are required for
training, which makes the training process fast. At the same, if the learning rate is
too high, numerical instabilities will occur and the training process will not be able
to converge.
Termination Criterion
The training is terminated either if an error is sufficiently small (see “error
measures” below) or if a maximum number of training epochs is reached. The latter
is helpful for avoiding that the algorithms encounter a situation where the error
cannot be reduced sufficiently and where one would end up in an “infinite loop.”
An obvious choice is termination if the error is zero. However, it can be formally
proven that the perceptron learning algorithm only converges if the dataset consists
of two linearly separable classes. Therefore, a more generous minimal error is often
more useful and should be chosen based on the dataset and the application.


================================================================================
PAGE 486
================================================================================

472 17 FromthePerceptrontoArtificialNeuralNetworks
The Peculiarity of the Perceptron Learning Rule (cid:3)
The perceptron learning rule is somewhat unusual as it does an individual
forward/update step for each of the training data records. Therefore, in
particular for small datasets, the order of the data records matters. This
is an important difference to, e.g., the ADALINE algorithm introduced in
Sect.17.3: there, the weights and bias update is determined based on all data
records at once.
Error Measures
How can the error be measured for the perceptron learning rule? Assuming that the
number of data records is M , thea bsolute error (AE) for the classification can be
easily computed as the total number of misclassified data,
(cid:2)M (cid:12) (cid:12)
(cid:12) (cid:12)
absoluteclassificationerrorAE= (cid:12)ytrue −ypred (cid:12) , (17.24)
. i i
i=1
where ytrue is the true class label of data record number i and ypred is corre-
. i . i
sponding predicted class label. This error measure can only take integer values that
represent the number of misclassified data and can therefore be easily interpreted.
Another useful error measure, that we already encountered in Sect.11.7, is the mean
absolute error (MAE), i.e., the number of mislabeled data divided by the number of
samples
(cid:2)M (cid:12) (cid:12)
1 (cid:12) (cid:12)
meanabs.classificationerrorMAE= (cid:12)ytrue −ypred (cid:12) . (17.25)
. M i i
i=1
This allows to specify an error tolerance that is independent of the size of the
dataset, which can be used to improve the termination criterion of the algorithm. For
example, one could weaken the originally very strict requirement that all samples
need to be correctly predicted and postulate that not more than5%of the data should
.
be misclassified.
A comment on “Learning” (cid:3)
As we saw in the previous chapters, learning typically requires a number
of examples from which the machine learning (ML) model is supposed to
generalize to new data. This has already been formulated by ROSENBLATT
[8], who in his early publications was wondering about the amount of
(continued)


================================================================================
PAGE 487
================================================================================

17.2 TheRosenblattPerceptron 473
information needed to recognize a geometrical shape: the strength of a good
and predictive model is not to learn all possible sizes and shapes of a triangle
but rather to learn the most important characteristics from a finite number of
examples.
Python Implementation of the Perceptron
A numerical implementation that uses the absolute classification error as a termina-
tion criterion can be found in Python Listing 17.2. The function train_1_epoch
Python Listing 17.2 Implementation of the perceptron learning rule. The code reuses the
function predict from Python Listing 17.1. Here, the training is stopped if the error is zero. Note
that contrary to all best practices in programming, we omitted docstrings, comments, type hints,
and “sanity checks” to ensure that the code stays compact


================================================================================
PAGE 488
================================================================================

474 17 FromthePerceptrontoArtificialNeuralNetworks
performs a complete training step consisting of the prediction and the back-
propagation for all samples of the dataset and returns the updated weights and bias.
The function train does the training for n_epochs . X_train is a 2D array and
y_train is a 1D array; X_train.shape returns the number of rows (number of
samples) and columns (number of features) of the 2D array. For each epoch, the
updated weights and bias values are computed and, based on that, the AE calculated.
For this, numpy’s abs functions compute the absolute value element-wise for each
entry of the vector y_pred - y_train . These values are then summed up. If the
error is zero, then the training is stopped and the break statement quits the for
loop. Note that changing the threshold can be counteracted by changing the bias.
This is why by fixing the bias to zero, one can say, that the algorithm is also learning
the threshold.
To check if the code works properly, a first “sanity check” is to predict the
training data: for X_train we already know that the prediction should be y_train .
Convergence Theorem
Taking a look at Eq.17.22, one observes that once a solution whas been found that
.
correctly predicts all training data, then w will never be changed again. A formal
.
proof is given by [9] and shortly afterward by [4] including a formal discussion of
the limitations of the Rosenblatt perceptron.
17.2.5 Geometric Interpretation of the Perceptron Training
Interpretation of the Weights Vector
Point of departure for this part is the mathematical representation of lines. Already
above we saw that the decision boundary for the case of a zero threshold is given by
w·x+b=0, (17.26)
.
which is the equation of a line (or hyperplane ifxis a position vector in an arbitrary
.
dimensional space). A non-zero threshold value can be included in an adjusted value
of b. From this form the normal of the line can be easily inferred: two vectorsaand
.
c are normal if and only if the scalar product a ·c = 0 (cf. Appendix A). To find
. .
a normal vector for a given vector x we therefore have to find a vector a such that
. .
a·x = 0. Neglecting b in Eq.17.26 we see that such a vector would be w.Thisis
. .
still true if we add b, as this only acts as an offset of the line but does not change
the direction. Therefore, w is a vector that is perpendicular to the line given by
.
Eq.17.26.
Choice of y Values
Sometimes the two classes are not chosen to be represented by the values 0 and 1 but
rather by −1and 1. Both variants work equally well and result in identical training
.
behavior if the learning rate is scaled accordingly. The former convention is rather
motivated by Boolean decision problems where 0 and 1 have a clear correspondence


================================================================================
PAGE 489
================================================================================

17.2 TheRosenblattPerceptron 475
to the Boolean values of False and True. Additionally, for problems with more
classes it is easy to simply count them starting from 0. The second convention
is motivated by the fact that for the geometrical interpretation, the normal to the
decision boundary, b, points into the area, where correctly classified y values are
.
positive. Those values in the area pointed at by the vector −b would be negative.
.
For the rest of this section, we follow the second convention.
Geometry of the Perceptron Update
How is this important for understanding how the perceptron learning rule works?
Figure 17.7a shows a hypothetical configuration with a given training datasetD .
.
According to Algorithm 17.1, a point number i (or equivalently the data record i)is
chosen, and the corresponding position vector is x(i). The current weights and bias
.
result in the shown decision boundary and the normal vector w. The update from
.
the above algorithm reads for data record number i
(cid:13) (cid:14)
wnew :=wold+η y(i)−yˆ(i) x(i) , (17.27)
.
where we used the short form y(i) := ytrue and yˆ(i) := ypred to avoid too lengthy
. .
superscripts.
The point i is misclassified with yˆ(i) =+1and the term in parenthesis evaluates
.
to 1−(−1) = 2 > 0. With λ = 2η the new weight vector is then wold+λx(i)—
. . .
addition of two vectors as shown in Fig.17.7b. At the same time, the value of the bias
is incremented byλ. This results in a rotation of the decision boundary in clockwise
.
direction, cf. Fig.17.7c. If the misclassification of a triangular maker would have
been considered, then the factor λwould be negative, which results in a rotation of
.
the decision line in the opposite, counter clockwise direction. If a point was correctly
classified, then the decision boundary does not change. A perceptron model without
a bias would have a hyper-plane that always goes through the origin.
Fig. 17.7 Illustration of the geometrical relations implied by the perceptron learning rule. The
decision boundary is given by.w and b,cf.(a). There, three points are misclassified: two rectangular
and a triangular point. An arbitrary point .x(i)is chosen that is then used for the update step. The
new weight vector in (b) is the sum of the current .w and a vector along the direction of .x(i): its
length is proportional to the training rate as indicated by the factor .λ . Additionally, the sign of the
line direction is determined by the difference .ytrue−ypred


================================================================================
PAGE 490
================================================================================

476 17 FromthePerceptrontoArtificialNeuralNetworks
17.3 The ADALINE Model
The Adaptive Linear Neuron (ADALINE) (also sometimes called “Adaptive Pattern
Classification Machine”) was developed in the 1960s by BERNARD WIDROW and
TED HOFF [12]. It is also based on the idea of the MCP neuron and the Rosenblatt
perceptron. However, the novelty of ADALINE is an enhanced training strategy as
compared to the perceptron. Below, we will see that the training strategy is in fact
an efficient gradient descent method for finding optimal weights.
17.3.1 ADALINE Training Strategy
The key difference between the Rosenblatt perceptron and the ADALINE model is
the way how the weights are adjusted: while the Rosenblatt perceptron used the
output y to determine the training increments for bias and weights, the ADALINE
model directly uses the agglomerated and weighted input valuesw·x+b. This can
.
be best seen in the schematic shown in Fig.17.8.
There, after summation we have now an activation function, which here is
just taken as the identity function ϕ(x) = x. The output is squeezed through a
.
“quantizer”3 —nothing but a threshold function, which, however, gives as output
either −1or +1(see below for the motivation). The backward direction computes
. .
the error based on the output of the activation function according to the so-called
Widrow & Hoff learning rule„ which consists of the following increments
Δw =(ytrain−a(xtrain;w,b))xtrain and Δb=(ytrain−a(xtrain;w,b)).
.
(17.28)
a(xtrain;w,b))is the output of the (identity) activation functions for given weights
.
and bias. Update of the weights and bias is then done using the learning rate η by
.
the update step
w ←w+ηΔw and b←b+ηΔb (17.29)
.
where these equations correspond to the perceptron’s Eqs.17.22 and 17.23. Addi-
tionally, the update is not done incrementally for each record or sample of the
dataset. Instead, the two increments are computed for the whole dataset such that
the update is done fo all records simultaneously. Everything else stays the same as
for the perceptron learning rule. A summary of the whole algorithm can be seen in
Algorithm 17.2.
3 In signal processing, “quantization” denotes the process of mapping continuous data to discrete
data.


================================================================================
PAGE 491
================================================================================

17.3 TheADALINEModel 477
Fig. 17.8 Schematic of the ADALINE model: The forward direction implies evaluating the
transfer, activation, and “quantizer” function for given weights, bias, and input values. For the
backward direction, the error from the activation function (here: the identity function) is used to
automatically adjust weights and bias
Algorithm 17.2: ADALINE Training with the Widrow & Hoff Learning Rule (cid:4)
1. Initialization: (training data must have ytrain ∈{−1;1})
a. initialize the weights w1,...,wn and bias b with zero
b. choose a suitable learning rate η ∈ (0, 1]
c. define a maximum number of epochs (=iterations) as termination criterion;
define an error tolerance as convergence criterion
2. Forward step:Makes imultaneous predictions for each record of the dataset with
current weights and bias:
a. compute the weighted sum a(xtrain; w,b)
b. evaluate the activation function (here: identity function): this gives again the
weighted sum a
c. evaluate the quantizer function: ypred = ϕ(a)
3. Backward direction: use difference between true output, ytrain, and the output
values of the activation function, a, to compute update increments for weights
and bias
Δy = ytrain − a(xtrain; w,b) (17.30)
Δw = Δy xtrain and Δb = Δy (17.31)
Update the weights and bias to obtain the new values
w ← w + ηΔw and b ← b + ηΔb (17.32)
4. GO TO step 2 and repeat until the norm of the error,‖Δy‖, is small enough or
until the maximum number of iterations is reached.


================================================================================
PAGE 492
================================================================================

478 17 FromthePerceptrontoArtificialNeuralNetworks
17.3.2 Example: Comparison Between the Perceptron and the
ADALINE Model
We now take a look at how well the perceptron and the ADALINE perform for a
dataset that consists of two data classes, which have a small amount of overlap and
are therefore, strictly speaking, not linearly separable. This was motivated by the
fact that often in many real problems, some of the assumptions used for the models
are slightly violated, but obviously a good model has a certain degree of tolerance.
In Fig.17.9, we show the training results for the perceptron and the ADALINE
model. All parameters, i.e., the training data itself, learning rate, initial values
for bias and weights, are chosen identically. There, it becomes obvious that the
ADALINE is numerically much more stable, which is the main side effect of using
the output of the (continuous) activation function and not the output of the step
functions.
For the perceptron, the error keeps fluctuating and does not reach the same small
error as the ADALINE. The decision boundary keeps jumping and rotating, which is
also not fundamentally changing if the learning rate is further decreased. Also, the
ADALINE model is not able to perfectly classify all the data, but this is not surprising
Fig. 17.9 Comparison of the training results for the Perceptron and the ADALINE model. Top
panel: final classification state, middle: error, bottom panel: evolution of weights and bias. The
perceptron oscillates and only reaches a larger error than the ADALINE model. The ADALINE
model is also numerically more stable. (a) Training of the Perceptron. (b) Training of the
ADALINE model (additionally, the MSE error is shown in the middle panel)


================================================================================
PAGE 493
================================================================================

17.3 TheADALINEModel 479
as it is a linear model. Overall, it looks numerically much more well-behaved and
converges to a set of weights and bias that result in the minimum possible error.
Is it surprising or problematic that the weights and bias values of the two models
have different magnitudes? For almost all problems, there is no unique solution to
the problem of finding a decision boundary. Thus, there exists a potentially large
range of parameters that work equally well. Additionally, a mathematical line is
fully determined by two parameters, whereas the two studied models contain two
weight parameters and the bias and therefore have one degree of freedom more than
needed.
17.3.3 The Relation to Gradient Descent Optimization
The ADALINE training uses the linear activation function a for computing an
error. This has the advantage that a is differential, and that we can define a cost
function J(w) that can then be minimized. Here and in the remainder of this
.
paragraph, we assume that the vector w˜ contains both the weights and the bias,
.
w˜ := [w ,...,w ,b]T as introduced in Eq.17.8, which also required a new
. 1 n
definition of the input vector as x˜ := [1,x,...,x ]T. Then we define the cost
. 1 n
function as the sum of the squared errors of all M data records,
(cid:2)M
1
J(w˜)= (y −a(x˜ ;w˜))2. (17.33)
. i i
2
i=1
x is the i-th data record, and the factor of one half is here to simplify the
. i
formulation after taking the derivative below and does not have any further
influence. Minimizing the cost function w.r.t the weight components then yields the
values of the trained weights. In the following, we use the simple gradient descent
method (cf. Chap.12 for further details). There, we should follow the direction
of the steepest descent, which is −∇J(w). Rewriting this component-wise lets us
.
explicitly perform the differentiation along the j-th component:
(cid:15) (cid:16)
(cid:2)M
∂J 1 ∂
= (y −a(x˜ ;w˜))2 (17.34)
. ∂w˜ 2∂w˜ i i
j j i=1
For taking the partial derivative of the lengthy expression, we note that the derivative
of a sum equals the sum of the derivatives. Therefore, we can move the partial
differentiation into the summation and apply it to the (...)2 term using the chain
.
rule. This results in
(cid:17) (cid:18)
∂J (cid:2)M ∂a(x˜ ;w˜)
= (y −a(x˜ ;w˜)) − i . (17.35)
. ∂w˜ i i ∂w˜
j i=1 j


================================================================================
PAGE 494
================================================================================

480 17 FromthePerceptrontoArtificialNeuralNetworks
For taking the partial derivative of a with(cid:19) respect to any
.
w ˜
j
, recall that with the
extended notation of x˜ and w˜ it is a = w˜ x˜ (hint: recall the definition of
. . . i j j ij
the data matrix, Sect.3.3.3). Therefore, there is only one element of the sum that
depends on w˜ , which would be the j-the element. Then, it is ∂a /∂w˜ = x˜ with
. j . i j ij
which follows that
(cid:2)M (cid:2)M
∂J
=− (y −a(x ;w˜))x˜ =− (y −x˜ ·w˜))x˜ (17.36)
. ∂w˜ i i ij i i ij
j i=1 i=1
This equation gives the sensitivity of the cost function with respect to the j-
th weight. Writing this in compact vectorial form for all weights eliminates the
occurrence of the index j and reads:
(cid:2)M
∂J
=− (y −x˜ ·w˜))x˜ . (17.37)
. ∂w˜ i i i
i=1
This is, by construction, a vector that points into the direction of the steepest descent.
For the iterative update of the search direction, the gradient descent method then
takes a small step along this new search direction, resulting in the weights increment
(cid:2)M
Δw˜ =−η∇J(w˜)=η (y −x˜ ·w˜))x˜ (17.38)
. i i i.
i=1
(cid:2)M
∂J
⇔ Δ w˜ =−η = η (y − x˜ · w˜)) x˜ . (17.39)
j ∂ w˜ i i ij
j i=1
Note that for comparison with the delta rule of the ADALINE model, we have to
convert the extended notation w˜ = w[ ,...,w ,b]T back to w and θ. This is the
. 1 n . .
same update formula that is used from the ADALINE model above which also shows
why ADALINE is indeed based on the gradient descent optimization method.
We conclude this derivation by mentioning that ∂J/∂w is not necessarily the
.
direction of the minimum. However, for problems that exhibit only a global
minimum and no local minima, the gradient descent method can be shown to always
converge to the global minimum. For cost functions that have local minima, this
guarantee is only restricted to the vicinity of the global minimum.
17.3.4 Limitations of the Perceptron and the ADALINE Model
We have seen that both models—even the simplistic perceptron—work very well
for linearly separable data. This gave rise to great hope that with such models truly
artificial intelligent machines could be created. This hope was vastly destroyed in
1969 by Minsky and Papert [4], who in their book formally proved that no linear,


================================================================================
PAGE 495
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 481
single layer model is able to learn the simple logical XOR operator. This limitation
(among others) was so severe, that after the rapid progress and enthusiasm of the
1950s and 1960s, further developments were dramatically reduced until the 1980s,
a period of which is sometimes called AI winter.
17.4 Increasing the Complexity: Assemblies of Neurons
From an early point in time on, it was clear that the perceptron and the ADALINE
model were very limited in terms of learnable the degrees of complexity. In fact,
already the very early units were combined to form assemblies of perceptrons to
increase the complexity. Again, the motivation came from the biological nervous
system. In the following, we first analyze networks of neurons and discuss what
kind of functions can be represented by them. In a next step, we then turn to the
investigation of aspects related to the training of such assemblies.
17.4.1 A First Network of Two Computational Neurons
The obvious approach is to combine multiple perceptrons in order to increase the
complexity of the model and—hopefully—thereby also to increase the complexity
of the predictable phenomena. In Fig.17.10a, two neurons with n input values/fea-
tures are combined.
Fig. 17.10 Combination of two perceptrons: (a) The output of two perceptrons with n input nodes
will be combined in the “??”-node. This gives then the output y, a real value or a categorical value
(class). (b) is a more compact notation with input nodes that are “reused” for each neuron; their
output is summed and tested against a threshold value (the large orange node), yielding the output
y that is used for classification. For a multi-class classification problem, more output nodes would
be used; for regression problems, there would be no thresholding component in the output node


================================================================================
PAGE 496
================================================================================

482 17 FromthePerceptrontoArtificialNeuralNetworks
Example with 2 Perceptrons
The output of each of the two perceptrons is fed as input into an output unit that—
depending on whether a regression problem, a binary or multi-class classification
problem is to be solved—has different functionality.
We turn now to a concrete example: a network of two perceptrons, each of which
has two inputs, to be used for a classification problem. This is shown in Fig.17.10b
where we also changed to a commonly used depiction: the two identical input nodes
of the perceptrons are merged, and from every input node connections go to both
units; their output is not depicted as an individual node.4 We assume that each of
the three shown units consists of a summation and activation function of the form
a(w·x;θ)where θ is again a threshold (the notation with the semicolon indicates
. .
thatθ is a constant parameter while everything before the semicolon is considered a
.
variable. Sometimes, the parameters are omitted for brevity). We can then start from
the right of Fig.17.10b to recursively derive the formulation for y:
y =a (y w +y w ;θ ) (17.40)
. 3 1 5 2 6 3
where
.
y
1
and
.
y
2
are the output values from the two TLUs. They can be computed as
y =a (b +x w +x w ;θ ) (17.41)
. 1 1 1 1 1 2 2 1 .
y = a (b + x w +x w ;θ ) . (17.42)
2 2 2 1 3 2 4 2
The a are the transfer and activation functions with the threshold values θ . The
. i . i
above equations are already sufficient to perform predictions if the weights and the
bias values are given. In Example 17.3, a step-by-step calculation is performed for
the case of two perceptrons.
Example 17.3 (Prediction with two Perceptrons) (cid:2)
The following figure depicts a simple network of two combined perceptrons
and an output unit:
(continued)
4 This is a convention that exists mainly for historical reasons. Additionally, in the context of
connected neurons, only one node is needed for storing the value, e.g., in case that the output of
the one is the input of the other.


================================================================================
PAGE 497
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 483
It require altogether six weights and two bias values, which are given as:
w =−2.0, w =2.0, b =0.5
1 2 1
. w 3 =1.5, w 4 =−2.1, b 2 =0
w =−1.8, w =−2.2
5 6
together with the threshold values θ = −1 and θ = −1 for the two
. 1 . 2
perceptrons and θ = −2.5 for the output unit (which we define to have
. 3
values of either 0 or 1). What is the output y for a given input ofx =0.8and
. 1
x =0.3?
. 2
We start by evaluating Eq.17.41:
y =a (0.5+0.8·(−2.0)+0.3·2.0)=a (−0.5)
. 1 1 1
To compute the first activation function a (−0.5), we compare the function’s
. 1
argument with the threshold −0.5 ≥ θ = −1from which follows that y =
. 1 . 1
a (−0.5) = 1. In analogy, we find for Eq.17.42 that y = a (0.57) ≥ θ =
1 . 2 2 2
−1. From this follows that y = 1. Finally, we compute the overall output
. 2
from Eq.17.41 via a (−1.8 · 1 + (−1.8) · 1) and the comparison with θ
. 3 . 3
resulting iny =0.
.
Is this a network with 2 or 3 neurons? (cid:3)
The network in Example 17.3 seems to show altogether three neurons but we
are only talking about two—was there a mistake?
Well, even though in the figure, y is computed as the output of a perceptron
with the same mathematical form as the two first units, it is composed of
a distinct function. This will become more clear, e.g., in case of a multi-
class classification or regression problem where this unit will take a different
mathematical form. This is why this network is considered a “2-neuron
network.” We will also discuss this when we analyze the decision boundary
of such a network.


================================================================================
PAGE 498
================================================================================

484 17 FromthePerceptrontoArtificialNeuralNetworks
17.4.2 The Boolean XOR Decision Problem
Is this simple network able to perform better on the Boolean XOR problem than a
single neuron? Using the same network structure as in Example 17.3, we perform
computations with all possible permutations of 0s and 1s as input, as shown in
Table 17.4 (note that there we used the parameters given in the caption of the table).
Then, it is easy to show that the Boolean XORoperator can indeed be represented
.
by this network.
Note that the output of the two neurons is −1or +1, whereas the final output y
. .
will be either 0 or 1 as we wanted to represent the Boolean states False and True
(another hint that the output unit is different from the two neurons).
Why are two neurons able to solve the XOR problem and in which aspects is
.
their combination different from a single neuron? To understand this, we visualize
the function a depending on the input x and x as shown in Fig.17.11 (recall that
. 3 . 1 . 2
in Eq.17.40 the variables y and y depend on x and x and therefore y implicitly
. 1 . 2 . 1 . 2
depends on them as well).
The “decision-making” of the two individual TLUs shows the expected lin-
ear decision boundary separating regions of y (x ,x ) = −1 from those with
. 1 1 2
y (x ,x ) = +1 (y in analogy). These two regions are also called half-spaces
. 1 1 2 . 2
as they start at the decision boundary and from there continue infinitely. The output
node essentially adds weighted versions of y and y and then compares it with a
. 1 . 2
threshold value. In our case, the values of the weightsw andw are not too different
. 5 . 6
so that effectively y and y are averaged, as can be clearly seen in the bottom left
. 1 . 2
plot of Fig.17.11. The values between which the output node should choose are
expected to represent Boolean values. Therefore, we set it such that the values can
only be 0 or 1.
In conclusion, we observe that the composition of two perceptrons effectively
combines two decision boundaries, which allows to create a more complex resulting
decision boundary that is not restricted to linearly separable problems and—
finally—is able to solve the Boolean XOR decision problem. Do we also have to
.
show that the other Boolean decision problems (AND NOT and OR) can still be
. . .
Table 17.4 Representation of the logical XOR by two connected perceptrons. Parameters used
are .w 1 = 0.7,w2 =−1.1,b1 =1,w3 =−w4 =2,b2 =0,θ1 = 0.4,θ2 = 0.8,w5 =−w6 =
0.5,b3 = 0,θ3 =0.1. The two output columns show the model output as well as the true output
.x 1 ∨˙x2
Input Data Processing Output
.x1 .x2 .a1 .a1 ≥ θ1 ? .y1 .a2 .a2 ≥θ2 ? .y2 .a3 .a3 ≥ θ3 ? .y =ϕ(a3) .x 1 ∨˙x2
0 0 1.0 True 1 0.0 False -1 1.0 True 0 0
1 0 1.7 True 1 2.0 True 1 0.0 False 1 1
0 1 -0.1 False . − 1 . − 2.0 False -1 0.0 False 1 1
1 1 0.6 True 1 0.0 False -1 1.0 True 0 0


================================================================================
PAGE 499
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 485
Fig. 17.11 Results from an assembly of two perceptrons with parameters according to Exam-
ple 17.3. The weights, bias, and threshold values are chosen as in Table 17.4. The top left and top
right panels are the intermediate outputs of neuron 1 and 2, the bottom panel shows the results of
the output node. .y 1and .y 2are the output of perceptrons and are either . −1or . + 1. As the output y
should represent Boolean values, it was here defined to be either 0 (False)or1(True). In the
bottom left panel, the composition of the effect of the two perceptrons becomes directly visible in
the resultant decision boundaries
represented? The proof that this still holds is simple as we can set one of the weights
of the output unit to zero such that effectively only one unit is contained.
17.4.3 Python Implementation of the Two-Neuron-Assembly
In Python Listing 17.3, an implementation of the assembly of two neurons is shown,
which is intuitively understandable and matches the mathematical formulation.
There, we don’t even reuse functions from the previous sections.
Why not directly implement this for an arbitrary number of inputs or neurons?
Because such code would not be very flexible, e.g., if the number of neurons or
inputs changes, a substantial part of it needs to be rewritten. The purpose of this
code example is twofold: First of all, it shows the information flow very clearly and
thereby paves the way for more complex and general networks introduced in the
following sections. The second important aspect is that we can use these lines of


================================================================================
PAGE 500
================================================================================

486 17 FromthePerceptrontoArtificialNeuralNetworks
Python Listing 17.3 An implementation of the network of two neurons. Even though it is a very
simplistic implementation, it is able to handle a number of input records at the same time as can
be seen by the fact that x1 and x2 are both numpy array s
code to produce “reference solutions” to be used as validation of more complicated
networks.
Parameters used are those for solving the XOR problem from above. With the
.
given comments, the code should be mainly self explanatory. We note, that a1
in line 9 is again an array where each entry corresponds to one of the four data
record of the input x1 and x2 . y1 uses numpy’s where function that gives +1 for
each element that fulfills the condition (e.g. a1 >= theta1 ) and -1 otherwise. The
function where is able to do this for all elements of an array.
Words of advice (cid:2)
In case that the reader skipped the sections on vectorization in the chapters on
regression (Sect.12.4) we suggest to try to understands the “vector operations” in
Python Listing 17.3 (i.e., the way how the four data records in x1 and x2 are
handled). This will be needed for many of the subsequent implementations. Printing
the output of each line and adding more entries to x1 and x2 (=increasing the
number of data records) might also be helpful.


================================================================================
PAGE 501
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 487
17.4.4 A Fully Connected Network with Input, Output and a Single
Hidden Layer
Already a few years after ROSENBLATT published the perceptron [8] he also
published the concept of a multilayer perceptron (MLP), which contained an input
layer, a hidden layer, and a output layer [9] (“layers” denote arrays of similar
neurons or nodes, stacked on top of each other, as shown in Fig.17.12). The hidden
layer was initialized with random values for the weights. However, there was no
mechanism such that the hidden layer was able to adjust the weights. MLPs has
been used in many variations and could also be studied mathematically. They are
a predecessor of DL, and therefore, we will introduce them now in somewhat more
detail.
Starting from single units, we can assemble more complex structures in the same
spirit as when we assembled two perceptrons above. As the assemblies are now
getting more complex, also the notations become more and more involved. As this
can quickly become overwhelming (who likes dealing with variables with four or
five indices?!), we first take an intermediate step considering a network with only
three different types of “layers.” Figure 17.12 shows an example of such a network.
Fig. 17.12 Example of a neural network with one input layer (containing three input nodes), a
hidden layer (with four perceptrons), and an output layer (with two output nodes). On the right is
the symbolic representation that is used in the following and that is in detail explained in the text.
Note that the “bias input node,” usually indicated by the “1,” is not shown but implicitly assumed
to be always present. Two distinct sets of weights are present, the.w
i
(
j
1)and the.w
j
(2
k
)


================================================================================
PAGE 502
================================================================================

488 17 FromthePerceptrontoArtificialNeuralNetworks
There are two layers that are visible or accessible from the outside of the model:
the input layer containing nodes that store the input data x , and the output layer
. i
collecting data and returning them in form of the values y . In between is a hidden
. j
layer that is not visible from outside the model. All layers can have an arbitrary
number of nodes or neurons. Note that there is a fundamental difference between
the nodes of the input and the output layer: the output nodes perform computations,
while the nodes of the input layer only store data.
Words of advice (cid:2)
Naming conventions are often not standardized and rather based on “common
agreements” or habits. For example, a number of different names exist for the
network shown in Fig.17.12: sometimes it is called a two-layer network as there are
two layers that do actual computations the hidden and the output layer (or because
there are two “layers of weights”), but it really consists of three layers of nodes (the
input, hidden, and output layer). Sometimes the network is also called single layer
withhiddenunits or networkwithonehiddenlayer.
Fully Connected Layers
All nodes of the two neighboring layers are interconnected as shown in Fig.17.12:
each node on the origin layer (e.g. the input layer) is connected with all nodes of
the destination layer (e.g. the hidden layer). Layers with this kind of connection are
called fully connected layers. The two layers can have arbitrary numbers of nodes.
For example, if the first layer has n nodes and the second layer has m nodes, then
there are altogether n×mconnections between the two layers.
.
Notations for Weights
Each of these connections is associated with a weight, where the weights between
the first layer (the input layer) and the hidden layer are indicated by a superscript
(1), e.g., w (1) , and those between hidden and output layer are denoted as w (2) . To
. . ij . ij
uniquely indicate the start and the end node of the sets of weights, we will use the
following index notation:
Definition 17.1 Notation for Weights Each weight of the fully connected
layers is uniquely characterized by one superscript and two subscripts:
(continued)


================================================================================
PAGE 503
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 489
number of the origin layer
( )
number of the origin neuron number of the destination
or node neuron or node
For example, the connection from input layer (layer no. 1) to hidden layer (layer no.
2) has k = 1and the connection from the hidden to the output layer is k = 2.The
. .
(2)
subscripts inw indicate that the first node/feature of the starting layer is connected
. 1,2
to the second node or neuron of the receiving layer (starting and receiving are meant
in terms of the information flow); a starting node with the number 0 indicates that
(1)
this “weight” is in fact the bias, e.g., w denotes the bias of the first neuron in the
. 0,1
hidden layer.
Words of advice (cid:2)
The above introduced “index notation” should not be confused with a short notation
for partial derivatives where, e.g.,.fi,x ≡∂fi/∂xindicates the partial derivative w.r.t.
x of the i-the component of (the vector-valued).f .
Mathematical Formulation of the Network
We will now derive the governing equation for the first output node for the network
shown in Fig.17.12. For this we start from the output node and make our way
through the hidden layer, back to the input layer. For simplicity, we only assume a
single data record; the more general case of multiple data records that can be handled
simultaneously will be formulated below in Sect.17.4.5. We start our derivations
from the first outputy . The corresponding output nodes simply sum up all incoming
. 1
(2)
values together with a bias value b (again, the bias is not shown in Fig.17.12):
. 1
y =b (2)+w (2) ξ +w (2) ξ +w (2) ξ +w (2) ξ . (17.43)
. 1 1 11 1 21 2 31 3 41 4
There, ξ is the output of the i-th neuron of the hidden layer and therefore, strictly
. i
speaking, also should have the superscript(i). If the bias is again treated as a weight,
.
(2)
w , then Eq.17.43 can be written as a scalar product of two vectors,
. 01
y =ξ ·w (2) with ξ =[1,ξ ,...,ξ ]
. 1 1 1 4


================================================================================
PAGE 504
================================================================================

490 17 FromthePerceptrontoArtificialNeuralNetworks
and w (2) =[w (2) ,...,w ( 2) ]T . (17.44)
1 01 41
The scalar product ξ ·w (2) is shorter and looks more elegant than Eq.17.43 but is
. 1
also less explicit—in particular keep the “special” definition of ξ from Eq.17.44 in
.
mind and note that w
(2)
is the vector that contains the weights for the output node
. 1
(2) (2)
number 2. The second output is computed in analogy, exchanging w with w .
. 1 . 2
ξ is the output from neuron number i and can be computed by just following the
. i
same rule as we did already for the single Rosenblatt perceptron:
(cid:13) (cid:14)
ξ =ϕ x·w (1) with x =[1,x ,x ,x ]
. 1 1 1 1 2 3
and w (1) =[w (1) ,...,w (2)]T . (17.45)
1 01 41
As before, we treat the bias as 0-th weight and therefore again extend the vectors x
.
(1) (1)
and w with 1 and w , respectively. The outputs from the other neurons are then
. 1 . 01
computed in analogy, e.g.,
(cid:13) (cid:14)
ξ =ϕ x·w (1) . (17.46)
. 2 2 2
17.4.5 “Vectorized” Operations: The Equations for the Network
with a Hidden Layer
This can now be summarized for the network with three features and inputs, four
units contained in the hidden layer, and two output nodes, starting from the final
output of the network Additionally, for the computational efficiency, it is beneficial
to “vectorize” these steps. Vectorization implies that iterating over elements of
a vector or matrix by using for loops can be avoided. Instead, by performing
matrix–matrix or matrix–vector products, all calculations can be done virtually
simultaneously because Python’s numpy library or MATLAB® is able to strongly
optimize these operations.5 As compared to using regular for loops over individual
elements of arrays, this may result in a speed up of the computational time by factors
of 10 and even up to 1000 or more! Last but not least, for the current mathematical
problem, such a notation additionally looks compact and thereby helps to understand
the structure of the equations.
5 This is typically done by using pre-compiled and possibly strongly optimized code, which itself
is typically written in a low-level language such as C or Fortran. Vectorization does not imply
any parallel execution of code even though an already vectorized code often can easily be run on
multi-cores/multi-processors as well.


================================================================================
PAGE 505
================================================================================

17.4 IncreasingtheComplexity:AssembliesofNeurons 491
⎡ ⎤
(2) (2)
w w
⎢ 01 02⎥
(cid:4) (cid:5) (cid:4) (cid:5) ⎢ ⎢ w 1 (2 1 ) w 1 (2 2 )⎥ ⎥
. y = y 1 y 2 = 1 ξ 1 ξ 2 ξ 3 ξ 4 ·⎢ ⎢ w 2 (2 1 ) w 2 (2 2 )⎥ ⎥ . (17.47)
⎣ (2) (2)⎦
w w
31 32
(2) (2)
w w
(cid:20) 41(cid:21)(cid:22) 42 (cid:23)
W(2)
Next, the intermediate values ξ can be computed by applying a different activation
. i
function to each of them:
(cid:4) (cid:5) (cid:24) (cid:13) (cid:14) (cid:13) (cid:14)(cid:25)
. ξ = ξ 1 ξ 2 ξ 3 ξ 4 = ϕ 1 x·w ( 1 1) ... ϕ 4 x·w ( 4 1) . (17.48)
Defining ϕ as the vector-valued function that consists of the ϕ , this can be written
. . i
in short as
⎛ ⎡ ⎤⎞
w (1) w (1) w (1) w (1) (cid:13) (cid:14)
⎜ ⎢ 01 02 03 04⎥⎟
. ξ =ϕ⎝x·⎣w 1 (1 1 ) w 1 (1 2 ) w 1 (1 3 ) w 1 (1 4 )⎦⎠=ϕ x·W(1) , (17.49)
(1) (1) (1) (1)
w w w w
21 22 23 24
where we additionally introduced the matrix of all weights between the input layer
and the hidden layer as W(1). These are now formulations that are straightforward
.
to be generalized to a network with one hidden layer, an arbitrary number of
features, arbitrary number of units contained in the hidden layer, and output nodes.
Furthermore, without much of additional work, it is also possible to consider not
just a single data record but all data records of the data matrix at once and in a
vectorized manner.
We can now easily turn the vectorized equations into a Python implementation
as shown in Python Listing 17.4.
17.4.6 Capabilities of the Networks for Classification and
Regression
One of the most important components that determines whether the network
performs a regression or a classification task is the output node. So far we
encountered two different types of output nodes: the first one contained a threshold
function—indicator for the fact that this is about classification. With a True or
False (i.e., 1 or 0) output, we can perform a binary classification for deciding
if something belongs to class A or to class B. The first more complex network,
Fig.17.12, contained two output nodes that were only performing (weighted)
summations resulting in real numbers y and y and therefore is a regression
. 1 . 2
model. This particular example takes three features as input, e.g., the spatial x,
y, and z coordinates of a point and results in two real valued outputs, e.g., the


================================================================================
PAGE 506
================================================================================

492 17 FromthePerceptrontoArtificialNeuralNetworks
PythonListing17.4 General implementation of the vectorized equations for a network with one
hidden layer. Note that np.where also works with the row vector theta1 : it automatically
applies it to each row of X1 @ W1 so that creating THETA1 is not required
temperature and a chemical concentration. Generalizing these approaches to, e.g.,
multiple classes and increasing their performance is a subject that will be covered
at a later point after the training of these networks was introduced.
17.5 Summary and Historical Remarks
We saw how increasingly complex networks developed, starting from the example
of the biological neuron as a “template.” All of the above developed networks of
increasing complexity are able to represent increasingly complicated functions and,
altogether, looked very promising in the 1960s. Problems intensively studied back
then included the structure of “error surfaces” for Boolean operators of various
complexities and how to represent assemblies of Boolean operators with possibly
many inputs (see e.g. [6,10] and references therein).
The greatest shortcoming, however, turned out to be a suitable “training algo-
rithm” for more complex networks: transferring the strategy of the ADALINE training
with the “delta rule” (or the Widrow & Hoff learning rule) from a single unit to
more complex networks was not straightforward as the delta rule is based on the
difference of the actual and desired output. The latter is available only for the unit
that yields the output. Therefore, it is not clear how the weights and bias values of
the other neurons in any of the hidden layers should be adjusted during the training.


================================================================================
PAGE 507
================================================================================

17.6 Exercises 493
This is why after a sustained period of excitement, ANNs was seen as a research
“dead end” and, what is commonly known as the “AI winter” began and lasted until
the end of the 1970s.
17.6 Exercises
17.1 Given are two Boolean variables x and y. Show how the Boolean operations
AND, OR, and NOT can be expressed using “regular” number arithmetic, i.e., by
interpreting True as 1 and False as 0.
17.2 Show that the perceptron is not able to represent the Boolean XOR operation.
17.3 Visualize a perceptron with two inputs that would be able to represent the
logical AND operation. Also show the decision boundary in analogy to Fig.17.4.
17.4 Assuming that we have Boolean functions with two input variables. Show that
then there exist 16 different Boolean functions. Show geometrically that only two
of them are not linearly separable
17.5 Assuming that we have Boolean functions with three input variables. How
many different Boolean functions exist then and how many of them are linearly
separable?
17.6 Visualize a perceptron with two inputs that would be able to represent the
logical AND operation. Also show the decision boundary in analogy to Fig.17.4.
17.7 Use the following function to create a dataset that consists of two normal
distributions so that the data set has 2 features (the x and y coordinate) and the label
0or1.
def create_bivariate_normal_distribution ( n_points=500):
1
X = np . empty ((0 , 2))
2
y = np . empty (0)
3
4
mean = [6, -2]
5
cov = [[1 , -0.6] , [-0.6 , 3]]
6
x1 , x2 = np . random . multivariate_normal (
7
mean , cov , n_points
8
).T
9
X = np . append ( X , np . stack ((x1 , x2) , axis=1),
10
axis=0)
11
y = np . append ( y , np . zeros_like ( x1 ))
12
13
mean = [-1, 1]
14
cov = [[2 , 0.6] , [0.6 , 12]]
15
x1 , x2 = np . random . multivariate_normal (
16
mean , cov , n_points
17
).T
18


================================================================================
PAGE 508
================================================================================

494 17 From the Perceptron to Artificial Neural Networks
X = np . append ( X , np . stack ((x1 , x2) , axis=1),
19
axis=0)
20
y = np . append ( y , np . ones_like ( x1 ))
21
22
return X , y
23
Use the Python Listing 17.2 to perform a classification. Create a plotting function
that shows the decision boundary. Additionally, adjust the code such that the error
is plotted as a function of the number of epoch.
17.8 The ADALINE model can also be used for regression problems. Which are
the parts of the ADALINE model that need to be adapted? Apply your solution for
predicting the inclination of the linear elastic regime of the MDS-1 (Tensile Test)
dataset.
17.9 Assume that all weights and biases in a network of perceptrons are scaled by
a constant value, λ> 0. How does this influence the network?
17.10 In Sect.17.4.2 it was shown that for a particular set of weights, biases and
thresholds a network of two TLUs is able to represent the Boolean operator XOR.
The solution resulted in three regions for the output value y: two regions where
y = 1 and one regions where y =0.
(i) Show by an example that there exists another class of solutions that consist of
two regions where y =0 and one regions where y =1.
(ii) Explain a systematic approach for finding these weights starting from the
solution in Sect.17.4.2.
References
1. G. Berlucchi and H. A. Buchtel. Neuronal plasticity: historical roots and evolution of meaning.
Exp Brain Res, 192:307–319, 2009. DOI https://doi.org/10.1007/s00221-008-1611-6.
2. B. Blaus. Multipolar neuron. URL https://commons.wikimedia.org/wiki/File:Blausen_0657_
MultipolarNeuron.png.
3. W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity.
Bulletin of Mathematical Biophysics, 5(4):115–133, 1943.
4. M. Minsky and S. Papert. Perceptrons: An Introduction to Computational Geometry. MIT
Press, Cambridge, MA, USA, 1969.
5. G. Nagy. Neural networks-then and now. IEEE Transactions on Neural Networks, 2(2):316–
318, 1991. DOI https://doi.org/10.1109/72.80343.
6. R. Rojas. Neural Networks—A Systematic Introduction. Springer, Berlin, Heidelberg, 1st
edition, 1996. ISBN 978-3-540-60505-8. DOI https://doi.org/10.1007/978-3-642-61068-4.
7. F. Rosenblatt. The Perceptron—A Perceiving and Recognizing Automaton Project Para.
Report: 85-460-1, Cornell Aeronautical Laboratory. Cornell Aeronautical Laboratory, Buffalo,
New York, Ithaca, New York, January 1957.
8. F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization
in the brain. Psychological Review, 65(6):386–408, 1958. DOI https://doi.org/10.1037/
h0042519.


================================================================================
PAGE 509
================================================================================

References 495
9. F. Rosenblatt. Principles of neurodynamics: Perceptrons and the theory of brain mechanisms,
1962.
10. Q. Sheng. Threshold Logic. Academic Press Inc, 1969. ISBN 978-0126398502.
11. W. Waldeyer. Ueber einige neuere Forschungen im Gebiete der Anatomie des Centralnerven-
systems). DMW—Deutsche Medizinische Wochenschrift, 17(44):1213–1218, Nov. 1891. DOI
https://doi.org/10.1055/s-0029-1206824.
12. B. Widrow. An Adaptive “Adaline” Neuron Using Chemical “MEMISTORS”, Technical
Report No. 15553-2, Stanford University, USA, 1960.


================================================================================
PAGE 510
================================================================================

A Gentle Introduction to Deep Learning 18
More data beats clever algorithms, but better data beats more
data.
Peter Norvig (born 1956)
American Computer Scientist
18.1 Overview of the Historical Developments
Even though research activities were strongly reduced during “first AI winter”,
which lasted from the early 1970s until 1980, there still was some progress. One ot
the important developments was the introduction of nonlinear step functions (such
as the sigmoidal function). Furthermore, in 1974, PAUL JOHN WERBOS described
a new type of training process where the errors were backpropagated through the
layers of the network1 [10]. While WERBOS was the first to describe backpropaga-
tion, this approach was also (re)discovered in other research communities. It took
until approximately 1985 when DAVID E. RUMMELART, GEOFFREY E. HINTON,
and RONALD J. WILLIAMS introduced the algorithm for backpropagation of errors
in the context of multilayer perceptrons (MLPs) in [8]. This made many of the
developments possible, which lead to today’s deep learning.
The following sections start by introducing the most important building blocks
for deep learning (DL): new types of activation functions and the backpropagation
algorithm.
1 The PhD thesis [10] is the original publication; additionally, the thesis was later published in the
book [11].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 497
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_18


================================================================================
PAGE 511
================================================================================

498 18 AGentleIntroductiontoDeepLearning
18.2 Activation Functions
In the previous chapter, we only encountered one type of activation function, the
binary step function (or Heaviside function). However, it turns out that for back-
propagation, non-zero gradients are required. Also, having a certain degree of
nonlinearity in the activation function generally helps with the training and increases
the predictive power of a network. In fact, the activation function is the first place
where a non-linearity can enter the network. Thus, there are many options for
mathematically formulating activation functions. One of their common properties
is that they all are elementary mathematical functions. They are always monotonic
functions, i.e., their function values are always increasing. Additionally, some of
them have the property to saturate at large absolute values: they are squashing
functions that map an arbitrary value range to a bounded output. Figure 18.1
shows an overview of some activation functions and the respective mathematical
formulations; further discussions can be found, e.g., in [12] and in the references
therein.
Activation Functions (cid:2)
Usually, all hidden layers use the same activation function. The output layer
(as we already saw in the previous chapter) may use a different type of
activation function. This depends on the kind of prediction (classification,
regression) that is to be made. For example, for regression problems, the last
layer is a layer with linear activation.
18.2.1 Step Function
The step function is the simplest approach, which, however, does not work for multi-
class classification problems because it can only give two different outputs, 0 and 1.
Additional, the gradient is zero, which poses a problem for back-propagation.
18.2.2 Linear Function
Using a linear activation function, which is also called identity function , the
performance of the neural network will not or only slowly improve due to the
constant gradient during the back-propagation: a constant gradient implies that it
is insensitive to the value of the input. If all hidden layers have linear activation
functions, then each layer is a linear function of the previous one and, effectively,
all layers can be replaced by a single layer with linear activation functions.


================================================================================
PAGE 512
================================================================================

18.2 ActivationFunctions 499
Fig. 18.1 Activation functions (left column) and their derivatives (right column)


================================================================================
PAGE 513
================================================================================

500 18 AGentleIntroductiontoDeepLearning
18.2.3 Sigmoidal Function
The logistic or sigmoid activation function is a nonlinear function. The function is
non-symmetric with f(x) > 0 around x = 0. As a consequence, the outputs of
. .
all neurons have the same sign, which makes training less stable. Due to its range
of the function values of [0,1], it is used in the context of predicting probabilities.
.
The function has only small gradients further away from the center; the fact that
df(x)/dx = f(x)· (1 − f(x))implies that for every iteration the values are
.
getting smaller (below we will discuss this in the context of the “vanishing gradient
problem”).
18.2.4 The Family of ReLU Functions
One of the often used type of activation functions today is the rectified linear unit
(ReLU) function: The ReLU activation function looks extremely simple: it consists
of a horizontal line and an 45 ◦ -inclined line. It is not differentiable at x = 0,
. .
which might lead to problems during gradient-based optimization. A property of
the ReLU function is that it is scale-invariant. However, one of the side effects of
this mathematical formulation is that it does not activate all neurons at the same
time; due to the zero gradient, it even might lead to “dead neurons” that never get
activated. This is also the reason, why we typically prefer to initialize the neurons
directly connected to ReLU activation functions with a slightly positive bias by, e.g.,
adding a small “offset” to the initial random values.
A variant of ReLU that aims to remedy the problem of dead neurons is the leaky
ReLU. This variant avoids non-zero function values and derivatives by adding a
small slope to the region of x ≤0. The slope is the “leakage” parameter:
.
(cid:2)
x if x >0,
f(x)= (18.1)
. .
0.01x otherwise.
(cid:2)
∂f (x) 1 if x >0 ,
= (18.2)
∂x 0.01 otherwise.
The first equation can also be implemented as f(x) = max(0.01x, x) .
Furthermore, the parameterized ReLU treats the leakage parameter as a parame-
ter that needs to be identified during the training. These are just a few examples,
there are many more formulations, some of which are specialized to particular
network architectures.


================================================================================
PAGE 514
================================================================================

18.3 Back-Propagation:IntroductionandExample 501
18.2.5 Hyperbolic Tangent Function
The second, most commonly used type of activation function, is the hyperbolic
tangent (tanh) function. It is very similar to the sigmoid function, but it is symmetric
.
around 0. It also has a steeper gradient, which can be seen by the value of the
derivative at x = 0 in Fig.18.1. Usually, it performs better than the sigmoid
.
function. As opposed to ReLU, it is not scale-invariant.
18.3 Back-Propagation: Introduction and Example
Already a few years after ROSENBLATT published the perceptron , 6[], he also
published the concept of a multilayer perceptron (MLP), which contained an input
layer, a hidden layer, and an output layer [7]. The hidden layer was initialized with
random values for the weights. However, there was no mechanism such that the
hidden layer was able to adjust the weights. Because a DL network has the ability
to adjust the weights in the hidden layer, the multilayer perceptron would not yet be
considered a proper DL network.
Even though the gradient descent method in the context of training was indirectly
established through the delta rule, at this point the concepts of a cost function in
conjunction with gradient descent minimization did not exist for artificial neural
networks (ANNs). The great achievement of RUMMELHARD et al. [8] was to
generalize the gradient descend framework to the context of training multilayer
neural networks with nonlinear activation functions in a way such that all weights
get automatically adjusted.
18.3.1 The General Concept
The back-propagation algorithm (which is sometimes also called in short “back-
prop” algorithm) consists of two processes: the forward pass (or propagation phase)
where the inputs are propagated through the network and the output predictions are
computed, and the backward pass during which the gradient of the loss function
of the output layer is computed with respect to all weights; this gradient is then
recursively propagated through the network from the output layer all the way to the
input layer. The chain rule for differentiation is key for finding how each individual
weight of all hidden layer(s) has to be adjusted (the so-called weight update) in order
to minimize the loss function.
18.3.2 A First Example
We will now investigate a very simple system, similar to that from Sect.17.4.1, and
introduce the backpropagation formalism. The network has two input nodes, two


================================================================================
PAGE 515
================================================================================

502 18 AGentleIntroductiontoDeepLearning
Fig. 18.2 Network with one
hidden layer and a sigmoidal
activation function. The
output is just aggregated, and
there is no activation function
nodes in the hidden layer, and a single output. The hidden nodes contain sigmoidal
activation functions, and the whole network is shown in Fig.18.2.
Furthermore, we only consider a single data record consisting of two features
and a single target variable,P =(x ,x ;y).
. 1 2
Activation Function
The activation functions of the hidden layer both have the following mathematical
form:
1
ϕ(z)= (18.3)
. 1+exp(−z)
Loss and Cost Function
Additionally, we need to quantify the “badness” of the performance of the network.
This is done using a quadratic loss function, in analogy to what has been introduced
in Sect.12.3.3 and in particular in Definition 12.6. For each training example, it is
(cid:3) (cid:4)
1 2
L(w; (x,y))= y−ypred(w; x ,x ) . (18.4)
. 1 2
2
Note that the factor of 1/2 is there to simplify the equation during computing the
.
derivative; this does not have any effect on finding the minimum of the cost function
below. From this equation, we see that L depends on the current values of all
.
weights that are varied during the training, as well as on a given record from the
training dataset.
The loss function gives a single value for each data record. The cost function
on the other hand is the average loss for all records of the dataset. Similar to
Definition 12.5 we can now define the cost function J. Writing this directly for
.
only a single data record hides some details; thus, we write it first in all generality
as the average cost of M data records:
(cid:5)M (cid:3) (cid:4)
1 2
J= y −ypred (w; x ,x ) , (18.5)
. 2M i i i1 i2
i=1


================================================================================
PAGE 516
================================================================================

18.3 Back-Propagation:IntroductionandExample 503
which, for a dataset that consists of only one record, would simplify to
(cid:3) (cid:4)
1 2
J= y−ypred(w; x ,x ) . (18.6)
. 1 2
2
The first and second features for several data records are given by the column vectors
X and X (compare the definition of the feature matrix, Sect.3.3.3), the vector of
. 1 . 2
all labels is Y. The resulting cost function is then a function that depends on six
weights and two bias values as variables:
J=J(w , ..., w ,b ,b ; X ,X ; Y). (18.7)
. 1 6 1 2 1 2
Words of advice (cid:3)
In the field of deep learning, the difference between cost and loss is very often not
recognized, and ML practitioners only talk about the “loss functions.” To be consistent
withtheformulationsfromearlierchapters,wewillcontinuetodifferentiatebetween
cost and loss.
Forward Pass
The feed-forward step consists of evaluating the network predictionypredfor a given
.
input, in analogy to Sect.17.4.1, together with computing the cost. The quantities
can be best computed starting from the output layer and then moving toward the
input layer. The cost function can then be written as
1
J= (y−a )2 , (18.8)
. 3
2
where we replaced ypredwith the result from the agglomeration function a
. . 3
a =b +y w +y w =ypred , (18.9)
. 3 3 1 5 2 6
This function performs the weighted summation, and y and y are the outputs of
. 1 . 2
the two units of the hidden layer. They can be computed as
y = ϕ(a ) (18.10)
. 1 1 .
y = ϕ(a ). (18.11)
2 2
where ϕis the sigmoidal activation function. The weighted summations are:
.
a =b +x w +x w (18.12)
. 1 1 1 1 2 2.
a =b + x w +x w . (18.13)
2 2 1 3 2 4


================================================================================
PAGE 517
================================================================================

504 18 AGentleIntroductiontoDeepLearning
For brevity, we refrained from stating the function arguments. For example, we
automatically implied the short notation a ≡a (w ,w ,b ; x ,x ).
. 1 1 1 2 1 1 2
Backward Pass
Training implies to find the weights that minimize the cost function,
Findw , ..., w ,b ,b suchthat J→MIN . (18.14)
. 1 6 1 2
Thus, so solve this minimization problem with the gradient descent (GD) method,
we will need to compute the derivatives ∂J/∂w ...∂J/∂w , as well as ∂J/∂b
. 1 . 6 . 1
and∂J/∂b2. Again, we start from the output of the network and then move through
.
the network toward the input layer.
We start to compute the derivative w.r.t. one of the weights of the output node
using the chain rule and Eq.18.8:
∂J ∂J ∂a
= · 3 =−(y−a )·y (18.15)
. 3 1
∂w ∂a ∂w
5 3 5
Hint for Writing the Chained Products of Derivatives (cid:2)
Start writing the chain of derivatives from the right to the left. For example, if
the partial derivative is w.r.t.w then search for the term that containsw :itis
. 1 . 1
a . Thus the rightmost term is∂a /∂w . Next, watch out for the equation that
. 1 . 1 1
contains a : it is y , giving you the second derivative term ∂y /∂a . Then,
. 1 . 1 . 1 1
search for the equation that contains y : it is a , and we found a third term.
. 1 . 3
It is ∂a /∂y . Last but not least, the function that contains a is J, hence,
. 3 1 . 3 .
∂J/∂a and we’re done.
. 3
The result is:
∂J ∂J ∂a ∂y ∂a
= · 3 · 1 · 1
.
∂w ∂a ∂y ∂a ∂w
1 3 1 1 1
The derivation of ∂J/∂w follows in analogy:
. 6
∂J ∂J ∂a
= · 3 =−(y−a )·y . (18.16)
. 3 2
∂w ∂a ∂w
6 3 6
Next, the expressions for ∂J/∂b , ∂J/∂w , and ∂J/∂w are derived:
. 2 . 3 . 4


================================================================================
PAGE 518
================================================================================

18.3 Back-Propagation:IntroductionandExample 505
∂J ∂J ∂a ∂y ∂a
= · 3 · 2 · 2 =−(y−a )·w ·y (1−y ) (18.17)
. 3 6 2 2 .
∂b ∂a ∂y ∂a ∂b
2 3 2 2 2
∂J ∂J ∂a ∂y ∂a
= · 3 · 2 · 2 =−(y − a ) · w · y (1− y )· x (18.18)
3 6 2 2 1.
∂w ∂a ∂y ∂a ∂w
3 3 2 2 3
∂J ∂J ∂a ∂y ∂a
= · 3 · 2 · 2 =−(y − a ) · w · y (1− y )· x (18.19)
3 6 2 2 2
∂w ∂a ∂y ∂a ∂w
4 3 2 2 4
where we used that for a sigmoidal function f , its derivative is f (cid:6) (x) = f(x)(1−
.
f(x)), which helped to write
∂y ∂ϕ(a )
2 = 2 = ϕ(a )(1−ϕ(a ))=y (1−y ). (18.20)
. 2 2 2 2
∂a ∂a
2 2
The derivation for ∂J/∂b , ∂J/∂w , and ∂J/∂w follows along the same line and
. 1 . 1 . 2
are summarized as
∂J ∂J ∂a ∂y ∂a
= · 3 · 1 · 1 =−(y−a )·w ·y (1−y ) (18.21)
. 3 5 1 1 .
∂b ∂a ∂y ∂a ∂b
1 3 1 1 1
∂J ∂J ∂a ∂y ∂a
= · 3 · 1 · 1 =−(y − a )·w ·y (1− y )· x (18.22)
3 5 1 1 1.
∂w ∂a ∂y ∂a ∂w
1 3 1 1 1
∂J ∂J ∂a ∂y ∂a
= · 3 · 1 · 1 =−(y − a )·w ·y (1− y )· x . (18.23)
3 5 1 1 2
∂w ∂a ∂y ∂a ∂w
2 3 1 1 2
These are all possible “sensitivities” of the cost function w.r.t. the network parame-
ters, and the backward pass is finished.
Update of a Weight in a Gradient Descent Approach
The above results can now be used in a GD method, where, e.g., the weight . w 4 is
updated according to
∂J
wnew =wold−η , (18.24)
. 4 4 ∂w
4
where η is the learning rate. Note that if the algorithm is performed with multiple
.
data records, then each record would result in a different weight increment. The
weight update has to be done using the mean value of all weight increments. For
weight w , this is then written as
. j
1 (cid:5)n ∂J(x )
wnew =wold−η i , (18.25)
. j j n ∂w
i=1 j


================================================================================
PAGE 519
================================================================================

506 18 AGentleIntroductiontoDeepLearning
where n denotes the number of records. With this, all components for a complete
training process are now available. The following Python implementation almost
exactly uses the mathematical formulations.
18.3.3 A Complete Python Implementation
In the previous subsection, we derived the formulation for a forward and backward
pass under the assumption that there is only one example in the dataset. What
would change if, instead, we have n data records? The partial derivative operations
will be “simultaneously” applied to all data records, but there are no vector–matrix
products involved. Only for computing the cost function, we have to return to the
formulation in Eq.18.5. Therefore, we can directly implement the case of multiple
data records using vectorization over data records. However, we will not generalize
the formulation to arbitrary numbers of, e.g., input nodes or hidden layers but rather
hard-code the network architecture.
Python Listing 18.1 shows a Python implementation of the formulations derived
in Sect.18.3.2, which will be briefly explained in what follows. We assume that
the training data are provided in form of two feature vectors X1 and X2 together
with the target vector Y , all of which are numpy arrays. The goal was to keep the
implementation as readable as possible. For this we keep all weights and bias value
in the sets W and B when functions are called. This requires that inside the function
we again extract (or “unpack”) the individual weight and bias values from these two
variables—and similar, when, e.g., the dJdw1 , dJdw2 are returned. Using object
oriented programming and classes could help to avoid this kind of “book-keeping
overhead.”
(cid:129) In line 3 we define the sigmoidal function. Due to the fact that we use numpy’s
exponential function, it is possible to compute it for all elements of an array.
(cid:129) Starting with line 6, we define a function for performing the forward pass. After
extracting the individual weight and bias variables from W and B , we can directly
generate Python code from the mathematical equations. All operations work for
X1 and X2 being scalars as well as numpy arrays. The individual weights and
biases are always scalar values.
(cid:129) Predicting a label for given weights, biases, and feature values in Line 16 consists
just in a forward pass. The “ _, _, _, _ ” in Line 15 are placeholders’ return
values that are not required (but any other variable name would do as well).
(cid:129) Doing the backward pass in Line 20 does not require anything new. The
implementation is again identical to the mathematical formulation and works
both for scalar and array data for X1 and X2 as all operations are performed
element-wise.
(cid:129) For updating the weights in Line 37 we again start by unpacking the weights,
biases, and derivatives. Then, Eq.18.25 is implemented and the updated weights
and biases are returned


================================================================================
PAGE 520
================================================================================

18.3 Back-Propagation:IntroductionandExample 507
import numpy as np
def sigmoidal_function(x):
return 1 / (1 + np.exp(-x))
def forward_pass(W, B, X1, X2):
(w1, w2, w3, w4, w5, w6), (b1, b2, b3) = W, B
a1 = b1 + X1 * w1 + X2 * w2
a2 = b2 + X1 * w3 + X2 * w4
y1 = sigmoidal_function(a1)
y2 = sigmoidal_function(a2)
a3 = y1 * w5 + y2 * w6 + b3
return a1, a2, y1, y2, a3
def predict(W, B, X1, X2):
_, _, _, _, Y_pred = forward_pass(W, B, X1, X2)
return Y_pred
def backward_pass(W, B, X1, X2, Y):
w1, w2, w3, w4, w5, w6 = W
a1, a2, y1, y2, a3 = forward_pass(W, B, X1, X2)
dJdb1 = (-Y - a3) * w5 * y1 * (1 - y1)
dJdw1 = dJdb1 * X1
dJdw2 = dJdb1 * X2
dJdb2 = (-Y - a3) * w6 * y2 * (1 - y2)
dJdw3 = dJdb1 * X1
dJdw4 = dJdb1 * X2
dJdb3 = (-Y - a3)
dJdw5 = (-Y - a3) * y1
dJdw6 = (-Y - a3) * y2
dJdW, dJdB = (dJdw1, dJdw2, dJdw3, dJdw4, dJdw5, dJdw6), (dJdb1, dJdb2, dJdb3)
return dJdW, dJdB
def update_weights(W, B, dJdW, dJdB, learning_rate):
(w1, w2, w3, w4, w5, w6), (b1, b2, b3) = W, B
(dJdw1, dJdw2, dJdw3, dJdw4, dJdw5, dJdw6), (dJdb1, dJdb2, dJdb3) = dJdW, dJdB
w1 -= learning_rate * np.mean(dJdw1)
w2 -= learning_rate * np.mean(dJdw2)
w3 -= learning_rate * np.mean(dJdw3)
w4 -= learning_rate * np.mean(dJdw4)
w5 -= learning_rate * np.mean(dJdw5)
w6 -= learning_rate * np.mean(dJdw6)
b1 -= learning_rate * np.mean(dJdb1)
b2 -= learning_rate * np.mean(dJdb2)
b3 -= learning_rate * np.mean(dJdb3)
W_new, B_new = (w1, w2, w3, w4, w5, w6), (b1, b2, b3)
return W_new, B_new
Python Listing 18.1 Implementation of a network with two input nodes, two nodes in a hidden,
layer and a single output node. See the text for more information. Note that contrary to all best
practices in programming, we omitted docstrings, comments, type hints, and “sanity checks” to
ensure that the code stays compact
With this we can now attempt to use the implementation for a concrete problem:
classification using only the first two features of the “Iris dataset.” We import the
dataset using the scikit-learn module (or the online resources of this book, or any
other resources—as long as X contains the four features in columns and Y is the
single target variable) and then extract only those records that belong to the first two
classes.


================================================================================
PAGE 521
================================================================================

508 18 AGentleIntroductiontoDeepLearning
The data are split into a training and a testing dataset, using the function
train_test_split that has already been introduced in Sect.16.2.2.
Next, we initialize the six weights with random values and set the bias values to
zero. The number of epochs (i.e., the number of complete forward/backward passes)
together with the learning rate are set
with which now the gradient descent update iterations can be performed:
In [4]: mean_losses = []
for epoch in range(epochs):
Y_pred = predict(W, B, X1_train, X2_train)
mean_losses.append(mean_loss(Y_train, Y_pred))
dJdW, dJdB = backward_pass(W, B, X1_train, X2_train, Y_train)
W, B = update_weights(W, B, dJdW, dJdB, learning_rate)
print('MSE difference:', mean_losses[0], mean_losses[-1])
Out [4]: MSE difference: 0.26667596178013414 0.019281451533922658
Comparing the initial training MSE to the final training MSE suggests that at least
the numerical scheme is stable and no undefined values or other errors occurred.
Since the training dataset is two-dimensional, it is a good approach to visualize the
data. Here, we show two different plot types. The first plot just shows the training
mean squared error (MSE) as a function of the number of epochs:


================================================================================
PAGE 522
================================================================================

18.3 Back-Propagation:IntroductionandExample 509
In [5]: fig, ax = plt.subplots()
ax.plot(mean_losses)
ax.set(xlabel='number of epochs', ylabel='mean training loss (MSE)')
The second plot is a more complex plot. It is a heat map using imshow , which shows
the training MSE evaluted at a grid of equidistant points of the feature space. This
was superimposed with a scatter plot of the position of the training and testing data.
Additionally, the decision boundary was shown where all target values smaller than
0.5 indicated class #1 and all values larger than 0.5 indicate class #2.
The resulting plots are similar to those shown in Fig.18.3 (plotting the loss curve
for the testing follows the same path and is left as an exercise). Additionally, we
also showed the evolution of the mean testing loss, which is left as an exercise for
the reader.
The learning curve shows that after approximately 400 epochs, a stationary
state is reached, both for training and testing. The testing MSE is slightly below
the training MSE, but using different random splits does not result in significant
Fig. 18.3 Results from the network with one hidden layer for the first two features of the Iris
dataset. The left panel shows the learning curves, the right panel shows the datasets together with
the predicted value.Ypredand the decision boundary


================================================================================
PAGE 523
================================================================================

510 18 AGentleIntroductiontoDeepLearning
difference. Here, a cross-validation could be useful as well. Visually inspecting
the dataset reveals that the data points should be linearly separable; the decision
boundary that was found by the network does this perfectly.
To quantify how well our model performs, we evaluate the metric of true
positives, true negatives, false negatives, and false positives, as introduced in
Sect.11.8.1:
In [7]: TP = np.sum((Y_pred == 1) & (Y_test == 1))
TN = np.sum((Y_pred == 0) & (Y_test == 0))
FP = np.sum((Y_pred == 1) & (Y_test == 0))
FN = np.sum((Y_pred == 0) & (Y_test == 1))
print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')
Out [7]: TP: 17, FP: 0, TN: 13, FN: 0
The value of 0.5is the threshold above which the output is considered to be True.
.
The result confirms what we already observed in the right panel of Fig.18.3: all
examples have been correctly predicted. Even though this comes as no surprise since
the two classes are linearly separable, it is nonetheless a good validation of our
network.
18.4 General Formulation of Backpropagation
In the last section, it became obvious that determining all chained partial derivatives
of all layers is very tedious. It is also inflexible as for every new network architecture
(i.e., the particular combination of different layers and nodes), the derivations have
to be performed anew. An alternative way is to treat each layer as a basic building
block that takes an input, processes the data, and returns an output. For example,
for layer number k the input is x(k) and the output is y(k). For simplicity of the
. .
formulations, both vectors represent only a single data record and are thus row
vectors. Also, we treat the activation functions as an individual layer, the activation
layer (AL) while the layer where the summation is done is called fully connected
layer (FCL).
This is also the way how current DL frameworks are designed. Therefore, we will
go through these derivations in some more detail.
18.4.1 Forward Pass
Fully Connected Layer
We will now derive the formulations for the forward pass and start with the fully
connected layer, which is layer number k. A sketch of the considered situation is
shown in Fig.18.4.
The forward pass works as expected: it takes as input the output from the previous
layer (step 1. in Fig.18.4), computes the values of the target variable (step 2. in


================================================================================
PAGE 524
================================================================================

18.4 GeneralFormulationofBackpropagation 511
fully
connected
activation prediction
or input
layer cost
function
prediction output of current layer = input of prediction of
next layer or final prediction
Fig. 18.4 Notations and visualization of the forward pass for a fully connected layer. Note that
the sizes n and m of the input and output vectors can change from layer to layer even though we
do not explicitly indicate this by subscripts such as. ( k)
Fig.18.4), and provides this as the output to the next layer (step 3. in Fig.18.4). If
the next layer is the output layer, then this is the prediction of the network, and we
additionally can evaluate the cost function (step 4. in Fig.18.4).
What is new in this section is the notation of using superscripts to indicate the
layer. Thereby, we only have to define each layer type ones and can then use them as
basic building blocks for general networks. For example, the output of layer number
k−1is given by
.
(cid:6) (cid:7) (cid:6) (cid:7)
. y(k−1) = y 1 (k−1) ··· y m (k−1) ≡ x 1 (k) ··· x n (k) =x(k) , (18.26)
which serves as the input of layer number k. The number of input nodes of each
layer is denoted by n, while the number of output nodes of each layer is m .Tobe
fully consistent, also the m and n would need a superscript to indicate the number
of the layer; however, given that for m and n usually the context is clear and for the
sake of readability, we will remember that, e.g., the number of input nodes n of a
layer k can be different from the number of input nodes n of any other layer, and
that writing n in fact would imply n(k)for the k-th layer.
.
The agglomeration in the layer is then given by
x(k)W(k)+b(k) =y(k) (18.27)
.
where the matrix of weights and the vector of bias values of layer k are given by
⎡ ⎤
. W(k) = ⎢ ⎣ w 1 . . . (k 1 ) · . · .. · w 1 . . . (k m ) ⎥ ⎦ and b(k) = (cid:6) b 1 (k) ··· b m (k) (cid:7) . (18.28)
w (k) ···w (k)
n1 nm


================================================================================
PAGE 525
================================================================================

512 18 AGentleIntroductiontoDeepLearning
The r-th component of y(k)can then be written as
.
⎡ ⎤
(k)
. y r (k) = (cid:6) x 1 (k) ···x n (k) (cid:7) ⎢ ⎣ w 1 . . . r ⎥ ⎦+b r (k) =x 1 (k) w 1 (k r )+···+x n (k)w n (k r )+b r (k) .
(k)
w
nr
(18.29)
Activation Layer
The formulation for the activation layer is rather straightforward, and all that needs
to be done, is, to apply the activation function to each input of the layer. For example,
for a layer number k+1it is
.
(cid:6) (cid:7) (cid:3) (cid:4)
. y(i+1) = ϕ(x 1 (i+1) ), ..., ϕ(x n (i+1) ) =ϕ x(i+1) , (18.30)
where the function ϕ is applied to each element of x individually. x is the input of
. . .
the activation layer, which is typically given by the output of a fully connected layer.
In an activation layer, y has always the same dimensions as x.
. .
Cost Function
The forward propagation step is then repeated for each layer, until we arrive at the
output layer for which we obtain the predicted outputy. With this, the cost function
.
can be evaluated; as we only have a single data record, it is simply
(cid:5)m (cid:3) (cid:4)
1 2
J(y; ytrain)= ytrain −ypred (18.31)
. 2m i i
i=1
18.4.2 Backward Pass
The backward pass is more complicated than the forward pass and requires the
computation of a number of derivatives. The strategy is to establish a backward
pass for one layer, which can then be generalized to more complex networks by
connecting such layers.
Our point of departure for the derivation is the output layer where we already
computed the cost function. The goal is now to use the established framework of a
gradient descent method, which is supposed to find values of all weights and all bias
values of the N layers in the network such that the cost function is minimized,
. layers
(cid:14) (cid:15)
Find W(k),b(k) suchthat J→MIN . (18.32)
.
k=1...Nlayers
The numerical approach is the same as before: we calculate the sensitivity of the
cost function w.r.t. all weights and bias values,


================================================================================
PAGE 526
================================================================================

18.4 GeneralFormulationofBackpropagation 513
dJ dJ
and , (18.33)
. (k) (k)
dw db
ij j
and use this to define an update increment for all weights and bias values. While
computing these derivatives manually is possible in case that there is only one
fully connected layer (as we saw in the previous chapter), for networks with several
layers, this is more difficult as one layer depends on the output of the previous layer,
and we thus have an implicit dependency of the cost function on the weights. This
is where the chain rule for a function that implicitly depends on a third variable
through two variables will come into play:
Definition 18.1. Chain Rule If f is a function of two variables, f(y ,y ),
. 1 2
and each of the y and y are again functions of a variable x, then it is
. 1 . 2
df ∂f ∂y ∂f ∂y
= 1 + 2 . (18.34)
.
dx ∂y ∂x ∂y ∂x
1 2
In our case, the function under consideration is the cost function J, and we are
.
looking for the derivatives w.r.t. the weights and biases, see Eq.18.33. However, we
are only able to obtain the derivatives w.r.t. to the final output y(k), because this is
.
the only variable that shows up in Eq.18.31. What other information could be used?
We already know that y implicitly depends also on the weights and bias values.
.
Thus, we can write the first term from Eq.18.33 using the chain rule as
dJ ∂J ∂y (k) ∂J ∂y (k)
= 1 +···+ m
. (k) (k) (k) (k) (k)
dw ∂y ∂w ∂y ∂w
ij 1 ij m ij
(cid:5)m ∂J ∂y (k)
= r . (18.35)
(k) (k)
∂y ∂w
r=1 r ij
Each of the summands in Eq.18.35 consists of two factors. The first terms only
can be easily computed for the final layer using the definition of the cost function.
For example, for the r-th component of y we get:
(cid:3) (cid:4)
∂J 1
=− ytrain −y , (18.36)
. ∂y (k) m r r
r
where m is the number of output nodes which is identical to the number of elements
of y. For all other layers, we first have to propagate the error from the output layer
.
to the respective layer of interest. For doing so, the chain rule comes in handy again,


================================================================================
PAGE 527
================================================================================

514 18 AGentleIntroductiontoDeepLearning
and we can write:
dJ (cid:5)m ∂J ∂y (k) dJ ∂J ∂y(k)
= r or = . (18.37)
.
dx
(k)
∂y
(k)
∂x
(k) dx(k) ∂y(k)∂x(k)
i r=1 r i
Next, we realize that the output of layer (k−1)serves as the input of layer k. With
.
this we are now able to rewrite Eq.18.37 as
dJ ∂J ∂y(k)
= , (18.38)
. dy(k−1) ∂y(k)∂x(k)
which also very clearly shows where “back-propagation” got its name from: we
start from the end of the network and iteratively compute the derivative of the cost
function w.r.t. the weights for layer k − 1 using the already known value from
.
the previous layer k. Thereby, the change of the overall error is moving backward
through all layers.
For the following derivations, we again differentiate between a fully connected
layer and an activation layer.
Fully Connected Layer
First, we continue with the previous expression, Eq.18.38. For a fully connected
layer, we can specify ∂y/∂x
.
dJ ∂J
= W(k) . (18.39)
. dy(k−1) ∂y(k)
Next, the second term in Eq.18.35 still requires to be determined. We start from
explicitly rewriting them in terms of the components of Eq.18.29:
(k) (cid:3) (cid:4)
∂y ∂
1 = x (k) w (k)+x (k) w (k)+···+x(k)w (k)+b (k)
. (k) (k) 1 11 2 21 n n1 1
∂w ∂w
ij ij
(k) (cid:3) (cid:4)
∂y ∂
2 = x (k) w (k)+x (k) w (k)+···+x(k)w (k)+b (k)
(k) (k) 1 12 2 22 n n2 2
∂w ∂w
ij ij
.
.
.
(cid:3) (cid:4)
(k)
∂y ∂
m = x (k) w (k)+x (k) w (k)+···+x(k)w(k)+b(k) . (18.40)
(k) (k) 1 1m 2 2m n nm m
∂w ∂w
ij ij
All derivatives are zero except for the only term containing a w :
. ij


================================================================================
PAGE 528
================================================================================

18.4 GeneralFormulationofBackpropagation 515
(k) (cid:3) (cid:4)
∂y ∂
j = x (k) w (k) =x (k) (18.41)
. (k) (k) i ij i
∂w ∂w
ij ij
which is the expression that contains the j-th component of y. Hence, we can
simplify Eq.18.35 to
dJ ∂J
= x (k) , (18.42)
. (k) (k) i
dw ∂y
ij j
which can also be written as the outer product of two vectors
(cid:3) (cid:4)
dJ ∂J T ∂J
=x(k)⊗ ≡ x(k) , (18.43)
. dW(k) ∂y(k) ∂y(k)
where the last term rewrites the outer product symbol ⊗as the product of a column
.
vector with a row vector, resulting in a matrix. The derivation for the derivative w.r.t.
the bias follows nearly the identical path, and we end up with
(k)
∂y
j =1, (18.44)
. (k)
∂b
j
such that the sensitivity of the cost function w.r.t. the bias is given by
dJ ∂J
= . (18.45)
. db(k) ∂y(k)
We can now summarize all details for a back-propagation step for a fully
connected layer in Algorithm 18.1:
Algorithm 18.1: Backpropagation Step of a Fully Connected Layer (cid:2)
The following are the required steps for backpropagation:
1. If the layer.k+1is the final, output layer, compute
∂J 1
. =− (ytrain−y); (18.46)
∂y m
otherwise just use the
value.∂J/∂y(k+1)from
the previous layer in step 2.
2. Propagate the error from layer.k+1to layer k:
dJ ∂J
. dy(k) = ∂y(k+1) W(k). (18.47)


================================================================================
PAGE 529
================================================================================

516 18 AGentleIntroductiontoDeepLearning
3. Computethesensitivityofthecostw.r.t.theweightsandbiasvaluesofthecurrent
layer
dJ ∂J dJ ∂J
. =x(k)⊗ and = (18.48)
dW(k) ∂y(k) db(k) ∂y(k)
4. Update the weights and biases using the GD methods
η dJ η dJ
.W(k)⇐W(k)− and b(k)⇐b(k)− (18.49)
N dW(k) N db(k)
with the learning rate.η and N the number of data records.
5. Continue to the next layer,.(k)⇔(k−1).
Activation Layer
The back-propagation of the errors for one activation layer is significantly easier.
We again start from Eq.18.38 for which we need to specify ∂y/∂x. Recall the
.
definition of the forward pass of the activation layer in Eq.18.30, which states that
the output is obtained from applying the activation function on each of the entries
of x individually. The derivative then reads
.
dJ ∂J ∂ϕ(x(k))
= (18.50)
. dy(k−1) ∂y(k) ∂x(k) .
(cid:16) (cid:17)
= ∂J ∂ϕ(x 1 ( k ) ) , ··· , ∂J ∂ϕ(xn ( k ) ) = ∂J . (18.51)
∂y
1
(k ) ∂x
1
( k ) ∂yn (k ) ∂xn ( k) ∂y(k)
Thus, the sensitivity of the error w.r.t. the output is not changed by the activation
layer. Note that the product in Eq.18.50 is the element-wise (or HADAMARD)
product as both terms result in row vectors. Nothing else needs to be done here
as there are no trainable weights or biases.
18.5 Python Implementation and Example for the Fully
Connected Network
The algorithm and the data structure for this network are already fairly complex.
As a consequence, the Python code is also getting more lengthy and requires
vectorization of the numpy operations. Therefore, we are approaching a point
at which showing code and commenting it are no longer feasible in this text.
Nonetheless, we will do this for the fully connected network, and, based on
this, explain the design strategy and the implementation details. For the Python


================================================================================
PAGE 530
================================================================================

18.5 PythonImplementationandExamplefortheFullyConnectedNetwork 517
implementation of all subsequent network architectures and the corresponding DL
experiments, we refer the reader to the accompanying website.
18.5.1 Python Implementation
The Python implementation can be best done in an object-oriented approach using
classes and inheritance,2 because each activation layer of a network has always
the same structure and needs to perform the same computations as derived above.
Additionally, different layers must be able to keep track of their “state,” governed
by the input values and, in the case of a fully connected layer, also of the values
of weights and biases. Thus, each fully connected layer is an instantiation of the
same class. The same also holds for the activation layer. Since we want to ensure
that all layers should have the same “structure,” in terms of input and output of the
functions (or rather methods, as it is called in the context of a class), we create an
abstract base class from which both the activation layer and the fully connected
layer are derived:
In [1]: from abc import ABC # For implementing an abstract base class (ABC)
import numpy as np
class NNLayer(ABC):
def __init__(self):
self.x = np.empty()
self.y = np.empty()
def feed_forward(self, x):
raise NotImplementedError("Any layer needs to implement the \
feed_forward method.")
def backward_propagation(self, dJdy, learning_rate):
raise NotImplementedError("Any layer needs to implement the \
backward_propagation method.")
Thereby, we ensure that any derived class implements the feed-forward method and
the backward propagation method.
We now begin with the concrete implementation of the fully connected layer,
which we will explain in two steps. FullyConnectedLayer derived from the abstract
base class NNlayer and therefore inherits all attributes and methods. The __init__
method initializes the random number generator using the integer value seed as a
seed. It also initializes the weights and biases with random values. The “state” of
the class (i.e., the member variables x and y ) is set during the forward pass. Note,
that all numpy operations are vectorized and need to have correct dimensions; for
brevity, we do not ensure this.
2 We are fully aware that not everyone is familiar with object-oriented programming, and, so far,
we entirely avoided it. However, the object-oriented code is, in our opinion, much more readable
than a code written with functional programming. For those who are looking for an introduction,
[3] is one out of many good introductions.


================================================================================
PAGE 531
================================================================================

518 18 AGentleIntroductiontoDeepLearning
In [2]: class FullyConnectedLayer(NNLayer):
def __init__(self, n_inputs, n_outputs, seed=None):
rng = np.random.default_rng(seed)
self.weights = rng.random(size=(n_inputs, n_outputs)) - 0.5
self.biases = rng.random(size=(1, n_outputs)) - 0.5
def feed_forward(self, x):
self.x = np.atleast_2d(x)
self.y = np.dot(self.x, self.weights) + self.biases
return self.y
def backward_propagation(self, dJdy, learning_rate): ...
The implementation of the backward propagation is shown in the next cell. The class
method implements the sensitivity from step 3 as well as the update step 4 from the
above algorithm Algorithm 18.1. If we would not implement these methods, the
corresponding methods of the base class (cf. In [1]: ) would be called. This causes
an exception to be raised and the program stops.
In [2]: class FullyConnectedLayer(NNLayer):
def __init__(self, n_inputs, n_outputs, seed=None): ...
def feed_forward(self, x): ...
def backward_propagation(self, dJdy, learning_rate):
dJdW = np.dot(self.x.T, dJdy)
dJdb = np.dot(np.ones(self.x.shape[0]), dJdy)
self.weights -= learning_rate * dJdW
self.biases -= learning_rate * dJdb
dJdy_prev = np.dot(dJdy, self.weights.T)
return dJdy_prev
As a preparation for the activation layer, we now define a activation function and
its derivative.
In [3]: def tanh_function(x):
return np.tanh(x)
def tanh_derivative(x):
return 1 - np.tanh(x) ** 2
We are now able to implement the activation layer, which is again derived from
the abstract base class NNLayer . The specific activation function is hard-coded in
the __init__ method, because all activation layers should use the same activation
function. The feed-forward and the backward propagation methods follow the
mathematical form.


================================================================================
PAGE 532
================================================================================

18.5 PythonImplementationandExamplefortheFullyConnectedNetwork 519
In [4]: class ActivationLayer(NNLayer):
def __init__(self):
self.phi = tanh_function
self.dphidx = tanh_derivative
def feed_forward(self, x):
self.x = x
self.y = self.phi(self.x)
return self.y
def backward_propagation(self, dJdy, learning_rate):
return self.dphidx(self.x) * dJdy
For later use during the training, we now implement the cost function and its
derivative based on the MSE.3
In [5]: def MSE(y_true, y_pred):
return np.mean((y_true - y_pred) ** 2)
def dMSE_dy(y_true, y_pred):
return 2 / y_pred.size * (y_pred - y_true)
We are now ready to implement the Network class, which contains functionality
for assembling the network, for training, and for predicting. Note that the following
three code snippets always show the same class but hide the methods that are not
being explained, indicated by the three dots. We start by storing a reference to
the cost function and its derivative, which are to be defined during creation of the
network. All layers are simply stored in a list by the method add_layer .
In [6]: class Network:
def __init__(self, cost_function, derivative_of_cost):
self.layers = []
self.cost_function = cost_function
self.derivative_of_cost = derivative_of_cost
def add_layer(self, layer):
self.layers.append(layer)
def train(self, X_train, Y_train, epochs, learning_rate): ...
def predict(self, X): ...
The method that does the actual training needs some more explanations. The
training method takes the training dataset, the number of training epochs (i.e., the
number of iterations), and the learning rate. During one epoch, it iterates through
the whole training dataset and for each pair x_train, y_train , it performs the
forward propagation through all layers. The line y_pred = x_train might need a
short explanation: in the following for loop, the forward propagation is done by
using the output of the previous as the input of the current layer. This procedure
3 Sometimes, a factor of 2 be found in the MSE. However, this factor does not affect the training
behavior if the learning rate is scaled accordingly.


================================================================================
PAGE 533
================================================================================

520 18 AGentleIntroductiontoDeepLearning
is sequentially done using the variable y_pred , which is why y_pred needs to be
properly initialized with the output of the “previous” layer, the inputs x_train. At
the end of this sequence, we obtain the cost function for the prediction of the output
layer.
Next, the errors are back-propagated through all layers, which adjusts all weights
and biases (see the back-propagation method in the above code cell “ In [2]:'' ). At
the end of each epoch, the value of the cost function is stored in the list all_costs ,
which will be returned at the end of the training. It then can be used, e.g., for plotting
the training error.
In [6]: class Network:
def __init__(self, cost_function, derivative_of_cost): ...
def add_layer(self, layer): ...
def train(self, X_train, Y_train, epochs, learning_rate):
all_costs = []
for i in range(epochs):
cost = 0
for x_train, y_train in zip(X_train, Y_train):
# compute prediction through forward propagations
y_pred = x_train
for layer in self.layers:
y_pred = layer.feed_forward(y_pred)
cost += self.cost_function(y_train, y_pred)
# perform backprop of errors
error = self.derivative_of_cost(y_train, y_pred)
for layer in reversed(self.layers):
error = layer.backward_propagation(error,
learning_rate)
all_costs.append(cost / X_train.shape[0])
return all_costs
def predict(self, X): ...
In this part of the code, the variable all_costs is a list of the cost values for each
epoch that can then be used for plotting. Since the training sometimes takes longer,
it can be useful to get some feedback about the progress. The Python package
tqdm is very useful for this purpose. For example, in a jupyter notebook , after
importing tqdm with from tqdm.notebook import trange , you can replace range
with trange and then will see a continuously updated progress bar.
The last method of the Network class is used for making predictions. It takes a
feature matrix which, as usual, has records in rows, and each column is a feature.
We iterate over all records and for each of them compute the output by a complete
forward pass. For consistency, the resulting prediction is returned as a numpy array
and not as a list.


================================================================================
PAGE 534
================================================================================

18.5 PythonImplementationandExamplefortheFullyConnectedNetwork 521
In [6]: class Network:
def __init__(self, cost_function, derivative_of_cost): ...
def add_layer(self, layer): ...
def train(self, X_train, Y_train, epochs, learning_rate): ...
def predict(self, X):
Y = []
for x in X:
y = x
for layer in self.layers:
y = layer.feed_forward(y)
Y.append(y)
return np.array(Y)
With this, our implementation is finished, and we can perform a short test.
18.5.2 Application: Identification of Handwritten Digits
We use the dataset of handwritten digits, DS-2-small (UCI Handwritten Digits), and
seek to find the labels 0...9foreach image in an supervised learning approach. We
start by importing the data. Note that the dataset already contains the “flattened”
images, i.e., each N ×N image array was converted into a long one-dimensional
.
array of
.
N×N elements. We then perform one-hot encoding (OHE) (cf. Sect.16.1.1)
such that we can have an output layer with 10 units. The dataset is then split into a
training and a testing portion with the function introduced in Sect.16.2.2.
In [7]: from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data / 16. # Scale the feature value range to [0, 1]
# Target variable: One-hot-encoding: map each number of the interval [0,9]
# 9 zeros and a one, such that, e.g., 2 is mapped to [0, 0, 1, 0, 0, 0, 0
Y = digits.target
Y = np.eye(10, dtype=int)[Y]
X_train, Y_train, X_test, Y_test = \
train_test_split(X, Y, fraction=0.7, seed=None)
We can now assemble the network. We chose an architecture consisting of as
many input nodes as there are features, which is why in the code we use
X_train.shape[-1] . These are 64 nodes, since the images contain . 8×8=64pixel.
The hidden layer has 80 units. The next fully connected layer reduces the number
of nodes to 40, the next but one reduced them to 10, which are the number of output
nodes, consistent with the one-hot encoding. Finally, we train the model for 200
epochs and with a learning rate of 0.1. These are values that were experimentally
found. A good strategy is to start with a few epochs and a learning rate in between
0.001 and 0.1. If the resulting MSE (shown as output of the last cell) does not blow
up, then one could increase the number of epochs or increase the learning rate.


================================================================================
PAGE 535
================================================================================

522 18 AGentleIntroductiontoDeepLearning
Fig. 18.5 Classification of the handwritten digits dataset using the implementation of the fully
connected network. The left panel shows the training and testing error, the right panel shows the
confusion matrix
In [8]: nn = Network(MSE, dMSE_dy)
nn.add_layer(FullyConnectedLayer(X_train.shape[-1], 80))
nn.add_layer(ActivationLayer())
nn.add_layer(FullyConnectedLayer(80, 40))
nn.add_layer(ActivationLayer())
nn.add_layer(FullyConnectedLayer(40, 10))
nn.add_layer(ActivationLayer())
train_mse = nn.train(X_train, Y_train, epochs=200, learning_rate=0.01)
train_mse[0], train_mse[-1]
Out [8]: (0.35482118936344176, 0.0018668214460151819)
There are (at least) two very useful methods to investigate the performance of
the training. The first is to simply plot the training and testing error, as shown in
Fig.18.5.
It is important to ascertain that not only the training error decreases but also
that the testing error decreases. With the above implementation, only the training
error can directly be plotted. The testing MSE is obtained by using the weights and
biases of each epoch for making predictions on the testing dataset. This is left as an
exercise.
Creating the confusion matrix comes down to counting those digits that got
correctly predicted. In the next Python cell, yp is an array of 10 entries (one for
each digit, i.e., output node). Finding the one that has the largest value gives us the
respective digit. This is done, using the np.argmax function.
In [9]: confusion_matrix = np.zeros((10, 10), dtype=int)
for yt, yp in zip(Y_test, Y_pred):
pred_digit = np.argmax(yp)
true_digit = np.argmax(yt)
confusion_matrix[true_digit, pred_digit] += 1


================================================================================
PAGE 536
================================================================================

18.6 FurtherConceptsandTechniques 523
We can now plot the confusion matrix, as shown in the next Python cell, where we
used imshow to plot the two-dimensional . 10×10array.
In [10]: fig, ax = plt.subplots()
im = ax.imshow(confusion_matrix, cmap='magma_r', origin='lower')
plt.colorbar(im)
ax.set(xlabel='predicted digit', ylabel='true digit',
xticks=range(10), yticks=range(10));
We observe that both the train as well as the test loss decrease quite smoothly. The
test loss is only slightly higher than the train loss. Both losses continue to very
slowly decrease. The classification accuracy (for this particular random split into
training and testing data) is after 200 epochs already at 96.5%, and this doesn’t
.
increase significantly even if we train for another 1000 epochs. The confusion matrix
also shows a very good pattern, and only few images are misclassified. We observe
that the two most misclassified digits are the 4 (which is identified as a 9) and the
8 (which is identified as a 1 but also as a 4 or 5). Quite surprising, on a first look,
this agree only partially with the results from the principal component analysis in
Fig.15.9. However, the dataset investigated there was the full MNIST dataset DS-2
(MNIST) and not the DS-2-small (UCI Handwritten Digits) used here. Furthermore,
there are only very few images misclassified at all, and these might be among the
extreme values, hidden behind other data points in Fig.15.9. There is an exercise
that gives some hint on how to investigate this observation, cf. Exercise 18.11.
Machine Learning vs. Deep Learning (cid:2)
The above example also demonstrates a key difference between DL and
classical ML: while in classical (or statistical) ML we need features, DL does
not require features. We could simply use every pixel of a whole image as the
input.
18.6 Further Concepts and Techniques
The fully connected neural network introduced above is already quite powerful and
can perform complex tasks that would be difficult to achieve with classical ML
methods. Nonetheless, there are a number of measures that can be implemented,
in order to enhance the performance. We will now introduce some of them briefly.
18.6.1 Mini-Batches
There are two obvious ways for calculating the loss: we can either calculate it for
one data record, or we could calculate it for the whole dataset. The former would


================================================================================
PAGE 537
================================================================================

524 18 AGentleIntroductiontoDeepLearning
not help to improve the performance w.r.t. the whole dataset and would result in
very unstable gradients. The latter may take a long time and require a huge amount
of memory. A min-batch is a compromise between the two extremes. A mini-batch
contains only a subset of the whole training data, with the number of contained
data records called the batch size. The batch size if a hyperparameter to the model.
During training, all mini-batches are then processed, until the whole training dataset
is covered. The step-by-step algorithm is shown in Algorithm 18.2.
Algorithm 18.2: Training with Mini-batches (cid:2)
For the use of mini-batches, the following steps need to be taken:
1. shuffle the training dataset
2. split the training dataset such that each “fold” has as many records as given by the
mini-batch size.
3. pick the first mini-batch
4. do a forward and backward pass simultaneously with all data of the current mini-
batch
5. sum up the average contribution of the training cost for the mini-batch
6. choose the next mini-batch and go to step 4 until all mini-batches are processed.
Choosing a good batch size is essential for an efficient and accurate training
process. This is another parameter that we, as the DL modeler, need to fix.
Exercise 18.7 considers this aspect further.
18.6.2 Batch Normalization
When mini-batches are used, each of them is described by different summary
statistics. Batch normalization is a method that centers and normalizes the data
of a mini-batch and does so for each (or at least most of the) layers [4]. This
is done just before the input is given to the activation functions, which seems to
reduce fluctuations in the hidden layers activations. In practice, batch normalization
is added in form of an extra layer. By centering and normalizing, we denote the
transformation,
xˆ(k) =
x(k)−μB
, (18.52)
.
σB
where
.
B denotes the minibatch under consideration and
.
μB and
.
σB are the
respective values of the mean and standard deviation.
However, such a normalization would imply that from a sigmoidal activation
function only the linear regime would be used. Thus, two additional parameters are


================================================================================
PAGE 538
================================================================================

18.6 FurtherConceptsandTechniques 525
introduced for each layer k, the scaleγ(k)and the shift of the normalized vectorβ(k)
. .
with
x˜(k) =γ(k)xˆ(k)+β(k) . (18.53)
.
The two new parameter are not minibatch-specific and are learned along with
the other weights and biases of the model. This transformation even allows to
approximately undo the centering and normalization, if this would be the optimal
approach. However, the new parameter are directly able to “tune” the variance and
the mean, which are parameters that are otherwise only indirectly accessible by the
network. This can significantly change the training behavior.
While there are still some speculations about the mathematical details of why
this works, there is significant evidence that batch normalization strongly improves
the performance of a network. In particular, the learning rate can be chosen much
more generous.
18.6.3 Weight Initialization
So far, we have always initialized the weights with small random numbers, e.g., in
the above example, taken from a uniform distribution with a range of [−0.5,0.5].
.
We have already seen in Sect.12.3.7 that during a minimization, such as (stochastic)
gradient descent, the initial values can make a difference. In a deep neural net, the
initialization of the weights may have a different influence depending on which kind
of activation function is used. This also was recognized by XAVIER GLOROT and
coworker [1], who suggested to scale the uniform distributions when using a linear
activation. That strategy is known as the Xavier initialization for which the weights
w are sampled from the following symmetric distribution:
(cid:18) √ √ (cid:19)
w ∼U −1/ n, +1/ n , (18.54)
.
where n is the number of input nodes and U(a,b)denotes the uniform distribution
.
within the limits a and b. As a consequence of the dependency on the number of
input nodes, the range of the random values is larger for smaller n. Figure 18.6
shows the distribution of random samples as a function of the number of input nodes.
A variant of this initialization is the normalized Xavier initialization , which
additionally takes the number of outputs m into account. The number of outputs
are usually identical to the number of units in the layer under consideration. Then,
the weights w are sampled from the following distribution:
(cid:3) √ √ √ √ (cid:4)
w ∼U − 6/ n+m, + 6/ n+m . (18.55)
.
The two variants of the Xavier initialization work well for sigmoidal and tanh
.
activation functions (even though, the former is only rarely used, today).


================================================================================
PAGE 539
================================================================================

526 18 AGentleIntroductiontoDeepLearning
Fig. 18.6 Xavier
initialization: the range of the
random uniform distribution
is chosen according to the
number of input nodes. For
each value of n, 40 random
values are drawn
Fig. 18.7 He initialization:
as opposed to the Xavier
initialization, here a normal
distribution is chosen to be
sampled from. For each value
of n, 100 random values are
drawn
Words of advice (cid:3)
The two variants of the Xavier initialization do not work well together with the ReLU
acitvation.
If the ReLU activation function is used, then the alternative formulation,
introduced by KAIMING HE [2] is preferred. In this case, the random weight values
are drawn from a centered normal distribution:
(cid:20)
w ∼N(μ=0,σ= 2/n) (18.56)
.
Figure 18.7 shows the distribution of random weight values as a function of the
number of input nodes.
Note, that the range of the weight values for small values of n is much larger than
for the Xavier initialization. For larger n, the range of the values stays approximately
constant (at least over a reasonable numbers of input nodes).


================================================================================
PAGE 540
================================================================================

18.7 LessIsMore:TheConceptofDropout 527
18.7 Less Is More: The Concept of Dropout
A serious problem of networks with large numbers of weights is overfitting. This
applies in particular to fully connected networks, which, by construction, contain a
maximum number of possible weights and thereby tend to be slow during training.
Dropout is a technique for remedying this, first published in 2014 by Srivastava
et al. [9]. The idea of dropout is to randomly drop nodes from the network and also
to remove the respective connections. Such a kind of training is then performed
for a number of “thinned” networks, each of which uses a random dropout. During
testing, a fully connected network is used that represents the average of the trained
networks (according to the probability with which a unit is found in one of the
thinned realizations). Figure 18.8 shows a sketch of a network with two hidden
layers.
The original publication [9] proposed to additionally include the units of the input
layer in the set of those that can be randomly removed (they proposed a retention rate
of 50% for the units in hidden layers and 80% (i.e. only every fifth unit is dropped)
for those in the input layers). There are, however, situations where thinning the input
layer might not be the most appropriate approach. For example, if the input consists
of one-hot encoded categorical data, in cases where the number of inputs are very
small, or if each input carries a unique and significant amount of information, one
might want to keep all input nodes.
Why is the dropout technique useful? The quick answer is that it adds a new
type of randomness to the training process, which is, generally speaking, always
beneficial against overfitting. Part of the explanation is also that removing random
units and using such network realizations in an ensemble, acts as a stochastic
regularization. On a technical level, dropout is implemented by sampling, e.g., from
aBERNOULLI distribution (cf. Sect.10.3) and using the result to determine if a unit
is taken into account during forward propagation. E.g., for layer k and the i-th unit
r (k) ∼Bernoulli(p) (18.57)
. i .
Fig. 18.8 “Drop out” transforms a fully connected network into a network where not all nodes
are fully connected. This is achieved by randomly removing nodes from the network. Here, this
was only applied to the hidden layers


================================================================================
PAGE 541
================================================================================

528 18 AGentleIntroductiontoDeepLearning
x˜(k) = r(k)(cid:12)x(k) (18.58)
.
yk+1 = w (k+1) · x˜(k) + b (k+1) (18.59)
i i i
where the parameter p of the BERNOULLI distribution is chosen according to the
(k)
desired retention rate. Thus,r effectively “switches” units on or off, ensuring that
. i
the desired percentage of units is disabled. The thinned output of this layer is used
as input for the following layer. Effectively, the dropout network is one of the 2n
.
possible “sample” of the fully connected network, with n being the number of units.
Ideally, one would then like to obtain a sample average of all 2n networks. As this
.
is usually unfeasible due to computational cost, the solution is that during testing,
each weight obtained during training, is scaled by the probability p, and only a
single fully connected network is used.
In terms of an implementation, taking out individual units amounts to removing
the respective rows and columns from the linear systems of equations. This may
significantly reduce the size of the minimization problem. Furthermore, all the
other techniques used in “regular” networks, such as momentum and weight decay,
seem to work nicely for dropout networks as well. However, keep in mind that this
requires that the network is adjusted during the testing phase.
Therefore, for implementation reasons, often a different type of dropout is used,
the inverted dropout. There, the scaling by the probability is changed to a scaling
during the training, but with the inverse probability 1/p.
.
When the dropout strategy was developed around 2012, it gave the development
and the performance of DL method a significant boost; it is one of the major
contributors that helped DL to become able to solve highly complex problems,
sometimes even with super-human performance.
18.8 Example: Microstructure Classification and Property
Prediction
As an example, we use the MDS-2 light (Ising Model, small images) dataset for
the task of predicting if an image with a particular microstructure corresponds to a
temperature that is above or below the Curie temperature (cf. Sect.4.3 for further
details). For this, we define a network that consists of a fully connected layer that
takes the input (the image in form of a “flattened” vector), which is reduced to 128
output nodes. The following layer has 128 input and 64 output nodes, 64 and 32
input/output nodes, and finally 32 input and 2 output nodes that give the class label
of the image for the one-hot encoding. Thus, the model has altogether approximately
43,000 weights to be determined. Figure 18.9 shows the model performance.
We observe that the model reaches a prediction accuracy of around 80%. Com-
paring this to the baseline of ≈50% for a binary classification with approximately
.
balanced classes (cf. Sect.16.3), this represents a good (but not a great) accuracy.


================================================================================
PAGE 542
================================================================================

18.8 Example:MicrostructureClassificationandPropertyPrediction 529
Fig. 18.9 Classification of the Ising microstructures w.r.t. to temperature. The left panel shows
the training and testing errors, the right panel shows the confusion matrix
From the confusion matrix, we can see that the model has more difficulties to
predict microstructures that were obtained at higher temperatures: 136 examples
out of 522 examples for microstructures above the Curie temperature were wrongly
classified. Note that due to the random split into training and testing data, there
is a small class imbalance, which is why we should not directly compare the
number of misclassified images and instead should use relative numbers: for the
“high temperature” data, 25% of the images were misclassified, while for the “low
temperature” data only 15.6% were misclassified.
To analyze if there specific temperature values for which microstructures are
more prone to misclassifications than others, we could turn the two-class classi-
fication problem into a multi-class classification problem, e.g., using 16 classes
of temperature ranges. Obviously, one could even increase the number of classes
such that in the limit case, we use the classifer for solving a regression problem.
Alternatively, we can also directly turn this into a regression problem. The network
architecture requires then that the output layer contains only a single unit. Addition-
ally, the labels of the data records are numeric labels containing the temperature.
Figure 18.10 shows the results after training.
Ideally, all data points should be located on the dark, solid line. To quantify how
well the data fits to this line, we calculate the coefficient of determinationR2and the
.
correlation coefficient corr, compare Chap.9. We find R2 ≈ 0.28 where a perfect
. .
fit would be denoted by 1.0. Furthermore, it is corr ≈ 0.64 which according to
. .
Sect.8.7.3 denotes a positive correlation that is in between moderate and strong.
It is interesting to observe that the microstructure examples that correspond to the
Curie temperature are those that can be easiest predicted. Both implementations
show good results from which it is obvious that the network learned something
about the microstructures. Using more advanced network architectures, some of
which will be introduced in the next chapter, however, can drastically increase the
predictive capabilities. A systematic study where the performance of a range of
different models is investigated can be found in NGUYEN et al. [5].


================================================================================
PAGE 543
================================================================================

530 18 AGentleIntroductiontoDeepLearning
Fig. 18.10 The Ising dataset as a regression problem. The left panel shows the training and testing
errors, the right panel shows the predicted vs the true temperature for the testing dataset
18.9 Exercises
18.1. In Sect.18.3.3, we implemented a first model that performs backpropagation.
In this exercise, you should implement the tracking of the mean testing loss and
reproduce the evolution of the train/test losses from Fig.18.3. Hint: in the for loop
over all epochs, use the current weights and bias values to predict the labels for
training data. Use the previously introduced function for splitting the dataset into
training and testing data.
18.2. The derivation and Python implementation from Sect.18.3.3 uses three
biases.
(a) Is the third bias really needed or can this be “taken over” by the hidden units?
Do an experiment with the code and discuss the results.
(b) How important is the type of initialization of the weights and biases? Perform
an experiment with the Python code.
18.3. Use the code of the fully connected network to create a network architecture
with which the XOR problem can be solved. The XOR function takes two input
values, each of which is either 0 or 1. If both inputs have the same value, then the
XOR function yields zero; if the inputs have different values (0 and 1, or 1 and 0),
the output is 1.
Which is a “minimal network architecture”, e.g., in terms of number of layers
and number of nodes, that still gives the desired result?
18.4. Compare the sigmoidal activation functions with the tanh and the ReLU
activation function. Use the code for a fully connected network from the companion
website, pick a dataset of your choice, and compare their performance. Also,
measure the compute time required for evaluating the activation functions for a


================================================================================
PAGE 544
================================================================================

18.9 Exercises 531
single number and for an array of 100,000 elements. Which activation function is
preferrable?
18.5. Extend the code for a fully connected network used in Sect.18.5.2 and
perform some experiments.
(a) Extend the code such that during the training also the test cost (MSE) is
computed and recorded.
Hint: In the method train of the class Network add your code after back-
propagation of the errors. The training cost is then computed with the current
values of the weights and biases, based on the testing dataset.
(b) What do you observe for the training and testing MSE when you increase the
number of epochs or/and the learning rate? Discuss the extreme parameter
combinations.
(c) Can you manage to bring the model to a point where it overfits?
18.6. Use the code of the fully connected network from Sect.18.5.2 to investigate
the influence of different activation functions. In particular, compare the perfor-
mance and discuss the behavior of the (i) sigmoidal, (ii) tanh, (iii) ReLU, and (iv) a
leaky ReLU function. Which one is the best performing function for the investigated
problem? For (iv) try different leakage values.
18.7. Extend the code for a fully connected network used in Sect.18.5.2 such that
the training is done using mini-batches. Compare this implementation with the one
without mini-batches.
(a) How do they perform in terms of stability or efficiency?
(b) What is a good batch-size and what is the difference between a too large or too
small mini-batch size?
18.8. Use the code from Sect.18.5.2 and the extended version from Exercise 18.6
and answer the following questions:
(a) What is the influence of the learning rate on the performances of the network
concerning the training/testing data?
(b) What is the impact of the number of epochs on the performances? (Hint:
Investigate the training in terms of overfitting)
(c) Does increasing the size of the network (i.e., the number of layers and the
number of nodes per layers) lead to better performances? Discuss it with an
example
18.9. In Sect.18.6.3, we saw how weights can or should be initialized. Try different
initialization methods (uniform, normal, constant, Xavier, He) and discuss the
results and performances.


================================================================================
PAGE 545
================================================================================

532 18 AGentleIntroductiontoDeepLearning
18.10. We saw that a neural network can solve the XOR problem. Can a neural
network solve any combination of logical gate? Try this out for a number of
combinations of different logical (Boolean) elements. What would be the condition
to ensure that there exists a solution?
18.11. Investigate the “most confused” images further. Identify from the confusion
matrix those images that get most often confused with a different digit. Locate these
images and try to understand why they are misclassified the way how they are.
Bonus task: perform a principal component analysis on the same dataset as used
here and interpret your findings in view of these results.
18.12. Use the code of the fully connected network from Sect.18.5.2, and train it on
the dataset DS-2-small (UCI Handwritten Digits). How do the predictions change if
◦
a new image is rotated by multiples of 90 ? Translational invariance is more difficult
to be investigated since the image are very small and most of the digits fill the whole
image. You can still investigate this for one of the more narrow digits, e.g., for a “1”.
What do you observe?
References
1. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Y. W. Teh and M. Titterington, editors, Proceedings of the Thirteenth Interna-
tional Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine
Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010.
PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html.
2. K. He, X. Zhang, S. Ren and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification. IEEE International Conference on Computer Vision
(ICCV), Santiago, Chile, 2015, pp. 1026–1034, 2015. DOI https://doi.org/10.1109/ICCV.2015.
123.
3. C. Hill. Learning Scientific Programming with Python. Cambridge University Press, 2nd
edition. ISBN 978-1108745918.
4. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift, 2015.
5. B. D. Nguyen, P. Potapenko, A. Dermici, K. Govind, and S. Sandfeld. Forward modeling of
materials science simulations: learning the microstructure-property relation. under review.
6. F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization
in the brain. Psychological Review, 65(6):386–408, 1958. DOI https://doi.org/10.1037/
h0042519.
7. F. Rosenblatt. Principles of neurodynamics: Perceptrons and the theory of brain mechanisms,
1962.
8. D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating
errors. Nature 323:533–536, 1986. DOI https://doi.org/10.1038/323533a0
9. N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research,
15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.
10. P. J. Werbos. Beyond regression—new tools for prediction and analysis in the behavioral
sciences, 1974.
11. P. J. Werbos. The Roots of Backpropagation: From Ordered Derivatives to Neural Networks
and Political Forecasting. Wiley-Interscience, USA, 1994. ISBN 0471598976.
12. B. Yuen, M. T. Hoang, X. Dong, and T. Lu. Universal activation function for machine learning.
Scientific Reports, 11(1), Sept. 2021. DOI https://doi.org/10.1038/s41598-021-96723-8.


================================================================================
PAGE 546
================================================================================

Advanced Deep Learning Architectures and 19
Techniques
Any sufficiently advanced technology is indistinguishable from
magic.
— Arthur C. Clarke (1917–2008),
English Science Fiction Writer
19.1 Convolutional Neural Networks
Convolutional Neural Networks (CNNs) have become one of the work horses of
microscopy image analysis in materials science and engineering. In the following
subsections we will go through all required mathematical and conceptual ingredi-
ents.
19.1.1 Motivation of the Architecture
If you just finished the previous chapter, then you might get the idea that Artificial
Neural Networks (ANNs) have made a great leap forward and that many more com-
plex application cases are within reach. While this holds true for some classification
and regression tasks, which otherwise would have been tackled by a combination
of feature engineering and classical machine learning (ML), unfortunately, this is
only partially true, when it comes to general image data. The back-of-the-envelope
calculation in Example 19.1 demonstrates this.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 533
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9_19


================================================================================
PAGE 547
================================================================================

534 19 AdvancedDeepLearningArchitecturesandTechniques
Example 19.1 (The number of Weights in a Fully Connected Network)(cid:2)
Imagine the example images from dataset “DS-2-light (Alpaydin Digits).”
They have8×8=64pixels. Now, imagine a fully connected artificial neural
.
network (ANN) consisting of 64 input nodes, a fully connected layer with 80
input and 40 output nodes, and another one with 40 inputs and 10 outputs.
This amounts to 64×80+80×40+40×10=8,720weights.
.
Let us continue with the dataset “MDS-2 light (Ising Model, small)” where
the images have 16×16=256pixels and where for the binary classification
.
task the first layer has 128 outputs, the second has 64, a third has 32, and the
fourth output layer has 2. This amounts to256×128+128×64+64×32+
.
32×2=43,072weights.
Last but not least, imagine a small microscopy image on which we wish to
perform a binary classification task. The image has1024×1024=1,048,576
.
pixels. For an ANN with three hidden layers, each of which has 512 units, we
get a total number of1,048,576×512+512×512+512×2=537,134,080
.
weights, which, if using double precision floating point numbers, would
already require about 4GB of computer memory. Using RGB images with
.
three channels would be another factor of three. Additionally, an ANN with
three such layers would not be suitable because having an input size of about
a million nodes and an output size of only 512 is far from being optimal for a
network (see Exercise 19.1).
The memory requirement and therefore also the computational performance of
fully connected networks scales so badly that alternative solution approaches are
required. Additionally, ANNs are not translation invariant, and the spatial structure
of an image is lost when the two-dimensional image array is “flattened” to a
one-dimensional vector. This is a major problem as, typically, nearby pixels are
correlated. Hence, in fully connected ANNs, this information is lost.
CNNs are motivated by what was done in image analysis with filter functions.
There, the most important mathematical operation is the convolution of the image
with a “kernel” function. Additionally, CNNs also come with a number of new
techniques and layer types, all of which makes the CNN extremely efficient and
effective.
19.1.2 Convolutional Layer
The convolutional layer is a key ingredient for a CNN. Such a layer is based on the
mathematical concept of a convolution of two functions where one is “sliding” over
the other (which is called folding), similar to what happens when we calculate a
moving average.


================================================================================
PAGE 548
================================================================================

19.1 ConvolutionalNeuralNetworks 535
Definition 19.1 (Convolution of Two Functions) Assume two functions,
f(x)and g(x). Then, the convolution of f and g is defined by the integral
. .
(cid:2)+∞
(f ∗g)(x)= f(t)g(x− t)d t . (19.1)
.
−∞
The asterisk between f and g represents the convolution operation. The
convolution of two functions is a new function, (f ∗g)(x).
.
This definition can easily be transferred to the two-dimensional case.
Discrete Convolution
For convolutional layer, however, the two-dimensional image consists of pixel, and
therefore, we require a discrete convolution operation. In two dimensions and for an
input (or image) array I and a discrete kernel K, this is obtained for pixel number
. .
m,nas:
.
O=I∗K (19.2)
. .
(cid:3)+∞ (cid:3)+∞
⇔ O[m, n]= I[m+ i, n + j ]· K[i, j] , (19.3)
j=−∞ i=−∞
whereO[m,n]is the value in the resulting output array. The limits in the summation
.
are for a kernel that is infinite in all directions. Usually, the kernel has a limited
size such that the limits need to adjusted, accordingly. An example for a discrete
convolution is shown in Fig.19.1. The discrete kernel has 3×3elements, and the
.
above summations would be for all elements j and i ∈ [−1,0,+1]. To obtain all
.
values I∗K in Fig.19.1, the kernel w.r.t. I is moved to all possible locations, and,
. .
e.g., by moving the kernel to the leftmost and topmost position in I, performing the
.
respective multiplication and summation, the upper left element of Ois obtained.
.
Such convolution operations are used in image processing, e.g., to detect edges,
or to smooth an image; whether it is the former or the latter depends on the details of
the kernel. In classical image processing a kernel often has odd numbers of elements
along each direction. In deep learning (DL), however, this is different, and a kernel
also can have even numbers of elements along each direction. For example, the
operation shown in Fig.19.1 can also easily be performed using a 4×4kernel.
.
For the data structure that is used in CNNs, several dimensions have to be
considered, because an image (unless it is a grayscale image) consists of several
channels. For example, an RGB image has one channel for the red components,
one for the green, and another channel for the blue components, resulting in the
composite color image. Figure 19.2 shows convolution operations that result in a


================================================================================
PAGE 549
================================================================================

536 19 AdvancedDeepLearningArchitecturesandTechniques
Fig. 19.1 A discrete convolution operation between a two-dimensional matrix .I and a discrete
kernel .K . The mathematical convolution operation is indicated by a star (. ∗ ). Each element of the
two shaded.3×3matrices is multiplied, and all nine products are summed up, giving the highlighted
value in the right resultant matrix
Fig. 19.2 Convolution of an
image with.32×32pixels
and 3 channels with a
.5×5×3kernel which
results in a single “slice”
Fig. 19.3 Convolution of an
image with.32×32pixels
and 3 channels with a total of
six kernel functions that have
.5×5×3pixels in size,
resulting in six different
activation maps
M ×M ×1 layer. This requires a kernel function that has the same depth as the
.
image.
Figure 19.3 shows a different type of convolution operation that operates with
altogether six kernels, each of which reduces the image to a single layer and thus
results in a M ×M ×6layer.
.
Additionally, a dataset has many records. Thus, we have to operate with four-
dimensional arrays which, in the DL community, are often denoted as “tensors.”
For convolutional layers, there are two parameters that describe further details
of the convolution process: the padding and the stride, as introduced below. An
overview of “convolution arithmetic for deep learning” can be found in [8].


================================================================================
PAGE 550
================================================================================

19.1 ConvolutionalNeuralNetworks 537
Fig. 19.4 Ninety-six convolutional kernels learned by the first convolutional layer of the AlexNet
[23]. (With permission of the authors A. KRIZHEVSKY,I.SUTSKEVER and G. E. HINTON)
Learning
In convolutional layers, the weights are the discrete kernels; training is tantamount
with finding the values of the kernel such that the convolution process is able
to “filter out” the relevant features. Often, the kernels at the very beginning of a
network are responsible for the most generic features such as edges or dark spots
(also called the low-level features), while the further one gets to the end of the
network, the more concrete but also complex the kernels are getting (high-level
features), e.g., whole faces or complex compositions of different colors. An example
of a visualization of the trained kernels of the AlexNet (which we discuss further
below in Sect.19.1.6) is shown in Fig.19.4
Padding
Counting the number of elements of the matrices I and O we observe that O is
. . .
smaller thanI, exactly by the width or height of the kernel (minus the kernel center).
.
In case that there are a number of convolutional layers, the image shrinks after each
of these layers. This can be a desired effect—but most of the time this is not the
case. As another side effect, this also means that the corner elements of the input
Iare only represented once in O while all inner elements are represented as many
. .
times as there are elements in K.
.
Padding remedies this by adding an extra “frame” around I. The padding can
.
either be initialized to zero or it can be set to the value of the boundary elements (as
in a constant extrapolation).
Example 19.2 gives an example about dimensions of the padding, input, and
output image.
Example 19.2 (Padding) (cid:2)
If the original imageIhas8×8pixel, then by adding a padding of one element
. .
around it and using a3×3discrete kernel, we would get an outputOthat has
. .
the same dimensions as Iafter the convolution operation is performed.
.


================================================================================
PAGE 551
================================================================================

538 19 AdvancedDeepLearningArchitecturesandTechniques
In general, it is easy to see that to preserve the size of the input also as output, and
a kernel of2n+1×2n+1is used, then the padding must be of width n, resulting in
.
an input image that is 2n larger in each direction. This is also called same padding.
If no padding is used, this is called valid padding.
Stride
So far, the convolution kernel is moved across the input by steps of one pixel. If
every other (or more) pixel is skipped, then the output will get a smaller dimension.
The number of pixels to be skipped is governed by the parameter called stride.
Things to Remember About Convolution Layer (cid:3)
(cid:129) Convolution operations are nonlinear transformations and are therefore
capable of learning very complex aspects of an image.
(cid:129) Padding is used such that during the convolution the output does not
“shrink.”
(cid:129) If during shifting across the input image the convolution kernel should not
cover pixel multiple times, striding is used to repeatedly skip some pixel.
(cid:129) Stride effectively downsamples the input.
19.1.3 The Pooling Layer
The convolution can represent an arbitrarily complex transformation with an
increasing number of weights. The computational performance, on the other hand,
suffers from large numbers of to-be-trained weights. Pooling is an alternative to
using stride regarding downsampling of the input. It summarizes the content of a
number of neighboring pixel. Using only the maximum value is called max pooling,
averaging the values is called average pooling. Figure 19.5 shows a sketch of
pooling operations. Pooling typically is accompanied by a corresponding stride.
Even though this is not a necessity, it is often encountered because pooling is
used for reducing the number of weights. Max pooling has the advantage that it
preserves fluctuations and maximal signals. During pooling, some information is
lost, which can make it more difficult for the network to learn. If the benefits from
the reduced number of weights outweigh the disadvantage of lost information, it
is not always obvious and needs to be investigated for the respective problem at
hand. Pooling layers used to be very beneficial for computational reasons and in
times when the hard- and software were not so developed. However, today, their
importance is somewhat reduced, and there are even approaches where exclusively
the convolution layer is used.


================================================================================
PAGE 552
================================================================================

19.1 ConvolutionalNeuralNetworks 539
Fig. 19.5 Sketch of max
pooling and average pooling,
used together with a stride
value of two. The colored
squares on the left correspond
to the numbers on the same
colors on the right
19.1.4 The Receptive Field and Dilated Convolutions
The receptive field of a CNN describes the size of the region of the input that
influences a pixel in the output. For example, for object detection a receptive
field that is much smaller than the object may not be able to properly find the
object. ARAUJO et al. [1]“observe a logarithmic relationship between classification
accuracy and receptive field size, which suggests that large receptive fields are
necessary for high-level recognition tasks, but with diminishing rewards,” where
the authors investigated network performance based on classification accuracy of
ImageNet. The receptive field can be increased in an obvious manner by adding
more convolutional layers, i.e., by increasing the network depth or by adding
pooling layers or higher strides.
Another approach is to apply dilated convolutions. There, the convolution kernel
is reduced to only every rth row and column where r is called dilation rate. All other
kernel elements are effectively “dropped” and do not have to be considered for the
calculation. A sketch of the dilated convolution for r = 2 is shown in Fig.19.6.
.
From the sketch it is obvious that a 3×3 kernel with a dilation rate of 2 has the
.
same receptive field as a5×5kernel. We can compute the effective kernel sizem
. . eff
as
m =(m−1)·r +1, (19.4)
. eff
where m is the number of considered elements in the dilated kernel for each
dimension. For example, for the situation in Fig.19.6 it is m = 3, r = 2, and
. .
m =5.
. eff
Dilated Convolutional Layer (cid:2)
In DL networks a dilated convolutional layer is often used instead of the last
standard convolutional layer.


================================================================================
PAGE 553
================================================================================

540 19 AdvancedDeepLearningArchitecturesandTechniques
Fig. 19.6 Schematic of a dilated convolution. Even though the kernel has the same number of
elements as the one shown in Fig.19.1, the receptive field has increased
19.1.5 Feed Forward Step and Backpropagation
The feed forward step of a convolutional layer consists in performing a convolution
operation, where additionally needs to be considered that images often consist of
more than one channel (i.e., a color image consists of one pixel layer for each of the
colors red, green, and blue). Furthermore, a convolutional layer typically contains
not only a single convolution operation for each input channel but several, so that
during training each filter kernel of a layer learns to identify a different characteristic
of the image. This is what adds “depth”, i.e. a third dimension, to a convolutional
layer.
The derivation of the mathematical formulation for backpropagation is based
on the fact that the mathematical convolution operation consists of a summation of
products between the kernel elements (the weights) and the input values of the layer.
Other than that, the same steps as also required for a fully connected layer need to
be performed.
Last but not least, the convolution operation is typically followed by an activation
operation where an activation function is applied to each output value of the
convolution. The formulations of the feed forward step and the backpropagation
are again obtained in analogy to a fully connected network.
19.1.6 CNN Architectures
Subsequently, we introduce two early CNN architecture that had a significant
influence on the development of CNN DL technology.
LeNet
One of the earliest convolutional neural networks was LeNet developed in 1989
by YANN LECUN et al. [24]. After several research iterations, in 1995 LeNet-5 was
developed, shortly after the MNIST dataset of handwritten digits (in this text used as
dataset DS-2 [MNIST]) was created. A comparison with several other methods for


================================================================================
PAGE 554
================================================================================

19.1 ConvolutionalNeuralNetworks 541
Fig. 19.7 Summary of the architecture of the LeNet-5 [24]
identifying the handwritten digits revealed that the CNNs architecture outperformed
all other methods. The architecture is shown in Fig.19.7. It consists of an input layer
that takes a single-channel image of32×32pixel. This is followed by convolutional
.
layers with a 5 × 5 kernel and 2-element padding. Then, average pooling was
.
performed with a stride of 2, effectively reducing the 28 × 28 output from the
.
convolution layer to half the size. This was followed by another convolution layer
with the same kernel and padding, followed by an average pooling layer. Finally,
there were two fully connected layers. Note that pooling requires no learnable
weights, so weights need to be allocated only for the convolutional and the fully
connected layers. As activation functions (after the convolution layer and the fully
connected layers), a sigmoidal function was used.
The LeNet was a milestone for the development of CNNs, and even though
today, the CNN architectures are quite different, all important elements were already
present.
AlexNet
As opposed to the original LeNet, the AlexNet was designed to run on multiple
GPU hardware. It is one of the early, deep CNNs. It consisted of eight layers:
five convolutional layers, partially followed by max-pooling layers, and at the end
completed by three fully connected layers; cf. Fig.19.8. The “specialty” of this
architecture was that it ran on two GPUs, each of which contained a copy of the
whole network except for the last, fully connected layer. Each GPU then learned
different kernels, some of which were already shown above in Fig.19.4. AlexNet
easily won the “ImageNet competition” in 2012. It is considered the most important
impetus for subsequent advancements in DL for computer vision.
ResNet
The residual neural network (ResNet) is a more recent network and, in 2015, won
the ImageNet1 competition. One of the problems for deep network is the problem of
1 ImageNet (https://www.image-net.org/) is a huge image database with millions of annotated
images. Since 2010, the ImageNet regularly organizes contests about classification and detection
of objects where networks compete against each other.


================================================================================
PAGE 555
================================================================================

542 19 AdvancedDeepLearningArchitecturesandTechniques
Fig. 19.8 The AlexNet [23]: summary of the architecture
vanishing gradients, where gradients are becoming smaller and smaller while being
propagated through the many layers of a deep network until they nearly disappear,
and the performance of the network saturates.
The solution, used by the ResNet architecture [13], is based on the fact that in a
deep neural network, a significant part of the learning takes place in the first layers,
while the learning increment from the layers closer to the output is much smaller,
sometimes even approaching zero. Thus, these layers could, in principle, be replaced
with the identity function. However, it was shown by K. HE et al. [13] that a better
approach is to add a small increment to the identity mapping 1(x)=x of the input
.
x in the form
.
y =F(x;{w })+1(x ), (19.5)
. i
where Fis the “residual mapping” to be learned,
.
F(x;{w })=y−1(x ). (19.6)
. i
from which we see why Fhas the function of a residual.
.
For this purpose the ResNet contains a new element, the residual block . This
is a network part that, in the simplest version, consists of two connected 3 × 3
.
convolutional layers and a residual connection, also called skip connection ; see
Fig.19.9.
These connections have the ability of initially “doing nothing,” i.e., the infor-
mation bypasses the two convolutional layers and reuses the activations from the
previous layer. This speeds up the training during the initial phase, as effectively a
much shallower network is trained. Later on, the remaining, previously deactivated
layers are also included in the training process.


================================================================================
PAGE 556
================================================================================

19.2 DeepLearningTechniques 543
Fig. 19.9 Residual block in
a deep residual Network. The
identity function skips two
layers
In further developed ResNet-type networks one can also find bottleneck blocks,
a variant of the residual block, consisting of three convolutional layers. Currently,
there are architectures (the HighwayNets) which have the ability to decide how many
layers to skip, adapted to the respective situation.
19.2 Deep Learning Techniques
In what follows, we briefly explain a few advanced DL techniques that help to
increase the performance of networks.
19.2.1 Optimization Methods
In the previous parts of this text, we encountered a number of times the gradient
descent (GD) methods which were used for minimizing a cost function. The
stochastic gradient descent (SGD) is the counterpart of the GD method. In each
iteration it is applied to a randomly chosen subset of the data. The benefit for
high-dimensional minimization problems is that the computational cost is reduced,
which speeds up the minimization. However, this often comes at the price of slower
convergence. Note that it is not obvious that performing the minimization on a
subset of the dataset works (even though it does).
If instead of randomly choosing subsets of data one uses mini-batches, then we
are somewhere in between a GD and the SGD. This method is called mini-batch
gradient descent.InDL this is usually the preferred algorithm.
The variants of the GD method are, by far, not the only methods for cost
minimization and finding parameters. We only mention two of them and refer the
reader to the extensive literature on numerical optimization. The first is the Adam
optimization algorithm [20] which was designed for non-convex optimization tasks
and which is able to operate with adaptive learning rates. Last but not least, there is
a classical optimization algorithm, called BFGS (after the four persons Broyden,
Fletcher, Goldfarb, and Shanno) which includes information of the curvature
through the Hessian matrix.


================================================================================
PAGE 557
================================================================================

544 19 AdvancedDeepLearningArchitecturesandTechniques
19.2.2 Momentum
In cases where the minimizer encounters very shallow “valleys,” a technique called
momentum is useful (in fact, it is useful in many situations). While GD (or SGD)
uses only the current gradient, in this method, the gradients are “accumulated” over
a few epochs. A typical parameter that needs to be fixed is a coefficient that scales
the strength of the summed gradients, the coefficient of momentum . With this, if the
optimizer just reaches a valley, then there is still “enough gradient left” from before,
to do larger update steps.
19.2.3 Early Stopping
In practical DL frameworks, one often stumbles across the notion of early stopping.
The purpose is to automatically detect the point during the training when any
further training would deteriorate the ability to generalize. A number of different
methods exist, some of them can be analytically derived, some are rather heuristic
methods, and others are highly tailored to the minimizer or to the network under
consideration.
19.2.4 Transfer Learning
Transfer learning is a technique often used in DL where a model is first trained on
one dataset of particular objects. The trained model is then used for further training
or for inference on data of different objects. For example, a network that was trained
on peas (e.g., for counting them) probably is not too different from a network trained
on lentils.
19.2.5 Data Augmentation
Last but not least, an important technique to increase the number of training images
are image transformations, such as flipping, rotating, cropping, (color) jittering,
warping, adding image “defects” or noise, etc. Even though these are not entirely
new images, usually this technique is already sufficient to significantly increase the
accuracy of a model. Data augmentation can also be understood as an oversampling
technique (cf. Sect.8.4.4) as the amount of data is increased by “reusing” existing
images.
Care must be taken that none of the used augmentation transformations violates
the physical reality. For example, if the straight lines in an image should always be
parallel, then warping the training images might deteriorate the prediction accuracy.


================================================================================
PAGE 558
================================================================================

19.3 TwoExamplesforDeepLearninginMicroscopy 545
19.3 Two Examples for Deep Learning in Microscopy
19.3.1 Semantic Segmentation of TEM Images of Nanoparticles
The first example uses a dataset that has been published along with the journal
publication by SYTWU et al. in [37]. The data is obtained from an transmission
electron microscopy (TEM) experiment with gold nanoparticles of 2.2nm diameter.
The resolution of the TEM was such that the crystalline structure of the nanoparticle
can be distinguished from the amorphous background. For more information please
refer to the original publication [37]. The dataset also contains a label, i.e., a domain
expert annotated the images, indicating which part of the image is a nanoparticle and
which part is not. Finding such disjunct regions is called semantic segmentation.
For training, we used the U-Net architecture by RONNEBERGER et al. [33]. The
U-Net is a CNN which consists of a contracting path and an expansive path, similar
to an encoder/decoder structure of an autoencoder architecture (cf. Sect.19.4).
The contracting path consists of a sequence of convolutional layers together with
max pooling downsampling operation, resulting in reduced spatial resolution of the
layers where at the same time the represented feature complexity can be very high.
The expansive path projects the learned lower-dimensional features back into the
pixel space. These two steps give the network the u-shaped architecture. U-Nets
have been proven to be very efficient and accurate for semantic segmentation, but
can also be used for other types of problems. Figure 19.10 shows examples of the
images and the corresponding predictions.
One can see that for some of the sections in the images it is even for us very
difficult to see differences and to exactly pinpoint where the crystalline regions
Fig. 19.10 Semantic segmentation of crystalline Au nanoparticles, embedded into an amorphous
matrix. The DL task is to identify the crystalline regions. The ground truth was prepared by a
domain expert. The superposition (bottom row) shows both the microscopy image and the predicted
masks


================================================================================
PAGE 559
================================================================================

546 19 AdvancedDeepLearningArchitecturesandTechniques
end and the amorphous phase begins. In example 2, our model did not separate
the two bottom nanoparticles correctly. In example 6 the prediction of the DL model
looks more accurate. Given that we only used an “off-the-shelf” U-Net, initialized
the weights from the ImageNet, and performed basic data augmentation (such as
rotation, warping, jittering, etc.), the results are very satisfying. The performance of
the U-Net seems to be at least comparable to human performance. Given how much
time it takes to manually annotate such images, it is clear that such DL approaches
are extremely useful tools for image analysis in microscopy. The analysis does not
have to stop here, though. With the information about the nanoparticle, one could
now start to analyze their shape or to perform statistical analyses and thereby turn
these images possible even into knowledge (cf. Sect.2.3). The next example shows
some possible solution strategies if labeled images are not available.
19.3.2 Binary Segmentation Using Synthetic Training Data for Grain
Microstructures
This example is a typical materials science and microscopy application case, where a
convolutional neural network is used for binary segmentation of microscopy images.
The study is the own work of—among others—the author of this text and has
been previously published in [38] under the terms and conditions of the Creative
Commons Attribution (CC BY) license.2
Binary segmentation of microscopy images is one of the tasks where CNN-
based DL methods already became a very useful tool. Binary segmentation typically
involves finding contours or edges of features, e.g., of the grain boundaries in grain
microstructures. The top panel of Fig.19.11 shows typical grayscale images with
grain distributions, obtained from scanning electron microscopy (SEM) microscopy.
A main problem of ML and DL approaches is that any model requires data for
training. As a rule of thumb one can say, the higher the degree of complexity of the
features contained in the images to be analyzed, the more training data is needed.
This makes microscopy images rather difficult to learn, but on the other hand, the
effort for hand-labeling (i.e., manually identifying and “drawing” the contours) is
also significant.
Therefore, in this study, another approach was chosen. Synthetic grain
microstructures were created using a variant of the Voronoi tessellation. Then,
a range of different types of “randomness” was added to increase the variability
of the dataset. From the Voronoi tessellation, we also could directly obtain the
“masks,” i.e., the black lines that indicate the objects of interest. Since this was
generated by a Python script, it was easily possible to create thousands of images
with the corresponding masks for training. The question was, how far a CNN (see3
2 https://creativecommons.org/licenses/by/4.0/
3 In this study a ResNet-50-based U-Net was used where the original U-Net architecture was
adapted, such that standard convolutional layers were replaced by residual blocks. Furthermore,


================================================================================
PAGE 560
================================================================================

19.3 TwoExamplesforDeepLearninginMicroscopy 547
Fig. 19.11 Images of grain microstructures, obtained from scanning electron microscopy (top
row). The middle row shows the masks, obtained from manually “labeling” the grain boundaries,
while the bottom row shows the DL predictions using a network that was trained on synthetic
images. Previously published in [38] under the terms and conditions of the Creative Commons
Attribution (CC BY) license
for further details), which was only trained on synthetic images, can be used for
predictions on real microscopy images. There, an additional technique was used:
adaptive data generation and fine-tuning. We used the trained model after each
training cycle to find the images with the worst predictions. Since the generation
procedure of the synthetic images was fully parameterized, we could then create
small variations of these images. This effectively increased the available data of
images with particular features.
Taking a look at the predictions in Fig.19.12 for the synthetic images shows
nearly perfect results. This may be not entirely surprising since these images have
significantly less noise, randomness, and artifacts than real microscopy images.
Therefore, the real challenges is then to make predictions for the real microscopy
images (Fig.19.11 [bottom row]). There, we observe that the accuracy of the binary
segmentation is very high. This was particularly surprising, as the microscopy
images came from very different sources and materials. Some small sections of
grain boundaries were missing; however, with more specialization for a particular
microscopy experiment, even this could be mended. However, in this study, the goal
was to develop a very generic approach and to not use anything but synthetic training
data and a convolutional neural network.
a ResNet50 was used as backbone of the hourglass-like U-Net architecture for the segmentation
process.


================================================================================
PAGE 561
================================================================================

548 19 AdvancedDeepLearningArchitecturesandTechniques
Fig. 19.12 Synthetically generated grain microstructures (top row). The middle row shows the
masks, also automatically obtained, while the bottom row shows the DL predictions. Previously
published in [38] under the terms and conditions of the Creative Commons Attribution (CC BY)
license
19.4 Autoencoder: How to Learn with Networks Without
Supervision
An autoencoder is a specific type of a feedforward neural network, where for each
input node there exists a corresponding output node. An autoencoder is a neural
network that learns in an unsupervised manner two different functions. The first is
the encoder which takes the input data and turns it into an encoded (i.e., compressed)
representation, which is then contained in the code (also called latent space); see the
left panel of Fig.19.13. The other function, which is learned, is the decoder which
reconstructs the input data again. One of the applications of autoencoders is, similar
to principal component analysis (PCA) from Sect.15.2,thed imensionality reduction
of a given dataset. Similarly, it can also be used as a generative model which, after
training, is able to generate new random data that is similar to the training data.
The first version of an autoencoder is attributed to MARK A. KRAMER [22] who
introduced it as a nonlinear kind of a PCA method.
19.4.1 Single-Layer and Multilayer Architectures
In the simplest case, an autoencoder consists of only one hidden layer, as shown
in the sketch in the right panel of Fig.19.13. However, one has to avoid that
the autoencoder just duplicates the training data. This is why typically, the code


================================================================================
PAGE 562
================================================================================

19.4 Autoencoder:HowtoLearnwithNetworksWithoutSupervision 549
Fig. 19.13 The autoencoder
architecture consists of an
encoder and a decoder (left
panel). The low “code” is in
the intersection of the two.
The right panel shows a
simple one-layer autoencoder
network
reyaL
tupnI
reyaL
tuptuO
Fig. 19.14 Visualization of a multilayer autoencoder: during the forward pass the input informa-
tion moves through the encoder, which here is a fully connected ANN with multiple hidden layers,
to produce the code. The decoder consists of a similar but “mirrored” layer structure and produces
output only using the code as input
represents a lower-dimensional space, i.e., the corresponding single-code layer has
a lower number of units than the number of inputs. In more complex situations, e.g.,
in case of complex features or large differences between the input dimension and
the code dimension, multilayer autoencoder architectures are required. Figure 19.14
shows an architecture with three hidden layers for both the encoder and decoder in
a compact visualization. The code always consists of only one layer; its exact size
is a hyperparameter.
The training of all autoencoders relies on a cost function that characterizes how
different the input from the output is. Typical loss functions (remember, “loss”
in the DL community always means “mean loss,” i.e., the cost) are in general the
mean squared error (MSE) and in particular for image data the binary cross-entropy
function. Then a minimization algorithm then tries to find the weights that minimize
the cost function. With this, predictions can be made for any input.
19.4.2 The Relation to Principal Component Analysis
There is a surprising similarity between autoencoder and principal component
analysis: both store the encoded information as a lower-dimensional representation,
the “code,” and both are unsupervised learners that can be used for similar tasks. The
main difference is the way how the data is compressed: PCA is a linear method, i.e.,
the relation between the original data, the code, and the reconstructed data is given


================================================================================
PAGE 563
================================================================================

550 19 AdvancedDeepLearningArchitecturesandTechniques
by linear transformations. Autoencoder, on the other hand, usually uses nonlinear
transformation because the activation functions are nonlinear.
In the case that only linear activation functions are used together with the same
cost function as for PCA4 then the result will be comparable—however, note that
PCA enforced that the subspace of the code is constructed such that all basis vectors
are orthogonal. This is not the case for autoencoders.
19.4.3 Convolutional Autoencoder
In particular for image data, fully connected architectures quickly become inefficient
and deep CNN autoencoders are preferred. The encoders consist of a number of 2D
convolutional and max-pooling layers, while the decoder consists of 2D transposed
convolutional layers for upsampling. One of the typical applications in materials
science is denoising and image reconstruction. Furthermore, autoencoders are also
used for anomaly detection.
19.5 Generative Adversarial Networks: How to Create Data?
A branch of generative models that have become more and more popular are gener-
ative adversarial networks (GANs), first introduced in 2014 by IAN GOODFELLOW
et al. [11]. In the following, we will introduce the most important aspects of GANs
based on an image dataset. However, everything that is said can be generalized to
arbitrary data as well.
19.5.1 Model
Generative implies that the model is able to produce new data, while adversarial
denotes the fact that the model will compete against another one. The two models
are termed generator and discriminator.
The task of the generator is to produce plausible data that should represent
the main aspects of the training dataset. The goal of the discriminator is then to
distinguish between the real and the generated data. In the most simple form, this is
just a classifier.
4 There is an alternative derivation to the one shown in Sect.15.2 which is based on the MSE and
which yields the same results.


================================================================================
PAGE 564
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 551
Fig. 19.15 Sketch of a GAN
architecture consisting of a
discriminator, which predicts
iftheimageisafakeorreal
image, and a generator, which
creates fake images. The top
is the forward direction, and
the middle and bottom are the
backward directions for
computing the loss for
discriminator and generator
19.5.2 Training
Training a GAN is more involved than training a regular ANN, because one has
to alternate between training the discriminator and training the generator. For
a successful training, it is crucial that none of the two networks significantly
outperforms the other. This kind of equilibrium between the generator and the
discriminator is called Nash equilibrium. For this, the steps shown in Algorithm 19.1
need to be followed.
Algorithm 19.1: Main Steps for Training a GAN (cid:4)
1. Sample random noise from the normal distribution,.z∼N.
2. Use the generator to produce “fake” data from the noise,.yfake=G(z).
3. Obtain a prediction .ypred from the discriminator for the fake data .yfake and the
real data.yreal=D(xreal).
4. Update the weights of the discriminator according to the cost.
5. Update the generator based on the discriminator.
6. Repeat previous step until “convergence.”
A sketch of the principle of training a GAN is shown in Fig.19.15. Note that in
step 5 we first need to back-propagate through the discriminator in order to obtain
the information, how to update the fake images. This needs to be done with the
results from step 3. Therefore, one has to store the data from the previous iteration
to get the gradient for the generator.
19.5.3 Loss Functions for the Min-Max Game
Additionally, Generative Adversarial Networks (GANs) require specific loss func-
tions to perform well. The loss function also described in the 2014 paper [11]isthe


================================================================================
PAGE 565
================================================================================

552 19 AdvancedDeepLearningArchitecturesandTechniques
so-called min-max GAN loss which is obtained from the binary cross-entropy loss
(cid:4) (cid:5)
L(yreal,ypred) =− yreal·ln(ypred)+(1−yreal)·ln(1−ypred) . (19.7)
.
Here, yreal and ypred is one image (i.e., one data record). The generalization to
. .
several data records will be done after the resulting loss has been derived for a
single record.
The Min-Max Game (cid:2)
The word “game” in the title of this subsection is reminiscent of the origin
of the below introduced algorithm, which stems from the field of game
theory. There, it was originally formulated for a “two-player zero-sum game,”
where the player either makes simultaneous moves or alternatingly takes their
moves. One player tried to maximize a score while the other tries to minimize
it.
Discriminator Loss
There,ypredis the prediction obtained from the discriminator (for real or fake data),
.
and yreal is the correct label. As the discriminator is a classifier, we use a value of
.
1 as being a real image and 0 as being fake. We can then simplify the equation and
get the loss function for the discriminator as:
L (ypred,real,ypred,fake)=L(1,ypred,real)+L(0,ypred,fake) (19.8)
. disc .
(cid:4) (cid:5)
=− ln(ypred,real)+ ln(1− ypred,fake) (19.9)
There, we used the notations from Algorithm 19.1 with ypred,real = D(x) and
.
ypred,fake = D(G(z)). The goal of the discriminator is to minimize its loss:
.
L (yreal,ypred)→MIN, (19.10)
. disc
which is identical to maximizing the sum of the two ln(·)terms from Eq.19.9:
.
ln(ypred,real)+ln(1−ypred,fake)→MAX. (19.11)
.
Generator Loss
When the generator is trained, the discriminator is kept fixed. The only possible
input to the discriminator is therefore a fake image. As a consequence, the first term
in the equations of the discriminator loss becomes zero. The generator is supposed
to generate fake images G(z) that cannot be differentiated from real images.
.


================================================================================
PAGE 566
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 553
Consequently, the discriminator must predict it as real with label 1. Effectively, the
discriminator has the goal of obtaining a label of 1 forD(G(z)), while the generator
.
attempts to obtain a label of 0. The loss for the generator is again obtained from the
cross-entropy formulation:
L (ypred,fake)=ln(1−ypred,fake), (19.12)
. gen
which the generator tries to minimize during the training, resulting in:
ln(1−ypred,fake)→MIN. (19.13)
.
Resulting Loss for the GAN
Both networks have to achieve their optimization goals: the generator seeks to
minimize D(G(z)) (Eq.19.13), while the discriminator attempts to maximize
.
D(G(z)) (Eq.19.11). This is why this section is called “Loss Functions for the
.
Min-Max Game.” The Nash equilibrium will be reached if in the discriminator the
probability for an image to be real or fake is 50%, which translates to getting a loss
value that is close to 0.7.
This is the simplest loss function which, however, has the drawback that it can
cause the generator to get stuck in the early stage of the training where the task for
the discriminator is very easy. A more robust version of this loss is also proposed in
the original publication [11]. That type of loss would be a non-saturating loss form:
L (ypred,fake)=L(1,ypred,fake) =−ln(ypred,fake). (19.14)
. gen
Applying this strategy to multiple records at once and usingE[·]for the expected
.
value, we obtain the following formulation that will result in the training of the GAN
network:
MIN MAX [E[lnD(X)]+E[ln(1−D(G(z)))]] . (19.15)
. G D
Training Algorithm
For the whole training, we note that the generator and discriminator are not trained
at the same time but sequentially. During training of the generator, the discriminator
is frozen and vice versa. Then the training can be summarized in the following
algorithm, adapted from [11]:


================================================================================
PAGE 567
================================================================================

554 19 AdvancedDeepLearningArchitecturesandTechniques
Algorithm 19.2: Algorithm for Training of a GAN (cid:4)
FOR number of epochs DO :
FOR k steps DO :
(cid:129) sample minibatch of m noise samples. {z1,...,zm }fromN .
(cid:129) sample minibatch of m examples. {x1,...,xm }from data generating distribu-
tion
(cid:129) update the discriminator by ascending its stochastic gradient:
(cid:3)m
1
. ∇ θdm [lnD(xi)+ln(1−D(G(zi)))] ,
i=1
where. ∇ θd is the directional derivative.
END FOR
(cid:129) Sample minibatch of m noise samples. {z1,...,zm }fromN .
(cid:129) Update the generator by descending its stochastic gradient:
(cid:3)m
1
. ∇ θdm [ln(1−D(G(zi)))] .
i=1
19.5.4 Common Problems of GANs
GANs are less robust than, e.g., CNNs and are more difficult to train. Intuitively, this
is clear, as it is easier to predict the temperature of one of the images of the MDS-2
(Ising model) than to generate a new image according to a given temperature. Due to
the complex architecture, there are a number of additional points where the training
can fail, two of which will be briefly covered here.
Mode Collapse
Probably the most important one is mode collapse. This may occur when a generator
creates a fake image (or any other data structure) that is particularly convincing.
This can easily happen since at the early stage of the training, only a few modes
are covered. The discriminator then will concentrate on only the features that occur
in the limited set and can quickly overfit. Then, the generator may learn to produce
only this particular output without any variance. This scenario can happen whenever
the generator is learning much faster than the discriminator. While a complete
collapse is not too common but a partial collapse happens often. Mode collapse
is a problem for which remedies are still being investigated.


================================================================================
PAGE 568
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 555
Vanishing Gradients
We encountered the problem of vanishing gradients already before; however, in the
context of GANs this is a slightly different kind of a problem. Here, the problem
can occur when the discriminator is learning much faster than the generator. Then,
the discriminator is perfectly able to differentiate between fake and real data, and
the generator would always face a very high loss, resulting in a very shallow “loss
landscape,” which is a very difficult optimization problem.
19.5.5 Alternative Architectures and Variations
A number of variations exists, and the original architecture is rarely used today. One
of the closest to the original architecture is the conditional GAN (CGAN) which is
only a slight variation of the standard GAN [11]. The main idea is to provide the
class label also to the generator such that there is a “side condition” for the generator
(the respective class), which restricts the space of possibilities of the generator.
In 2017, an alternative of CGAN introduced as auxiliary classifier GAN (AC-
GAN) was published in [29]. Here, instead of simply providing the label to the
networks, an auxiliary network is also used to classify the real and the generated
images. A new loss term is then used to ensure that the generated images correspond
to the expected class. Additionally, the developer of the AC-GAN also introduced a
number of other technical improvements to achieve a better performance.
The Wasserstein GAN (WGAN), proposed in 2017 in [2], is a further develop-
ment of the original GAN where the discriminator is not acting as classifier. Rather,
for each new instance, the discriminator predicts a number 0 ...1which serves as a
.
score. The Wasserstein GAN is a way of solving the problem of vanishing gradients,
due to the used Wasserstein distance which provides a better gradient of the loss as
compared to the regular GAN. However, it has also been shown that the loss does
not become stable [28], which was a major drawback. A number of efforts, such as
[12], were made to make the training more stable.
Other developments have been made to determine better loss functions (see, e.g.,
[5, 27]) or to stabilize the training by introducing regularization terms to the loss
[28]. One of the key milestones in the developments of GAN was reached with the
publication of pix2pix [14] and the CycleGAN [42]. This allows to use generative
models to produce an image based on a given input, e.g., for generating colorized
photos from black and white images. This also might enable a number of exciting
applications in materials science.
With the development of ProGAN [16], it has became possible to progressively
train a GAN to produce high0resolution images with an outstanding quality based on
low-resolution input. The StyleGAN [18] is based on this approach; the current state
of research is given by [17]. A somewhat different direction is used by approaches


================================================================================
PAGE 569
================================================================================

556 19 AdvancedDeepLearningArchitecturesandTechniques
based on transformers [39] which open a door for new GAN development [9]. The
other major improvement results from the combination of large language models
(LLMs) such as CLIP [30] as a conditioning mechanism for generative models.
By combining the VQGAN and the CLIP transformer, [7] allows prompt base-
generated images—which might not seem to have a direct application in materials
science; however, thinking along the line of metadata and ontologies, one also could
see potential applications.
19.5.6 Python Implementation
We will now return to a minimalistic GAN architecture. Even for this, the algorithm
of a GAN is, due to the different learning strategies, more complex than the one that
we introduced for fully connected networks in Sect.18.5 (we assume that the reader
is already familiar with that). Even though we cannot show the whole code, we will
still introduce the most important steps and hints that would be required for a full
Python implementation, because we are convinced that writing code is very useful
for an in-depth understanding. The full code, which in part is still quite similar to the
one for the fully connected network, can be found in the supplementary material.
We start with the class that also contains the training (this was the class Network
for a fully connected network), which is here called GAN . Here, we show the first
two methods:
In [1]: # First part of the implementation
class Gan:
def __init__(self, cost_function, derivative_of_cost):
self.layers_gen = []
self.layers_disc = []
self.cost_function = cost_function
self.derivative_of_cost = derivative_of_cost
def add_layer(self, layer, model):
if model == 'gen':
self.layers_gen.append(layer)
elif model == 'disc':
self.layers_disc.append(layer)
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
...
def generate(self, batch, z_dim): ...
Because for the GAN we need two types of layers, the layers for the generator
and for the discriminator ( self.layers_gen and self.layers_disc , respectively),
the __init__ method was slightly extended as compared to the Network class.
This also reflects in the method for adding new layers where the parameter model
indicates to which list of layers a new layer should be added.
The training method requires further adaption as compared to the original, fully
connected version. We will now go step by step through the whole training method:


================================================================================
PAGE 570
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 557
In [1]: # 2nd part of the implementation
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
all_costs = []
for epoch in range(epochs): # or use trange from package tqdm
np.random.shuffle(X_train)
X_train_batch = np.array_split(X_train,
int(len(X_train) / batch))
for x_train in X_train_batch:
# 1 - sample the noise vector from a normal distribution
noise = np.random.normal(0, 1, (x_train.shape[0], z_dim))
# 2 - generate 'fake' example by giving noise as input
# to the forward propagation of the generator
fake = noise
for layer in self.layers_gen:
fake = layer.feed_forward(fake)
What is new in the signature of the above method is that we now have the size of the
random vector, z_dim , as additional parameter. The parameter decay implements a
decreasing learning rate ηof the form:
.
1
ηnew :=ηold· , (19.16)
. 1+decay·epoch
by which initially a larger learning rate is used than later during the training.
However, note that the learning rate in general needs to be significantly smaller than
that of a “non-GAN” network, as can be observed in the two examples, Sect.19.5.7.
The parameter batch defines the mini-batch size. For each epoch, the split into
mini-batches is performed randomly anew. The first part inside the two nested for
loops starts by sampling from a random normal distribution which is then used as
input for the generator, propagated through all layers of the generator. At the end, a
fake image is the result.
The next block of code (still in the training method) is responsible for obtaining
predictions:
In [1]: # 3rd part of the implementation
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
...
...
# 3 - obtain prediction for the fake image
fake_pred = fake
for layer in self.layers_disc:
fake_pred = layer.feed_forward(fake_pred, store=True)
# 4 - obtain prediction for the real image
real_pred = x_train
for layer in self.layers_disc:
real_pred = layer.feed_forward(real_pred)
... which is done by forward propagating both the fake and the real image through
all layers of the discriminator. The method feed_forward is mostly the same as in


================================================================================
PAGE 571
================================================================================

558 19 AdvancedDeepLearningArchitecturesandTechniques
Sect.18.5. The only difference is that using store=True the current value of the
input is stored in the object for later reuse.
We can now compare the predictions for the real and for the fake image. For this,
we compute the loss for both of them and take the mean of them and for all items in
the mini-batch:
In [1]: # 4th part of the implementation
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
...
...
# 5 - compare predictions of real and fake with reference
real_loss = self.cost_function(1., real_pred)
fake_loss = self.cost_function(0., fake_pred)
cost_D = 0.5 * np.mean(fake_loss + real_loss)
Note that for the real example, the discriminator should output 1, while for the fake
example the discriminator should output 0.
Having the cost (i.e., the mean loss, for those readers, who are not used to the
notion of “cost”) of the discriminator at hand, we can now back-propagate the error
for both predictions:
In [1]: # 5th part of the implementation
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
...
...
# 6 - back-propagate errors from both predictions
real_error = self.derivative_of_cost(1., real_pred)
for layer in reversed(self.layers_disc):
real_error = layer.backward_propagation(
real_error, learning_rate,
store_update=True, update_weights=False
)
fake_error = self.derivative_of_cost(0., fake_pred)
for layer in reversed(self.layers_disc):
fake_error = layer.backward_propagation(
fake_error, learning_rate, use_memory=True
)
In the above code, it becomes also obvious that the implementation of the
back-propagation also will need to be changed: as compared to the original
implementation, the “state” of the layer (which was stored during the forward
propagation; see the parameter store=True above) is recalled and used.
The remainder of the training method consists of two blocks inside the two nested
for loops and two “update” lines after the inner for loop is finished:


================================================================================
PAGE 572
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 559
In [1]: # 6th part of the implementation
def train(self, X_train, z_dim, epochs, learning_rate, decay, batch):
...
...
# 7 - update generator based on feedback from discrimi.
cost_G = np.mean(self.cost_function(1., fake_pred))
# 8 - backpropagate errors for the generator
gen_error = self.derivative_of_cost(1., fake_pred)
for layer in reversed(self.layers_disc):
gen_error = layer.backward_propagation(
gen_error, learning_rate,
update=False, use_memory=True
)
for layer in reversed(self.layers_gen):
gen_error = layer.backward_propagation(
gen_error, learning_rate
)
all_costs.append([cost_D, cost_G])
learning_rate *= (1. / (1. + decay * epoch))
return np.array(all_costs)
Here, the cost of the generator was computed based on the fake prediction. For the
final update of the weights, all errors are back-propagated through all layers of the
discriminator and the generator. In the last two lines inside the for loop over all
epochs, the costs are stored in a list for monitoring their changes, and the learning
rate was adapted according to Eq.19.16. Finally, the list with the training errors for
generator and discriminator is returned.
The last method of the GAN class is the method corresponding to predict from
the fully connected network:
In [1]: # 7th part of the implementation
def generate(self, batch, z_dim):
noise = np.random.normal(0, 1, (batch, z_dim))
fake = noise
for layer in self.layers_gen:
fake = layer.feed_forward(fake)
return fake
... where based on the learned weights, new images are created, starting from noise.
What else is needed for the implementation? As the layer needs to be able to
store the input of a forwards pass, the class FullyConnectedLayer as well as the
classes for different activation layers needs to be adapted. Since there is nothing
fundamentally complex in there, we do not show the implementation and refer the
reader to the supplementary material.


================================================================================
PAGE 573
================================================================================

560 19 AdvancedDeepLearningArchitecturesandTechniques
19.5.7 Two Examples: Handwritten Digits and Microstructures
Training GANs takes a considerable amount of time and computational power,
and any real-world application would require GPU computing. However, for small
images, we still can set up “toy networks,” which help to understand the architecture,
as introduced next.
Example 1: Generating Handwritten Digits
We use the dataset DS-2-light (Alpaydin Digits) for training and are interested
in generating images that look similar to the original ones. The images are small
enough (8×8) to have a fast training process. We begin by importing the data:
.
In [2]: from sklearn.datasets import load_digits
digits = load_digits()
X = 2 * (digits.data / 16.) - 1
Y = digits.target
Next, we define the network architecture both for the discriminator and the
generator:
In [3]: nn = Gan(BCE, dBCE_dy)
nn.add_layer(FullyConnectedLayer(X.shape[-1], 32), 'disc')
nn.add_layer(LeakyReLULayer(0.02), 'disc')
nn.add_layer(FullyConnectedLayer(32, 1), 'disc')
nn.add_layer(SigmoidLayer(), 'disc')
nn.add_layer(FullyConnectedLayer(z_dim, 32), 'gen')
nn.add_layer(ReLULayer(), 'gen')
nn.add_layer(FullyConnectedLayer(32, 48), 'gen')
nn.add_layer(ReLULayer(), 'gen')
nn.add_layer(FullyConnectedLayer(48 , .Xshape[-1]), 'gen')
nn.add_layer(TanhLayer(), 'gen')
... where BCE and dBCE_dy is the function for computing the binary cross-entropy
loss and its derivative, respectively. Now, we train the network for 2000 epochs,
where we start the training with a learning rate of 10
−4
and plot the cost functions:
.
In [4]: train_gan = nn.train(X, z_dim, epochs=2000, learning_rate=1e-4,
decay=1e-5, batch=64)
plt.plot(train_gan[:, 0], label='Discriminator loss')
plt.plot(train_gan[:, 1], label='Generator loss')
plt.legend()
The resulting plot is similar to the one shown in the left panel of Fig.19.16. Images
can be generated by:
In [5]: n_images = 4
generated_imgs = nn.generate(n_images, z_dim)
generated_imgs = generated_imgs.reshape(-1, 8, 8)
plt.imshow(generated_imgs[1], cmap='gray') # use index 0..(n_images-1)


================================================================================
PAGE 574
================================================================================

19.5 GenerativeAdversarialNetworks:HowtoCreateData? 561
1.0
0.9
0.8
0.7
0.6
0.5
0 500 1000 1500 2000
epochs
s soln
aem
Discriminator
Generator
Fig. 19.16 Generating handwritten digits: training loss for the GAN (left panel) where the darker
lines show moving averages; the right panel shows images of handwritten digits, generated by the
simple GAN implementation
The generated images are shown in the right panel of Fig.19.16. Taking a look
at them, we do see that in most of them, we can clearly identify the number that
would correspond to the patterns. Even though the results are not perfect, this is
still a very good outcome, given that the training took a few seconds and no GPU
was required. The training loss is plotted separately for the discriminator and for the
generator. The strong fluctuations are a typical behavior for GANs and also underline
that the training for such architecture is more complicated. Above, we mentioned
that if the Nash equilibrium is achieved, then the loss values are approximately 0.7.
While we are still away from that, both loss values are, on average, about equally
far away from that value, which suggests that none of the two network parts is much
“stronger” than the other.
Example 2: Generating Microstructure Images
In this example our goal now is to generate artificial microstructure images. For this
we start with the dataset MDS-2 light (Ising model, small), which consists of larger
images, 16×16, and therefore contain four times as many pixel as the DS-2-light
.
(Alpaydin Digits). Additionally, just by looking at the images, we probably would
estimate that the variance of the features in the dataset MDS-2 light (Ising Model,
small) is larger than in the handwritten digit dataset. This will be confirmed during
the training, where it turns out that we need significantly more epochs (10000) and
also need to use 5000 (instead of ≈1800) training images. Additionally, we also
.
found that just using leaky ReLU as activation for the generator was beneficial. The
training takes more than two hours on a regular laptop (as opposed to a few seconds
for the previous example). In Fig.19.17 the losses are shown as well as true and the
generated images. The losses exhibit much larger fluctuation, and it might be helpful
to show a moving average in such cases. While the real images are binary images
(only black and white), the generated one also contains gray values in between.
This could easily be removed by a simple thresholding. Visually, the images are
quite similar, but a person who already looked at several such images would be


================================================================================
PAGE 575
================================================================================

562 19 AdvancedDeepLearningArchitecturesandTechniques
1.5
1.0
0.5
0.0
0 2500 5000 7500 10000
epochs
s
soln
aem
Discriminator
Generator
Fig. 19.17 Generating microstructure images: training loss for the GAN (left panel) and true and
fake Ising microstructures (middle and right panel). Note that for comparison, the real images are
a particular choice of images that roughly exhibit the same characteristics of the structures as the
generated once. Otherwise, the “image distribution” would be random
able to tell the difference. For example, in the fake images the noise on the level of
individual pixel looks as if it were uniformly random generated and superimposed
with other, more structures of larger wavelengths.
Furthermore, it seems as if there are mainly two types of microstructures (i.e.,
images with two predominant wavelengths). This might indicate the early stage of
a mode collapse (cf. Sect.19.5.4).
19.6 Physics-Informed Machine Learning and Beyond
A specialized subfield of DL that has seen a huge interest in recent years is the
field of scientific machine learning . There, the goal is to incorporate knowledge
of a physical problem, e.g., in form of given partial differential equations (PDEs)
or boundary conditions. Clearly, this might be a very useful approach because the
space of possible solutions that a deep neural net could learn can thereby strongly
be reduced. Additionally, the consideration of physical constraints should help to
increase the accuracy of a training process. Such approaches are also known as
physics informed machine learning [3]. Physical knowledge can be included, e.g.,
via a number of different bias types (see, e.g., the overview by KARNIADAKIS et al.
[15]).
For example, using observational bias is a very simply way of leveraging
physical knowledge. Such knowledge can be “injected” into a ML model by simply
building a dataset of physics-based data, for instance, from simulations. Then,
enforcing certain physical properties in the dataset, such as symmetry or periodicity,
helps to improve the training process.
Introducing a learning bias is another way of leveraging physical knowledge.
Consider, for example, the heat equation which consists of a PDE along with
boundary conditions and initial values. Assume that a DL model is to be trained.


================================================================================
PAGE 576
================================================================================

19.7 SummaryandConclusion 563
Then, it is possible to introduce the information from the heat equation during
the training process by incorporating additional terms for the PDE, the initial and
the boundary condition into the loss function. Such models have recently gained
considerable amount of interest and are called physics-informed neural network
(PINN) [31, 32]. Typically, the network architecture is a simple, fully connected
neural network. PINNs have been applied for predicting PDEs, i.e., forward problems
[4,31,35] as well as inverse problems [32,41]. If the total loss function is the sum
of the loss for the PDE, the initial and boundary conditions, then such a method is
known as deep Galerkin method [36]. Additionally, for these methods, the dataset
is not given but rather generated by a random sampling strategy during the training
(this is possible as the PDE is “provided”).
A number of further developments and variations exist, and there have been many
attempts to use various architectures as a basis for PINNs instead of fully connected
layers, such as convolutional neural networks [10], recurrent neural networks [40],
graph networks [34] networks, and even mixed architectures, such as convolutional
transformers [19]. However, many of these methods are still rather limited (e.g., for
each new boundary condition or initial condition the network needs to be trained
from scratch) and far from being able to replace simulations. Also, they still require
a substantial amount of development work.
As a last and very promising subject, we briefly mention the topic of operator
learning. While PINNs attempt to learn the solution of a set of equations, an alter-
native approach is to learn the mathematical operators instead. The most common
approach and network architecture is DeepONet [26]. By applying the so-called
Chen and Chen theorem [6] for nonlinear operator approximation, it is possible
to separate the network into two parts: the trunk NN network and the branch
. t
NN network. The trunk network handles the spatial coordinate-based information
. b
and performs nonlinear mapping into a higher-dimensional space. At the same
time, the branch network operates on the mesh-specific data by providing the
information about an input function at a number of different points (the “sensors”).
The DeepONet architecture provides a number of potential customization options
and is one of the very promising and new models (see, e.g., [25] for an application).
A thorough overview of neural operators can be found in the work of KOVACHKI
et al. [21].
Clearly, this is a very exciting field in which very active development is taking
place. Unfortunately, it is beyond the scope of this book to go into further details of
the theory or the implementation and we hope that the above short review serves as
a good starting point for further reading or own research.
19.7 Summary and Conclusion
With this, we are at the end of the DL part. We began this journey in the mid of
the twentieth century and followed the historical developments starting with single
artificial neurons. We then, step by step, developed the single neurons into various
assemblies of neurons, arriving at larger and more complex networks.


================================================================================
PAGE 577
================================================================================

564 19 AdvancedDeepLearningArchitecturesandTechniques
In the current chapter, we covered many of the more recent developments,
ranging from convolutional neural networks and autoencoders to generative models.
At the same time, also a number of specialized techniques for creating efficient and
stable deep learning models were introduced.
One of the goals of these chapters was to show how simple neural networks and
a number of deep learning approaches can still be implemented by just a few lines
of Python code. For more advanced network architectures, this is no longer possible
and one should rather switch to some of the very mature and computationally
efficient DL frameworks such as PyTorch5 or TensorFlow.6
As a consequence, we did not cover some relevant architectures in depth such as
the U-Net or recurrent neural networks for time-dependent problems. Nonetheless,
we hope that the reader is now in possession of all relevant foundations and a good
amount of hands-on experience; she or he should now be easily able to explore new
network types and exciting machine learning methods with the help of the large
body of offline and online literature.
19.8 Exercises
19.1 Use the implementation of an ANN with hidden layers from the previous
chapter to study the influence of the number of nodes in the hidden layers. Following
Sect.18.5.2, vary the number of output nodes of the hidden layer and compare the
testing accuracy for different numbers. Which constellation of node numbers for the
hidden layers would be optimal?
19.2 Using the example of the DS-2-light (Alpaydin Digits) dataset using fully
connected layers in Sect.18.5.2, extend the code so that a convolution layer can be
used.
(a) What shape does the input need?
(b) Comparing the performance of both model, which one gives the best results?
(c) How does the number of weights in each model influence the performances
in terms of accuracy and in terms of computational time? Show the scaling
behavior in two diagrams.
19.3 Using the Python implementation of the GAN for the DS-2-light (Alpaydin
Digits) dataset from above (the full code can be obtained from supplementary
webpage (https://MDS-book.org)), discuss the following points:
(a) How does the number of layer impact the performance of the GAN?
(b) What happens if you only increase the capacity of one of the layers?
(c) What is the influence of the number of training example on the generated
images?
5 https://pytorch.org/
6 https://www.tensorflow.org/


================================================================================
PAGE 578
================================================================================

References 565
References
1. A. Araujo, W. Norris, and J. Sim. Computing receptive fields of convolutional neural networks.
Distill, 4(11), Nov. 2019. DOI https://doi.org/10.23915/distill.00021.
2. M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In
D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pages 214–223. PMLR,
06–11 Aug 2017. URL https://proceedings.mlr.press/v70/arjovsky17a.html.
3. N. Baker, F. Alexander, T. Bremer, A. Hagberg, Y. Kevrekidis, H. Najm, M. Parashar, A. Patra,
J. Sethian, S. Wild, K. Willcox, and S. Lee. Workshop report on basic research needs for
scientific machine learning: Core technologies for artificial intelligence. Technical report, Feb.
2019. DOI https://doi.org/10.2172/1478744.
4. T. Bandai and T. A. Ghezzehei. Forward and inverse modeling of water flow in unsaturated
soils with discontinuous hydraulic conductivities using physics-informed neural networks with
domain decomposition. Hydrology and Earth System Sciences, 26(16):4469–4495, 2022.
DOI https://doi.org/10.5194/hess-26-4469-2022. URLhttps://hess.copernicus.org/articles/26/
4469/2022/.
5. D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial
networks, 2017.
6. T. Chen and H. Chen. Universal approximation to nonlinear operators by neural networks with
arbitrary activation functions and its application to dynamical systems. IEEE Trans Neural
Netw, 6(4):911–917, 1995.
7. K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan, L. Castricato, and E. Raff.
Vqgan-clip: Open domain image generation and editing with natural language guidance, 2022.
8. V. Dumoulin and F. Visin. A guide to convolution arithmetic for deep learning. ArXiv e-prints,
mar 2016.
9. P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image
synthesis, 2020.
10. H. Gao, L. Sun, and J.-X. Wang. PhyGeoNet: Physics-informed geometry-adaptive convo-
lutional neural networks for solving parameterized steady-state PDEs on irregular domain.
Journal of Computational Physics, 428:110079, mar 2021. DOI https://doi.org/10.1016/j.jcp.
2020.110079.
11. I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial networks, 2014.
12. I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of
wasserstein gans. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS’17, page 5769–5779, Red Hook, NY, USA, 2017. Curran Associates
Inc. ISBN 9781510860964.
13. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, 2015.
14. P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional
adversarial networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5967–5976, 2017. DOI https://doi.org/10.1109/CVPR.2017.632.
15. G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-
informed machine learning. Nature Reviews Physics, 3(6):422–440, Jun 2021. ISSN 2522-
5820. DOI https://doi.org/10.1038/s42254-021-00314-5.
16. T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved
quality, stability, and variation. In International Conference on Learning Representations,
2018.
17. T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen, and T. Aila. Alias-free
generative adversarial networks. In Neural Information Processing Systems, 2021a.
18. T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial
networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(12):4217–
4228, Dec 2021b. ISSN 1939-3539. DOI https://doi.org/10.1109/TPAMI.2020.2970919.


================================================================================
PAGE 579
================================================================================

566 19 AdvancedDeepLearningArchitecturesandTechniques
19. S. Kim, S.-B. Yun, H. Bae, M.-Y. Lee, and Y. Hong. Physics-informed convolutional
transformer for predicting volatility surface, 2022.
20. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.
21. N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandku-
mar. Neural operator: Learning maps between function spaces, 2021. URL https://arxiv.org/
abs/2108.08481.
22. M. A. Kramer. Nonlinear principal component analysis using autoassociative neural networks.
AIChE Journal, 37(2):233–243, Feb. 1991. DOI https://doi.org/10.1002/aic.690370209.
23. A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional
neural networks. Communications of the ACM, 60(6):84–90, May 2017. DOI https://doi.org/
10.1145/3065386.
24. Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–
551, Dec. 1989. DOI https://doi.org/10.1162/neco.1989.1.4.541.
25. W. Li, M. Z. Bazant, and J. Zhu. Phase-field deeponet: Physics-informed deep operator neural
network for fast simulations of pattern formation governed by gradient flows of free-energy
functionals, 2023.
26. L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via
DeepONet based on the universal approximation theorem of operators. Nature Machine
Intelligence, 3(3):218–229, mar 2021. DOI https://doi.org/10.1038/s42256-021-00302-5.
27. X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley. Least squares generative
adversarial networks. In Proceedings of the IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
28. L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for gans do actually
converge? In International Conference on Machine learning (ICML), 2018.
29. A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier GANs.
In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2642–
2651. PMLR, 06–11 Aug 2017.
30. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from
natural language supervision, 2021.
31. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part i): Data-
driven solutions of nonlinear partial differential equations, 2017a. URL https://arxiv.org/abs/
1711.10561.
32. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part ii): Data-
driven discovery of nonlinear partial differential equations, 2017b. URL https://arxiv.org/abs/
1711.10566.
33. O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image
segmentation, 2015.
34. S. Seo and Y. Liu. Differentiable physics-informed graph networks, 2019. URL https://arxiv.
org/abs/1902.02950.
35. Y. Shin. On the convergence of physics informed neural networks for linear second-order
elliptic and parabolic type PDEs. Communications in Computational Physics, 28(5):2042–
2074, Jun. 2020. DOI https://doi.org/10.4208/cicp.oa-2020-0193.
36. J. Sirignano and K. Spiliopoulos. Dgm: A deep learning algorithm for solving partial
differential equations. Journal of Computational Physics, 375:1339–1364, 2018. ISSN 0021-
9991. DOI https://doi.org/10.1016/j.jcp.2018.08.029. URL h ttps://www.sciencedirect.com/
science/article/pii/S0021999118305527.
37. K. Sytwu, C. Groschner, and M. C. Scott. Understanding the influence of receptive field and
network complexity in neural network-guided TEM image analysis. Microscopy and Micro-
analysis, 28(6):1896–1904, Dec. 2022. DOI https://doi.org/10.1017/s1431927622012466.


================================================================================
PAGE 580
================================================================================

References 567
38. P. Trampert, D. Rubinstein, F. Boughorbel, C. Schlinkmann, M. Luschkova, P. Slusallek,
T. Dahmen, and S. Sandfeld. Deep neural networks for analysis of microscopy
images—synthetic data generation and adaptive sampling. Crystals, 11(3):258, Mar. 2021.
DOI https://doi.org/10.3390/cryst11030258.
39. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and
I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Process-
ing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
40. B. Wu, O. Hennigh, J. Kautz, S. Choudhry, and W. Byeon. Physics informed rnn-dct networks
for time-dependent partial differential equations. In D. Groen, C. de Mulatier, M. Paszynski,
V. V. Krzhizhanovskaya, J. J. Dongarra, and P. M. A. Sloot, editors, Computational Science—
ICCS 2022, pages 372–379, Cham, 2022. Springer International Publishing. ISBN 978-3-031-
08754-7.
41. M. Yin, X. Zheng, J. D. Humphrey, and G. E. Karniadakis. Non-invasive inference of thrombus
material properties with physics-informed neural networks. Computer Methods in Applied
Mechanics and Engineering, 375:113603, Mar. 2021. DOI https://doi.org/10.1016/j.cma.2020.
113603.
42. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks, 2020.


================================================================================
PAGE 581
================================================================================

Linear Algebra for Machine Learning A
In the following we give an overview over the most important definitions and
concepts of linear algebra. This serves as an refresher for those readers who already
took a class in linear algebra. Even though a big part of this book has been written
such that readers without a deep background in vector, matrix, and tensor calculus
and analysis should still be able to follow, there are a number of derivations where
vector calculus is indispensable.
Nonetheless, this appendix is not an rigorous mathematical introduction. It only
aims to introduce all relevant notions and definitions. As a simplification, we always
assume that the problems considered are real-valued problems. There are also nearly
no examples; for this the reader is referred to the many great text books on this topic.
Instead, we use this subject as an opportunity to introduce the corresponding Python
(and in particular numpy) functions. For an introduction to (scientific) Python
programming, refer to one of the many good introduction texts, e.g., [1,2].
For symbolic mathematical computations the Python package sympy is a good
choice.1 sympy is part of Python’s scientific stack and also can be used for
symbolic calculus and analysis; sympy expressions also can be converted into
numpy expression. While we encourage the interested reader to dive into sympy ,
we nonetheless will only focus on numpy examples here, as sympy requires the
user to be a bit familiar with its “philosophy.”
We differentiate between three different types of notations:
1. A purely mathematical formulation using vector/matrix calculus such as scalar
products, or, more general, contractions over one or more indices. This notation
does not consider multiple data records and just very “mathematically” writes
down equations for a single data point. An example is:
1 https://docs.sympy.org/
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 569
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9


================================================================================
PAGE 582
================================================================================

570 A LinearAlgebraforMachineLearning
y = ax2+bx (A.1)
.
2. The next variant is similar to the first (in terms of notation) but considers the
whole data sets and therefore operates with, e.g., vectors for, e.g., a scalar output.
As an intermediate step we sometimes show index notations. As an example
consider the variable x, which represents a number of measurements of a scalar
.
variable (i.e., each element ofx is a single measurementx ):
. . i
y = ax2+bx ⇔ y =ax2+bx (A.2)
. i i i
(note thatx2is the square of all elements,x2).
. . i
3. The last step is with view on Python implementation and we often have to think
about whether an object is a row or column vector. Also here, index notation
comes in handy, in particular when the mathematical operations consist of vector-
matrix products or similar operations. For example, assuming thatx is a column
.
vector that contains a number of measurement, the result y is also a column
.
vector. This can be directly implemented in Python:
y i = ax i 2 + bx i → y = a * x ** 2 + b * x (A.3)
For this simple equation it does not matter which shape the numpy array x
has. The difference between Eq. A.2 and Eq. A.3 becomes more significant once
Eq. A.2 consists of more complex operations, such as computing the mean of all
records, as we already saw in, e.g., Chap.13.
A.1 Vector Calculus
A.1.1 Definition of Vectors
A position vector to a point P starts at the origin O of the chosen coordinate system
and ends at the point. It is thus a means to describe the position of points with
respect to the coordinate origin. A vector in an n-dimensional vector space has n
components.
A.1.2 Conventions: Row and Column Vectors
As a commonly accepted convention, vectors are, if not stated differently, consid-
ered as column vectors. A column vectoraand a row vectorbare shown as:
. .


================================================================================
PAGE 583
================================================================================

A LinearAlgebraforMachineLearning 571
⎡ ⎤
a
1
⎢ ⎥
⎢a 2⎥ (cid:8) (cid:9)
. a =⎢ ⎣ . . ⎥ ⎦ and b= b 1 b 2 ···b n where a i ,b i ∈R. (A.4)
.
a
n
There, the a and b are the vector components. The reason why column vectors
. i . i
are the “default” is that one of the common operations is the matrix-vector product
where the matrix represents a linear function and “acts” on the vector. This product
(for more details see below) is only defined ifais a column vector.
.
Row vectors are also used but at least in this text we are very explicit when we
consider a vector a row vector.
A column vector can be turned into a row vector (and vice versa) by transposing
it; see Appendix A.1.3.
A.1.3 Transposition of a Vector
A row vector is converted into a column vector by transposing it. This is indicated
by the superscript T, e.g., the column vector a becomes a row vector b by b = aT,
. . . .
and the row vectorbbecomes a column vectorabya =bT.
. . .
A.1.4 Scalar Product of Vectors
The scalar product of two vectors results in a scalar, i.e., a single number. It is also
called or dot product and is defined as follows:
Definition A.1 (Scalar Product of Two Vectors) Given are two vectors
a,b ∈ Rn, i.e., each vector consists of n entries. The scalar product a · b
. .
is then defined as:
(cid:10)n
a·b= a b ∈R (A.5)
. i i
i=1
The dot (“·”) between the two vectors symbolizes the scalar product.
.
In this definition, it is not necessary to differentiate between row and column
vector. However, if we assume as convention that a and bare column vectors, then
. .
their scalar product is written without the dot using the transposition symbol (which
turns column vectors into row vectors and vice versa; cf. the general definition
Appendix A.2.7):


================================================================================
PAGE 584
================================================================================

572 A LinearAlgebraforMachineLearning
c=aTb. (A.6)
.
This notation is often followed in the context of matrix-vector multiplications, and
it is not advisable to mix the two notations.
A.1.5 Orthogonal Vectors
Geometrically speaking, two vectors are orthogonal if they are perpendicular to
each. The mathematical definitions use the dot or scalar product:
Definition A.2 (Orthogonal Vectors) Two vectors a and b are said to be
. .
orthogonal if their scalar product vanishes:
a·b=0. (A.7)
.
As the dot product gives the projection of one vector onto the other vector, it is
clear thata·b=0implies an angle of 90 ◦ between them —at least in the considered
. .
two-dimensional plane. This definition can easily be generalized to an arbitrary set
of vectors:
Definition A.3 (Mutually Orthogonal Vectors) A set of n vectors
{v ,...,v }aresaidtobe mutually orthogonal if each possible pair of vector
. 1 n
is orthogonal, i.e.:
v ·v foralli andj withi /= j . (A.8)
. i j
A.1.6 Euclidean Norm of a Vector
The Euclidean norm of a vector can be written in terms of the scalar product.
Definition A.4 (Euclidean Norm of a Vector) Given is a vector a ∈ Rn,
.
i.e., the vector consists of n entries. The Euclidean norm ‖a‖is then defined
.
as:
(continued)


================================================================================
PAGE 585
================================================================================

A LinearAlgebraforMachineLearning 573
(cid:11)
(cid:12)
√ (cid:12)(cid:10)n
‖a ‖= a·a =(cid:13) (a )2 ∈R. (A.9)
. i
i=1
The Euclidean norm is also called L norm (or L2-norm). It gives the distance
. 2
of a point a from the origin of the Euclidean coordinate system which is why we
.
often say that “‖a‖is the length of the vectora.” To differentiate theL -norm from
. . . 2
other norms, sometimes the “2” is added and we use‖·‖ . In the context of machine
. 2
learning (ML) also other norms are used; however, in the context of linear algebra
this is by far the most commonly used norm.
If the length of a vectorais 1, then the vector is called a unit vector:
.
‖a ‖=1 ⇔ aisaunitvector. (A.10)
.
The most well-known unit vectors are the Euclidean unit basis vectors, e.g., e =
. 1
[1,0,...,0]∈Rnore =[0,1,0,...,0]∈Rn.
. 2
A.1.7 Euclidean Distance
Closely related to the Euclidean norm is the Euclidean distance which gives the
distance between two points:
Definition A.5 (Euclidean Distance) Given two points p and p , repre-
. 1 . 2
sented by the vectors v and v ∈ Rn, respectively, the Euclidean distance
. 1 . 2
D between the points is defined as:
. 2
D (v ,v ) =‖v −v ‖ ∈R. (A.11)
. 2 1 2 2 1
In this definition, the subscript “ ” indicates that the L norm is used. Further
.2 . 2
distance metrics are defined in the main text, e.g., in Chap.14.
A.1.8 Orthogonal Projection of a Vector
Projecting one vector a onto another vector u is a task that this encountered fre-
. .
quently in machine learning and data mining, e.g., in the context of dimensionality
reduction and PCA in Sect.15.2. Figure A.1 shows the geometry: there, a is the
.
vector to be projected orthogonally onto u. Orthogonal vector projection implies
.


================================================================================
PAGE 586
================================================================================

574 A LinearAlgebraforMachineLearning
Fig. A.1 Projection of a 2
vector.a on a vector.u :.a is
decomposed into a vector.p n (=a )
⊥
parallel to.u and a vector.n a
perpendicular to.u such that
u
.a =p+n
p (=a )
‖
1
thatais decomposed into a vector that is parallel touand one that is perpendicular
. .
tousuch that:
.
.
a =a‖ +a⊥ . (A.12)
The derivation of . a ‖ is based on the fact that there exists a scalar value c such
that . a‖ = cu, i.e., . a ‖ is just a scaled version of . u .As . a ⊥ and . a ‖ are perpendicular
to each other, their scalar product must vanish, and it follows:
.
a⊥ ·u=0
.
(A.13)
⇔ (a − a‖) · u = (a − cu)· u = 0 (A.14)
We can now use the distributive law for vectors, separate the two terms, and obtain
the constant c:
a·u=cu·u (A.15)
. .
x · u
⇒ c = . (A.16)
u· u
From this it follows for the parallel component ofathat:
.
x·u
.
a‖ =
u·u
u. (A.17)
In the notation used in the ML chapters, quite a number of different super- and
subscripts are used. Therefore, we often rather use
.
p (for parallel) instead of
.
a ‖and
.
n (for normal) instead of
.
a ⊥.
A.1.9 Gradient of a Scalar Field and a Vector Field
Assume that at each point x = (x ,x ,x ) ∈ R3 there is a vector-valued quantity,
. 1 2 3
a(x ,x ,x ). This defines a vector field). Then the gradient of the vector field,∇a,
. 1 2 3 .
is given by:


================================================================================
PAGE 587
================================================================================

A LinearAlgebraforMachineLearning 575
⎡ ⎤
∂a1 ∂a2 ∂a3
⎢∂x1 ∂x1 ∂x1⎥
. ∇a =⎢ ⎣ ∂ ∂ a x 1 2 ∂ ∂ a x 2 2 ∂ ∂ a x 3 2 ⎥ ⎦ (A.18)
∂a1 ∂a2 ∂a3
∂x3 ∂x3 ∂x3
and the jth derivative of the ith component ofais:
.
∂a
i
, (A.19)
.
∂x
j
which is the same as the short form ∂ a . A dimensionality of the field can be
. j i
handled in analogy
A scalar field ϕ(x ,x ,x ) is understood as a special case of the vector field.
. 1 2 3
Therefore, it is:
⎡ ⎤
∂ϕ
⎢∂x1⎥
. ∇ϕ =⎢ ⎣ ∂ ∂ x ϕ 2 ⎥ ⎦ (A.20)
∂ϕ
∂x3
and the jth derivative ofϕis ∂ϕ which can also be abbreviated as∂ ϕ.
. . ∂xj . j
A.2 Matrices and Matrix Operations
A.2.1 Basic Definition and Creation of Matrices in Python
In analogy to Sect.3.2.4 we define a matrix Aconsisting of m rows and n columns
.
of real-valued elements.
Definition A.6 (Matrix Consisting of m Rows and n Columns) AmatrixA
.
consisting of m rows and n columns of real-valued elements has the following
structure:
⎡ ⎤
a a ··· a
11 12 1n
⎢ ⎢a 21 a 22 ··· a 2n ⎥ ⎥
. A=⎢ ⎣ . . . . . . ⎥ ⎦ witha ij ∈R. (A.21)
. . .
a a ··· a
m1 m2 mn
There, an element in row number i (counted from the top) and column number
j (counted from left) is denoted by a . The matrix in Eq. A.21 is called a m×n
. ij .


================================================================================
PAGE 588
================================================================================

576 A LinearAlgebraforMachineLearning
matrix. Sometimes, we use more complex expressions as indices. In these cases we
might use a comma in between the row and the column index, as in, e.g., . A i,2(j+1) .
For matrices a number of basic mathematical operations exists which are
introduced in the following sections.
Python implementation: In Python, a number of methods for creating matrices
exists. A matrix consisting of 4 rows and 4 columns of zeros can be created by:
A3×4matrix consisting of the Boolean values True can be created by:
.
A matrix populated with random values in between 0 and 1 can be created using
numpy.random.rand(n_rows, n_columns) , and to create a matrix A with undefined
(or uninitialized) entries and with the same shape as a matrix B , one can use
numpy.empty_like(B) , to name but a few options.
A.2.2 Addition of Matrices
Definition A.7 (Addition of Matrices) Addition of matrices is defined by
element-wise addition if the two matrices A and B have the same number of
rows and columns:
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
a ··· a b ··· b a +b ··· a +b
11 1n 11 1n 11 11 1n 1n
A+B= ⎢ ⎣ . . . . ⎥ ⎦+ ⎢ ⎣ . . . . ⎥ ⎦= ⎢ ⎣ . . . . ⎥ ⎦ ,
. . . . . . .
a ··· a b ··· b a +b ··· a +b
m1 mn m1 mn m1 m1 mn mn
(A.22)


================================================================================
PAGE 589
================================================================================

A LinearAlgebraforMachineLearning 577
Each element of the resulting matrix is again a real number. Furthermore, it is
directly clear thatA+B=B+A.
.
Python implementation: Addition works as expected in numpy–simply by
adding up the two matrices:
In [1]: import numpy
A = numpy.array([[1.1, 2.3, -0.1],
[0., 0.5, 2.1]])
B = numpy.array([[4.3, 0.2, 0.],
[0.5, 1.7, 0.3]])
A + B
Out [1]: array([[ 5.4, 2.5, -0.1],
[ 0.5, 2.2, 2.4]])
A.2.3 Multiplication of a Matrix with a Scalar
Definition A.8 (Multiplication of a Matrix with a Scalar) A matrixA is
multiplied with a scalar c by multiplying each element with the scalar:
⎡ ⎤
ca ··· ca
11 1n
cA=Ac= ⎢ ⎣ . . . . ⎥ ⎦ (A.23)
. . .
ca ··· ca
m1 mn
Scalar multiplication works equally for multiplying with a scalar from right or from
left. Note that the “scalar multiplication” should not be confused with the “scalar
product” which is an operation between two vectors, resulting in a scalar (hence the
name).
Python implementation: In Python this type of product is performed as expected,
e.g., the statement 2.4 * A multiplies each element of A with 2.4.
A.2.4 Matrix Product
The matrix product is, in a way, a generalization of the scalar product. It is given by
the following definition:


================================================================================
PAGE 590
================================================================================

578 A LinearAlgebraforMachineLearning
Fig. A.2 Multiplication of
two matrices: visualization of 11 12 + 12 22 11 1122 1133 B
the involved rows and
columns. The “inner” 31 13 + 32 23 21 2222 2233
dimensions of the matrices.A
and.B must agree for the
summation to work. Here, the
product can be computed as.A
hastwoc olumnsand.B has
1111 1122
two rows. The product.B A
would not be possible
21 22
A
3311 3322
41 42
Definition A.9 (Product of Two Matrices) The multiplication of two matri-
ces requires that the number of columns of the left equals the number of rows
of the right matrix. Then, the productABresults in a new matrixCwhich has
. .
the same number of rows as the left matrix and the same number of columns
as the right matrix. The elementc is given by:
. ij
(cid:10)n
c =a b +a b +···+a b = a b . (A.24)
. ij i1 1j i2 2j in nj ip pj
p=1
In other words, c is obtained by multiplying all elements of the ith row of the
. ij
left matrix with the jth column of the right matrix and then summing all results of
the products up. A visualization of the operation is shown in Fig.A.2.
Python implementation: numpy has several options for matrix-matrix multipli-
cation as, e.g., the function dot , matmul , or the operator @ which is a short form
for matmul :
In [1]: import numpy
A = numpy.array([[1.1, 2.3, -0.1],
[0., 0.5, 2.1]])
B = numpy.array([[4.3, 0.2, 0.],
[0.5, 1.7, 0.3],
[0.5, 1.7, 0.3]])
A @ B # numpy.dot(A, B) and numpy.matmul(A, B) give same results as A @ B
Out [1]: array([[5.83, 3.96, 0.66],
[1.3 , 4.42, 0.78]])


================================================================================
PAGE 591
================================================================================

A LinearAlgebraforMachineLearning 579
Further Rules for Matrices
Matrix multiplication obeys the following rules (the proofs can be found in most
textbooks on linear algebra, and the reader is encouraged to do them as an exercise):
Associativity: (AB)C=A(BC) (A.25)
. .
Left Distributivity: (A+B)C = AC +BC (A.26)
.
Right Distributivity: C(A + B) = CA+ CB (A.27)
Note that in general AB /= BA. As an example, we show numerically that the left
.
distributive law holds for a particular choice of numbers (this is not a mathematical
proof!). We start by creating the three matrices:
In [1]: import numpy
A = numpy.random.rand(2, 3)
B = numpy.random.rand(2, 3)
C = numpy.random.rand(3, 4)
and then compute the two terms of the left distributive law:
In [2]: (A + B) @ C
Out [2]: array([[1.52760876, 1.58518409, 1.68986657, 1.62772657],
[1.14112041, 1.06058372, 1.19346825, 1.38466784]])
In [3]: A @ C + B @ C
Out [3]: array([[1.52760876, 1.58518409, 1.68986657, 1.62772657],
[1.14112041, 1.06058372, 1.19346825, 1.38466784]])
Looking at the numbers reveals already that both formulations seem to be equiv-
alent. A better way of comparing all elements of the two arrays is to use the
function allclose , which results in True if all elements are identical within a
certain absolute and relative tolerance:
In [4]: numpy.allclose((A + B) @ C,
A @ C + B @ C)
Out [4]: True
A.2.5 Multiplication of a Matrix with a Vector
Multiplication of a matrix with a vector is the special case of multiplication of a
matrix with a matrix that has only one column (see Appendix A.2.4). For example,
assume thatx is a column vector. Then we have
.


================================================================================
PAGE 592
================================================================================

580 A LinearAlgebraforMachineLearning
.
Ax :=x
1
a:,1 +x
2
a:,2 +···+x
n
a:,n , (A.28)
where . a :,i is the ith column of . A . Note that we do not use a dot between . A and . x .In
index notation this is written as
⎡ ⎤⎡ ⎤
a a ··· a x ⎡ ⎤
. A x := ⎢ ⎢ ⎢ ⎣ a 1 2 . . 1 1 a 1 2 2 2 ··· a 1 2 . . n n ⎥ ⎥ ⎥ ⎦ ⎢ ⎢ ⎢ ⎣ x . . 1 2 ⎥ ⎥ ⎥ ⎦ = ⎢ ⎣ a 11 x 1 +a 12 x 2 . . . +···+a 1n x n ⎥ ⎦
. . .
a x +a x +···+a x .
a a ···a x m1 1 m2 2 mn n
m1 m2 mn n
(A.29)
The following python example is also used to remind the reader that for multiplica-
tion from the right the vector b has to be a column vector:
In [1]: import numpy
import numpy
A = numpy.array([[1, 2, 3], [4, 5, 6]])
b = numpy.array([[10], [20], [30]])
# Alternative: b = numpy.array([10, 20, 30]).reshape(-1, 1)
Sometimes it is useful to start with defining the elements of a row vector and then
convert it into a column vector as shown in the commented outline above. The
multiplication can be done using the abbreviation @ as in
In [2]: A @ b
Out [2]: array([[140],
[320]])
or equivalently using A.dot(b) (where the function dot is a method of the object
A )or numpy.dot(A, b) (where dot is a function of the module numpy ).
A.2.6 Hadamard Product of Matrices
There are a number of situations in machine learning where two matrices need to be
multiplied element-wise. This is called Hadamard product, indicated by the symbol
⊙. It is defined for matricesAandBas:
. . .
C=A⊙B (A.30)
. .
c = a b . (A.31)
ij ij ij


================================================================================
PAGE 593
================================================================================

A LinearAlgebraforMachineLearning 581
Python implementation: In numpy this type of multiplication simply uses the
multiplication symbol and looks as follows:
A.2.7 Transposition of a Matrix
The transposition of a matrix A is obtained by “mirroring” its elements across the
.
diagonal, i.e., effectively interchanging rows with columns.
Definition A.10 (Transposition of a Matrix) If A is a matrix with m rows
.
and n columns and its elements are given bya withi ∈[1,m]andj ∈[1,n],
. ij . .
then the elements of the transposed matrixATare given bya .
. . ji
The transposition operation is indicated by the superscript (•)T (where the bullet
.
symbol “•” serves as a placeholder), e.g.,AT. In more detail the transposition looks
. .
like this:
⎡ ⎤ ⎡ ⎤
a a ··· a a a ··· a
11 12 1n 11 21 m1
⎢ ⎢a 21 a 22 ··· a 2n ⎥ ⎥ ⎢ ⎢a 12 a 22 ··· a m2 ⎥ ⎥
. A=⎢ ⎣ . . . . . . ⎥ ⎦ ⇒ AT =⎢ ⎣ . . . . . . ⎥ ⎦ . (A.32)
. . . . . .
a a ··· a a a ··· a
m1 m2 mn 1n 2n mn
For the transposition operation a number of rules and identities can be proven:
AB=BTAT (A.33)
. .
(cA)T =c(AT ) (A.34)
.
(A+ B)T =AT +BT (A.35)
.
(AT )T =A . (A.36)
Python implementation: Transposing a matrix A in numpy is simple: we either
use the function numpy.transpose(A) or the transposition operator A.T .


================================================================================
PAGE 594
================================================================================

582 A LinearAlgebraforMachineLearning
A.2.8 Neutral Element for Matrix Addition
For matrix addition, the “zero matrix” 0 contains only zeros and is the neutral
.
element w.r.t. addition:
⎡ ⎤ ⎡ ⎤ ⎡ ⎤
a ··· a 0 ··· 0 a ··· a
11 1n 11 1n
A+0= ⎢ ⎣ . . . . ⎥ ⎦+ ⎢ ⎣ . . . . ⎥ ⎦= ⎢ ⎣ . . . . ⎥ ⎦=A. (A.37)
. . . . . . .
a ··· a 0 ··· 0 a ··· a
m1 mn m1 mn
This also holds for addition from the left and from the right such that A + 0 =
.
0+A=A.
Python implementation: Creating a “zero matrix” in Python is done using the
function numpy.zeros , as already shown above.
A.2.9 Neutral Element for Matrix Multiplication
For matrix multiplication, the neutral element is the identity matrixIwhich contains
.
ones on the main diagonal and zeros everywhere else. It has the same number of
rows and columns as the matrixA:
.
⎡ ⎤⎡ ⎤
10 ··· 0 a a ··· a
11 12 1n
⎢ ⎢01 ··· 0 ⎥ ⎥ ⎢ ⎢a 21 a 22 ··· a 1n ⎥ ⎥
. IA=⎢ ⎣ . .
.
... ⎥ ⎦ ⎢ ⎣ . .
.
... . .
.
⎥ ⎦ =A, (A.38)
00 ··· 1 a a ··· a
m1 m2 mn
which can be proven by performing the multiplication element-wise. I is also the
.
neutral element for multiplication from the right such thatIA=AI=A.
.
Python implementation: Creating a square identity matrix with 3 rows and 3
columns in python can be done as follows:
A.2.10 Square Matrix
A square matrix has the same number of rows as number of columns. If the number
of rows is n, then n is called the order of the matrix. Two square matrices that have


================================================================================
PAGE 595
================================================================================

A LinearAlgebraforMachineLearning 583
the same order can be added or multiplied. Square matrices play an important role
in many applications and have a number of interesting properties, in particular when
they are additionally symmetric (see Appendix A.2.12).
Python implementation: Testing if a matrix has the correct number of rows and
columns can be very helpful. For example, for ensuring that the matrix A is in fact
a square matrix, one can do the following:
Note the use of assert which gives a (hopefully helpful) error message in case that
the assertion conditions resulted in False .
A.2.11 Inverse Matrix
Definition A.11 (Inverse Matrix) Assume that A is a square matrix. Then,
B is termed inverse matrix of A if:
AB=I, (A.39)
.
where I is the identity matrix. A is then called invertible.
The inverse of a matrix is usually denoted by the superscript (•) −1, e.g., the
.
inverse of A is A
−1.
Note that matrix inversion is not possible for all kinds of
. .
values, i.e., only in cases where the matrix is non-singular. The determinant (see
Appendix A.2.16) can be used to determine if a matrix is invertible or not. For
invertible matrices the following rules hold:
AA −1 =A −1A=I (A.40)
. .
(AB) −1 = A −1B −1 (A.41)
There exist various methods and strategies for computing the inverse of a matrix,
e.g., Gaussian elimination which is also used for solving linear systems of equations
and using the so-called adjoint of a matrix together with the determinant are two of


================================================================================
PAGE 596
================================================================================

584 A LinearAlgebraforMachineLearning
such methods. For further details, the reader is referred to the literature on linear
algebra.
Python implementation: The linalg module of numpy has a function for
computing the inverse of a matrix:
where the resulting identity matrix shows that A_inv is indeed the inverse matrix of
A .
A.2.12 Symmetric Matrix
Symmetric matrices play an important role in linear algebra as they have a number
of useful properties. The following is their definition:
Definition A.12 (Symmetric Matrix) A matrixA is called symmetric if
.
transposing it does not change the values of the matrix elements:
A=AT . (A.42)
.
This definition implies that a symmetric matrix is also a square matrix. Addition-
ally, it can be shown that symmetric matrices exhibit the following properties:
• All eigenvalues of symmetric matrices are real-valued.
• Eigenvectors ofAare orthogonal.
.
• Ann×nmatrix is symmetric if and only if there are n eigenvectors that form an
.
orthonormal basis inRn.
.
• Symmetric matrices are diagonalizable.
More details and derivations are given in Sect.A.3, and in particular the aspect
of diagonalization is a consequence of the spectral theorem, Appendix A.3.4. If a
symmetric matrixAis also invertible, then additionally the following relation holds:
.
(AT) −1 =(A −1)T . (A.43)
.


================================================================================
PAGE 597
================================================================================

A LinearAlgebraforMachineLearning 585
The “chaining” of transposition and inversion operations in arbitrary order is often
abbreviated as(•) −Twhere the bullet symbol “•” serves as a placeholder.
. .
A.2.13 Triangular Matrices
An often encountered task in linear algebra is to decompose a matrix into a product
of typically simpler matrices. One among such matrix types are triangular matrices:
Definition A.13 (Triangular Matrix) A square matrixAis called a triangu-
.
lar matrix if all elements above or below the main diagonal are zero.
Ais called lower triangular matrix if the elements a of Afulfill a = 0
. . ij . . ij
for j > i, i.e., all elements above the main diagonal are zero. The matrix is
.
called upper triangular matrix if the elements fulfill a = 0for j < i, i.e.,
. ij .
all elements below the main diagonal are zero.
An example for a lower triangular matrix is the following matrix:
⎡ ⎤
1. 3 0 0 0
⎢ ⎥
.
A=⎢
⎣
2. 7 .4 0 0 ⎥
⎦
0
. (A.44)
9. 6 .33 .1 8 0
7. 4 .00 .24 .21
The numpy function numpy.tril(A) returns the lower triangular matrix of A , and
numpy.triu(A) returns the upper triangular matrix.
A.2.14 Trace of a Matrix
The sum of all entries on the main diagonal is called the trace of a matrix:
Definition A.14 (Trace of a Matrix) The trace of a matrix Ais abbreviated
.
astr(A)and defined as:
.
tr(A)=a +a +···+a whereA∈Rn,n. (A.45)
. 11 22 nn
The trace occurs, e.g., in many continuum mechanical problems as well as in the
eigenvalue equation Eq.A.50.


================================================================================
PAGE 598
================================================================================

586 A LinearAlgebraforMachineLearning
Python implementation: Computing the trace of a matrix could be done by
element-wise multiplication of the matrix with the identity matrix, followed by
summation—or we use the numpy function as follows:
A.2.15 Rank of a Matrix
Definition A.15 (Matrix Rank) Assume that a matrix A consists of n
columns, the column vectors a . The rank of A, rank(a), is then the maximum
i
number of linearly independent vectors a , or alternatively, the dimension of
i
the vectors’ space spanned by these vectors.
It can be shown that in this definition we can also equivalently use the number of
linearly independent rows of the matrix to determine the rank. In case of a square
matrix, a matrix is said to have full rank if the number of independent rows or
columns is equal to the total number of rows or columns. Only a matrix that has full
rank is invertible.
Python implementation: numpy provides a numerical function which is based
on singular value decomposition (SVD). This function is based on the calculation
of eigenvalues; it offers a parameter for prescribing a tolerance as a guard against
numerical errors. In practice this becomes relevant only for an extreme value range
or badly scaled numbers. Here is how to compute the rank of a matrix:
In [1]: import numpy
A = numpy.array([[1, 2, 3],
[2, 4, 6],
[0, 8, 9]], dtype=float)
numpy.linalg.matrix_rank(A)
Out [1]: 2
Obviously, the first two rows are just multiples of each other which explains the rank
of 2. In the next example, the matrix is an upper triangular matrix with full rank:


================================================================================
PAGE 599
================================================================================

A LinearAlgebraforMachineLearning 587
In [2]: import numpy
A = numpy.array([[1, 2, 3],
[0, 4, 6],
[0, 0, 9]], dtype=float)
numpy.linalg.matrix_rank(A)
Out [2]: 3
A.2.16 Determinant
In ML there are a number of situations where linear systems of equations need to be
solved—which in matrix-vector form requires or results in the inversion of a matrix.
Linear algebra provides a concept for analysis of such matrices: the determinant.
Determinants are additionally used in the context of computing eigenvalues and
eigenvectors.
Again, we assume thatAis a square matrix. Then, the determinant is a scalar that
.
characterizes certain properties of the matrix. One of such property is that a matrix
is invertible if and only if its determinant is nonzero. We write the determinant as
det(A);intheliteraturealsothenotation|A|iscommonly used.Thefollowingshows
. .
without proof how the determinant of a2×2matrix looks like (it is therefore not a
.
strict mathematical definition):
Definition A.16 (Determinant of a 2×2 Matrix) If A is a 2×2 matrix,
. . .
then the determinant is:
(cid:14)(cid:15) (cid:16)(cid:17) (cid:18) (cid:18)
(cid:18) (cid:18)
. det(A)=det a 11 a 12 =(cid:18) (cid:18) a 11 a 12(cid:18) (cid:18) =a 11 a 22 −a 12 a 21 . (A.46)
a a a a
21 22 21 22
(A general definition forn×nmatrices can make use of the Leibniz formula
.
or the Laplace expansion, but for brevity we have to refer the interested reader
to textbooks on linear algebra for this.)
In the above case of the2×2matrix we can use the determinant to compute the
.
inverse ofAas follows:
.
(cid:15) (cid:16)
A −1 = 1 a 22 −a 12 , (A.47)
. det(A) −a a
21 11
which is whyAis invertible only ifdet(A)/=0. Determinants of matrices with more
. .
rows (and equal number of columns) can be computed either with specialized rules
(e.g., for 3 ×3 matrices this is the Sarrus rule) recursively using the concept of
.


================================================================================
PAGE 600
================================================================================

588 A LinearAlgebraforMachineLearning
sub-determinants through the Laplace expansion or numerically. We conclude this
brief paragraph with listing a few rules for computing with determinants:
• Product of matrices: det(AB)=det(A)det(B)
.
• Invariance w.r.t. transposition: det(A)=det(AT)
.
• Scalar multiplication: det(λA)=λndet(A)where n is the rank of the matrix
.
• Invariance w.r.t. change of basis: The determinant of A stays unchanged if a
.
matrixisrepresentedinanewbasissystems(e.g.,afterarotationoftheEuclidean
coordinate frame)
• Square matrices: The determinant of a square matrix is the product of all
diagonal elements.
Python implementation: For numerical computation of the determinant, numpy
provides the function det which is contained in the linalg module.
A.2.17 Orthogonal Matrix
Definition A.17 (Orthogonal Matrix) An orthogonal matrix Q is a matrix
for which the following relation holds:
QQT =QTQ=I, (A.48)
.
where Q is a quadratic matrix and I is the identity matrix.
As a consequence the inverse of an orthogonal matrix is identical to its transpose:
Q −1 =QT (A.49)
.
which is why orthogonal matrices are very useful as computing the inverse is
extremely simple. The following additional properties holds:
• The determinant of an orthogonal matrix is either+1or−1.
. .
• Orthogonality implies that the eigenvectors of the matrix are linearly independent
and therefore form a basis of a vector space.
• While matrices in general represent linear transformations, any orthogonal
matrixQwithdet(Q) =+1represents a rotation.
. .
There are many applications for orthogonal matrices. Among those is the MP3
compression (a normalized version of the discrete cosine transform) which can be


================================================================================
PAGE 601
================================================================================

A LinearAlgebraforMachineLearning 589
represented by an orthogonal matrix. Further applications are principal component
analysis as discussed in Sect.15.2.
Python implementation: To show that a matrix is orthogonal we only need to
show that the product QQT results in the identity matrix. We do this by creating a
.
“rotation matrix” from which we already know that it should be orthogonal:
In [1]: from numpy import array, sin, cos, allclose, eye, pi
# create a matrix Q
theta = pi / 8
Q = array([[cos(theta), -sin(theta)], [sin(theta), cos(theta)]])
# create a 2x2 identiy matrix
I = eye(2)
# show that it's an orthogonal matrix
allclose(Q @ Q.T, I), allclose(Q.T @ Q, I)
Out [1]: (True, True)
Here, we used the numpy function numpy.allclose which compares all elements
of the two given matrices and returns True if and only if all elements are identical
(up to roundoff errors).
A.3 Derived Properties, Further Theorems, and Advanced
Definitions
A.3.1 Eigenvalues, Eigenvectors, and the Characteristic Polynomial
Mathematical details: Eigenvalues are way to express what happens to a matrix
(and in particular to its eigenvectors) when it is subjected to a linear mapping, i.e., a
particular kind of change of “coordinate frame.” The German word “eigen” can be
translated as characteristic or intrinsic. This stems from the fact that eigenvectors
often contain information of fundamental (physical) properties, e.g., in vibrations
of mechanical systems the eigenvalues are the frequencies with which the system
vibrates, and the Hartree-Fock theory defines atomic orbitals as the eigenvectors of
the so-called Fock operator.
We start with the following definition:
Definition A.18 (Eigenvalue Equation) For the real-valued, square matrix
A, the numberλ∈Ris called an eigenvalue ofAif the eigenvalue equation:
. . .
Av =λv (A.50)
.
(continued)


================================================================================
PAGE 602
================================================================================

590 A LinearAlgebraforMachineLearning
holds. There, v is the corresponding eigenvector which is assumed to be a
.
nonzero column vectorv ∈Rn.
.
Eigenvalues and eigenvectors are sometimes also called characteristic values
and characteristic vectors, respectively. To put the above definition in simpler
words, eigenvectors have the property that their direction is unchanged by a linear
transformation. Such a transformation is represented by a square matrixAwhich is
.
applied to a vectorv, i.e., the matrix-vector productAvgives the transformed vector.
. .
If only the length of the vector changes, then all vector components are scaled by the
same scalarλand we again arrive at the eigenvalue equationAv =λv. By bringing
. .
both terms to the same side of the equation, we get:
(λI−A)v =0 (A.51)
.
whereIis the identity matrix which has the same shape as the square matrixA.The
. .
situation ofv =0is not considered an eigenvector, and therefore, it must be a vector
.
with nonzero entries. As a consequence, the other factor of the product, i.e., the term
in parentheses, must be singular because otherwise it would be possible to bring it to
the right-hand side (by inversion); multiplying it with 0, however, results in v = 0
. .
which violates the assumption. Thus, (λI−A) must be singular, i.e., it cannot be
.
inverted. For singular matrices, the determinant must vanish, which is expressed in
the so-called characteristic equation:
Definition A.19 (Characteristic Equation) For a real-valued, square matrix
A, the characteristic equation is:
.
p (λ)=det(λI−A)=0 (A.52)
. A
for some scalarλ. Then,p (λ)is the characteristic polynomial.
. . A
The characteristic polynomial p (λ) is a polynomial with degree of up to
. A
rank(A). The solutions of the characteristic equation, Eq. A.52, are the eigenvalues
.
λ , which are also called the spectrum ofA.
. i .
Surprisingly, the characteristic equation is no longer dependent on v! Note that
.
sometimes this equation is also encountered with changed signs asdet(A−λI)=0
.
which, however, does not influence subsequent derivations as long as this is done
consistently. For solving this equation, we first state two useful properties: If a
quadratic n × n matrix A has the eigenvalues λ ,...,λ , then the trace of A is
. . . 1 n .
the sum of all eigenvalues:


================================================================================
PAGE 603
================================================================================

A LinearAlgebraforMachineLearning 591
(cid:10)n
tr(A)=λ +···+λ = λ . (A.53)
. 1 n i
i=1
The product of all eigenvalues ofAis given by the determinant ofA:
. .
(cid:19)n
det(A)=λ ×λ ×···×λ = λ (A.54)
. 1 2 n i
i=1
where “×” is used to indicate the scalar multiplication. Before we state the general
.
solution of this equation we first write it out for a2×2matrix:
.
(cid:14)(cid:15) (cid:16)(cid:17)
λ−a a
det(λI−A)=det 11 12 (A.55)
. a λ−a .
21 22
= (λ − a )(λ− a ) −a a (A.56)
11 22 12 21.
= λ2− λ(a + a )+ (a a − a a ) =0 (A.57)
11 22 11 22 12 21
Hence, the determinant is a polynomial of degree 2 with λ as the variable. If we
.
additionally make use of the definition of the trace and determinant, we obtain the
characteristic polynomial:
Characteristic polynomial for a2×2matrix:
.
λ2−λtr(A)+det(A)=0. (A.58)
.
Now, the eigenvalues can be easily computed as:
(cid:20)
tr(A) tr2(A)
λ= ± −det(A). (A.59)
.
2 4
Finally, we can also compute the eigenvectors solving Eq. A.50 by insertion of each
of the eigenvalues and solving for the corresponding eigenvectorv.
.
Computing the characteristic polynomial and the resulting eigenvalues and
eigenvectors for larger matrices is quickly getting quite involved. For many ML
applications and dataset, the involved matrices are rather large, and therefore, the
use of numerical algorithms is recommended. However, the main idea still stays the
same, and in particular the structure of the equation—a λn term in the beginning
.
followed by new terms, the trace, and determinant terms at the end—stays roughly
the same.


================================================================================
PAGE 604
================================================================================

592 A LinearAlgebraforMachineLearning
Python implementation: We conclude this section with a Python example that the
reader can also use as an exercise to solve manually. For computing eigenvalues and
eigenvectors we use the numpy function eig from the module linalg :
In [1]: import numpy
A = numpy.array([[2, 3],
[3, -6]], dtype=float)
eig_values, eig_vectors = numpy.linalg.eig(A)
lambda1, lambda2 = eig_values
lambda1, lambda2
Out [1]: array([ 3., -7.])
As a test we now insert the first eigenvalue and eigenvector into the eigenvalue
equation Eq.A.51 and show that evaluating the right-hand side results in a vector
with zero entries:
In [2]: lambda1, lambda2 = eig_values
v1, v2 = eig_vectors[:, 0], eig_vectors[:, 1] # two column vectors
(lambda1 * numpy.eye(2) - A) @ v1
Out [2]: array([0.00000000e+00, 2.22044605e-16])
The result is a vector where both entries are approximately zero since the value of
≈2.2·10 −16can, within the range of the machine accuracy, be considered as zero.
.
A.3.2 Diagonal Matrices and Diagonalization of Matrices
Diagonal matrices play an important role in many applications of linear algebra
such as diagonalization and have a special relationship with eigenvalues and
eigenvectors: in particular, for diagonal matrices the eigenvectors are already
known.
Definition A.20 (Diagonal Matrix) A diagonal matrix is a matrix in which
all entries that are not located on the main diagonal are zero. For the elements
on the main diagonal there are no restrictions and some of them also can be
zero.
To construct a diagonal matrix we use the operator diag which is defined
.
as:
⎡ ⎤
λ 0 ··· 0
1
. diag(λ 1 ,...,λ n ):=
⎢ ⎢
⎢ ⎢ ⎣ 0 . .
.
.
0
..
. 0 ..
0
. .
.
⎥ ⎥
⎥ ⎥ ⎦ withλ i ∈R. (A.60)
0 ··· 0 λ
n


================================================================================
PAGE 605
================================================================================

A LinearAlgebraforMachineLearning 593
Some of the diagonal elements also can be zero, e.g., the following matrix is also
a diagonal matrix:
⎡ ⎤
30 0
.
A=⎣
00 0
⎦
. (A.61)
00−2
In this book we only consider square diagonal matrices. Thus, all diagonal matrices
are symmetric matrices withAT =A.
.
Diagonalization of a matrixAis the process of finding an alternative representa-
.
tion of a square matrix A; this representation consists of an invertible matrix Pand
. .
a diagonal matrixD. Based on this, we define a diagonalizable matrix as follows:
.
Definition A.21 (Diagonalizable Matrix) A square matrix Ais diagonaliz-
.
able if there exists a diagonal matrixDand an invertible matrixPsuch thatA
. . .
has the representation:
D=P −1AP. (A.62)
.
An alternative formulation to Eq. A.62 is the factorization of A into a diagonal
.
matrix D and a matrices P whose columns are the eigenvectors of A–the so-called
. . .
eigendecompositionA:
.
A= PDP −1 . (A.63)
.
If A has full rank, then all eigenvectors are orthogonal. If the eigenvectors are
.
additionally scaled to unit length,2 P is an orthogonal matrix, which implies that
.
P −1 = PT; P represents different types of transformations. P is also called modal
. . .
matrix, and D is also called spectral matrix as it has all eigenvalues as diagonal
.
elements.
Diagonalizable matrices act as to anisotropically scale the space, i.e., the coordi-
nate system spanned by the eigenvectors is scaled by the corresponding eigenvalues.
The intimate relationship between eigenvectors/eigenvalues and diagonal matrices
also shows in the fact that diagonalization is identical to finding eigenvalues and
eigenvectors. This is summarized in Algorithm A.1.
A very convenient aspect of diagonalizable matrices is that their inverse can very
easily be computed as long as none of their eigenvalues is zero. It is simply:
2 Scaling the vectors does not change the result of the eigendecomposition.PDP−1as the inversion
of.P cancels out the scaling factors from BP.


================================================================================
PAGE 606
================================================================================

594 A LinearAlgebraforMachineLearning
(cid:14) (cid:17)
1 1
A −1 = PD −1PT with D −1 =diag ,..., . (A.64)
.
λ λ
1 n
A.3.3 Similarity Transformations
An alternative approach uses the notion of similarity between matrices:
Definition A.22 (Similarity of Matrices) Two square matrices Aand Dare
. .
similar if there is an invertible matrixPsuch that:
.
D=P −1AP. (A.65)
.
As a consequence, a matrix A is diagonalizable if it is similar to a diagonal
.
matrix. In case that the two matrices are similar, then they must have the same
rank, trace, determinant, and eigenvalues. It is sufficient to show that any of these
properties is not fulfilled in order to conclude that two matrices are not similar. On
the other hand, proving that two general matrices are similar is not straightforward.
Often, the Jordan canonical form of the matrices is used, but this is beyond the
scope of this short overview.
A.3.4 The Spectral Theorem
A very useful theorem in linear algebra is the spectral theorem which is also
sometimes called the principal axis theorem. It essentially tells if and how a matrix
can be diagonalized; compare Algorithm A.1. This is important for many derivation
but also for practical purposes as diagonal matrices are easier to handle and the
formulations tend to be simpler.
The name of this theorem stems from the fact that the set of all eigenvalues of
a matrix is called a s pectrum (although for some of the readers that only might
explain one unusual name with another somewhat unusual name). The theorem is
formulated as follows:
Spectral Theorem: Assume thatAis an arbitrary but symmetric matrix. Then
.
A can be diagonalized such that it has only eigenvectors v ,...,v that are
. . 1 n
mutually orthogonal with:
Av =λ v withi =1,...,n (A.66)
. i i i
Theλ ∈Rare the eigenvalues.
. i


================================================================================
PAGE 607
================================================================================

A LinearAlgebraforMachineLearning 595
Algorithm A.1: Summary of the Algorithmic Steps for Matrix Diagonalization (cid:2)
Letusassumeforsimplicitya.3×3squarematrix.A .Thegeneralcaseofa.n×nmatrix
can be handled analogously. These are the steps required for diagonalization of.A :
1. Determine the eigenvalues.λ using the determinant of the eigenvalue equa-
tion. For this, we use Eq.A.52 which reads:
.
det(λI−A)=0,
where .I is the identity matrix. The solution of the equation is obtained using the
characteristicpolynomialandwillresultin(upto)threeeigenvalues.λ iforthe.3 ×3
matrix.
2. For each eigenvalue .λ i find the corresponding eigenvector .v i. We use the
eigenvalue equation Eq.A.50 for each.λ i:
.( λiI−A)vi =0
and use elementary row operations on the term in parentheses. As the system
of equations can be an undetermined, we fix one component of .v i and then
determine the others.
3. “Assemble” the modal matrix. As a preparation, scale the eigenvectors such
that they have unit length, .vi := vi/‖vi ‖. The modal matrix .P consists of the
eigenvectors, ordered in columns:
.P =[v1v2v3] .
.P is now an orthonormal matrix.
4. Compute the diagonal matrix .D . For this we use Eq. A.62 and the spectral
theorem (cf. Appendix A.3.4) to conclude that the eigenvectors are mutually
orthogonal, and thus, .P is an orthogonal matrix with .P −1 = PTwhich is helpful
for computing.D :
.D =PTAP.
Thus,.D will contain the three eigenvalues.λ
1
,.λ
2
,.λ
3
with the following structure:
⎡ ⎤
λ1 0 0
.D
=⎣
0 λ2 0
⎦
,
0 0 λ3
so that from now on we can directly go from the first step (determining the
eigenvalues) to setting up.D .


================================================================================
PAGE 608
================================================================================

596 A LinearAlgebraforMachineLearning
The property of orthogonality implies that the eigenvectors are linearly inde-
pendent and therefore form a basis of a vector space. From the spectral theorem a
number of useful properties and equalities can be derived:
1. For an arbitrary real-valued m×n matrix A, the n×n matrix product ATA is
. . . .
symmetric and so is the m×mproduct AAT. Therefore, the spectral theorem is
. .
applicable for both of these matrix products which is an important prerequisite
for the following claim.
2. The productsATAandAAThave identical nonzero eigenvalues.
. .
This is extremely helpful in case that A has a very large number of rows but
.
a considerably smaller number of columns (or vice versa): we are now able to
choose whether we useAATorATAfor computing eigenvectors and eigenvalues!
. .
3. The eigenvalues ofAATandATAare all larger than or equal to zero.
. .
For brevity, we do not show the proofs here even though they are quite short. The
interested reader is referred to any textbook on linear algebra.
As an example for the usefulness of the spectral theorem, imagine a dataset
consisting of 10000 records and 3 features. The resulting feature matrixAhas 10000
.
rows but only 3 columns. If we need to compute the eigenvalues of the productAAT,
.
then there is an easy way to do this: compute the 9 eigenvalues of the 3×3matrix
.
ATA. Then all other 9997 eigenvalues ofAATare (by help to the claim in 2.) simply
. .
zero!
A.3.5 Eigendecomposition and Singular Value Decomposition
A method closely related to diagonalization which is also used for decomposition
of matrices is singular value decomposition (SVD). There, for a real matrix
.
A the
eigendecomposition has the form of A = PΣQT, where P and Q are orthogonal
. . .
matrices that, however, need not be the inverse of each other. The entries of
Σ are called the singular values of A. Additionally, the diagonal matrix Σ has
. . .
only real values that are larger or equal to zero, as opposed to D from the
.
eigendecomposition which may even consist of complex values. Last but not least,
the eigendecomposition requires . U to be a square matrix while SVD does not have
any requirements.
A comment on the use in principal component analysis: Eigendecomposition as
well as SVD can be used for principal component analysis (compare Sect.15.2); SVD
would directly be applied to the centered data matrix while eigendecomposition acts
on the covariance matrix. In both cases the resulting eigenvalues can be interpreted
as the lengths of the semiaxes of a two-dimensional ellipse or (hyper-) ellipsoid with
the eigenvectors defining the directions of the axes. For some implementations, SVD
can be more memory intensive than the eigendecomposition which is why we focus
here on the latter.


================================================================================
PAGE 609
================================================================================

A LinearAlgebraforMachineLearning 597
A.4 Matrix-Related Operation That Only Exists in Numpy
We conclude this linear algebra overview with a comment on potential pitfalls:
Thereareafewdifferencesbetweenhowmatricesandvectorsarehandledin numpy
as compared to the mathematical, linear algebra definitions and conventions.
A.4.1 Addition of a Scalar to a Matrix
Adding a scalar to all elements of a matrix can be differently done in numpy, which
is mainly related to performance aspects. For example, in numpy the following
operation is valid:
In [1]: import numpy as np
A = np.array([[1, 2, 3],
[4, 5, 6],
[7, 8, 9]], dtype=float)
A + 0.5 # which works equally well as 0.5 + A
Out [1]: np.array([[1.5, 2.5, 3.5],
[4.5, 5.5, 6.5],
[7.5, 8.5, 9.5]])
Mathematically, it is not allowed to sum up two objects with different dimensions.
However, in numpy the product is internally “broadcasted” and optimized. In a
Jupyter notebook the execution time can be measured using the “magic” function
numpy|
In [2]: %timeit A + 0.5
788 ns ± 19.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
Comparing this with addition of A with a matrix with constant values
In [3]: %timeit A + np.array([[0.5, 0.5, 0.5], \
[0.5, 0.5, 0.5], \
[0.5, 0.5, 0.5]])
1.61 µs ± 12.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each
shows that this is a much slower operation as now a full matrix needs to be allocated
in the memory and then it is element-wise added, whereas in the previous version
each element of A is simply incremented by 0.5. As a warning, keep in mind that
even though Python is very forgiving, you should not get used to writingA+0.5in
.
any mathematical document.


================================================================================
PAGE 610
================================================================================

598 A LinearAlgebraforMachineLearning
A.4.2 One-Dimensional vs. Two-Dimensional “Vectors”
Furthermore, Python is also quite forgiving concerning the multiplication of vectors
and matrices—in some cases, that is. The numpy function numpy.dot(a, b)
performs different types of “multiplications” depending on the dimension of the
two inputs. For example, if the second variable is a one-dimensional array, then the
function calculates a sum product over the last axis of a and b .Tobeonthes afe
side, it is always advisable not to mix “1D vectors” (an array with dimensions 1)
with “2D vectors” (1 × n or n × 1 matrices), unless you really know what you
. .
are doing. Usually, it is advisable to turn all one-dimensional vectors into two-
dimensional arrays, e.g., using numpy.reshape .
As this is quite a critical subject we repeat the most important steps here. We
start by constructing a matrix and then extract a row from that matrix:
a is now a one-dimensional array as can also be seen by taking a look at its shape
and dimensions and comparing this to the original matrix A :
Now, we use reshape to put the same data into a new shape. This can be imagined
as first, “flattening” a matrix, i.e., row by row, to concatenate each row vector until
we have a very long vector with all elements of the matrix. Next, these numbers
are then—again row by row—used to create a new matrix, this time with possibly
different numbers of columns and/or rows. For both steps, we always start at the top
left corner of the matrix.


================================================================================
PAGE 611
================================================================================

A LinearAlgebraforMachineLearning 599
In [3]: b = a.reshape(4, 1) # here: identical to a.reshape(-1, 1)
print("shape(b) :", b.shape)
print("dim(b) :", b.ndim)
print(b)
shape(b) : (4, 1)
dim(b) : 2
[[ 9.]
[10.]
[11.]
[12.]]
A commonly used way of “reshaping” is to set one dimension to −1, e.g.,
.
np.reshape(-1, 1) . This implies that the second dimension is fixed as one, from
which the other, remaining dimension (the one with−1) can be inferred. Obviously,
.
the total number of elements cannot change.


================================================================================
PAGE 612
================================================================================

Proofs and Derivations B
Below, a number of chosen proofs and derivations are given, which are not required
for the understanding of the main text but which nonetheless can be useful for
understanding some relations and theorems1 in more detail.
B.1 Proofs of the Theorems About Expectation Values
Theorem B.1 (Expected Value of a Product) Let us assume two indepen-
dent random variables, X and Y. Then, the expectation of the product of the
two random variables is equal to the product of the expectations:
E[XY]=E[X]·E[Y].
.
Proof To prove this theorem for the case of discrete random variables, we assume
that X can take on N values and Y takes on M values. The product XY is again a
random variable which can take the values x y with i = 1 ...N and j = 1 ...M.
i j
If all values are equally likely, then the proof is very simple:
(cid:10)N (cid:10)M
1
E[XY]= x y
. i j
NM
i=1j=1
1 A theorem is a mathematical statement or relation that can be derived or proved.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 601
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9


================================================================================
PAGE 613
================================================================================

602 B ProofsandDerivations
(cid:21) (cid:22)⎛ ⎞
(cid:10)N (cid:10)N
= 1 x ⎝ 1 y ⎠
i j
N M
i=1 j=1
= E[X]· E[Y] . □ (B.1)
B.2 Proofs of Some Theorems About Variances
Theorem B.2 (Multiplication with a Scalar) Assuming that X is a random
variable and c a scalar value. Then the following identity holds:
Var(cX)=c2Var(X)
.
Proof
(cid:27) (cid:28)
Var(cX)=E (cX−cE[X])2
.
(cid:27) (cid:28)
=E c2(X−E[X])2 (B.2)
with the relation E[cX] = cE[X] according to Eq. 7.6 we can take c2 out of the
operator leading to
(cid:27) (cid:28)
=c2E (X−E[X])2
.
=c2Var(X). □ (B.3)
Theorem B.3 (SumofTwoRandomVariables) Assuming that X and Y are
two random variables that are independent from each other. Then the variance
of their sum is the sum of their variances:
Var(X+ Y)=Var(X)+Var(Y)
.


================================================================================
PAGE 614
================================================================================

B ProofsandDerivations 603
Proof
(cid:27) (cid:28)
Var(X+ Y)=E ((X+ Y)−E[X+Y])2
.
(cid:27) (cid:28)
=E ((X−E[X])+(Y −E[Y]))2
(cid:27) (cid:28)
=E (X−E[X])2+2(X−E[X])(Y −E[Y])+(Y −E[Y])2
(B.4)
Because X and Y are independent variables, also X − E[X] and Y − E[Y] are
independent variables. Thus, their product is zero. Additionally, the expectation
value of a sum is the sum of the expectation values and we get:
(cid:27) (cid:28) (cid:27) (cid:28)
Var(X+ Y)=E (X−E[X])2 +0+E (Y −E[Y])2
.
=Var(X)+Var(Y). □ (B.5)
B.3 Simplification of Pearson Moment Coefficient of Skewness
Pearson moment coefficient of skewness as introduced in Sect.9.3.6 is given by:
(cid:29)(cid:14) (cid:17) (cid:30)
X−μ 3
γ =E with μ=E[X]. (B.6)
. 1
σ
There, σ is the standard deviation. Expand the terms of the expectation values to
.
arrive at a simpler term that can easily be written down for multiple data x (with
. i
i =1 ...N):
.
(cid:8) (cid:9)
E (X−μ)3
γ = (B.7)
. 1 σ3 .
(cid:8) (cid:9) (cid:8) (cid:9)
E X3 − 3μE X2 + 3μ2E[X]− μ3
= (B.8)
σ3 .
(cid:8) (cid:9) (cid:31) (cid:8) (cid:9)
E X3 − 3μ E X2 − μE[X] − μ3
= (B.9)
σ3 .
(cid:8) (cid:9)
E X3 − 3μσ2− μ3
= (B.10)
σ3 .
(cid:8) (cid:9)
E X3 − 3μσ2− μ3
= . (B.11)
(σ2) 3 2


================================================================================
PAGE 615
================================================================================

604 B ProofsandDerivations
This can now be easily computed with:
(cid:10)N
1
μ= x (B.12)
. i.
N
i=1
(cid:10)N
1
σ = (x − μ)2 (B.13)
i .
N
i=1
(cid:27) (cid:28) (cid:10)N
1
E X3 = x3 . (B.14)
N i
i=1
B.4 Proofs and Additional Derivations for Distributions
B.4.1 Property of the Binomial Coefficient
We start with the definition of the binomial coefficient which is given as:
(cid:14) (cid:17)
n n!
= (B.15)
. k (n−k)!·k!
Next, we introduce one of the most well-known identities that are often used for
proofs and derivations in the context of binomial coefficients:
(cid:14) (cid:17) (cid:14) (cid:17)
n n−1
k =n , (B.16)
. k k−1
which is a way to link the binomial coefficient of the “n-1/k-1” quantit(cid:31)ies t o the
original ones. For our derivation, the point of departure is the expression
n−1
:
. k−1
(cid:14) (cid:17)
n−1 (n−1)!
= (B.17)
. k−1 ((n−1)−(k−1))!·(k−1)!.
(n− 1)!
= (B.18)
(n− k)!· (k − 1)!
Subsequently, we employ the property of the factorial for which it isx !=(x−1)!·x.
.
Insertion of this relation into Eq. B.15 gives:
(cid:14) (cid:17)
n n! (n−1)!·n
= = (B.19)
. k (n−k)!·k! (n−k)!·(k−1)!·k .
n (n − 1)!
= · (B.20)
k (n− k)!·( k − 1)!


================================================================================
PAGE 616
================================================================================

B ProofsandDerivations 605
This expression now allows us to rewrite the left-hand side of Eq. B.16 as:
(cid:14) (cid:17)
n n (n−1)!
k =k (B.21)
.
k k
(n−k)!·(k−1)!.
(n− 1)!
= n (B.22)
(n−k)!·(k − 1)!
As alaststepweuse:
(cid:14) (cid:17)
n−1 (n−1)!
= (B.23)
. k−1 (n−k)!·(k−1)!
from above and obtain as a final result:
(cid:14) (cid:17) (cid:14) (cid:17)
n n−1
k· =n· . (B.24)
. k k−1
B.4.2 Binomial Distribution: Derivation of Mean and Variance
The binomial distribution is, according to Sect.10.4, given by:
(cid:14) (cid:17)
n
f(k)= pk(1−p)n−k . (B.25)
.
k
To derive the formulation of the mean we start with the general definition of the
(population) mean:
(cid:10)n
μ= x p(x ) (B.26)
. i i
i=0
where the population, for which the mean is calculated, consists of altogethern+1
.
items. We then substitute the probability mass function (PMF), Eq. B.25,for
.
p(x):
(cid:14) (cid:17)
(cid:10)n
n
.
μ= k
i
pki (1−p)n−ki. (B.27)
k
i=0 i
If we compute the first element of the sum for i = 0, we find that it is zero. For all
.
following values it is in general:
(cid:14) (cid:17) (cid:14) (cid:17)
n n−1
k =n (B.28)
. k k−1


================================================================================
PAGE 617
================================================================================

606 B ProofsandDerivations
which then gives:
(cid:14) (cid:17)
(cid:10)n n−1
.
μ= n
k −1
pki (1−p)n−ki. (B.29)
i=1 i
(cid:14) (cid:17)
(cid:10)n n− 1
= np pki −1 (1−p)n−ki (B.30)
k − 1
i=1 i
where we took np out of the summation as it does not depend on i. The last term
.
(1−p)n−ki
can be replaced by
.
(1−p)n−1(1−p) −(ki −1).
With this, we obtain:
(cid:14) (cid:17)
(cid:10)n n−1
.
μ=np
k −1
pki −1(1−p)n−1(1−p) −(ki −1). (B.31)
i=1 i
To get rid of all then−1andk−1terms, we substitutem:=n−1andj =k −1:
. . . . i i
(cid:14) (cid:17)
(cid:10)m
m
.
μ=np pji (1−p)m−ji. (B.32)
j
i=0 i
where the summands have the same structure as the binomial PMF (except for the
variable names). Thus, summing this PMF-expression must result in 1, as it is the
probability of all outcomes:
(cid:14) (cid:17)
(cid:10)m
m
.
pji (1−p)m−ji =1. (B.33)
j
i=0 i
and therefore we get for the mean:
μ=np . (B.34)
.
B.4.3 Isocontour Lines of a Bivariate Normal Distribution
Assume a bivariate normal distributionN (x ,x ; μ ,μ ,σ ,σ )whose probabil-
. 1 2 1 2 1 2
ity density function (PDF) has the following form:
(cid:21) (cid:22)
1 (x −μ )2 (x −μ )2
f(x ,x ; μ ,μ ,σ ,σ )= exp − 1 1 − 2 2
. 1 2 1 2 1 2 2πσ 1 σ 2 2σ 1 2 2σ 2 2
(B.35)


================================================================================
PAGE 618
================================================================================

B ProofsandDerivations 607
Then the level set of all points (x ,x )for given shape parameters μ ,μ ,σ , and
. 1 2 . 1 2 1
σ that have the constant valuec∈Ris given by:
. 2 .
.
f(x ,x ; μ ,μ ,σ ,σ )= c. (B.36)
. 1 2 1 2 1 2
The question is now, what the geometrical shape of the isocontour lines is. Our
intuition points us into the direction of an ellipse, which is in general given in terms
of its major and minor semiaxes, a and b, by:
x2 x2
1 + 2 =1 (B.37)
. a2 b2
This equation assumes that the ellipse is centered at the origin of the coordinate
system. To prove that the contour lines of the bivariate normal distribution have
elliptical shape, we start by determining all points (x ,x )that fulfill Eq.B.36 and
. 1 2
will bring it into the governing equation for the ellipse, Eq. B.36:
(cid:21) (cid:22)
1 (x −μ )2 (x −μ )2
c= exp − 1 1 − 2 2 . (B.38)
. 2πσ 1 σ 2 2σ 1 2 2σ 2 2
Multiplying both sides with2πσ σ and applying thelnfunction results in:
. 1 2 .
(x −μ )2 (x −μ )2
− 1 1 − 2 2 =ln(2πcσ σ ). (B.39)
. 2σ2 2σ2 1 2
1 2
This can now be simplified by multiplying with −1 and utilizing that −ln(x) =
. .
ln(1/x):
(cid:14) (cid:17)
(x −μ )2 (x −μ )2 1
1 1 + 2 2 =ln . (B.40)
. 2σ 1 2 2σ 2 2 2πcσ 1 σ 2
Furthermore, dividing both sides of the equation by the logarithm term gives:
(x −μ )2 (x −μ )2
1! 1 " + 2! 2 " =1. (B.41)
.
2σ2ln 1 2σ2ln 1
1 2πcσ1σ2 2 2πcσ1σ2
This is already quite close to Eq. B.37, and with the definitions of the two semiaxes
as:
(cid:20) (cid:20)
(cid:14) (cid:17) (cid:14) (cid:17)
1 1
a = 2σ2ln and b= 2σ2ln , (B.42)
. 1 2πcσ σ 2 2πcσ σ
1 2 1 2


================================================================================
PAGE 619
================================================================================

608 B ProofsandDerivations
where we assumed that a is the major semiaxis and b is the minor semiaxis (we
simply switch them ifa <b). Insertion of these abbreviations yield:
.
(x −μ )2 (x −μ )2
1 1 + 2 2 =1, (B.43)
. a2 b2
which is the equation for an ellipse that is shifted along the two directions by μ
. 1
and μ . In terms of the distribution, this equation describes the line that indicates a
. 2
value of the PDF of c.
References
1. C. Hill. Learning Scientific Programming with Python, 2nd edition. Cambridge University
Press, 2016. ISBN 978-1108745918.
2. Q. Kong, T. Siauw, and A. M. Bayen. Python Programming and Numerical Methods. Elsevier,
2021. ISBN 978-0-12-819549-9. DOI https://doi.org/10.1016/c2018-0-04165-1.


================================================================================
PAGE 620
================================================================================

List of Python Listings
9.1 A Python function for reading the file iris.csv. The function
returns the feature matrix and the target vector. Note the
specification of the dtype when the two np.array s are created.
Due to the mixed data type (numbers and strings) in the
file iris.csv, we import all of it as strings and then create X
as a float and y as a string . Section 4.7 provides more
information on how to obtained the file .................................... 181
10.1 A minimal example for plotting the PDF and CDF of a normal
distribution ................................................................... 219
12.1 Python function for performing the train-test split. Note that
contrary to all best practices in programming, we omitted
docstrings, comments, type hints, and “sanity checks” to ensure
that the code stays compact ................................................. 286
13.1 A Python function that returns piecewise polynomial
approximations at equidistant points. The main difference to
regression codes in the previous section is that we use idx to
“filter out” all points that are not in the current interval. The
resulting data for each interval are stored as a list of tuple of the
x and y coordinates .......................................................... 345
13.2 Kernel regression with radial basis functions: the Python function
for the RBF and for creating the feature matrix (to be used with
the code snippets in the text) ................................................ 350
14.1 An implementation of the kNN algorithm. In case that there is a
tie during voting (here implemented using the argmax function,
which returns the index of the element with the maximum value),
then only the first class with the highest count contained in
count will be used .......................................................... 378
16.1 An implementation of the train-test split ................................... 445
17.1 Implementation of the Rosenblatt perceptron for making a
prediction. Here, the prediction will be a value of either 0 or
1. However, it is also possible to use, e.g., −1 and +1 (see
. .
Sect.17.2.5 for an explanation) ............................................. 465
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 609
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9


================================================================================
PAGE 621
================================================================================

610 ListofPythonListings
17.2 Implementation of the perceptron learning rule. The code reuses
the function predict from Python Listing 17.1. Here, the training
is stopped if the error is zero. Note that contrary to all best
practices in programming, we omitted docstrings, comments,
type hints, and “sanity checks” to ensure that the code stays
compact ...................................................................... 473
17.3 An implementation of the network of two neurons. Even though
it is a very simplistic implementation, it is able to handle a
number of input records at the same time as can be seen by the
fact that x1 and x2 are both numpy array s .............................. 486
17.4 General implementation of the vectorized equations for a
network with one hidden layer. Note that np.where also works
with the row vector theta1 : it automatically applies it to each
row of X1 @ W1 so that creating THETA1 is not required .................. 492
18.1 Implementation of a network with two input nodes, two nodes
in a hidden, layer and a single output node. See the text for
more information. Note that contrary to all best practices in
programming, we omitted docstrings, comments, type hints, and
“sanity checks” to ensure that the code stays compact .................... 507


================================================================================
PAGE 622
================================================================================

Index
A Bar diagram, 199
Abaqus, 3 Baseline model, 452
Abstract base class, 517 Basis for polynomials, 330
Accuracy, 258 Basis functions, 329, 330
Activation functions, 459, 498 Basis vector, 397
linear, 498 Batch normalization, 524
logistic, 500 Batch size, 524
ReLU, 500 Bayes, T., 10
sigmoid, 500 Bayes’ theorem, 99
tanh, 501 Bayesian information criterion (BIC), 429
Activation layer, 512 Bellman, R.E., 26
ADALINE model, 476 Bernoulli trial, 210
Adam, 543 Bessel’s correction, 190
Adaptive data generation, 547 Bias, 153, 258
Additive principle, 70 Bias-variance tradeoff, 320
Aether, 6 Bimodal, 189
Aggregation function, 459 Binary classification, 359
AI winter, 481, 497 Binary cross-entropy loss, 552
AlexNet, 541 Binary segmentation, 262, 546
Anaximenes, 6 Bishop, C., 237
Anscombe’s quartet, 196 Bivariate distributions, 110
Antecedent, 361 Black box model, 42, 239
Aquinas, T., 6, 8 Boolean decision problem, 461
Aristotle, 6 Bootstrap, 158, 450
Array, 32 Bottleneck blocks, 543
Artificial intelligence, 16 Box-Cox transformation, 251
Ashby, M., 19 Box plot, 200
Ashby map, 19 Box-whisker plot, 200
Atomistic simulations, 20 Brackets, 32
Attribute, 257 Broyden, Fletcher, Goldfarb, and Shanno
Autoencoder, 545, 548 (BFGS), 543
Average, 87, 133
Average pooling, 538
C
Cahn-Hilliard equation, 59
B Candidate model, 441
Babylonia, 4 Categorical encoding, 438
Backpropagation, 469, 501, 510, 513 Centered data matrix, 168
Backward pass, 501, 512 Centering, 251
Bandwidth, 351 Central limit theorem, 163, 218
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 611
S. Sandfeld, Materials Data Science, The Materials Research Society Series,
https://doi.org/10.1007/978-3-031-46565-9


================================================================================
PAGE 623
================================================================================

612 Index
Central tendency, 183, 187 Cumulative frequencies, 112
Chain rule, 504, 513 Curse of dimensionality, 9, 26, 256, 321, 390,
Characteristic equation, 590 429
Characteristic polynomial, 589–591
Cherry picking, 10, 163
Class IDs, 367 D
Classification, 238, 253 Data, 23
binary, 359, 470 augmentation, 544
Class imbalance, 373, 449 bins, 198
Clustering, 253, 424 categorical, 89
Cnidus, E., 6 cleaning, 245
Code, 548 collection, 244
Coefficient of determination, 529 compression, 394
Coefficient of momentum, 544 covariance matrix, 223
Coefficient of variation, 261 matrix, 45
Collinearity, 334 preparation, 245
Column vector, 38, 570 preprocessing, 245
Combinations, 81, 84 record, 45
without repetition, 81 space, 290
with replacement, 84 structured, 43
Combinatorics, 69 tabular, 45
Concentric half-spheres, 6 transformation, 247
Conditional, 555 visualization, 196, 256
Conditional probability, 99, 125 Datalogy, 11
Condition number, 334 Data matrix, 47, 49
Confounding variable, 175 Data mining, 11, 21
Confusion matrix, 263 Data science, 15, 18
Consequent, 361 Dataset, 44
Constrained optimization, 402 Cahn-Hilliard, 58
Contingency table, 186 element properties, 60
Continuous probability distribution, 113 handwritten digits, 63
Control points, 346 iris flower, 62
Convolution, 534 Ising, 57
Convolutional layer, 534 MNIST, 63
Convolutional neural network, 533 nanoindentation, 60
Copernican Revolution, 7 tensile test, 53
Copernicus, N., 7 DBSCAN, 430
Correlation, 172 de Brahe, T., 8
Correlation coefficient, 10, 529 Decision boundary, 254, 359, 363, 367, 468
Correlation matrix, 175 Decoder, 548
Correlation of variables, 184 Deep CNN autoencoders, 550
Cost function, 293, 298, 502 Deep Galerkin method, 563
Covariance, 165 Deep learning, 497
matrix, 167 Deep neural network, 17
Cross-entropy, 267 DeepONet, 563
Cross tab, 186 Denoising, 418, 550
Cross validation, 441 Descriptive statistics, 152
k-fold, 447 Design matrix, 49, 325
leave-one-out, 446 Design of experiment, 28
repeated k-fold, 448 Deviation
stratified k-fold, 449 mean squared, 259
Cumulative distribution function, 115 Diagonalization, 593


================================================================================
PAGE 624
================================================================================

Index 613
Dice coefficient, 265 Euclidean distance, 369, 573
Digital twin, 26 Euclidean norm, 572
DIKW pyramid, 25, 237 Event, 69, 92, 94
Dilated convolutions, 539 impossible, 94
Dilation rate, 539 independent, 96
Dimensionality reduction, 253, 391, 408, 548 Evidence, 100
Discovery of new materials, 394 Example, 45
Discriminator, 550 Expectation operator, 132
Disjoint, 71 Expectation step, 427
Dislocation, 21 Expectation value, 132
Dispersion, 184, 190 homogeneity, 134
Distribution linearity, 134
Bernoulli, 210 scaling, 134
bimodal, 202 translating, 135
binomial, 211 Experiment, 69
bivariate normal, 127, 219 Explanatory variable, 257, 283
categorical, 109 Exploratory data analysis (EDA), 179
discrete uniform, 208 Extended feature matrix, 324
exponential, 229 Extrapolation error, 278
Gaussian, 217, 425 Extreme outlier, 201
geometric, 213 Extrinsic dimensionality, 390
logistic, 230
lognormal, 227
multivariate, 110 F
multivariate normal, 222 Factorial, 73
normal, 217 False negative, 262, 360
Poisson, 215 False positive, 262, 360
univariate, 110 Feature, 257, 283
Distribution shape, 184 creation, 438
Domain knowledge, 17, 438 engineering, 19, 411, 437
Dot product, 571 selection, 440
Dropout, 527 transformation, 437
Feature matrix, 48
extended, 336
E Features, 45
Early exaggeration, 422 Feed forward step, 469
Early stopping, 544 Fence, 201
Eigendecomposition, 408, 593 inner, 201
Eigenvalue, 589 outer, 201
Eigenvalue equation, 403 Final model, 441
Eigenvector, 589, 593 Fine-tuning, 547
Embedding, 410 First central moment, 144
Encoder, 548 First principal component, 404
Ensemble methods, 420 First raw moment, 142
Epsem, 156 Forward pass, 501, 510
Error, 258 Fourier basis functions, 341, 353
extrapolation, 278 Fourier series, 353
mean absolute, 259 Fourth moment, 195
mean squared, 259 Frequency distribution table, 110
root mean squared, 260 Frequency table, 184
root mean squared logarithmic, 261 F1 score, 266
testing, 284 Full rank, 586
training, 275, 284 Fully connected layer, 488, 510, 514
Error measure, 257 Fully connected network, 487


================================================================================
PAGE 625
================================================================================

614 Index
G J
Galton, F., 10, 271 Jaccard similarity, 263
Gauss, C.F., 10 Joint CDF, 123
Gaussian mixture model, 425 Joint distribution, 122
Gaussian Naive Bayes, 378 Joint PMF, 123
Generator, 550 Joint probability, 99, 121
Global minimum, 298 Joint probability density function, 124, 126
Glorot, X., 525 Joint probability distribution, 122
Goodfellow, I., 550
Gradient descent, 299, 512
mini-batch, 543 K
stochastic, 543 KDE plot, 202
Gram matrix, 327 Kepler, J., 8
Greedy maximization, 402 Kernel density estimation, 352
Grid search, 294 Kernel projection, 387
Kernel regression, 348
Kernel smoothing, 349
H Kernel trick, 387, 420
Hamming distance, 370 k-medians, 424
Hand-labeling, 546 kNN regression, 355
He, K., 526 Knots, 344
Hidden layer, 488 Knowledge, 23, 25
Hinge loss, 385 Knowledge discovery, 20
Histogram, 198 Kolmogorov, A., 97
Hoff, T., 476 Kolmogorov axioms, 97
Holdout method, 283 Kramer, M.A., 548
Hyperparameter, 243, 318, 428 Kurtosis, 195
Hyperparameter optimization, 320, 374
Hyperparameter study, 441
Hypothesis, 279, 280 L
Hypothesis class, 366 Label, 252, 257, 283
Hypothesis space, 280, 289 Label encoding, 438
Langrangian multiplier, 402
Laplace, P.S., 10
I Lasso regression, 354
Image reconstruction, 417 Latent space, 548
Independent events, 97, 122 Law of truly large numbers, 163
Independent variable, 42 Lazy learning, 374
Independent variables, 283 Leaky ReLU, 500
Inference, 152, 237, 253, 281 Learning
Inferential statistics, 152 instance-based, 240
Information, 23 lazy, 242
Input data matrix, 48 memory-based, 240
Input layer, 488 unsupervised, 253
Input variable, 41, 257 Learning bias, 562
Instance, 43, 45 Learning rate, 299
Instance-based learning, 374 Least square (LSQ) method, 10, 274, 293
Instance space, 290 Lecun, Y., 540
Interquartile range, 200 Legendre, A.-M., 10
Intersection over union, 264 Lenet, 540
Interval, 36 Libration, 9
Intrinsic dimensionality, 390 Likelihood, 87, 100
Inverted dropout, 528 Linear function, 317
Ising, E., 57 Linearly separable, 359


================================================================================
PAGE 626
================================================================================

Index 615
Linear model, 288 Max pooling, 538
Linear sampling, 156 Mayer, J.T., 9
List comprehension, 161 McCulloch, W., 459
L1 loss, 259 Mcculloch-Pitts model, 459
L2 loss, 259 Mean, 133, 142
.L 2norm, 573 arithmetic, 187
Local minimum, 298 geometric, 187
Logistic function, 231 harmonic, 187
Lookup table, 42 Mean error, 259
Loss function, 292, 502 Median skewness, 194
Loss landscape, 307 Metadata, 3
Method of least squares, 10
Metric, 323
M Microscopy, 21
Machine epsilon, 299 Microstates, 74
Machine learning definition, 235 Mild outlier, 201
Macrostates, 74 Min-batch, 524
Mahalanobis distance, 225 Minimization, 298
Majority class, 158 Minkowski distance, 370
Making inference, 152 Min-max GAN loss, 552
Manhatten distance, 370 Min/max scaler, 251
Manifold learning, 421 Minority class, 158
Margin, 385 Mitchell, T.T., 237
Marginalization, 141 MNIST database, 63
Marginal probability, 99 Modal value, 189
Material property charts, 18 Mode, 189
Materials data science, 18 Mode collapse, 554, 562
Materials Genome Initiative, 20 Model, 5, 238, 239, 254
Matrix, 37, 575 Model class, 275
addition, 576 Model parameter, 254, 275
covariance, 223 Model selection, 428, 429
determinant, 587 Moment
diagonal, 592 central, 142
diagonalization, 592 first raw, 142
Hadamard product, 580 standardized, 145
inverse, 583 zero-th raw, 141
lower triangular, 585 Momentum, 544
modal, 593 Monomials, 329
neural element, 582 Most general hypothesis, 368
orthogonal, 588 Most specific hypothesis, 368
product, 577 Moving averages, 348
rank, 586 Mulicolinearity, 354
scalar multiplication, 577 Müller, A.C., 237
spectral, 593 Multiclass classification, 360
square, 582 Multicollinearity, 334
symmetric, 584 Multidimensional scaling, 421
trace, 585 Multilayer perceptron, 487, 501
transposition, 581 Multimodal, 189
triangular, 585 Multiple linear regression, 323, 335
upper triangular, 585 Multiple linear regression models, 310
vector multiplication, 579 Multiple polynomial regression, 339
Maximization step, 427 Multiplication principle, 71, 79
Maximum likelihood estimation, 426 Multivariate distribution, 185
Maximum margin estimator, 385 Mutually exclusive, 94


================================================================================
PAGE 627
================================================================================

616 Index
N Pearson, K., 10, 271
Naive Bayes methods, 378 Pearson’s measure of kurtosis, 195
Nash equilibrium, 551 Pearson’s mode skewness, 194
Natural splines, 346 Pearson’s 2nd coefficient of skewness, 194
Naur, P., 11 Percentile, 188, 200
Negative example, 363 Perceptron, 463
Network Perceptron learning rule, 469
architecture, 510 Permutation, 74
Neuron Permutation feature importance, 439
biological, 457 Permutations, 79, 80
model, 457 Perplexity, 421
multipolar, 458 Physical units, 3
Non-linear function, 317 Physics-informed neural network (PINN), 563
Nonlinear scaling, 250 Physics informed machine learning, 562
Non-parametric dimensionality reduction, 421 Piecewise polynomial function, 344
Non-parametric methods, 342 Piecewise regression splines, 346
Normal distribution, 425 Piecewise step function, 345
Normalization, 250, 261 Pitts, W., 459
Normalized Xavier initialization, 525 p-norm, 370
Normal matrix, 327 Poisson point process, 216
Poisson process, 216
Polynomial interpolation, 277
O
Polynomial regression, 318, 336
Object detection, 264
Pooling, 538
Objective function, 294, 298
Population, 151
Observation, 89
Position vector, 570
Observational bias, 562
Positive example, 363
Occam, W., 7, 8
Posterior probability, 100
Occam’s razor, 7, 8, 429
Precision, 258, 266
One-hot encoding, 251
Prediction, 253, 281
Operator learning, 563
Predictors, 283
Optimal hyperplane, 385
Preprocessing, 245
Ordinary least squares, 327
Principal
Orthogonal projection, 573
direction, 404
Orthogonal vector projection, 573
stress, 392
Orthogonal vectors, 572
stress components, 393
Outcome, 92, 93
Principal component analysis (PCA), 391
Outlier, 188, 201, 246
application, 430
global, 246
kernel, 420
point, 246
randomized, 420
Output data matrix, 49
Principle of parsimony, 8
Output layer, 488
Prior probability, 100
Output variable, 257
Probabilistic model, 378
Overfitting, 6, 282, 320, 351
Probability, 86
Oversampling, 158
density function, 108, 109
function, 108
P joint, 97
Padding, 537 mass function, 108, 109
Parameterized ReLU, 500 table, 111
Parameter space, 280, 289 Probability theory, 87
Parametric models, 322 Problem of vanishing gradients, 542, 555
Parametric regression, 342 Product rule, 99
Parentheses, 32 ProGAN, 555
Pattern, 237 Ptolemy, C., 6


================================================================================
PAGE 628
================================================================================

Index 617
Q over, 544
Quartile, 200 simple random, 155
stratified, 159
systematic, 156
R take from top, 158
Radial basis function, 349 weighted, 158
Random experiment, 88 Samuel, A., 236
Random noise, 278 Scalar field, 575
Random variable, 86, 103 Scalar product, 571
continuous, 106 Scatter plot, 197
discrete, 105 Scatter plot matrix, 203
image, 106 Scientific machine learning, 562
image of a, 105 Scientific Python Stack, 182
scaling, 134 Score value, 404
Range, 184 Scree plot, 415, 430
Range scaling, 251 Seaborn, 203
Rare events, 163, 188, 246 Second principal component, 407
Raw moment, 140 Segmentation, 264
Real numbers, 32 Selection bias, 153
Recall, 266 Semantic segmentation, 545
Regressand, 257 Semi-parametric method, 343
Regression, 238, 253, 271 Set, 32
multiple linear, 287 difference, 34
simple linear, 287 disjoint, 34
Regression plane, 311 empty, 34
Regression splines, 346 intersection, 34
Regression to the mean, 162 operation, 32, 33
Regressor, 257 union, 34
Regularization, 354 Shallow neural network, 17
Reinforcement learning, 253 Shrinkage penalty, 354
Relative cumulative frequency, 112 Simple linear regression, 335
Research data management, 20 Singular value decomposition, 596
Residual, 258 Skewness, 193
block, 542 Skip connection, 542
connection, 542 Spectral theorem, 594
error, 291 Spectrum, 590, 594
Resnet, 541 Spinodal decomposition, 58
Response variable, 257 Spurious correlation, 174
Retrograde motion, 4, 5 Squashing functions, 498
Ridge regression, 354 Standard deviation, 10, 139
Rosenblatt perceptron, 463 Standardization, 250
Row matrix, 325 Statistical
Row vector, 38 error, 291
Rummelhart, D.E., 497 inference, 152, 278
Runge, C.D., 346 Statistical sampling, 440
Statistics, 149
Steepest descent, 299
S Step function, 498
Same padding, 538 Stirling’s approximation, 73
Sample, 151 Stochastic regularization, 527
representative, 153 Strata, 159
Sample mean, 187 Stratification, 159
Sample space, 92, 93 Stratified k-fold cross-validation, 449
Sampling, 69, 151, 153, 154 Stratified sampling, 449


================================================================================
PAGE 629
================================================================================

618 Index
Stride, 538 V
Structured dataset, 43 Validation dataset, 441
StyleGAN, 555 Valid padding, 538
Summary statistics, 183 Vandermonde matrix, 337, 341
Supervised learning, 22, 252 Vanishing gradients, 542
Support vector, 385 Variability, 258
Support vector regression, 355 Variable, 257
Systematic sampling, 156 Variance, 137
Variance explained, 416
Vector, 37, 570
T component, 571
Take from top, 158 field, 574
Tanzi, E., 459 product, 571
Target, 283 space, 570
Target variable, 252, 257 transposition, 571
Testing cost, 294 Vectorization, 302
Testing dataset, 441 Venn diagram, 34
t-distributed stochastic neighbor embedding Visualization, 196
(t-SNE), 421, 430 Voronoi tesselation, 372
Threshold logical unit (TLU), 459
Total covariance, 409
Training cost function, 294 W
Training data, 239 Waldeyer, H.W., 457, 459
Train/test split, 446 Wasserstein distance, 555
Transfer learning, 544 Wasserstein GAN, 555
Transformers, 556 Wavelet regression, 354
True function, 278 Weighted kNN, 373
True negative, 262 Weight initialization, 525
True positives, 262 Weight space, 289
Tukey, J.W., 11 Weight update, 501
Tuple, 32, 35 Werbos, P.J., 497
Whisker, 201
White box model, 240
U Widrow, B., 476
Uncertainty, 87 Wisdom, 25
Underfitting, 282, 319 Wu, C.F.J., 11
Undersampling, 158
U-Net, 545
Uniform, 189 X
Uniform manifold approximation and Xavier initialization, 525
projection (UMAP), 21 XOR problem, 462, 467, 468, 481, 484
Unimodal, 189
Unit vector, 573
Univariate distribution, 184 Z
Unsupervised learning, 389 Zahavi, J., 11

