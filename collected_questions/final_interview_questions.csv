question_text,company,difficulty,question_type,topics,source,answer_text,created_at
"More Data: Generally reduces variance, but can also help a high-bias model better capture underlying patterns.",,medium,stats,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639618
Feature Selection/Engineering: Aims to reduce overfitting by focusing on the most relevant features.,,medium,coding,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639658
Simpler Models: Helps alleviate overfitting; reduces variance but might increase bias.,,medium,stats,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639684
"Regularization: A technique that adds a penalty term for model complexity, which can help decrease overfitting.",,medium,ml,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639712
"Ensemble Methods: Combine multiple models to reduce variance and, in some cases, improve bias.",,medium,stats,ensemble,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639737
"Cross-Validation: Helps estimate the performance of a model on an independent dataset, providing insights into both bias and variance. <br> ## 6. Explain the concept of _Cross-Validation_ and its importance in ML. Cross-Validation (CV) is a robust technique for assessing the performance of a machine learning model, especially when it involves hyperparameter tuning or comparing multiple models. It addresses issues such as overfitting and ensures a more reliable performance estimate on unseen data. ### Kinds of Cross-Validation",,medium,stats,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639810
Holdout Method: Data is simply split into training and test sets.,,medium,ml,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639831
"K-Fold CV: Data is divided into K folds; each fold is used as a test set, and the rest are used for training.",,medium,ml,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639857
"Stratified K-Fold CV: Like K-Fold, but preserves the class distribution in each fold, useful for balanced datasets.",,medium,stats,probability,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639882
Leave-One-Out (LOO) CV: A special case of K-Fold where K equals the number of instances; each observation is used as a test set once.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.639919
"Time Series CV: Specifically designed for temporal data, where the training set always precedes the test set. ### Benefits of K-Fold Cross-Validation - Data Utilization: Every data point is used for both training and testing, providing a more comprehensive model evaluation. - Performance Stability: Averaging results from multiple folds can help reduce variability. - Hyperparameter Tuning: Helps in tuning model parameters more effectively, especially when combined with techniques like grid search. ### Code Example: K-Fold Cross-Validation Here is the Python code: `python import numpy as np from sklearn.model_selection import KFold # Create sample data X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) y = np.array([1, 2, 3, 4, 5]) # Initialize K-Fold splitter kf = KFold(n_splits=3) # Demonstrate how data is split fold_index = 1 for train_index, test_index in kf.split(X): print(f""Fold {fold_index} - Train set indices: {train_index}, Test set indices: {test_index}"") fold_index += 1 ` <br> ## 7. What is _Regularization_ and how does it help prevent _overfitting_? Regularization in machine learning is a technique used to prevent overfitting, which occurs when a model is too closely fit to a limited set of data points and may perform poorly on new data. Regularization discourages overly complex models by adding a penalty term to the loss function used to train the model. ### Types of Regularization #### L1 Regularization (Lasso Regression) $$ \text{Cost} + \lambda \sum_{i=1}^{n} |w_i| $$ L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), adds the absolute values of the coefficients to the cost function. This encourages a sparse solution, effectively performing feature selection by potentially reducing some coefficients to zero. #### L2 Regularization (Ridge Regression) $$ \text{Cost} + \lambda \sum_{i=1}^{n} w_i^2 $$ L2 regularization, or Ridge regression, adds the squared values of the coefficients to the cost function. This generally helps to reduce the model complexity by constraining the coefficients, especially effective when many features have small or moderate effects. #### Elastic Net Regularization $$ \text{Cost} + \lambda_1 \sum_{i=1}^{n} |w_i| + \lambda_2 \sum_{i=1}^{n} w_i^2 $$ Elastic Net is a hybrid of L1 and L2 regularization. It combines both penalties in the cost function and is useful for handling situations when there are correlations amongst the features or when you need to incorporate both attributes of L1 and L2 regularization. #### Max Norm Regularization Max Norm Regularization constrains the L2 norm of the weights for each neuron and is typically used in neural networks. It limits the size of the parameter weights, ensuring that they do not grow too large: `python from keras.constraints import max_norm ` This can be particularly beneficial in preventing overfitting in deep learning models. ### Code Examples #### L1 and L2 Regularization Example: For Lasso and Ridge regression, you can use the respective classes from Scikit-learn‚Äôs linear_model module: `python from sklearn.linear_model import Lasso, Ridge # Example of Lasso Regression lasso_reg = Lasso(alpha=0.1) lasso_reg.fit(X_train, y_train) # Example of Ridge Regression ridge_reg = Ridge(alpha=1.0) ridge_reg.fit(X_train, y_train) ` #### Elastic Net Regularization Example: You can apply Elastic Net regularization using its specific class from Scikit-learn: `python from sklearn.linear_model import ElasticNet # Elastic Net combines L1 and L2 regularization elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5) elastic_net.fit(X_train, y_train) ` #### Max Norm Regularization Example: Max Norm regularization can be specified for layers in a Keras model as follows: `python from keras.layers import Dense from keras.models import Sequential from keras.constraints import max_norm model = Sequential() model.add(Dense(64, input_dim=8, kernel_constraint=max_norm(3))) ` Here, the max_norm(3) constraint ensures that the max norm of the weights does not exceed 3. <br> ## 8. Describe the difference between _Parametric_ and _Non-Parametric_ models. Parametric and non-parametric models represent distinct approaches in statistical modeling, each with unique characteristics in terms of assumptions, computational complexity, and suitability for various types of data. ### Key Distinctions - Parametric Models: - Make explicit and often strong assumptions about data distribution. - Are defined by a fixed number of parameters, regardless of sample size. - Typically require less data for accurate estimation. - Common examples include linear regression, logistic regression, and Gaussian Naive Bayes. - Non-parametric Models: - Make minimal or no assumptions about data distribution. - The number of parameters can grow with sample size, offering more flexibility. - Generally require more data for accurate estimation. - Examples encompass k-nearest neighbors, decision trees, and random forests. ### Advantages and Disadvantages of Each Approach - Parametric Models - Advantages: - Inferential speed: Once trained, making predictions or conducting inference is often computationally fast. - Parameter interpretability: The meaning of parameters can be directly linked to the model and the data. - Efficiency with small, well-behaved datasets: Parametric models can yield highly accurate results with relatively small, clean datasets that adhere to the model's distributional assumptions. - Disadvantages: - Strong distributional assumptions: Data must closely match the specified distribution for the model to produce reliable results. - Limited flexibility: These models might not adapt well to non-standard data distributions. - Non-Parametric Models - Advantages: - Distribution-free: They do not impose strict distributional assumptions, making them more robust across a wider range of datasets. - Flexibility: Can capture complex, nonlinear relationships in the data. - Larger sample adaptability: Particularly suitable for big data or data from unknown distributions. - Disadvantages: - Computational overhead: Can be slower for making predictions, especially with large datasets. - Interpretability: Often, the predictive results are harder to interpret in terms of the original features. ### Code Example: Gaussian Naive Bayes vs. Decision Tree (Scikit-learn) Here is the Python code: `python # Gaussian Naive Bayes (parametric) from sklearn.naive_bayes import GaussianNB model = GaussianNB() # Decision Tree (non-parametric) from sklearn.tree import DecisionTreeClassifier model_dt = DecisionTreeClassifier() ` <br> ## 9. What is the _curse of dimensionality_ and how does it impact ML models? The curse of dimensionality describes the issues that arise when working with high-dimensional data, affecting the performance of machine learning models. ### Key Challenges",,hard,coding,regression|classification|probability|python|deep_learning|ensemble|feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.640936
"Sparse Data: As the number of dimensions increases, the data points become more spread out, and the density of data points decreases.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.640980
"Increased Volume of Data: With each additional dimension, the volume of the sample space grows exponentially, necessitating a larger dataset to maintain coverage.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641018
Overfitting: High-dimensional spaces make it easier for models to fit to noise rather than the underlying pattern in the data.,,medium,ml,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641046
"Computational Complexity: Many machine learning algorithms exhibit slower performance and require more resources as the number of dimensions increases. ### Visual Example Consider a hypercube (n-dimensional cube) inscribed in a hypersphere (n-dimensional sphere) with a large number of dimensions, say 100. If you were to place a ""grid"" or uniformly spaced points within the hypercube, you'd find that the majority of these points actually fall outside the hypersphere. This disparity grows more pronounced as the number of dimensions increases, leading to a ""density gulf"" between the data contained within the hypercube and that within the hypersphere. !curse-of-dimensionality.png?alt=media&token=24d3cde6-89ae-4eb3-8d05-1d6358bb5ac9) ### Recommendations to Mitigate the Curse of Dimensionality",Google,medium,coding,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641133
"Feature Selection and Dimensionality Reduction: Prioritize quality over quantity of features. Techniques like PCA, t-SNE, and LDA can help reduce dimensions.",,medium,coding,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641161
"Simpler Models: Consider using algorithms with less sensitivity to high dimensions, even if it means sacrificing a bit of performance.",,medium,coding,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641187
"Sparse Models: For high-dimensional, sparse datasets, models that can handle sparsity, like LASSO or ElasticNet, might be beneficial.",,medium,ml,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641215
Feature Engineering: Craft domain-specific features that can capture relevant information more efficiently.,,medium,mixed,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641243
"Data Quality: Strive for a high-quality dataset, as more data doesn't necessarily counteract the curse of dimensionality.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641273
"Data Stratification and Sampling: When possible, stratify and sample data to ensure coverage across the high-dimensional space.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641303
"Computational Resources: Leverage cloud computing or powerful hardware to handle the increased computational demands. <br> ## 10. Explain the concept of _Feature Engineering_ and its significance in ML. Feature engineering is a vital component of the machine-learning pipeline. It entails creating meaningful and robust representations of the data upon which the model will be built. ### Significance of Feature Engineering - Improved Model Performance: High-quality features can make even simple models more effective, while poor features can hamper the performance of the most advanced models. - Dimensionality Reduction: Carefully engineered features can distill relevant information from high-dimensional data, leading to more efficient and accurate models. - Model Interpretability: Certain feature engineering techniques, such as binning or one-hot encoding, make it easier to understand and interpret the model's decisions. - Computational Efficiency: Engineered features can often streamline computational processes, making predictions faster and cheaper. ### Common Feature Engineering Techniques",,hard,stats,hypothesis_testing|metrics|feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641434
"Handling Missing Data - Removing or imputing missing values. - Creating a separate ""missing"" category.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641465
"Handling Categorical Data - Converting categories into ordinal values. - Using one-hot encoding to create binary ""dummy"" variables. - Grouping rare categories into an ""other"" category.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641504
"Handling Temporal Data - Extracting specific time-related features from timestamps, such as hour or month. - Converting timestamps into different representations, like age or duration since a specific event.",,medium,mixed,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641544
Variable Transformation - Using mathematical transformations such as logarithms. - Normalizing or scaling data to a specific range.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641576
"Discretization - Converting continuous variables into discrete bins, e.g., converting age to age groups.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641609
Feature Extraction - Reducing dimensionality through techniques like PCA or LDA.,,medium,mixed,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641634
"Feature Creation - Engineering domain-specific metrics. - Generating polynomial or interaction features. <br> ## 11. What is _Data Preprocessing_ and why is it important in ML? Data Preprocessing is a vital early-stage task in any machine learning project. It involves cleaning, transforming, and standardizing data to make it more suitable for predictive modeling. ### Key Steps in Data Preprocessing",,medium,ml,metrics|feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641691
Data Cleaning: - Address missing values: Implement strategies like imputation or removal. - Outlier detection and handling: Identify and deal with data points that deviate significantly from the rest.,,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641732
Feature Selection and Engineering: - Choose the most relevant features that contribute to the model's predictive accuracy. - Create new features that might improve the model's performance.,,medium,coding,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641762
"Data Transformation: - Normalize or standardize numerical data to ensure all features contribute equally. - Convert categorical data into a format understandable by the model, often using techniques like one-hot encoding. - Discretize continuous data when required.",,medium,ml,feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641805
"Data Integration: - Combine data from multiple sources, ensuring compatibility and consistency.",,medium,mixed,,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.641847
"Data Reduction: - Reduce the dimensionality of the feature space, often to eliminate noise or improve computational efficiency. ### Code Example: Handling Missing Data Here is the Python code: `python # Drop rows with missing values cleaned_data = raw_data.dropna() # Fill missing values using the mean mean_value = raw_data['column_name'].mean() raw_data['column_name'].fillna(mean_value, inplace=True) ` ### Code Example: Feature Scaling Here is the Python code: `python from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) ` ### Code Example: Dimensionality Reduction Using PCA Here is the Python code: `python from sklearn.decomposition import PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) ` <br> ## 12. Explain the difference between _Feature Scaling_ and _Normalization_. Both Feature Scaling and Normalization are data preprocessing techniques that aim to make machine learning models more robust and accurate. While they share similarities in standardizing data, they serve slightly different purposes. ### Key Distinctions - Feature Scaling adjusts the range of independent variables or features so that they are on a similar scale. Common methods include Min-Max Scaling and Standardization. - Normalization, in the machine learning context, typically refers to scaling the magnitude of a vector to make its Euclidean length 1. It's also known as Unit Vector transformation. In some contexts, it may be used more generally to refer to scaling quantities to be in a range (like Min-Max), but this is a less common usage in the ML community. ### Methods in Feature Scaling and Normalization - Min-Max Scaling: Transforms the data to a specific range (usually 0 to 1 or -1 to 1). - Standardization: Rescales the data to have a mean of 0 and a standard deviation of 1. - Unit Vector Transformation: Scales data to have a Euclidean length of 1. ### Use Cases - Feature Scaling: Beneficial for algorithms that compute distances or use linear methods, such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM). - Normalization: More useful for algorithms that work with vector dot products, like the K-Means clustering algorithm and Neural Networks. <br> ## 13. What is the purpose of _One-Hot Encoding_ and when is it used? One-Hot Encoding is a technique frequently used to prepare categorical data for machine learning algorithms. ### Purpose of One-Hot Encoding It is employed when: - Categorical Data: The data on hand is categorical, and the algorithm or model being used does not support categorical input. - Nominal Data Order: The categorical data is nominal, i.e., not ordinal, which means there is no inherent order or ranking. - Non-Scalar Representation: The model can only process numerical (scalar) data. The model may be represented as the set $x = \{x_1, x_2, \ldots, x_k\}$ each $x_i$ corresponding to a category. A scalar transformation $f(x_i)$ or comparison $f(x_i) > f(x_j)$ is not defined for the categories directly. - Category Dimension: The categorical variable has many distinct categories. For instance, using one-hot encoding consistently reduces the computational and statistical burden in algorithms. ### Code Example: One-Hot Encoding Here is the Python code: `python import pandas as pd # Sample data data = pd.DataFrame({'color': ['red', 'green', 'blue', 'green', 'red']}) # One-hot encode one_hot_encoded = pd.get_dummies(data, columns=['color']) print(one_hot_encoded) ` ### Output: One-Hot Encoding | | color_blue | color_green | color_red | |---:|-----------:|------------:|----------:| | 0 | 0 | 0 | 1 | | 1 | 0 | 1 | 0 | | 2 | 1 | 0 | 0 | | 3 | 0 | 1 | 0 | | 4 | 0 | 0 | 1 | ### Output: Binary representation (alternatively) | Color | Binary Red | Binary Green | Binary Blue | |-------|------------|--------------|-------------| | Red | 1 | 0 | 0 | | Green | 0 | 1 | 0 | | Blue | 0 | 0 | 1 | <br> ## 14. Describe the concept of _Handling Missing Values_ in datasets. Handling Missing Values is a crucial step in the data preprocessing pipeline for any machine learning or statistical analysis. It involves identifying and dealing with data points that are not available, ensuring the robustness and reliability of the subsequent analysis or model. ### Common Techniques for Handling Missing Values #### Deletion - Listwise Deletion: Eliminate entire rows with any missing value. This method is straightforward but can lead to significant information loss, especially if the dataset has a large number of missing values. - Pairwise Deletion: Ignore specific pairs of missing values across variables. While this method preserves more data than listwise deletion, it can introduce bias in the analysis. #### Single-Imputation Methods - Mean/ Median/ Mode: Replace missing values with the mean, median, or mode of the variable. This method is quick and easy to implement but can affect the distribution and introduce bias. - Forward or Backward Fill (Last Observation Carried Forward - LOCF / Last Observation Carried Backward - LOCB): Substitute missing values with the most recent (forward) or next (backward) non-missing value. These methods are useful for time-series data. - Linear Interpolation: Estimate missing values by fitting a linear model to the two closest non-missing data points. This method is particularly useful for ordered data, but it assumes a linear relationship. #### Multiple-Imputation Methods - k-Nearest Neighbors (KNN): Impute missing values based on the values of the k most similar instances or neighbors. This method can preserve the original data structure and is more robust than single imputation. - Expectation-Maximization (EM) Algorithm: Model the data with an initial estimate, then iteratively refine the imputations. It's effective for data with complex missing patterns. #### Prediction Models - Use predictive models, typically regression or decision tree-based models, to estimate missing values. This approach can be more accurate than simpler methods but also more computationally intensive. ### Best Practices - Understanding the Mechanism of Missing Data: Investigating why the data is missing can provide insights into the problem. For instance, is the data missing completely at random, at random, or not at random? - Combining Techniques: Employing multiple imputation methods or a combination of imputation and deletion strategies can help achieve better results. - Evaluating Impact on Model: Compare the performance of the model with and without the imputation method to understand its effect. <br> ## 15. What is _Feature Selection_ and its techniques? Feature Selection is a critical step in the machine learning pipeline. It aims to identify the most relevant features from a dataset, leading to improved model performance, reduced overfitting, and faster training times. ### Feature Selection Techniques #### 1. Filter Methods - Description: Filter methods rank features based on certain criteria, such as their correlation with the target variable or their variance. - Advantages: They are computationally efficient and can be used in both regression and classification tasks. - Limitations: They do not take feature dependencies into account. #### 2. Wrapper Methods - Description: Wrapper methods select features based on their performance with a specific machine learning algorithm. Common techniques include Recursive Feature Elimination (RFE) and Forward-Backward Selection. - Advantages: They take feature dependencies into account and can improve model accuracy. - Limitations: They can be computationally expensive and prone to overfitting. #### 3. Embedded Methods - Description: Embedded methods integrate feature selection with the model building process. Techniques like LASSO (Least Absolute Shrinkage and Selection Operator) and decision tree feature importances are examples of this approach. - Advantages: They are computationally efficient and provide feature rankings. - Limitations: They may not be transferable to other models. ### Code Example: Filter Methods Here is the Python code: `python import pandas as pd from sklearn.feature_selection import VarianceThreshold # Generate example data data = {'feature1': [1, 2, 3, 4, 5], 'feature2': [0, 0, 0, 0, 0], 'feature3': [1, 0, 1, 0, 1], 'target': [0, 1, 0, 1, 0]} df = pd.DataFrame(data) # Remove features with low variance X = df.drop('target', axis=1) y = df['target'] selector = VarianceThreshold(threshold=0.2) X_selected = selector.fit_transform(X) print(X_selected) ` #### Code Example: Wrapper Methods Here is the Python code: `python from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression # Create the RFE object and rank features model = LogisticRegression(solver='lbfgs') rfe = RFE(model, 3) fit = rfe.fit(X, y) print(""Selected Features:"") print(fit.support_) ` <br> #### Explore all 100 answers here üëâ Devinterview.io - Data Scientist <br> <a href=""https://devinterview.io/questions/machine-learning-and-data-science/""> <img src=""https://firebasestorage.googleapis.com/v0/b/dev-stack-app.appspot.com/o/github-blog-img%2Fmachine-learning-and-data-science-github-img.jpg?alt=media&token=c511359d-cb91-4157-9465-a8e75a0242fe"" alt=""machine-learning-and-data-science"" width=""100%""> </a> </p>",Google,easy,coding,regression|classification|clustering|probability|python|deep_learning|metrics|feature_engineering,Devinterview-io/data-scientist-interview-questions,,2025-11-17T10:49:35.642628
TN / True Negative: case was negative and predicted negative,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.279389
TP / True Positive: case was positive and predicted positive,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.279432
FN / False Negative: case was positive but predicted negative,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.279466
"FP / False Positive: case was negative but predicted positive !alt text Now, your boss asks you three questions: * What percent of your predictions were correct? You answer: the ""accuracy"" was (9,760+60) out of 10,000 = 98.2% * What percent of the positive cases did you catch? You answer: the ""recall"" was 60 out of 100 = 60% * What percent of positive predictions were correct? You answer: the ""precision"" was 60 out of 200 = 30% See also a very good explanation of Precision and recall in Wikipedia. !alt text ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION) and is commonly used to measure the performance of binary classifiers. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more representative picture of performance. Remember, a ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION). Sensitivity is the other name for recall but specificity is not PRECISION. Recall/Sensitivity is the measure of the probability that your estimate is 1 given all the samples whose true class label is 1. It is a measure of how many of the positive samples have been identified as being positive. Specificity is the measure of the probability that your estimate is 0 given all the samples whose true class label is 0. It is a measure of how many of the negative samples have been identified as being negative. PRECISION on the other hand is different. It is a measure of the probability that a sample is a true positive class given that your classifier said it is positive. It is a measure of how many of the samples predicted by the classifier as positive is indeed positive. Note here that this changes when the base probability or prior probability of the positive class changes. Which means PRECISION depends on how rare is the positive class. In other words, it is used when positive class is more interesting than the negative class. * Sensitivity also known as the True Positive rate or Recall is calculated as, Sensitivity = TP / (TP + FN). Since the formula doesn‚Äôt contain FP and TN, Sensitivity may give you a biased result, especially for imbalanced classes. In the example of Fraud detection, it gives you the percentage of Correctly Predicted Frauds from the pool of Actual Frauds pool of Actual Non-Frauds. * Specificity, also known as True Negative Rate is calculated as, Specificity = TN / (TN + FP). Since the formula does not contain FN and TP, Specificity may give you a biased result, especially for imbalanced classes. In the example of Fraud detection, it gives you the percentage of Correctly Predicted Non-Frauds from the pool of Actual Frauds pool of Actual Non-Frauds Assessing and Comparing Classifier Performance with ROC Curves ## 6. Is it better to have too many false positives, or too many false negatives? It depends on the question as well as on the domain for which we are trying to solve the question. In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive. For spam filtering, a false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task. So, we prefer too many false negatives over many false positives. ## 7. How do you deal with unbalanced binary classification? Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally. For example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2. This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1. You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either. The remaining discussions will assume a two-class classification problem because it is easier to think about and describe.",,medium,stats,classification|probability|metrics,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.280457
Can You Collect More Data?</br> A larger dataset might expose a different and perhaps more balanced perspective on the classes. More examples of minor classes may be useful later when we look at resampling your dataset.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.280524
"Try Changing Your Performance Metric</br> Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading. From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy: - Confusion Matrix: A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned). - Precision: A measure of a classifiers exactness. Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV). Precision can be thought of as a measure of a classifiers exactness. A low precision can also indicate a large number of False Positives. - Recall: A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate. Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives. - F1 Score (or F-score): A weighted average of precision and recall. I would also advise you to take a look at the following: - Kappa (or Cohen‚Äôs kappa): Classification accuracy normalized by the imbalance of the classes in the data. ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.",,medium,ml,classification|metrics,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.281008
"Try Resampling Your Dataset * You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement) * You can delete instances from the over-represented class, called under-sampling.",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.281081
Try Different Algorithms,,medium,coding,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.281106
Try Penalized Models</br> You can use the same algorithms but give them a different perspective on the problem. Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class. Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA. Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you‚Äôre getting poor results. It provides yet another way to ‚Äúbalance‚Äù the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.,,medium,coding,classification,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.281321
"Try a Different Perspective</br> Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas. Two you might like to consider are anomaly detection and change detection. ## 8. What is statistical power? Statistical power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true‚Äîthat is, the ability of a test to detect an effect, if the effect actually exists. To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is). A type I error (or error of the first kind) is the incorrect rejection of a true null hypothesis. Usually a type I error leads one to conclude that a supposed effect or relationship exists when in fact it doesn't. Examples of type I errors include a test that shows a patient to have a disease when in fact the patient does not have the disease, a fire alarm going on indicating a fire when in fact there is no fire, or an experiment indicating that a medical treatment should cure a disease when in fact it does not. A type II error (or error of the second kind) is the failure to reject a false null hypothesis. Examples of type II errors would be a blood test failing to detect the disease it was designed to detect, in a patient who really has the disease; a fire breaking out and the fire alarm does not ring; or a clinical trial of a medical treatment failing to show that the treatment works when really it does. !alt text ## 9. What are bias and variance, and what are their relation to modeling data? Bias is how far removed a model's predictions are from correctness, while variance is the degree to which these predictions vary between model iterations. Bias is generally the distance between the model that you build on the training data (the best model that your model space can provide) and the ‚Äúreal model‚Äù (which generates data). Error due to Bias: Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value. The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model. The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise) in the training data, rather than the intended outputs (overfitting). Big dataset -> low variance <br/> Low dataset -> high variance <br/> Few features -> high bias, low variance <br/> Many features -> low bias, high variance <br/> Complicated model -> low bias <br/> Simplified model -> high bias <br/> Decreasing Œª -> low bias <br/> Increasing Œª -> low variance <br/> We can create a graphical visualization of bias and variance using a bulls-eye diagram. Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target. !alt text As an example, using a simple flawed Presidential election survey as an example, errors in the survey are then explained through the twin lenses of bias and variance: selecting survey participants from a phonebook is a source of bias; a small sample size is a source of variance. Minimizing total model error relies on the balancing of bias and variance errors. Ideally, models are the result of a collection of unbiased data of low variance. Unfortunately, however, the more complex a model becomes, its tendency is toward less bias but greater variance; therefore an optimal model would need to consider a balance between these 2 properties. The statistical evaluation method of cross-validation is useful in both demonstrating the importance of this balance, as well as actually searching it out. The number of data folds to use -- the value of k in k-fold cross-validation -- is an important decision; the lower the value, the higher the bias in the error estimates and the less variance. !alt text The most important takeaways are that bias and variance are two sides of an important trade-off when building models, and that even the most routine of statistical evaluation methods are directly reliant upon such a trade-off. We may estimate a model fÃÇ (X) of f(X) using linear regressions or another modeling technique. In this case, the expected squared prediction error at a point x is: Err(x)=E[(Y‚àífÃÇ (x))^2] This error may then be decomposed into bias and variance components: Err(x)=(E[fÃÇ (x)]‚àíf(x))^2+E[(fÃÇ (x)‚àíE[fÃÇ (x)])^2]+œÉ^2e Err(x)=Bias^2+Variance+Irreducible That third term, irreducible error, is the noise term in the true relationship that cannot fundamentally be reduced by any model. Given the true model and infinite data to calibrate it, we should be able to reduce both the bias and variance terms to 0. However, in a world with imperfect models and finite data, there is a tradeoff between minimizing the bias and minimizing the variance. That third term, irreducible error, is the noise term in the true relationship that cannot fundamentally be reduced by any model. Given the true model and infinite data to calibrate it, we should be able to reduce both the bias and variance terms to 0. However, in a world with imperfect models and finite data, there is a tradeoff between minimizing the bias and minimizing the variance. If a model is suffering from high bias, it means that model is less complex, to make the model more robust, we can add more features in feature space. Adding data points will reduce the variance. The bias‚Äìvariance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may underfit their training data, failing to capture important regularities. Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set. #### Approaches Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.: * (Generalized) linear models can be regularized to decrease their variance at the cost of increasing their bias. * In artificial neural networks, the variance increases and the bias decreases with the number of hidden units. Like in GLMs, regularization is typically applied. * In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below). * In Instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.[ * In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance. One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting) combines many ""weak"" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines ""strong"" learners in a way that reduces their variance. Understanding the Bias-Variance Tradeoff ## 10. What if the classes are imbalanced? What if there are more than 2 groups? Binary classification involves classifying the data into two groups, e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc. As the target variable is not continuous, binary classification model predicts the probability of a target variable to be Yes/No. To evaluate such a model, a metric called the confusion matrix is used, also called the classification or co-incidence matrix. With the help of a confusion matrix, we can calculate important performance measures: * True Positive Rate (TPR) or Recall or Sensitivity = TP / (TP + FN) * Precision = TP / (TP + FP) * False Positive Rate(FPR) or False Alarm Rate = 1 - Specificity = 1 - (TN / (TN + FP)) * Accuracy = (TP + TN) / (TP + TN + FP + FN) * Error Rate = 1 ‚Äì Accuracy F-measure = 2 / ((1 / Precision) + (1 / Recall)) = 2 (precision * recall) / (precision + recall) * ROC (Receiver Operating Characteristics) = plot of FPR vs TPR * AUC (Area Under the [ROC] Curve) Performance measure across all classification thresholds. Treated as the probability that a model ranks a randomly chosen positive sample higher than negative ## 11. What are some ways I can make my model more robust to outliers? There are several ways to make a model more robust to outliers, from different points of view (data preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human‚Äôs knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare. Outliers are usually defined in relation to the distribution. Thus outliers could be removed in the pre-processing step (before any learning step), by using standard deviations (Mean +/- 2*SD), it can be used for normality. Or interquartile ranges Q1 - Q3, Q1 - is the ""middle"" value in the first half of the rank-ordered data set, Q3 - is the ""middle"" value in the second half of the rank-ordered data set. It can be used for not normal/unknown as threshold levels. Moreover, data transformation (e.g. log transformation) may help if data have a noticeable tail. When outliers related to the sensitivity of the collecting instrument which may not precisely record small values, Winsorization may be useful. This type of transformation (named after Charles P. Winsor (1895‚Äì1951)) has the same effect as clipping signals (i.e. replaces extreme data values with less extreme values). Another option to reduce the influence of outliers is using mean absolute difference rather mean squared error. For model building, some models are resistant to outliers (e.g. tree-based approaches) or non-parametric tests. Similar to the median effect, tree models divide each node into two in each split. Thus, at each split, all data points in a bucket could be equally treated regardless of extreme values they may have. ## 12. In unsupervised learning, if a ground truth about a dataset is unknown, how can we determine the most useful number of clusters to be? The elbow method is often the best place to start, and is especially useful due to its ease of explanation and verification via visualization. The elbow method is interested in explaining variance as a function of cluster numbers (the k in k-means). By plotting the percentage of variance explained against k, the first N clusters should add significant information, explaining variance; yet, some eventual value of k will result in a much less significant gain in information, and it is at this point that the graph will provide a noticeable angle. This angle will be the optimal number of clusters, from the perspective of the elbow method, It should be self-evident that, in order to plot this variance against varying numbers of clusters, varying numbers of clusters must be tested. Successive complete iterations of the clustering method must be undertaken, after which the results can be plotted and compared. DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density. ## 13. Define variance Variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself. Var(X) = E[(X - m)^2], m=E[X] Variance is, thus, a measure of the scatter of the values of a random variable relative to its mathematical expectation. ## 14. Expected value Expected value ‚Äî Expected Value (Probability Distribution In a probability distribution, expected value is the value that a random variable takes with greatest likelihood. Based on the law of distribution of a random variable x, we know that a random variable x can take values x1, x2, ..., xk with probabilities p1, p2, ..., pk. The mathematical expectation M(x) of a random variable x is equal. The mathematical expectation of a random variable X (denoted by M (X) or less often E (X)) characterizes the average value of a random variable (discrete or continuous). Mathematical expectation is the first initial moment of a given CB. Mathematical expectation is attributed to the so-called characteristics of the distribution position (to which the mode and median also belong). This characteristic describes a certain average position of a random variable on the numerical axis. Say, if the expectation of a random variable - the lamp life is 100 hours, then it is considered that the values of the service life are concentrated (on both sides) from this value (with dispersion on each side, indicated by the variance). The mathematical expectation of a discrete random variable X is calculated as the sum of the products of the values xi that the CB takes X by the corresponding probabilities pi: `python import numpy as np X = [3,4,5,6,7] P = [0.1,0.2,0.3,0.4,0.5] np.sum(np.dot(X, P)) ` ## 15. Describe the differences between and use cases for box plots and histograms A histogram is a type of bar chart that graphically displays the frequencies of a data set. Similar to a bar chart, a histogram plots the frequency, or raw count, on the Y-axis (vertical) and the variable being measured on the X-axis (horizontal). The only difference between a histogram and a bar chart is that a histogram displays frequencies for a group of data, rather than an individual data point; therefore, no spaces are present between the bars. Typically, a histogram groups data into small chunks (four to eight values per bar on the horizontal axis), unless the range of data is so great that it easier to identify general distribution trends with larger groupings. A box plot, also called a box-and-whisker plot, is a chart that graphically represents the five most important descriptive values for a data set. These values include the minimum value, the first quartile, the median, the third quartile, and the maximum value. When graphing this five-number summary, only the horizontal axis displays values. Within the quadrant, a vertical line is placed above each of the summary numbers. A box is drawn around the middle three lines (first quartile, median, and third quartile) and two lines are drawn from the box‚Äôs edges to the two endpoints (minimum and maximum). Boxplots are better for comparing distributions than histograms! !alt text ## 16. How would you find an anomaly in a distribution? Before getting started, it is important to establish some boundaries on the definition of an anomaly. Anomalies can be broadly categorized as:",,medium,coding,regression|classification|clustering|hypothesis_testing|probability|python|deep_learning|ensemble|metrics|feature_engineering,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.284737
"Point anomalies: A single instance of data is anomalous if it's too far off from the rest. Business use case: Detecting credit card fraud based on ""amount spent.""",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.284796
"Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: Spending $100 on food every day during the holiday season is normal, but may be odd otherwise.",,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.284852
"Collective anomalies: A set of data instances collectively helps in detecting anomalies. Business use case: Someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack. Best steps to prevent anomalies is to implement policies or checks that can catch them during the data collection stage. Unfortunately, you do not often get to collect your own data, and often the data you're mining was collected for another purpose. About 68% of all the data points are within one standard deviation from the mean. About 95% of the data points are within two standard deviations from the mean. Finally, over 99% of the data is within three standard deviations from the mean. When the value deviate too much from the mean, let‚Äôs say by ¬± 4œÉ, then we can considerate this almost impossible value as anomaly. (This limit can also be calculated using the percentile). #### Statistical methods Statistically based anomaly detection uses this knowledge to discover outliers. A dataset can be standardized by taking the z-score of each point. A z-score is a measure of how many standard deviations a data point is away from the mean of the data. Any data-point that has a z-score higher than 3 is an outlier, and likely to be an anomaly. As the z-score increases above 3, points become more obviously anomalous. A z-score is calculated using the following equation. A box-plot is perfect for this application. #### Metric method Judging by the number of publications, metric methods are the most popular methods among researchers. They postulate the existence of a certain metric in the space of objects, which helps to find anomalies. Intuitively, the anomaly has few neighbors in the instannce space, and a typical point has many. Therefore, a good measure of anomalies can be, for example, the ¬´distance to the k-th neighbor¬ª. (See method: Local Outlier Factor). Specific metrics are used here, for example Mahalonobis distance. Mahalonobis distance is a measure of distance between vectors of random variables, generalizing the concept of Euclidean distance. Using Mahalonobis distance, it is possible to determine the similarity of unknown and known samples. It differs from Euclidean distance in that it takes into account correlations between variables and is scale invariant. !alt text The most common form of clustering-based anomaly detection is done with prototype-based clustering. Using this approach to anomaly detection, a point is classified as an anomaly if its omission from the group significantly improves the prototype, then the point is classified as an anomaly. This logically makes sense. K-means is a clustering algorithm that clusters similar points. The points in any cluster are similar to the centroid of that cluster, hence why they are members of that cluster. If one point in the cluster is so far from the centroid that it pulls the centroid away from it's natural center, than that point is literally an outlier, since it lies outside the natural bounds for the cluster. Hence, its omission is a logical step to improve the accuracy of the rest of the cluster. Using this approach, the outlier score is defined as the degree to which a point doesn't belong to any cluster, or the distance it is from the centroid of the cluster. In K-means, the degree to which the removal of a point would increase the accuracy of the centroid is the difference in the SSE, or standard squared error, or the cluster with and without the point. If there is a substantial improvement in SSE after the removal of the point, that correlates to a high outlier score for that point. More specifically, when using a k-means clustering approach towards anomaly detection, the outlier score is calculated in one of two ways. The simplest is the point's distance from its closest centroid. However, this approach is not as useful when there are clusters of differing densities. To tackle that problem, the point's relative distance to it's closest centroid is used, where relative distance is defined as the ratio of the point's distance from the centroid to the median distance of all points in the cluster from the centroid. This approach to anomaly detection is sensitive to the value of k. Also, if the data is highly noisy, then that will throw off the accuracy of the initial clusters, which will decrease the accuracy of this type of anomaly detection. The time complexity of this approach is obviously dependent on the choice of clustering algorithm, but since most clustering algorithms have linear or close to linear time and space complexity, this type of anomaly detection can be highly efficient. ## 17. How do you deal with outliers in your data? For the most part, if your data is affected by these extreme cases, you can bound the input to a historical representative of your data that excludes outliers. So that could be a number of items (>3) or a lower or upper bounds on your order value. If the outliers are from a data set that is relatively unique then analyze them for your specific situation. Analyze both with and without them, and perhaps with a replacement alternative, if you have a reason for one, and report your results of this assessment. One option is to try a transformation. Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent. ## 18. How do you deal with sparse data? We could take a look at L1 regularization since it best fits to the sparse data and do feature selection. If linear relationship - linear regression either - svm. Also it would be nice to use one-hot-encoding or bag-of-words. A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. ## 19. Big Data Engineer Can you explain what REST is? REST stands for Representational State Transfer. (It is sometimes spelled ""ReST"".) It relies on a stateless, client-server, cacheable communications protocol -- and in virtually all cases, the HTTP protocol is used. REST is an architecture style for designing networked applications. The idea is simple HTTP is used to make calls between machines. * In many ways, the World Wide Web itself, based on HTTP, can be viewed as a REST-based architecture. RESTful applications use HTTP requests to post data (create and/or update), read data (e.g., make queries), and delete data. Thus, REST uses HTTP for all four CRUD (Create/Read/Update/Delete) operations. REST is a lightweight alternative to mechanisms like RPC (Remote Procedure Calls) and Web Services (SOAP, WSDL, et al.). Later, we will see how much more simple REST is. * Despite being simple, REST is fully-featured; there's basically nothing you can do in Web Services that can't be done with a RESTful architecture. REST is not a ""standard"". There will never be a W3C recommendation for REST, for example. And while there are REST programming frameworks, working with REST is so simple that you can often ""roll your own"" with standard library features in languages like Perl, Java, or C#. ## 20. Logistic regression Log odds - raw output from the model; odds - exponent from the output of the model. Probability of the output - odds / (1+odds). ## 21. What is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients? When predictor variables are correlated, the estimated regression coefficient of any one variable depends on which other predictor variables are included in the model. When predictor variables are correlated, the precision of the estimated regression coefficients decreases as more predictor variables are added to the model. In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. The consequences of multicollinearity: * Ratings estimates remain unbiased. * Standard coefficient errors increase. * The calculated t-statistics are underestimated. * Estimates become very sensitive to changes in specifications and changes in individual observations. * The overall quality of the equation, as well as estimates of variables not related to multicollinearity, remain unaffected. * The closer multicollinearity to perfect (strict), the more serious its consequences. Indicators of multicollinearity:",,medium,coding,regression|clustering|probability|metrics|feature_engineering,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.286401
High R2 and negligible odds.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.286427
Strong pair correlation of predictors.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.286450
"High VIF - variance inflation factor. Confidence interval (CI) is a type of interval estimate (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level. Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the true population, the confidence interval obtained from the data is also random. If a corresponding hypothesis test is performed, the confidence level is the complement of the level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. If it is hypothesized that a true parameter value is 0 but the 95% confidence interval does not contain 0, then the estimate is significantly different from zero at the 5% significance level. The desired level of confidence is set by the researcher (not determined by data). Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%. Factors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample. A larger sample size normally will lead to a better estimate of the population parameter. A Confidence Interval is a range of values we are fairly sure our true value lies in. X ¬± Z*s/‚àö(n), X is the mean, Z is the chosen Z-value from the table, s is the standard deviation, n is the number of samples. The value after the ¬± is called the margin of error. ## 22. What‚Äôs the difference between Gaussian Mixture Model and K-Means? Let's says we are aiming to break them into three clusters. K-means will start with the assumption that a given data point belongs to one cluster. Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a red cluster. In the next iteration, we might revise that belief, and be certain that it belongs to the green cluster. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the ""hard assignment"". What if we are uncertain? What if we think, well, I can't be sure, but there is 70% chance it belongs to the red cluster, but also 10% chance its in green, 20% chance it might be blue. That's a soft assignment. The Mixture of Gaussian model helps us to express this uncertainty. It starts with some prior belief about how certain we are about each point's cluster assignments. As it goes on, it revises those beliefs. But it incorporates the degree of uncertainty we have about our assignment. Kmeans: find kk to minimize (x‚àíŒºk)^2 Gaussian Mixture (EM clustering) : find kk to minimize (x‚àíŒºk)^2/œÉ^2 The difference (mathematically) is the denominator ‚ÄúœÉ^2‚Äù, which means GM takes variance into consideration when it calculates the measurement. Kmeans only calculates conventional Euclidean distance. In other words, Kmeans calculate distance, while GM calculates ‚Äúweighted‚Äù distance. K means: * Hard assign a data point to one particular cluster on convergence. * It makes use of the L2 norm when optimizing (Min {Theta} L2 norm point and its centroid coordinates). EM: * Soft assigns a point to clusters (so it give a probability of any point belonging to any centroid). * It doesn't depend on the L2 norm, but is based on the Expectation, i.e., the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters. ## 23. Describe how Gradient Boosting works. The idea of boosting came out of the idea of whether a weak learner can be modified to become better. Gradient boosting relies on regression trees (even when solving a classification problem) which minimize MSE. Selecting a prediction for a leaf region is simple: to minimize MSE we should select an average target value over samples in the leaf. The tree is built greedily starting from the root: for each leaf a split is selected to minimize MSE for this step. To begin with, gradient boosting is an ensembling technique, which means that prediction is done by an ensemble of simpler estimators. While this theoretical framework makes it possible to create an ensemble of various estimators, in practice we almost always use GBDT ‚Äî gradient boosting over decision trees. The aim of gradient boosting is to create (or ""train"") an ensemble of trees, given that we know how to train a single decision tree. This technique is called boosting because we expect an ensemble to work much better than a single estimator. Here comes the most interesting part. Gradient boosting builds an ensemble of trees one-by-one, then the predictions of the individual trees are summed: D(x)=d‚Äãtree 1‚Äã‚Äã(x)+d‚Äãtree 2‚Äã‚Äã(x)+... The next decision tree tries to cover the discrepancy between the target function f(x) and the current ensemble prediction by reconstructing the residual. For example, if an ensemble has 3 trees the prediction of that ensemble is: D(x)=d‚Äãtree 1‚Äã‚Äã(x)+d‚Äãtree 2‚Äã‚Äã(x)+d‚Äãtree 3‚Äã‚Äã(x). The next tree (tree 4) in the ensemble should complement well the existing trees and minimize the training error of the ensemble. In the ideal case we'd be happy to have: D(x)+d‚Äãtree 4‚Äã‚Äã(x)=f(x). To get a bit closer to the destination, we train a tree to reconstruct the difference between the target function and the current predictions of an ensemble, which is called the residual: R(x)=f(x)‚àíD(x). Did you notice? If decision tree completely reconstructs R(x), the whole ensemble gives predictions without errors (after adding the newly-trained tree to the ensemble)! That said, in practice this never happens, so we instead continue the iterative process of ensemble building. ### AdaBoost the First Boosting Algorithm The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness. AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. Gradient boosting involves three elements:",,hard,coding,regression|classification|clustering|hypothesis_testing|probability|ensemble|metrics,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.287568
A loss function to be optimized.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.287593
A weak learner to make predictions.,,medium,mixed,,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.287616
"An additive model to add weak learners to minimize the loss function. #### Loss Function The loss function used depends on the type of problem being solved. It must be differentiable, but many standard loss functions are supported and you can define your own. For example, regression may use a squared error and classification may use logarithmic loss. A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used. #### Weak Learner Decision trees are used as the weak learner in gradient boosting. Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and ‚Äúcorrect‚Äù the residuals in the predictions. Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss. Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels. It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes. This is to ensure that the learners remain weak, but can still be constructed in a greedy manner. #### Additive Model Trees are added one at a time, and existing trees in the model are not changed. A gradient descent procedure is used to minimize the loss when adding trees. Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error. Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by reducing the residual loss. Generally this approach is called functional gradient descent or gradient descent with functions. The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model. A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset. ### Improvements to Basic Gradient Boosting Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting. In this section we will look at 4 enhancements to basic gradient boosting: * Tree Constraints * Shrinkage * Random sampling * Penalized Learning #### Tree Constraints It is important that the weak learners have skill but remain weak. There are a number of ways that the trees can be constrained. A good general heuristic is that the more constrained tree creation is, the more trees you will need in the model, and the reverse, where less constrained individual trees, the fewer trees that will be required. Below are some constraints that can be imposed on the construction of decision trees: * Number of trees, generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed. * Tree depth, deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels. * Number of nodes or number of leaves, like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used. * Number of observations per split imposes a minimum constraint on the amount of training data at a training node before a split can be considered * Minimum improvement to loss is a constraint on the improvement of any split added to a tree. #### Weighted Updates The predictions of each tree are added together sequentially. The contribution of each tree to this sum can be weighted to slow down the learning by the algorithm. This weighting is called a shrinkage or a learning rate. Each update is simply scaled by the value of the ‚Äúlearning rate parameter‚Äù v The effect is that learning is slowed down, in turn require more trees to be added to the model, in turn taking longer to train, providing a configuration trade-off between the number of trees and learning rate. Decreasing the value of v [the learning rate] increases the best value for M [the number of trees]. It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1. Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model. #### Stochastic Gradient Boosting A big insight into bagging ensembles and random forest was allowing trees to be greedily created from subsamples of the training dataset. This same benefit can be used to reduce the correlation between the trees in the sequence in gradient boosting models. This variation of boosting is called stochastic gradient boosting. At each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to fit the base learner. A few variants of stochastic boosting that can be used: * Subsample rows before creating each tree. * Subsample columns before creating each tree * Subsample columns before considering each split. Generally, aggressive sub-sampling such as selecting only 50% of the data has shown to be beneficial. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling. #### Penalized Gradient Boosting Additional constraints can be imposed on the parameterized trees in addition to their structure. Classical decision trees like CART are not used as weak learners, instead a modified form called a regression tree is used that has numeric values in the leaf nodes (also called terminal nodes). The values in the leaves of the trees can be called weights in some literature. As such, the leaf weight values of the trees can be regularized using popular regularization functions, such as: * L1 regularization of weights. * L2 regularization of weights. The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions. More details in 2 posts (russian): * https://habr.com/company/ods/blog/327250/ * https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf ## 24. Difference between AdaBoost and XGBoost. Both methods combine weak learners into one strong learner. For example, one decision tree is a weak learner, and an emsemble of them would be a random forest model, which is a strong learner. Both methods in the learning process will increase the ensemble of weak-trainers, adding new weak learners to the ensemble at each training iteration, i.e. in the case of the forest, the forest will grow with new trees. The only difference between AdaBoost and XGBoost is how the ensemble is replenished. AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. AdaBoost at each iteration changes the sample weights in the sample. It raises the weight of the samples in which more mistakes were made. The sample weights vary in proportion to the ensemble error. We thereby change the probabilistic distribution of samples - those that have more weight will be selected more often in the future. It is as if we had accumulated samples on which more mistakes were made and would use them instead of the original sample. In addition, in AdaBoost, each weak learner has its own weight in the ensemble (alpha weight) - this weight is higher, the ‚Äúsmarter‚Äù this weak learner is, i.e. than the learner least likely to make mistakes. XGBoost does not change the selection or the distribution of observations at all. XGBoost builds the first tree (weak learner), which will fit the observations with some prediction error. A second tree (weak learner) is then added to correct the errors made by the existing model. Errors are minimized using a gradient descent algorithm. Regularization can also be used to penalize more complex models through both Lasso and Ridge regularization. In short, AdaBoost- reweighting examples. Gradient boosting - predicting the loss function of trees. Xgboost - the regularization term was added to the loss function (depth + values ‚Äã‚Äãin leaves). ## 25. Data Mining Describe the decision tree model A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node. Each internal node represents a test on an attribute. Each leaf node represents a class. The benefits of having a decision tree are as follows: * It does not require any domain knowledge. * It is easy to comprehend. * The learning and classification steps of a decision tree are simple and fast. Tree Pruning Tree pruning is performed in order to remove anomalies in the training data due to noise or outliers. The pruned trees are smaller and less complex. Tree Pruning Approaches Here is the Tree Pruning Approaches listed below: * Pre-pruning ‚àí The tree is pruned by halting its construction early. * Post-pruning - This approach removes a sub-tree from a fully grown tree. Cost Complexity The cost complexity is measured by the following two parameters ‚àí Number of leaves in the tree, and Error rate of the tree. ## 26. Notes from Coursera Deep Learning courses by Andrew Ng Notes from Coursera Deep Learning courses by Andrew Ng ## 27. What is a neural network? Neural networks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output as shown in the graphic below. Although there are many different kinds of learning rules used by neural networks, this demonstration is concerned only with one: the delta rule. The delta rule is often utilized by the most common class of ANNs called 'backpropagation neural networks' (BPNNs). Backpropagation is an abbreviation for the backwards propagation of error. With the delta rule, as with other types of back propagation, 'learning' is a supervised process that occurs with each cycle or 'epoch' (i.e. each time the network is presented with a new input pattern) through a forward activation flow of outputs, and the backwards error propagation of weight adjustments. More simply, when a neural network is initially presented with a pattern it makes a random 'guess' as to what it might be. It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights. More graphically, the process looks something like this: !alt text Backpropagation performs a gradient descent within the solution's vector space towards a 'global minimum' along the steepest vector of the error surface. The global minimum is that theoretical solution with the lowest possible error. The error surface itself is a hyperparaboloid but is seldom 'smooth'. Indeed, in most problems, the solution space is quite irregular with numerous 'pits' and 'hills' which may cause the network to settle down in a 'local minimum' which is not the best overall solution. Since the nature of the error space can not be known a priori, neural network analysis often requires a large number of individual runs to determine the best solution. Most learning rules have built-in mathematical terms to assist in this process which control the 'speed' (Beta-coefficient) and the 'momentum' of the learning. The speed of learning is actually the rate of convergence between the current solution and the global minimum. Momentum helps the network to overcome obstacles (local minima) in the error surface and settle down at or near the global minimum. Once a neural network is 'trained' to a satisfactory level it may be used as an analytical tool on other data. To do this, the user no longer specifies any training runs and instead allows the network to work in forward propagation mode only. New inputs are presented to the input pattern where they filter into and are processed by the middle layers as though training were taking place, however, at this point the output is retained and no backpropagation occurs. The output of a forward propagation run is the predicted model for the data which can then be used for further analysis and interpretation. ## 28. How do you deal with sparse data? We could take a look at L1 regularization since it best fits the sparse data and does feature selection. If linear relationship - linear regression either - svm. Also it would be nice to use one-hot-encoding or bag-of-words. A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. ## 29. RNN and LSTM Here are a few of my favorites: * Understanding LSTM Networks, Chris Olah's LSTM post * Exploring LSTMs, Edwin Chen's LSTM post * The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy's blog post * CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM, Andrej Karpathy's lecture * Jay Alammar's The Illustrated Transformer the guy generally focuses on visualizing different ML concepts ## 30. Pseudo Labeling Pseudo-labeling is a technique that allows you to use predicted with confidence test data in your training process. This effectivey works by allowing your model to look at more samples, possibly varying in distributions. I have found this Kaggle kernel to be useful in understanding how one can use pseudo-labeling in light of having too few train data points. ## 31. Knowledge Distillation It is the process by which a considerably larger model is able to transfer its knowledge to a smaller one. Applications include NLP and object detection allowing for less powerful hardware to make good inferences without significant loss of accuracy. Example: model compression which is used to compress the knowledge of multiple models into a single neural network. Explanation ## 32. What is an inductive bias? A model's inductive bias is referred to as assumptions made within that model to learn your target function from independent variables, your features. Without these assumptions, there is a whole space of solutions to our problem and finding the one that works best becomes a problem. Found this StackOverflow question useful to look at and explore. Consider an example of an inducion bias when choosing a learning algorithm with the minimum cross-validation (CV) error. Here, we rely on the hypothesis of the minimum CV error and hope it is able to generalize well on the data yet to be seen. Effectively, this choice is what helps us (in this case) make a choice in favor of the learning algorithm (or model) being tried. ## 33. What is a confidence interval in layman's terms? Confidence interval as the name suggests is the amount of confidence associated with an interval of values to get the desired outcome. For example : if 100 - 200 range is a 95% confidence interval , it implies that someone can have 95% assurance that the data point or any desired value is present in that range.",,easy,coding,regression|classification|probability|deep_learning|ensemble|metrics|feature_engineering,iamtodor/data-science-interview-questions-and-answers,,2025-11-17T10:49:36.290245
Explain Difference between joins,,hard,mixed,,jayinai/data-science-question-answer,"* **(INNER) JOIN**: Returns records that have matching values in both tables
* **LEFT (OUTER) JOIN**: Return all records from the left table, and the matched records from the right table
* **RIGHT (OUTER) JOIN**: Return all records from the right table, and the matched records from the left table
* **FULL (OUTER) JOIN**: Return all records when there is a match in either left or right table



(#data-science-question-answer)


## Tools and Framework

The resources here are only meant to help you",2025-11-17T11:42:20.246097
Explain Spark,,hard,mixed,,jayinai/data-science-question-answer,"Using PySpark API.

* The best resource is of course [Spark's documentation](https://spark.apache.org/docs/latest/). Take a thorough review of the topics
* If you are really time constrained, scan the Spark's documentation and check [PySpark cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf) for the basics


(#data-science-question-answer)


## Statistics and ML In General

* [Project Workflow](#project-workflow)
* [Cross Validation](#cross-vali",2025-11-17T11:42:20.246124
Explain Project Workflow,,hard,mixed,,jayinai/data-science-question-answer,"Given a data science / machine learning project, what steps should we follow? Here's
how I would tackle it:

* **Specify business objective.** Are we trying to win more customers, achieve higher satisfaction, or gain more revenues?
* **Define problem.** What is the specific gap in your ideal world and the real one that requires machine learning to fill? Ask questions that can be addressed using your data and predictive modeling (ML algorithms).
* **Create a common sense baseline.** But before yo",2025-11-17T11:42:20.246149
Explain Cross Validation,,medium,mixed,,jayinai/data-science-question-answer,"Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a validation set to evaluate it. For example, a k-fold cross validation divides the data into k folds (or partitions), trains on each k-1 fold, and evaluate on the remaining 1 fold. This results to k models/evaluations, which can be averaged to get a overall model performance.



(#data-science-question-answer)",2025-11-17T11:42:20.246161
Explain Feature Importance,,medium,mixed,,jayinai/data-science-question-answer,"* In linear models, feature importance can be calculated by the scale of the coefficients
* In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree.  We can get a feature's importance for random forest by computing the averaging depth at which it appears across all trees in the forest.

(#data-science-question-answer)",2025-11-17T11:42:20.246169
Explain Mean Squared Error vs. Mean Absolute Error,,medium,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both measure the average model prediction error; range from 0 to infinity; the lower the better
* Mean Squared Error (MSE) gives higher weights to large error (e.g., being off by 10 just MORE THAN TWICE as bad as being off by 5), whereas Mean Absolute Error (MAE) assign equal weights (being off by 10 is just twice as bad as being off by 5)
* MSE is continuously differentiable, MAE is not (where y_pred == y_true)

(#data-science-question-answer)",2025-11-17T11:42:20.246178
Explain L1 vs L2 regularization,,hard,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both L1 and L2 regularization **prevent overfitting** by shrinking (imposing a penalty) on the coefficients
* **Difference**: L2 (Ridge) shrinks all the coefficient by the same proportions but eliminates none, while L1 (Lasso) can shrink some coefficients to zero, performing variable selection.
* **Which to choose**: If all the features are correlated with the label, ridge outperforms lasso, as the coefficients are never zero in ridge. If only a subset of features are correlate",2025-11-17T11:42:20.246187
Explain Correlation vs Covariance,,hard,mixed,,jayinai/data-science-question-answer,"* Both determine the relationship and measure the dependency between two random variables
* Correlation is when the change in one item may result in the change in the another item, while covariance is when two items vary together (joint variability)
* Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance
* Range: correlation is between -1 and +1, while covariance lies between negative infinity and infinity.


(#data-science-quest",2025-11-17T11:42:20.246195
Explain Would adding more data address underfitting,,medium,mixed,,jayinai/data-science-question-answer,"Underfitting happens when a model is not complex enough to learn well from the data. It is the problem of model rather than data size. So a potential way to address underfitting is to increase the model complexity (e.g., to add higher order coefficients for linear model, increase depth for tree-based methods, add more layers / number of neurons for neural networks etc.)

(#data-science-question-answer)",2025-11-17T11:42:20.246203
Explain Activation Function,,medium,mixed,,jayinai/data-science-question-answer,"For neural networks

* Non-linearity: ReLU is often used. Use Leaky ReLU (a small positive gradient for negative input, say, `y = 0.01x` when x < 0) to address dead ReLU issue
* Multi-class: softmax
* Binary: sigmoid
* Regression: linear

(#data-science-question-answer)",2025-11-17T11:42:20.246210
Explain Bagging,,hard,mixed,,jayinai/data-science-question-answer,"To address overfitting, we can use an ensemble method called bagging (bootstrap aggregating),
which reduces the variance of the meta learning algorithm. Bagging can be applied
to decision tree or other algorithms.

Here is a [great illustration](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py) of a single estimator vs. bagging.



* Bagging is when samlping is performed *with* replacement. When sampling is perfor",2025-11-17T11:42:20.246219
Explain Stacking,,hard,mixed,,jayinai/data-science-question-answer,"* Instead of using trivial functions (such as hard voting) to aggregate the predictions from individual learners, train a model to perform this aggregation
* First split the training set into two subsets: the first subset is used to train the learners in the first layer
* Next the first layer learners are used to make predictions (meta features) on the second subset, and those predictions are used to train another models (to obtain the weigts of different learners) in the second layer
* We can t",2025-11-17T11:42:20.246227
Explain Generative vs discriminative,,hard,mixed,,jayinai/data-science-question-answer,"* Discriminative algorithms model *p(y|x; w)*, that is, given the dataset and learned
parameter, what is the probability of y belonging to a specific class. A discriminative algorithm
doesn't care about how the data was generated, it simply categorizes a given example
* Generative algorithms try to model *p(x|y)*, that is, the distribution of features given
that it belongs to a certain class. A generative algorithm models how the data was
generated.

> Given a training set, an algorithm like log",2025-11-17T11:42:20.246240
Explain Parametric vs Nonparametric,,medium,mixed,,jayinai/data-science-question-answer,"* A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model.
* A model where the number of parameters is not determined prior to training. Nonparametric does not mean that they have no parameters. On the contrary, nonparametric models (can) become more and more complex with an increasing amount of data.

(#data-science-question-answer)",2025-11-17T11:42:20.246247
Explain Recommender System,,hard,mixed,,jayinai/data-science-question-answer,"* I put recommend system here since technically it falls neither under supervised nor unsupervised learning
* A recommender system seeks to predict the 'rating' or 'preference' a user would give to items and then recommend items accordingly
* Content based recommender systems recommends items similar to those a given user has liked in the past, based on either explicit (ratings, like/dislike button) or implicit (viewed/finished an article) feedbacks. Content based recommenders work solely with t",2025-11-17T11:42:20.246259
Explain Linear regression,,hard,mixed,,jayinai/data-science-question-answer,"* How to learn the parameter: minimize the cost function
* How to minimize cost function: gradient descent
* Regularization:
    - L1 (Lasso): can shrink certain coef to zero, thus performing feature selection
    - L2 (Ridge): shrink all coef with the same proportion; almost always outperforms L1
    - Elastic Net: combined L1 and L2 priors as regularizer
* Assumes linear relationship between features and the label
* Can add polynomial and interaction features to add non-linearity



(#data-sci",2025-11-17T11:42:20.246268
Explain Logistic regression,,medium,mixed,,jayinai/data-science-question-answer,"* Generalized linear model (GLM) for binary classification problems
* Apply the sigmoid function to the output of linear models, squeezing the target
to range [0, 1]
* Threshold to make prediction: usually if the output > .5, prediction 1; otherwise prediction 0
* A special case of softmax function, which deals with multi-class problems

(#data-science-question-answer)",2025-11-17T11:42:20.246275
Explain Naive Bayes,,hard,mixed,,jayinai/data-science-question-answer,"* Naive Bayes (NB) is a supervised learning algorithm based on applying [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)
* It is called naive because it builds the naive assumption that each feature
are independent of each other
* NB can make different assumptions (i.e., data distributions, such as Gaussian,
Multinomial, Bernoulli)
* Despite the over-simplified assumptions, NB classifier works quite well in real-world
applications, especially for text classification (e.g., spam f",2025-11-17T11:42:20.246284
Explain Decision tree,,hard,mixed,,jayinai/data-science-question-answer,"* Non-parametric, supervised learning algorithms
* Given the training data, a decision tree algorithm divides the feature space into
regions. For inference, we first see which
region does the test data point fall in, and take the mean label values (regression)
or the majority label value (classification).
* **Construction**: top-down, chooses a variable to split the data such that the
target variables within each region are as homogeneous as possible. Two common
metrics: gini impurity or informa",2025-11-17T11:42:20.246301
Explain Random forest,,hard,mixed,,jayinai/data-science-question-answer,"Random forest improves bagging further by adding some randomness. In random forest,
only a subset of features are selected at random to construct a tree (while often not subsample instances).
The benefit is that random forest **decorrelates** the trees.

For example, suppose we have a dataset. There is one very predicative feature, and a couple
of moderately predicative features. In bagging trees, most of the trees
will use this very predicative feature in the top split, and therefore making mos",2025-11-17T11:42:20.246309
Explain Boosting Tree,,hard,mixed,,jayinai/data-science-question-answer,"**How it works**

Boosting builds on weak learners, and in an iterative fashion. In each iteration,
a new learner is added, while all existing learners are kept unchanged. All learners
are weighted based on their performance (e.g., accuracy), and after a weak learner
is added, the data are re-weighted: examples that are misclassified gain more weights,
while examples that are correctly classified lose weights. Thus, future weak learners
focus more on examples that previous weak learners misclass",2025-11-17T11:42:20.246318
Explain RNN and LSTM,,hard,mixed,,jayinai/data-science-question-answer,"RNN is another paradigm of neural network where we have difference layers of cells,
and each cell not only takes as input the cell from the previous layer, but also the previous
cell within the same layer. This gives RNN the power to model sequence.



This seems great, but in practice RNN barely works due to exploding/vanishing gradient, which
is cause by a series of multiplication of the same matrix. To solve this, we can use
a variation of RNN, called long short-term memory (LSTM), which is c",2025-11-17T11:42:20.246333
Explain Clustering,,hard,mixed,,jayinai/data-science-question-answer,"* Clustering is a unsupervised learning algorithm that groups data in such
a way that data points in the same group are more similar to each other than to
those from other groups
* Similarity is usually defined using a distance measure (e.g, Euclidean, Cosine, Jaccard, etc.)
* The goal is usually to discover the underlying structure within the data (usually high dimensional)
* The most common clustering algorithm is K-means, where we define K (the number of clusters)
and the algorithm iterativel",2025-11-17T11:42:20.246342
Explain Principal Component Analysis,,hard,mixed,,jayinai/data-science-question-answer,"* Principal Component Analysis (PCA) is a dimension reduction technique that projects
the data into a lower dimensional space
* PCA uses Singular Value Decomposition (SVD), which is a matrix factorization method
that decomposes a matrix into three smaller matrices (more details of SVD [here](https://en.wikipedia.org/wiki/Singular-value_decomposition))
* PCA finds top N principal components, which are dimensions along which the data vary
(spread out) the most. Intuitively, the more spread out the",2025-11-17T11:42:20.246351
Explain Autoencoder,,medium,mixed,,jayinai/data-science-question-answer,"* The aim of an autoencoder is to learn a representation (encoding) for a set of data
* An autoencoder always consists of two parts, the encoder and the decoder. The encoder would find a lower dimension representation (latent variable) of the original input, while the decoder is used to reconstruct from the lower-dimension vector such that the distance between the original and reconstruction is minimized
* Can be used for data denoising and dimensionality reduction",2025-11-17T11:42:20.246359
Explain Generative Adversarial Network,,hard,mixed,,jayinai/data-science-question-answer,"* Generative Adversarial Network (GAN) is an unsupervised learning algorithm that also has supervised flavor: using supervised loss as part of training
* GAN typically has two major components: the **generator** and the **discriminator**. The generator tries to generate ""fake"" data (e.g, images or sentences) that fool the discriminator into thinking that they're real, while the discriminator tries to distinguish between real and generated data. It's a fight between the two players thus the name ",2025-11-17T11:42:20.246371
Explain Tokenization,,hard,mixed,,jayinai/data-science-question-answer,"* Tokenization is the process of converting a sequence of characters into a sequence of tokens
* Consider this example: `The quick brown fox jumped over the lazy dog`. In this case each word (separated by space) would be a token
* Sometimes tokenization doesn't have a definitive answer. For instance, `O'Neill` can be tokenized to `o` and `neill`, `oneill`, or `o'neill`.
* In some cases tokenization requires language-specific knowledge. For example, it doesn't make sense to tokenize `aren't` into",2025-11-17T11:42:20.246379
Explain Stemming and lemmatization,,hard,mixed,,jayinai/data-science-question-answer,"* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form
* Stemming usually refers to a crude heuristic process that chops off the ends of words
* Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words
* If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` ",2025-11-17T11:42:20.246387
Explain N gram,,hard,mixed,,jayinai/data-science-question-answer,"* n-gram is a contiguous sequence of n items from a given sample of text or speech
* An n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" size 3 is a ""trigram"". Larger sizes are sometimes referred to by the value of n in modern language, e.g., ""four-gram"", ""five-gram"", and so on.
* Consider this example: `The quick brown fox jumped over the lazy dog.`
  - bigram would be `the quick`, `quick brown`, `brown fox`, ..., i.e, every two consecutive words (or tokens)
  - trigram woul",2025-11-17T11:42:20.246394
Explain Bag of Words,,hard,mixed,,jayinai/data-science-question-answer,"* Why? Machine learning models cannot work with raw text directly; rather, they take numerical values as input.
* Bag of words (BoW) builds a **vocabulary** of all the unique words in our dataset, and associate a unique index to each word in the vocabulary
* It is called a ""bag"" of words, because it is a representation that completely ignores the order of words
* Consider this example of two sentences: (1) `John likes to watch movies, especially horor movies.`, (2) `Mary likes movies too.` We wo",2025-11-17T11:42:20.246402
Explain word2vec,,hard,mixed,,jayinai/data-science-question-answer,"* Shallow, two-layer neural networks that are trained to construct linguistic context of words
* Takes as input a large corpus, and produce a vector space, typically of several hundred
dimension, and each word in the corpus is assigned a vector in the space
* The key idea is **context**: words that occur often in the same context should have same/opposite
meanings.
* Two flavors
    - continuous bag of words (CBOW): the model predicts the current word given a window of surrounding context words
",2025-11-17T11:42:20.246413
Explain Cron job,,hard,mixed,,jayinai/data-science-question-answer,"The software utility **cron** is a **time-based job scheduler** in Unix-like computer operating systems. People who set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration -- though its general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular intervals.



Tools:
* [Apache Ai",2025-11-17T11:42:20.246423
Explain Linux,,medium,mixed,,jayinai/data-science-question-answer,"Using **Ubuntu** as an example.

* Become root: `sudo su`
* Install package: `sudo apt-get install <package>`

(#data-science-question-answer)


Confession: some images are adopted from the internet without proper credit. If you are the author and this would be an issue for you, please let me know.",2025-11-17T11:42:20.246431
Explain to me a technical concept related to the role that you‚Äôre interviewing for.,,medium,behavioral,communication,kojino/120-DS-Questions,,2025-11-18T16:04:43.447330
Introduce me to something you‚Äôre passionate about.,,medium,behavioral,communication,kojino/120-DS-Questions,,2025-11-18T16:04:43.447342
How would you explain an A/B test to an engineer with no statistics background? A linear regression?,,medium,behavioral,communication,kojino/120-DS-Questions,"- A/B testing, or more broadly, multivariate testing, is the testing of different elements of a user's experience to determine which variation helps the business achieve its goal more effectively (i.e. increasing conversions, etc..)¬† This can be copy on a web site, button colors, different user interfaces, different email subject lines, calls to action, offers, etc.",2025-11-18T16:04:43.447381
How would you explain a confidence interval to an engineer with no statistics background? What does 95% confidence mean?,,medium,behavioral,communication,kojino/120-DS-Questions,- [link](https://www.quora.com/What-is-a-confidence-interval-in-laymans-terms),2025-11-18T16:04:43.447402
How would you explain to a group of senior executives why data is important?,,medium,behavioral,communication,kojino/120-DS-Questions,,2025-11-18T16:04:43.447408
(Given a Dataset) Analyze this dataset and tell me what you can learn from it.,,medium,case,data_analysis,kojino/120-DS-Questions,,2025-11-18T16:04:43.447458
What is R2? What are some other metrics that could be better than R2 and why?,,medium,case,data_analysis,kojino/120-DS-Questions,"- goodness of fit measure. variance explained by the regression / total variance
  - the more predictors you add the higher R^2 becomes.
    - hence use adjusted R^2 which adjusts for the degrees of freedom¬†
    - or train error metrics",2025-11-18T16:04:43.447469
What is the curse of dimensionality?,,hard,case,data_analysis,kojino/120-DS-Questions,"- High dimensionality makes clustering hard, because having lots of dimensions means that everything is ""far away"" from each other.
  - For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases
  - All samples are close to the edge of the sample.¬†And this is a bad news because prediction is much more difficult near the edges of the training sample.
  - The sampling density decreases exponentially as p in",2025-11-18T16:04:43.447483
Is more data always better?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Statistically,
    - It depends on the quality of your data, for example, if your data is biased, just getting more data won‚Äôt help.
    - It depends on your model. If your model suffers from high bias, getting more data won‚Äôt improve your test results beyond a point. You‚Äôd need to add more features, etc.
  - Practically,
    - Also there‚Äôs a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more ",2025-11-18T16:04:43.447495
What are advantages of plotting your data before per- forming analysis?,,hard,case,data_analysis,kojino/120-DS-Questions,"- 1) Data sets have errors.¬† You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.  
2) Variables can have skewness, outliers etc.¬† Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.  
3) Variables can be multimodal!¬† If a variable is multimodal then anything based on its mean or median is going to be suspect.",2025-11-18T16:04:43.447506
How can you make sure that you don‚Äôt analyze something that ends up meaningless?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Proper exploratory data analysis.  
In every data analysis task, there's the¬†exploratory¬†phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.  
Then there's the¬†exploitatory¬†phase, where you look deeply into a set of hypotheses.¬†  
The exploratory phase will generate lots of possible hypotheses, and the exploitatory phase will let you really understand a few",2025-11-18T16:04:43.447523
What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?,,hard,case,data_analysis,kojino/120-DS-Questions,"- data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.
  - The scientific method is eminently¬†inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is.",2025-11-18T16:04:43.447536
How can you determine which features are the most im- portant in your model?,,medium,case,data_analysis,kojino/120-DS-Questions,"- run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
  - Look at the variables added in forward variable selection",2025-11-18T16:04:43.447545
How do you deal with some of your predictors being missing?,,hard,case,data_analysis,kojino/120-DS-Questions,"- Remove rows with missing values -¬†This works well if 1) the values are missing randomly (see¬†[Vinay Prabhu's answer](https://www.quora.com/How-can-I-deal-with-missing-values-in-a-predictive-model/answer/Vinay-Prabhu-7)¬†for more details on this) 2) if you don't lose too much of the dataset after doing so.
  - Build another predictive model to predict the missing values -¬†This could be a whole project in itself, so simple techniques are usually used here.
  - Use a model that can incorporate mis",2025-11-18T16:04:43.447558
"You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?",,hard,case,data_analysis,kojino/120-DS-Questions,"- Multicollinearity¬†refers to a situation in which two or more explanatory variables in a¬†[multiple regression](https://en.wikipedia.org/wiki/Multiple_regression ""Multiple regression"")¬†model are highly linearly related.¬†
  - Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data o",2025-11-18T16:04:43.447579
Let‚Äôs say you‚Äôre given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?,,medium,case,data_analysis,kojino/120-DS-Questions,- PCA,2025-11-18T16:04:43.447587
"Now you have a feasible amount of predictors, but you‚Äôre fairly sure that you don‚Äôt need all of them. How would you perform feature selection on the dataset?",,hard,case,data_analysis,kojino/120-DS-Questions,"- ridge / lasso / elastic net regression
  - Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores
  - ""Recursive Feature Elimination"":  
    - First, train a model with all the feature and evaluate its performance on held out data.
    - Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining feature",2025-11-18T16:04:43.447604
Your linear regression didn‚Äôt run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?,,medium,case,data_analysis,kojino/120-DS-Questions,"- p > n.
  - If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique.",2025-11-18T16:04:43.447614
"You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?",,medium,case,data_analysis,kojino/120-DS-Questions,"- The dataset might be¬†heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with¬†heterogeneity quite nicely.",2025-11-18T16:04:43.447627
"What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?",,hard,case,data_analysis,kojino/120-DS-Questions,"- The assumption is that a group of weak learners can be combined to form a strong learner.
  - Hence the combined model is expected to perform better than an individual model.
  - Assumptions:
    - average out biases
    - reduce variance
  - Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability le",2025-11-18T16:04:43.447660
"Given that you have wi data in your o ce, how would you determine which rooms and areas are underutilized and overutilized?",,medium,case,data_analysis,kojino/120-DS-Questions,"- If the data is more used in one room, then that one is¬†over utilized! Maybe account for the room capacity and normalize the data.",2025-11-18T16:04:43.447669
How could you use GPS data from a car to determine the quality of a driver?,,medium,case,data_analysis,kojino/120-DS-Questions,,2025-11-18T16:04:43.447673
"Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?",,medium,case,data_analysis,kojino/120-DS-Questions,,2025-11-18T16:04:43.447680
"Given position data of NBA players in a season‚Äôs games, how would you evaluate a basketball player‚Äôs defensive ability?",,medium,case,data_analysis,kojino/120-DS-Questions,,2025-11-18T16:04:43.447685
How would you quantify the influence of a Twitter user?,,medium,case,data_analysis,kojino/120-DS-Questions,- like page rank with each user corresponding to the webpages and linking to the page equivalent to following.,2025-11-18T16:04:43.447691
"Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?",,medium,case,data_analysis,kojino/120-DS-Questions,,2025-11-18T16:04:43.447697
"You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of di culty?",,hard,case,data_analysis,kojino/120-DS-Questions,"- One way you could do this is by storing a ""skill level"" for each user and a ""difficulty level"" for each problem.¬† We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem.*¬† Then we maximize the likelihood of the data to find the hidden skill and difficulty levels.
  - The Rasch model for dichotomous data takes the form:  
{\displaystyle \Pr\\{X_{ni}=1\\}={\frac {\exp({\beta _{n}}-{\delta _{i}})}{1+\exp({\beta _{n}}-{\d",2025-11-18T16:04:43.447714
You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?,,medium,case,data_analysis,kojino/120-DS-Questions,"- Some people would take the mean rank of each sushi.¬† If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong).",2025-11-18T16:04:43.447726
"Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?",,medium,case,data_analysis,kojino/120-DS-Questions,"- collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative
  - for liberal and republican parties, find the mean vector and find the representative closest to the center point",2025-11-18T16:04:43.447740
How would you come up with an algorithm to detect plagiarism in online content?,,medium,case,data_analysis,kojino/120-DS-Questions,"- reduce the text to a more compact form (e.g. fingerprinting, bag of words) then compare those with other texts by calculating the similarity",2025-11-18T16:04:43.447747
You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?,,medium,case,data_analysis,kojino/120-DS-Questions,"- KMeans
  - choose a small value of¬†k¬†that still has a low SSE (elbow method)
  - <https://bl.ocks.org/rpgove/0060ff3b656618e9136b>",2025-11-18T16:04:43.447759
Let's say you're building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?,,medium,case,data_analysis,kojino/120-DS-Questions,- [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering),2025-11-18T16:04:43.447768
(Given a Dataset) Analyze this dataset and give me a model that can predict this response variable.,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Start by fitting a simple model (multivariate regression, logistic regression), do some feature engineering accordingly, and then try some complicated models. Always split the dataset into train, validation, test dataset and use cross validation to check their performance.
- Determine if the problem is classification or regression
- Favor simple models that run quickly and you can easily explain.
- Mention cross validation as a means to evaluate the model.
- Plot and visualize the data.",2025-11-18T16:04:43.447814
What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- The model that has high training accuracy might have low test accuracy. Without further knowledge, it is hard to know which dataset represents the population data and thus the generalizability of the algorithm is hard to measure. This should be mitigated by repeated splitting of train vs test dataset (as in cross validation).
- When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely",2025-11-18T16:04:43.447836
What are some ways I can make my model more robust to outliers?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can have regularization such as L1 or L2 to reduce variance (increase bias).
- Changes to the algorithm:
  - Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.
  - Use robust error metrics such as MAE or Huber Loss instead of MSE.
- Changes to the data:
  - Winsorizing the data
  - Transforming the data (e.g. log)
  - Remove them only if you‚Äôre certain they‚Äôre anomalies not ",2025-11-18T16:04:43.447849
"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?",,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that‚Äôs not the case, we should use MSE.
- MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient
- MAE more robust to outliers. If the consequences of large errors are great, use MSE
- MSE ",2025-11-18T16:04:43.447866
What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Accuracy: proportion of instances you predict correctly. Pros: intuitive, easy to explain, Cons: works poorly when the class labels are imbalanced and the signal from the data is weak
- AUROC: plot fpr on the x axis and tpr on the y axis for different threshold. Given a random positive instance and a random negative instance, the AUC is the probability that you can identify who's who. Pros: Works well when testing the ability of distinguishing the two classes, Cons: can‚Äôt interpret predictions",2025-11-18T16:04:43.447887
"What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What‚Äôs the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)",,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage
- Logistic Regression
  - features roughly linear, problem roughly linearly separable
  - robust to noise, use l1,l2 regularization for model selection, avoid overfitting
  - the output come as probabilities
  - efficient and the computation can be distributed
  - can be used as a baseline for other algorithms
  - (-) can hardly handle categorical features
- SVM
  - with a ",2025-11-18T16:04:43.447919
What is regularization and where might it be helpful? What is an example of using regularization in a model?,,medium,ml,predictive_modeling,kojino/120-DS-Questions,"- Regularization is useful for reducing variance in the model, meaning avoiding overfitting . For example, we can use L1 regularization in Lasso regression to penalize large coefficients.",2025-11-18T16:04:43.447928
Why might it be preferable to include fewer predictors over many?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- When we add irrelevant features, it increases model's tendency to overfit because those features introduce more noise. When two variables are correlated, they might be harder to interpret in case of regression, etc.
- curse of dimensionality
- adding random noise makes the model more complicated but useless
- computational cost
- Ask someone for more details.",2025-11-18T16:04:43.447938
"Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?",,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Build a time series model with the training data with a seven day cycle and then use that for a new data with only 2 days data.
- Ask someone for more details.
- Build a regression function to estimate the number of retweets as a function of time t
- to determine if one regression function can be built, see if there are clusters in terms of the trends in the number of retweets
- if not, we have to add features to the regression function
- features + # of retweets on the first and the second da",2025-11-18T16:04:43.447956
How could you collect and analyze data to use social media to predict the weather?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can collect social media data using twitter, Facebook, instagram API‚Äôs. Then, for example, for twitter, we can construct features from each tweet, e.g. the tweeted date, number of favorites, retweets, and of course, the features created from the tweeted content itself. Then use a multi variate time series model to predict the weather.
- Ask someone for more details.",2025-11-18T16:04:43.447967
How would you construct a feed to show relevant content for a site that involves user interactions with items?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- We can do so using building a recommendation engine. The easiest we can do is to show contents that are popular other users, which is still a valid strategy if for example the contents are news articles. To be more accurate, we can build a content based filtering or collaborative filtering. If there‚Äôs enough user usage data, we can try collaborative filtering and recommend contents other similar users have consumed. If there isn‚Äôt, we can recommend similar items based on vectorization of items",2025-11-18T16:04:43.447980
How would you design the people you may know feature on LinkedIn or Facebook?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Find strong unconnected people in weighted connection graph
  - Define similarity as how strong the two people are connected
  - Given a certain feature, we can calculate the similarity based on
    - friend connections (neighbors)
    - Check-in‚Äôs people being at the same location all the time.
    - same college, workplace
    - Have randomly dropped graphs test the performance of the algorithm
- ref. News Feed Optimization
  - Affinity score: how close the content creator and the users are
",2025-11-18T16:04:43.447995
How would you predict who someone may want to send a Snapchat or Gmail to?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- for each user, assign a score of how likely someone would send an email to
- the rest is feature engineering:
  - number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.
- Ask someone for more details.
- People who someone sent emails the most in the past, conditioning on time decay.",2025-11-18T16:04:43.448006
How would you suggest to a franchise where to open a new store?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- build a master dataset with local demographic information available for each location.
  - local income levels, proximity to traffic, weather, population density, proximity to other businesses
  - a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.)
  - any data on the local franchise owner-operators, to the degree the manager
- identify a set of KPIs acceptable to the management that had requested the analysis ",2025-11-18T16:04:43.448022
"In a search engine, given partial data on what the user has typed, how would you predict the user‚Äôs eventual search query?",,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.
- To further improve this algorithm,
  - we can put more weight on past sequences which showed up more recently and near your location to account for trends
  - show your recent searches given partial data",2025-11-18T16:04:43.448036
"Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?",,medium,ml,predictive_modeling,kojino/120-DS-Questions,"- Based on frequency and amount of donations, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.",2025-11-18T16:04:43.448045
You‚Äôre Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- Based on the past pickup location of passengers around the same time of the day, day of the week (month, year), construct
- Ask someone for more details.
- Based on the number of past pickups
  - account for periodicity (seasonal, monthly, weekly, daily, hourly)
  - special events (concerts, festivals, etc.) from tweets",2025-11-18T16:04:43.448056
How would you build a model to predict a March Madness bracket?,,hard,ml,predictive_modeling,kojino/120-DS-Questions,"- One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament
- Some extensions:
  - Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc)
  - Consider using a RNN type model that ",2025-11-18T16:04:43.448069
"You want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?",,medium,ml,predictive_modeling,kojino/120-DS-Questions,"- This is equivalent to making the model more robust to outliers.
- See Q3.",2025-11-18T16:04:43.448079
"Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 offspring, respectively. Each of Bobo‚Äôs descendants also have the same probabilities. What is the probability that Bobo‚Äôs lineage dies out?",,hard,stats,probability,kojino/120-DS-Questions,- p=1/4+1/4*p+1/2*p^2 => p=1/2,2025-11-18T16:04:43.448114
"In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba- bility that you see at least one shooting star in the period of an hour?",,hard,stats,probability,kojino/120-DS-Questions,"- 1-(0.8)^4. Or, we can use Poisson processes",2025-11-18T16:04:43.448124
How can you generate a random number between 1 - 7 with only a die?,,hard,stats,probability,kojino/120-DS-Questions,"* Launch it 3 times: each throw sets the nth bit of the result. 
* For each launch, if the value is 1-3, record a 0, else 1.
The result is between 0 (000) and 7 (111), evenly spread (3 independent throw). Repeat the throws if 0 was obtained: the process stops on evenly spread values.",2025-11-18T16:04:43.448133
How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?,,hard,stats,probability,kojino/120-DS-Questions,"- Flip twice and if HT then H, TH then T.",2025-11-18T16:04:43.448139
You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?,,hard,stats,probability,kojino/120-DS-Questions,- more than two standard deviations,2025-11-18T16:04:43.448147
"Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?",,hard,stats,probability,kojino/120-DS-Questions,- plug in the value to the CDF of the same random variable,2025-11-18T16:04:43.448154
"A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?",,hard,stats,probability,kojino/120-DS-Questions,- 1/3,2025-11-18T16:04:43.448161
"You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?",,hard,stats,probability,kojino/120-DS-Questions,- gender ratio is 1:1. Expected number of children is 2. let X be the number of children until getting a female (happens with prob 1/2). this follows a geometric distribution with probability 1/2,2025-11-18T16:04:43.448174
How many ways can you split 12 people into 3 teams of 4?,,hard,stats,probability,kojino/120-DS-Questions,- the outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable,2025-11-18T16:04:43.448179
"Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.",,hard,stats,probability,kojino/120-DS-Questions,"- the probability of a hash collision:¬†1-(10!/10^10)
  - the expected number of hash collisions: 1-10*(9/10)^10
  - the expected number of hashes that are unused:¬†10*(9/10)^10",2025-11-18T16:04:43.448192
"You call 2 UberX‚Äôs and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX‚Äôs arrive first?",,hard,stats,probability,kojino/120-DS-Questions,"- All Lyft's first
    * probability that the first car is Lyft = 3/5
    * probability that the second car is Lyft = 2/4
    * probability that the third car is Lyft = 1/3
    Therefore, probability that all the Lyfts arrive first = (3/5) * (2/4) * (1/3) = 1/10
  - All Uber's first
    * probability that the first car is Uber = 2/5
    * probability that the second car is Uber = 1/4
    Therefore, probability that all the Ubers arrive first = (2/5) * (1/4) = 1/10",2025-11-18T16:04:43.448209
"I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?",,hard,stats,probability,kojino/120-DS-Questions,- 100+60-20=140,2025-11-18T16:04:43.448222
"On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?",,hard,stats,probability,kojino/120-DS-Questions,- 24C5*(1+5(24-5))/24C5*24C5 = 4/1771,2025-11-18T16:04:43.448232
"A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?",,hard,stats,probability,kojino/120-DS-Questions,- 1,2025-11-18T16:04:43.448241
"Let‚Äôs say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?",,hard,stats,probability,kojino/120-DS-Questions,- Shorter. Regression to the mean,2025-11-18T16:04:43.448250
What‚Äôs the expected number of coin flips until you get two heads in a row? What‚Äôs the expected number of coin flips until you get two tails in a row?,,hard,stats,probability,kojino/120-DS-Questions,"- After the first two flips, you can see this problem as a Markov chain, with states HH, HT, TH, TT. 
  - HH is the final state. You can than define the expected number of steps N before reaching HH: E(N) = 2 + 0.25nHH, 0.25nHT, 0.25nTH, 0.25nTT. nXX represents the expected number of steps before reaching HH starting from state XX.
  - Solve linear equation:
  * nHH = 0
  * nHT = 1 + 0.5nTT + 0.5nTH
  * nTH = 1 + 0.5nHH + 0.5nHT
  * nTT = 1 + 0.5nTH + 0.5nTT
  - Result gives E(N) = 6.",2025-11-18T16:04:43.448265
"Let‚Äôs say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?",,hard,stats,probability,kojino/120-DS-Questions,- less than $3,2025-11-18T16:04:43.448274
"You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?",,hard,stats,probability,kojino/120-DS-Questions,- 4/13,2025-11-18T16:04:43.448285
"You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What‚Äôs the chance that you picked up the fair coin, given the information that you observed?",,hard,stats,probability,kojino/120-DS-Questions,"* Events: F = ""picked a fair coin"", T = ""10 heads in a row""
  * (1) P(F|T) = P(T|F)P(F)/P(T) (Bayes formula)
  * (2) P(T) = P(T|F)P(F) + P(T|¬¨F)P(¬¨F) (total probabilities formula)
  * Injecting (2) in (1): P(F|T) = P(T|F)P(F)/(P(T|F)P(F) + P(T|¬¨F)P(¬¨F)) = 1 / (1 + P(T|¬¨F)P(¬¨F)/(P(T|F)P(F)))
  * Numerically: 1/(1 + 0.001 * 2^10 /0.999).
  * With 2^10 ‚âà 1000 and 0.999 ‚âà 1 this simplifies to 1/2",2025-11-18T16:04:43.448303
What is a P-Value ?,,hard,stats,probability,kojino/120-DS-Questions,"* The probability to obtain a similar or more extreme result than observed when the null hypothesis is assumed.
  * ‚áí If the p-value is small, the null hypothesis is unlikely",2025-11-18T16:04:43.448308
"What would be good metrics of success for an advertising-driven consumer product? (Buzzfeed, YouTube, Google Search, etc.) A service-driven consumer product? (Uber, Flickr, Venmo, etc.)",,medium,case,product_metrics,kojino/120-DS-Questions,"* advertising-driven: Pageviews and daily actives, CTR, CPC (cost per click)
    * click-ads  
    * display-ads  
  * service-driven: number of purchases, conversion rate",2025-11-18T16:04:43.448343
"What would be good metrics of success for a productiv- ity tool? (Evernote, Asana, Google Docs, etc.) A MOOC? (edX, Coursera, Udacity, etc.)",,medium,case,product_metrics,kojino/120-DS-Questions,"* productivity tool: same as premium subscriptions
  * MOOC:¬†same as premium subscriptions, completion rate",2025-11-18T16:04:43.448351
"What would be good metrics of success for an e-commerce product? (Etsy, Groupon, Birchbox, etc.) A subscrip- tion product? (Net ix, Birchbox, Hulu, etc.) Premium subscriptions? (OKCupid, LinkedIn, Spotify, etc.)",,hard,case,product_metrics,kojino/120-DS-Questions,"* e-commerce:¬†number of purchases, conversion rate,¬†Hourly, daily, weekly, monthly, quarterly, and annual sales, Cost of goods sold,¬†Inventory levels,¬†Site traffic,¬†Unique visitors versus returning visitors,¬†Customer service phone call count,¬†Average resolution time
  * subscription
    * churn, CoCA, ARPU, MRR, LTV
  * premium subscriptions:",2025-11-18T16:04:43.448365
"What would be good metrics of success for a consumer product that relies heavily on engagement and interac- tion? (Snapchat, Pinterest, Facebook, etc.) A messaging product? (GroupMe, Hangouts, Snapchat, etc.)",,medium,case,product_metrics,kojino/120-DS-Questions,"* heavily on engagement and interaction:¬†uses AU ratios, email summary by type, and push notification summary by type, resurrection ratio
  * messaging product:",2025-11-18T16:04:43.448377
"What would be good metrics of success for a product that o ered in-app purchases? (Zynga, Angry Birds, other gaming apps)",,medium,case,product_metrics,kojino/120-DS-Questions,"* Average Revenue Per Paid User
  * Average Revenue Per User",2025-11-18T16:04:43.448383
A certain metric is violating your expectations by going down or up more than you expect. How would you try to identify the cause of the change?,,medium,case,product_metrics,kojino/120-DS-Questions,"* breakdown the KPI‚Äôs into what consists them and find where the change is
  * then further breakdown that basic KPI by channel, user cluster, etc. and relate them with any campaigns, changes in user behaviors in that segment",2025-11-18T16:04:43.448394
Growth for total number of tweets sent has been slow this month. What data would you look at to determine the cause of the problem?,,medium,case,product_metrics,kojino/120-DS-Questions,"* look at competitors' tweet growth
  * look at your social media engagement on other platforms
  * look at your sales data",2025-11-18T16:04:43.448402
You‚Äôre a restaurant and are approached by Groupon to run a deal. What data would you ask from them in order to determine whether or not to do the deal?,,medium,case,product_metrics,kojino/120-DS-Questions,"* for similar restaurants (they should define similarity), average increase in revenue gain per coupon, average increase in customers per coupon, number of meals sold",2025-11-18T16:04:43.448412
You are tasked with improving the e ciency of a subway system. Where would you start?,,medium,case,product_metrics,kojino/120-DS-Questions,* define efficiency,2025-11-18T16:04:43.448417
Say you are working on Facebook News Feed. What would be some metrics that you think are important? How would you make the news each person gets more relevant?,,hard,case,product_metrics,kojino/120-DS-Questions,"* rate for each action, duration users stay, CTR for sponsor feed posts
  * ref. News Feed Optimization
    * Affinity score: how close the content creator and the users are
    * Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
    * Time decay: the older the less important",2025-11-18T16:04:43.448429
How would you measure the impact that sponsored stories on Facebook News Feed have on user engagement? How would you determine the optimum balance between sponsored stories and organic content on a user‚Äôs News Feed?,,medium,case,product_metrics,kojino/120-DS-Questions,* AB test on¬†different balance ratio and see,2025-11-18T16:04:43.448440
You are on the data science team at Uber and you are asked to start thinking about surge pricing. What would be the objectives of such a product and how would you start looking into this?,,hard,case,product_metrics,kojino/120-DS-Questions,"* ¬†there is a gradual step-function type scaling mechanism until that imbalance of requests-to-drivers is alleviated and then vice versa as too many drivers come online enticed by the surge pricing structure.¬†
  * I would bet the algorithm is custom tailored and calibrated to each location as price elasticities almost certainly vary across different cities depending on a huge multitude of variables: income, distance/sprawl, traffic patterns, car ownership, etc. With the massive troves of user da",2025-11-18T16:04:43.448460
Say that you are Netflix. How would you determine what original series you should invest in and create?,,medium,case,product_metrics,kojino/120-DS-Questions,* Netflix uses data to estimate the potential market size for an original series before giving it the go-ahead.,2025-11-18T16:04:43.448467
What kind of services would nd churn (metric that tracks how many customers leave the service) helpful? How would you calculate churn?,,medium,case,product_metrics,kojino/120-DS-Questions,* subscription based services,2025-11-18T16:04:43.448474
Let‚Äôs say that you‚Äôre are scheduling content for a content provider on television. How would you determine the best times to schedule content?,,medium,case,product_metrics,kojino/120-DS-Questions,,2025-11-18T16:04:43.448481
"Write a function to calculate all possible assignment vectors of 2n users, where n users are assigned to group 0 (control), and n users are assigned to group 1 (treatment).",,medium,coding,programming,kojino/120-DS-Questions,- Recursive programming (sol in code),2025-11-18T16:04:43.448517
"Given a list of tweets, determine the top 10 most used hashtags.",,medium,coding,programming,kojino/120-DS-Questions,- Store all the hashtags in a dictionary and get the top 10 values,2025-11-18T16:04:43.448523
Program an algorithm to find the best approximate solution to the knapsack problem1 in a given time.,,medium,coding,programming,kojino/120-DS-Questions,- Greedy solution (add the best v/w as much as possible and move on to the next),2025-11-18T16:04:43.448529
"You have a stream of data coming in of size n, but you don‚Äôt know what n is ahead of time. Write an algorithm that will take a random sample of k elements. Can you write one that takes O(k) space?",,medium,coding,programming,kojino/120-DS-Questions,- https://en.wikipedia.org/wiki/Reservoir_sampling,2025-11-18T16:04:43.448545
Write an algorithm that can calculate the square root of a number.,,medium,coding,programming,kojino/120-DS-Questions,"- <https://www.quora.com/What-is-the-method-to-calculate-a-square-root-by-hand?redirected_qid=664405>
  - https://en.wikipedia.org/wiki/Newton's_method#Square_root_of_a_number",2025-11-18T16:04:43.448552
"Given a list of numbers, can you return the outliers?",,medium,coding,programming,kojino/120-DS-Questions,- sort then select the highest and the lowest 2.5%,2025-11-18T16:04:43.448556
When can parallelism make your algorithms run faster?,,medium,coding,programming,kojino/120-DS-Questions,"When could it make your algorithms run slower?
  - Ask someone for more details.
  - compute in parallel when communication cost < computation cost
    - ensemble trees
    - minibatch
    - cross validation
    - forward propagation
    - minibatch
    - not suitable for online learning",2025-11-18T16:04:43.448565
What are the different types of joins? What are the differences between them?,,medium,coding,programming,kojino/120-DS-Questions,"- (INNER) JOIN: Returns records that have matching values in both tables
    LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table
    RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table
    FULL (OUTER) JOIN: Return all records when there is a match in either left or right table",2025-11-18T16:04:43.448575
Why might a join on a subquery be slow? How might you speed it up?,,medium,coding,programming,kojino/120-DS-Questions,- Change the subquery to a join.,2025-11-18T16:04:43.448580
Describe the difference between primary keys and foreign keys in a SQL database.,,medium,coding,programming,kojino/120-DS-Questions,- Primary keys are columns whose value combinations must be unique in a specific table so that each row can be referenced uniquely. Foreign keys are columns that references columns (often primary keys) in other tables.,2025-11-18T16:04:43.448587
"Given a COURSES table with columns course_id and course_name, a FACULTY table with columns faculty_id and faculty_name, and a COURSE_FACULTY table with columns faculty_id and course_id, how would you return a list of faculty who teach a course given the name of a course?",,medium,coding,programming,kojino/120-DS-Questions,- select faculty_name from faculty_id c join (select faculty_id from (select course_id from COURSES where course_name=xxx) as a join COURSE_FACULTY b on a.course_id = b.course_id) d on c.faculty_id = d.faculty_id,2025-11-18T16:04:43.448601
"Given a IMPRESSIONS table with ad_id, click (an indicator that the ad was clicked), and date, write a SQL query that will tell me the click-through-rate of each ad by month.",,medium,coding,programming,kojino/120-DS-Questions,"- select id, average(click) from (select count(click) as click from IMPRESSIONS group by id,month(date)) group by id",2025-11-18T16:04:43.448610
Write a query that returns the name of each department and a count of the number of employees in each:,,medium,coding,programming,kojino/120-DS-Questions,"EMPLOYEES containing: Emp_ID (Primary key) and Emp_Name  
EMPLOYEE_DEPT containing: Emp_ID (Foreign key) and Dept_ID (Foreign key)  
DEPTS containing: Dept_ID (Primary key) and Dept_Name
  - select Dept_Name, count(1) from DEPTS a right join EMPLOYEE_DEPT b on a.Dept_id = b.Dept_id group by Dept_Name",2025-11-18T16:04:43.448620
"In an A/B test, how can you check if assignment to the various buckets was truly random?",,hard,stats,statistical_inference,kojino/120-DS-Questions,"- Plot the distributions of multiple features for both A and B and make sure that they have the same shape. More rigorously, we can conduct a permutation test to see if the distributions are the same.
  - MANOVA to compare different means",2025-11-18T16:04:43.448663
"What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?",,hard,stats,statistical_inference,kojino/120-DS-Questions,- Verify the sampling algorithm is random.,2025-11-18T16:04:43.448670
What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups.",2025-11-18T16:04:43.448678
What would be some issues if blogs decide to cover one of your experimental groups?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- Same as the previous question. The above problem can happen in larger¬†scale.,2025-11-18T16:04:43.448684
How would you conduct an A/B test on an opt-in feature?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- Ask someone for more details.,2025-11-18T16:04:43.448690
"How would you run an A/B test for many variants, say 20 or more?",,hard,stats,statistical_inference,kojino/120-DS-Questions,"- one control, 20 treatment, if the sample size for each group is big enough.
  - Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD).",2025-11-18T16:04:43.448698
How would you run an A/B test if the observations are extremely right-skewed?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- lower the variability by modifying the KPI
  - cap values
  - percentile metrics
  - log transform
  - <https://www.quora.com/How-would-you-run-an-A-B-test-if-the-observations-are-extremely-right-skewed>",2025-11-18T16:04:43.448706
I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?,,hard,stats,statistical_inference,kojino/120-DS-Questions,- exclusive -> ok,2025-11-18T16:04:43.448713
What is a p-value? What is the difference between type-1 and type-2 error?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- A p-value is defined such that under the null hypothesis less than the fraction p of events have parameter values more extreme than the observed parameter. It is not the probability that the null hypothesis is wrong. 
  - type-1 error: rejecting Ho when Ho is true
  - type-2 error: not rejecting Ho when Ha is true",2025-11-18T16:04:43.448722
You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups.
  - Ask someone for more details.",2025-11-18T16:04:43.448733
How would you design an experiment to determine the impact of latency on user engagement?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, i.e., add a delay in an A/B test.",2025-11-18T16:04:43.448740
What is maximum likelihood estimation? Could there be any case where it doesn‚Äôt exist?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model).
  - maximum likelihood estimation¬†(MLE) is a method of¬†[estimating](https://en.wikipedia.org/wiki/Estimator ""Estimator"")¬†the¬†[parameters](https://en.wikipedia.org/wiki/Statistical_parameter ""Statistical parameter"")¬†of a¬†[statistical model](https://en.wikipedia.org/wiki/Statistical_model ""Statistical mod",2025-11-18T16:04:43.448765
"What‚Äôs the difference between a MAP, MOM, MLE estima\- tor? In which cases would you want to use each?",,hard,stats,statistical_inference,kojino/120-DS-Questions,"- MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution.
  - MOM sets moment values and solves for the parameters. MOM is not used much anymore because¬†maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.",2025-11-18T16:04:43.448777
What is a confidence interval and how do you interpret it?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- For example, 95% confidence interval is an interval that when constructed for a set of samples each sampled in the same way, the constructed intervals include the true mean 95% of the time.
  - if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.
  - [confidence intervals refresher from khanacademy](https://www.khana",2025-11-18T16:04:43.448794
What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However, this is not always desirable for data analysis or predictive modeling as there is the bias variance tradeoff. We sometimes want to prioritize the generalizability and avoid overfitting by reducing variance and thus increasing bias.",2025-11-18T16:04:43.448808
What is Selection Bias?,,hard,stats,statistical_inference,kojino/120-DS-Questions,"- Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn‚Äôt random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.
  - The types of selection bias include:
  - Sampling ",2025-11-18T16:04:43.448829
[Paste question here],,easy/medium/hard,coding/stats/ml/case/behavioral,,manual_templates/leetcode_manual_template.txt,,2025-11-17T11:27:21.133135
[Question title/description] [Full question text if available],"[Company Name or ""General""]",easy/medium/hard,sql/python,,manual_templates/stratascratch_manual_template.txt,,2025-11-17T11:27:21.133212
"Count the number of movies per genre Write a query to find the number of movies in each genre. Return the genre name and count, ordered by count descending.",Netflix,medium,sql,,manual_templates/stratascratch_manual_template.txt,,2025-11-17T11:27:21.133229
You are given a train data set having 1000 columns and 1 million rows based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133308
You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133323
You are given a data set on cancer detection. You've built a classification model and achieved an accuracy of 96%. Why shouldn't you be happy with your model performance? What can you do about it?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133338
You are working on a time series data set. You built a decision tree model but later tried a time series regression model and got higher accuracy. Can this happen? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133352
"You are assigned a new project helping a food delivery company save money. The company's delivery team can't deliver food on time, so customers get unhappy and receive free food. Which machine learning algorithm can save them?",,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133368
You came to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133378
"You are given a data set with many variables, some highly correlated. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?",,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133390
"After spending several hours, you built 5 GBM models thinking boosting would do magic. Unfortunately, none performed better than benchmark. You decided to combine those models but ensembled models didn't improve accuracy. Where did you miss?",,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133406
You have built a multiple regression model. Your model R¬≤ isn't as good as you wanted. You remove the intercept term and model R¬≤ becomes 0.8 from 0.3. Is it possible? How?,,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133420
"Your manager informed that your regression model is suffering from multicollinearity. How would you check if he's true? Without losing information, can you still build a better model?",,hard,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133434
"Is rotation necessary in PCA? If yes, why? What will happen if you don't rotate the components?",,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133449
"Why is Naive Bayes so 'naive'? Explain prior probability, likelihood and marginal likelihood in context of Naive Bayes algorithm.",,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133459
When is Ridge regression favorable over Lasso regression?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133465
"Both being tree based algorithms, how is random forest different from Gradient boosting algorithm (GBM)?",,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133474
You've built a random forest model with 10000 trees. Training error is 0.00 but validation error is 34.23. What is going on? Haven't you trained your model perfectly?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133486
You've got a data set where p (no. of variables) > n (no. of observations). Why is OLS a bad option? Which techniques would be best to use? Why?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133498
What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?,,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133506
"You are given a data set consisting of variables having more than 30% missing values. Out of 50 variables, 8 have missing values higher than 30%. How will you deal with them?",,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133519
How is kNN different from k-means clustering?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133530
How is True Positive Rate and Recall related? Write the equation.,,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133536
What is the difference between covariance and correlation?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133542
"Is it possible to capture the correlation between continuous and categorical variable? If yes, how?",,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133551
Running a binary classification tree algorithm is easy. But how does tree splitting take place? How does the tree decide which variable to split at the root node and succeeding nodes?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133566
"We know that one hot encoding increases the dimensionality of a data set, but label encoding doesn't. How?",,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133575
What do you understand by Type I vs Type II error?,,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133582
What is convex hull? (Hint: Think SVM),,medium,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133587
"'People who bought this, also bought‚Ä¶' recommendations seen on Amazon is a result of which algorithm?",,medium,case,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133602
Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?,,easy,stats,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133619
"While working on a data set, how do you select important variables? Explain your methods.",,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133631
What cross validation technique would you use on a time series data set? Is it k-fold or LOOCV? Explain the forward chaining strategy.,,medium,ml,,manual_templates/manual_scenario_questions.txt,,2025-11-17T11:27:21.133641
