question_text,company,difficulty,question_type,topics,source,answer_text,created_at
What is Machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"Machine learning is a branch of computer science which deals with system programming to automatically learn and improve with experience. For example, Robots are programmed so that they can perform the task based on data they gather from sensors. It automatically learns programs from data.",2025-11-18T17:20:54.000373
Mention the difference between Data Mining and Machine learning?,,hard,ml,machine_learning,165_ML_Interview_QA,"Machine learning relates to the study, design, and development of the algorithms that give computers the capability to learn without being explicitly programmed. While data mining can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. During this processing machine, learning algorithms are used.",2025-11-18T17:20:54.000398
What is ‘Overfitting’ in Machine learning?,,hard,ml,machine_learning,165_ML_Interview_QA,"In machine learning, when a statistical model describes random error or noise instead of underlying relationship ‘overfitting’ occurs. When a model is excessively complex, overfitting is normally observed, because of having too many parameters concerning the number of training data types. The model exhibits poor performance which has been overfitted.",2025-11-18T17:20:54.000416
Why does overfitting happen?,,medium,ml,machine_learning,165_ML_Interview_QA,The possibility of overfitting exists as the criteria used for training the model is not the same as the criteria used to judge the efficacy of a model.,2025-11-18T17:20:54.000424
How can you avoid overfitting ?,,hard,ml,machine_learning,165_ML_Interview_QA,"By using a lot of data overfitting can be avoided, overfitting happens relatively as you have a small dataset, and you try to learn from it. But if you have a small database and you are forced to come with a model based on that. In such a situation, you can use a technique known as cross-validation. We can use data augmentation techniques or regularization. We can also try to reduce the futures or we can try to simplify our model structure.",2025-11-18T17:20:54.000443
What is inductive machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The inductive machine learning involves the process of learning by examples, where a system, from a set of observed instances, tries to induce a general rule.",2025-11-18T17:20:54.000451
What are the five popular algorithms of Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Decision Trees Neural Networks (backpropagation) Probabilistic networks Nearest Neighbor Support vector machines,2025-11-18T17:20:54.000458
What are the different Algorithm techniques in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different types of techniques in Machine Learning are Supervised Learning Unsupervised Learning Semi-supervised Learning Reinforcement Learning Transduction Learning to Learn,2025-11-18T17:20:54.000467
What are the three stages to build the hypotheses or model in machine learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Model building Model testing Applying the model,2025-11-18T17:20:54.000472
What is the standard approach to supervised learning?,,easy,ml,machine_learning,165_ML_Interview_QA,The standard approach to supervised learning is to split the set of examples into the training set and the test.,2025-11-18T17:20:54.000479
What is ‘Training set’ and ‘Test set’?,,hard,ml,machine_learning,165_ML_Interview_QA,"In various areas of information science like machine learning, a set of data is used to discover the potentially predictive relationship known as ‘Training Set’. The training set is an example given to the learner, while the Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of examples held back from the learner. The training set is distinct from the Test set.",2025-11-18T17:20:54.000496
List down various approaches for machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different approaches in Machine Learning are Concept Vs Classification Learning Symbolic Vs Statistical Learning Inductive Vs Analytical Learning ** 13) What is not Machine Learning?** => Artificial Intelligence Rule-based inference,2025-11-18T17:20:54.000507
Explain what is the function of ‘Unsupervised Learning’?,,medium,ml,machine_learning,165_ML_Interview_QA,Find clusters of the data Find low-dimensional representations of the data Find interesting directions in data Interesting coordinates and correlations Find novel observations/ database cleaning,2025-11-18T17:20:54.000516
Explain what is the function of ‘Supervised Learning’?,,easy,ml,machine_learning,165_ML_Interview_QA,Classifications Speech recognition Regression Predict the time series Annotate strings,2025-11-18T17:20:54.000522
What is algorithm independent machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,Machine learning in where mathematical foundations are independent of any particular classifier or learning algorithm is referred to as algorithm independent machine learning?,2025-11-18T17:20:54.000531
What is the difference between artificial learning and machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"Designing and developing algorithms according to the behaviors based on empirical data are known as Machine Learning. While artificial intelligence in addition to machine learning, also covers other aspects like knowledge representation, natural language processing, planning, robotics, etc.",2025-11-18T17:20:54.000543
What is a classifier in machine learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"A classifier in Machine Learning is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class.",2025-11-18T17:20:54.000551
What are the advantages of Naive Bayes?,,medium,ml,machine_learning,165_ML_Interview_QA,"In Naïve Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. The main advantage is that it can’t learn the interactions between features.",2025-11-18T17:20:54.000561
In what areas Pattern Recognition is used?,,easy,ml,machine_learning,165_ML_Interview_QA,Pattern Recognition can be used in Computer Vision Speech Recognition Data Mining Statistics Informal Retrieval Bio-Informatics,2025-11-18T17:20:54.000568
What is Genetic Programming?,,medium,ml,machine_learning,165_ML_Interview_QA,Genetic programming is one of the two techniques used in machine learning. The model is based on testing and selecting the best choice among a set of results.,2025-11-18T17:20:54.000575
What is Inductive Logic Programming in Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Inductive Logic Programming (ILP) is a subfield of machine learning which uses logic programming representing background knowledge and examples.,2025-11-18T17:20:54.000582
What is Model Selection in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The process of selecting models among different mathematical models, which are used to describe the same data set is known as Model Selection. Model selection is applied to the fields of statistics, machine learning, and data mining.",2025-11-18T17:20:54.000593
What are the two methods used for the calibration in Supervised Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"The two methods used for predicting good probabilities in Supervised Learning are Platt Calibration Isotonic Regression These methods are designed for binary classification, and it is not trivial.",2025-11-18T17:20:54.000602
Which method is frequently used to prevent overfitting?,,easy,ml,machine_learning,165_ML_Interview_QA,When there is sufficient data ‘Isotonic Regression’ is used to prevent an overfitting issue.,2025-11-18T17:20:54.000609
What is the difference between heuristic for rule learning and heuristics for decision trees?,,medium,ml,machine_learning,165_ML_Interview_QA,The difference is that the heuristics for decision trees evaluate the average quality of many disjointed sets while rule learners only evaluate the quality of the set of instances that are covered with the candidate rule.,2025-11-18T17:20:54.000620
What is Perceptron in Machine Learning?,,easy,ml,machine_learning,165_ML_Interview_QA,"In Machine Learning, Perceptron is an algorithm for supervised classification of the input into one of several possible non-binary outputs.",2025-11-18T17:20:54.000627
Explain the two components of the Bayesian logic program?,,medium,ml,machine_learning,165_ML_Interview_QA,"Bayesian logic program consists of two components. The first component is a logical one; it consists of a set of Bayesian Clauses, which captures the qualitative structure of the domain. The second component is a quantitative one, it encodes the quantitative information about the domain.",2025-11-18T17:20:54.000639
What are Bayesian Networks (BN) ?,,easy,ml,machine_learning,165_ML_Interview_QA,Bayesian Network is used to represent the graphical model for the probability relationship among a set of variables.,2025-11-18T17:20:54.000645
Why instance-based learning algorithm sometimes referred to as a Lazy learning algorithm?,,medium,ml,machine_learning,165_ML_Interview_QA,Instance-based learning algorithm is also referred to as the Lazy learning algorithm as they delay the induction or generalization process until classification is performed.,2025-11-18T17:20:54.000654
What are the two classification methods that SVM ( Support Vector Machine) can handle?,,easy,ml,machine_learning,165_ML_Interview_QA,Combining binary classifiers Modifying binary to incorporate multiclass learning,2025-11-18T17:20:54.000661
What is ensemble learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"To solve a particular computational program, multiple models such as classifiers or experts are strategically generated and combined. This process is known as ensemble learning.",2025-11-18T17:20:54.000668
Why ensemble learning is used?,,easy,ml,machine_learning,165_ML_Interview_QA,"Ensemble learning is used to improve the classification, prediction, function approximation, etc of a model.",2025-11-18T17:20:54.000674
When to use ensemble learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Ensemble learning is used when you build component classifiers that are more accurate and independent from each other.,2025-11-18T17:20:54.000680
What are the two paradigms of ensemble methods?,,easy,ml,machine_learning,165_ML_Interview_QA,The two paradigms of ensemble methods are Sequential ensemble methods Parallel ensemble methods,2025-11-18T17:20:54.000686
What is the general principle of an ensemble method and what is bagging and boosting in the ensemble method?,,hard,ml,machine_learning,165_ML_Interview_QA,The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm to improve robustness over a single model. Bagging is a method in an ensemble for improving unstable estimation or classification schemes. While boosting methods are used sequentially to reduce the bias of the combined model. Boosting and Bagging both can reduce errors by reducing the variance term.,2025-11-18T17:20:54.000704
What is a bias-variance decomposition of classification error in the ensemble method?,,hard,ml,machine_learning,165_ML_Interview_QA,The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets.,2025-11-18T17:20:54.000719
What is an Incremental Learning algorithm in the ensemble?,,medium,ml,machine_learning,165_ML_Interview_QA,Incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from the already available dataset.,2025-11-18T17:20:54.000728
"What are PCA, KPCA, and ICA used for?",,medium,ml,machine_learning,165_ML_Interview_QA,"PCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis), and ICA ( Independent Component Analysis) are important feature extraction techniques used for dimensionality reduction.",2025-11-18T17:20:54.000737
What is dimension reduction in Machine Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,"In Machine Learning and statistics, dimension reduction is the process of reducing the number of random variables under consideration and can be divided into feature selection and feature extraction.",2025-11-18T17:20:54.000746
What are support vector machines?,,easy,ml,machine_learning,165_ML_Interview_QA,Support vector machines are supervised learning algorithms used for classification and regression analysis.,2025-11-18T17:20:54.000752
What are the components of relational evaluation techniques?,,medium,ml,machine_learning,165_ML_Interview_QA,The important components of relational evaluation techniques are Data Acquisition Ground Truth Acquisition Cross-Validation Technique Query Type Scoring Metric Significance Test,2025-11-18T17:20:54.000760
What are the different methods for Sequential Supervised Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,The different methods to solve Sequential Supervised Learning problems are Sliding-window methods Recurrent sliding windows Hidden Markow models Maximum entropy Markow models Conditional random fields Graph transformer networks,2025-11-18T17:20:54.000770
What are the areas in robotics and information processing where sequential prediction problem arises?,,medium,ml,machine_learning,165_ML_Interview_QA,The areas in robotics and information processing where sequential prediction problem arises are: Imitation Learning Structured prediction Model-based reinforcement learning,2025-11-18T17:20:54.000779
What is batch statistical learning?,,hard,ml,machine_learning,165_ML_Interview_QA,Statistical learning techniques allow learning a function or predictor from a set of observed data that can make predictions about unseen or future data. These techniques provide guarantees on the performance of the learned predictor on the future unseen data based on a statistical assumption on the data generating process.,2025-11-18T17:20:54.000792
What is PAC Learning?,,medium,ml,machine_learning,165_ML_Interview_QA,PAC (Probably Approximately Correct) learning is a learning framework that has been introduced to analyze learning algorithms and their statistical efficiency.,2025-11-18T17:20:54.000799
What are the different categories you can categorize the sequence learning process?,,easy,ml,machine_learning,165_ML_Interview_QA,Sequence prediction Sequence generation Sequence recognition Sequential decision,2025-11-18T17:20:54.000805
What is sequence learning?,,easy,ml,machine_learning,165_ML_Interview_QA,Sequence learning is a method of teaching and learning in a logical manner.,2025-11-18T17:20:54.000809
What are two techniques of Machine Learning ?,,hard,ml,machine_learning,165_ML_Interview_QA,The two techniques of Machine Learning are Genetic Programming Inductive Learning Here is one more explanation of Overfitting and Underfitting for you. https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171229/overfitting_1.png Overfitting – High variance and low bias Techniques to reduce overfitting: Increase the training data. Reduce model complexity. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop,2025-11-18T17:20:54.000899
What is SQL?,,medium,sql,sql_database,SQL_Interview_QA,SQL is Structured Query Language designed specifically for communicating with databases (for inserting and modifying in a relational database management system). SQL is an ANSI (American National Standards Institute) standard.,2025-11-18T17:20:54.032680
What are the Advantages of SQL?,,medium,sql,sql_database,SQL_Interview_QA,"SQL is not a proprietary language used by specific database vendors. Almost every major DBMS supports SQL, so learning this one language will enable programmers to interact with any database like ORACLE, SQL, MYSQL etc. SQL is easy to learn. The statements are all made up of descriptive English words, and there aren't that many of them. SQL is actually an immensely powerful language and by using its language elements you can perform very complex and sophisticated database operations",2025-11-18T17:20:54.032715
What is a field in a database?,,easy,sql,sql_database,SQL_Interview_QA,"A field is an area within a record reserved for a specific piece of data. Examples: Employee Name, Employee ID etc.",2025-11-18T17:20:54.032738
What is a Record in a database?,,easy,sql,sql_database,SQL_Interview_QA,"A record is the collection of values / fields of a specific entity: i.e. Employee, Salary etc.",2025-11-18T17:20:54.032757
What is a Table in a database?,,easy,sql,sql_database,SQL_Interview_QA,"A table is a collection of records of a specific type. For example, employee table, salary table etc.",2025-11-18T17:20:54.032766
What is a database transaction?,,hard,sql,sql_database,SQL_Interview_QA,"A database transaction takes the database from one consistent state to another. At the end of the transaction the system must be in the prior state if transaction fails or the status of the system should reflect the successful completion if the transaction goes through. A Database Transaction is a set of database operations that must be treated as whole, meaning either all operations are executed or none of them. An example can be a bank transaction from one account to another account. Either bo",2025-11-18T17:20:54.032793
What are the properties of a transaction?,,hard,sql,sql_database,SQL_Interview_QA,"Properties of the transaction can be summarized as ACID Properties. Atomicity A transaction consists of many steps. When all the steps in a transaction are completed, it will get reflected in DB or if any step fails, all the transactions are rolled back. Consistency The database will move from one consistent state to another, if the transaction succeeds and remain in the original state if the transaction fails. Isolation Every transaction should operate as if it is the only transaction in the sy",2025-11-18T17:20:54.032819
What is SQL Order of Execution?,,medium,sql,sql_database,SQL_Interview_QA,About 90% of data analysts will get the SQL order of execution WRONG The correct order is: FROM – Define which tables you’ll source data from WHERE – Apply filters to your base data GROUP BY – Aggregate your data HAVING – Filter the aggregated data SELECT – Display the final data ORDER BY – Sort data for easy viewing LIMIT – Restrict the number of results Why is SELECT not first in the order of execution you ask? Because it is different from the order of writing! Our machines FIRST look for the ,2025-11-18T17:20:54.032848
What is the difference between having and where clause?,,medium,sql,sql_database,SQL_Interview_QA,"Ans: HAVING is used to specify a condition for a group or an aggregate function used in select statement. The WHEREclause selects before grouping. The HAVINGclause selects rows after grouping. Unlike HAVINGclause, the WHEREclause cannot contain aggregate functions.",2025-11-18T17:20:54.032861
What is Join?,,medium,sql,sql_database,SQL_Interview_QA,"Ans: An SQL Join is used to combine data from two or more tables, based on a common field between them. For example, consider the following two tables. Student Table EnrollNo StudentName Address 1000 geek1 geeksquiz1 1001 geek2 geeksquiz2 1002 geek3 geeksquiz3 StudentCourse Table CourseID EnrollNo 1 1000 2 1000 3 1000 1 1002 2 1003 The following is a join query that shows names of students enrolled in different courseIDs. SELECT StudentCourse.CourseID, Student.StudentName FROM StudentCourse INNE",2025-11-18T17:20:54.032903
What is a view in SQL? How to create one,,easy,sql,sql_database,SQL_Interview_QA,Ans: A view is a virtual table based on the result-set of an SQL statement. We can create using create view syntax. CREATE VIEW view_name AS SELECT column_name(s) FROM table_name WHERE condition,2025-11-18T17:20:54.032913
What are the uses of a view? (Level: Intermediate),,medium,sql,sql_database,SQL_Interview_QA,"Views can represent a subset of the data contained in a table; consequently, a view can limit the degree of exposure of the underlying tables to the outer world: a given user may have permission to query the view, while denied access to the rest of the base table. Views can join and simplify multiple tables into a single virtual table. Views can act as aggregated tables, where the database engine aggregates data (sum, average etc.) and presents the calculated results as part of the data Views ca",2025-11-18T17:20:54.032941
What are Primary Keys and Foreign Keys? (Level: Beginner),,medium,sql,sql_database,SQL_Interview_QA,"Ans: Primary keys are the unique identifiers for each row. They must contain unique values and cannot be null. Due to their importance in relational databases, Primary keys are the most fundamental aspect of all keys and constraints. A table can have only one primary key. Foreign keys are a method of ensuring data integrity and manifestation of the relationship between tables.",2025-11-18T17:20:54.032958
What are the different types of SQL or different commands in SQL?,,medium,sql,sql_database,SQL_Interview_QA,"DDL– Data Definition Language. DDL is used to define the structure that holds the data. DML– Data Manipulation Language. DML is used for manipulation of the data itself. Typical operations are Insert, Delete, Update and retrieving the data from the table. DCL–Data Control Language. DCL is used to control the visibility of data like granting database access and set privileges to create tables etc. TCL -Transaction Control Language. TCL commands are basically used for managing and controlling the ",2025-11-18T17:20:54.032986
In SQL interviews you will often be asked what constraint commands are,,medium,sql,sql_database,SQL_Interview_QA,Here's the short answer for you: Constraints are rules that limit the types of data that can go into a table. The most important commands are: NOT NULL - Ensures that a column cannot have a NULL value UNIQUE - Ensures that all values in a column are different PRIMARY KEY - A combination of a NOT NULL and UNIQUE. FOREIGN KEY - Prevents actions that would destroy links between tables CHECK - Ensures that the values in a column satisfies a specific condition DEFAULT - Sets a default value for a col,2025-11-18T17:20:54.033013
"What is a foreign key, and what is it used for?",,medium,sql,sql_database,SQL_Interview_QA,"A foreign key is used to establish relationships among relations (tables) in the relational model. Technically, a foreign key is a column (or columns) appearing in one relation that is (are) the primary key of another table. Although there may be exceptions, the values in the foreign key columns usually must correspond to values existing in the set of primary key values. This correspondence requirement is created in a database using a referential integrity constraint on the foreign key.",2025-11-18T17:20:54.033033
What is Aggregate Functions?,,medium,sql,sql_database,SQL_Interview_QA,"Aggregate functions perform a calculation on a set of values and return a single value. Aggregate functions ignore NULL values except COUNT function. HAVING clause is used, along with GROUP BY, for filtering query using aggregate values. Following functions are aggregate functions. AVG, MIN, CHECKSUM_AGG, SUM, COUNT, STDEV, COUNT_BIG, STDEVP, GROUPING, VAR, MAX. VARP",2025-11-18T17:20:54.033049
What is CTE?,,medium,sql,sql_database,SQL_Interview_QA,CTE is an abbreviation Common Table Expression. A Common Table Expression (CTE) is an expression that can be thought of as a temporary result set which is defined within the execution of a single SQL statement. A CTE is similar to a derived table in that it is not stored as an object and lasts only for the duration of the query.,2025-11-18T17:20:54.033062
What are the Advantages of using CTE?,,medium,sql,sql_database,SQL_Interview_QA,"Using CTE improves readability and makes maintenance of complex queries easy. The query can be divided into separate, simple, logical building blocks which can be then used to build more complex CTEs until final result set is generated. CTE can be defined in functions, stored procedures, triggers or even views. After a CTE is defined, it can be used as a Table or a View and can SELECT, INSERT, UPDATE or DELETE Data.",2025-11-18T17:20:54.033080
What is PRIMARY KEY?,,medium,sql,sql_database,SQL_Interview_QA,A PRIMARY KEY constraint is a unique identifier for a row within a database table. Every table should have a primary key constraint to uniquely identify each row and only one primary key constraint can be created for each table. The primary key constraints are used to enforce entity integrity.,2025-11-18T17:20:54.033093
What is UNIQUE KEY constraint?,,medium,sql,sql_database,SQL_Interview_QA,"A UNIQUE constraint enforces the uniqueness of the values in a set of columns, so no duplicate values are entered. The unique key constraints are used to enforce entity integrity as the primary key constraints.",2025-11-18T17:20:54.033103
What is FOREIGN KEY?,,medium,sql,sql_database,SQL_Interview_QA,A FOREIGN KEY constraint prevents any actions that would destroy links between tables with the corresponding data values. A foreign key in one table points to a primary key in another table. Foreign keys prevent actions that would leave rows with foreign key values when there are no primary keys with that value. The foreign key constraints are used to enforce referential integrity.,2025-11-18T17:20:54.033118
What is CHECK Constraint?,,easy,sql,sql_database,SQL_Interview_QA,A CHECK constraint is used to limit the values that can be placed in a column. The check constraints are used to enforce domain integrity.,2025-11-18T17:20:54.033125
What is NOT NULL Constraint?,,easy,sql,sql_database,SQL_Interview_QA,"A NOT NULL constraint enforces that the column will not accept null values. The not null constraints are used to enforce domain integrity, as the check constraints.",2025-11-18T17:20:54.033134
"Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success?",,hard,case,data_science,DS_Interview_Notebook,"* In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction. * One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data! * First step will be to do EDA and understand our data and intesity of class inbalance. * In order to handle inbalance data prob",2025-11-18T17:20:54.033738
Explain inner working on linear regression,,easy,ml,data_science,DS_Interview_Notebook,,2025-11-18T17:20:54.033749
What are the assumptions for linear regression,,hard,ml,data_science,DS_Interview_Notebook,"Linear regression assumptions are as below * Data should have linear relationship between X and Y (actually mean of Y) * Data should be normally distributed * No or little multicollinearity (observations should be independent of each other) * Assumption of additivity: This means that each feature (X) should affect the target (Y) independently. In other words, the influence of one feature on the target does not change because of another feature. * Example: Let's say you're predicting house prices",2025-11-18T17:20:54.033787
Explain inner working on logistic regression,,easy,ml,data_science,DS_Interview_Notebook,,2025-11-18T17:20:54.033793
How can AI be used in spam email detection?,,hard,mixed,data_science,DS_Interview_Notebook,"AI, particularly through NLP techniques, analyzes the content of emails to determine if they are spam. It does this by identifying patterns, keywords, and stylistic choices that are common in spam messages. Here's a simplified breakdown of the process: **Steps in AI-Based Spam Detection** 1. **Data Collection:** * A large dataset of emails, both spam and legitimate (""ham""), is gathered. 2. **Preprocessing:** * Emails are cleaned and standardized. * This often involves: * Removing punctuation and",2025-11-18T17:20:54.033948
How to build sentiment analysis model from scratch?,,hard,ml,data_science,DS_Interview_Notebook,"**1. Data Gathering:** * Collect a set of text data (e.g., movie reviews, tweets, product feedback) where each piece of text is labeled with its corresponding sentiment (positive, negative, or neutral). **2. Text Preprocessing:** * Clean and standardize the text: * Remove punctuation, special characters, and HTML tags. * Convert all text to lowercase. * Remove stop words (common words like 'the', 'and', etc.) * Tokenize the text into individual words. * Consider stemming or lemmatization to redu",2025-11-18T17:20:54.034003
When to use tokenization and stemming/Lemmatization?,,hard,mixed,data_science,DS_Interview_Notebook,"* **Tokenization:** * **Always** use tokenization as the first step in any NLP task that involves analyzing the text at the word level. * It's fundamental for breaking down sentences into individual words or subwords, which is necessary for further processing and analysis. * **Stemming/Lemmatization:** * Use these when you want to reduce words to their base or root forms. This can be helpful for: * **Reducing dimensionality:** Fewer unique words to deal with. * **Improving generalization:** Mode",2025-11-18T17:20:54.034056
What are the advantages and disadvantages of neural networks?,,hard,ml,data_science,DS_Interview_Notebook,"**Here are some advantages of Neural Networks** * Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. * The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missi",2025-11-18T17:20:54.034124
What is the difference between bias and variance?,,hard,mixed,data_science,DS_Interview_Notebook,"* Bias comes from model underfitting some set of data, whereas variance is the result of model overfitting some set of data. * Underfitting models have high error in training as well as test set. This behavior is called as ‘High Bias’ * Consider below example of bias(underfitting) where we are trying to fit linear function for nonlinear data. ![Underfitting]( * Overfitting models have low error in training set but high error in test set. This behavior is called as ‘High Variance’ * Consider belo",2025-11-18T17:20:54.034160
What is bias-variance tradeoff,,hard,mixed,data_science,DS_Interview_Notebook,"* As we increase the complexity of the model, error will reduce due to lower bias in the model. However, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance. * The goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is referred as bias-variance tradeoff. We can acheive bias-variance tradeoff by selecting optimum model complexity. <img src="" width=""500"" ",2025-11-18T17:20:54.034226
What is more important model accuracy or model performance?,,hard,ml,data_science,DS_Interview_Notebook,"* Short answer is: Model accuracy matters the most! inaccurate information is not usefull. * Model performance can be improved by increasing the compute resources. * Model accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction. * Some applications require real time performance, even if this",2025-11-18T17:20:54.034258
What is the difference between machine learning and deep learning?,,hard,ml,data_science,DS_Interview_Notebook,"Deep Learning out performs traditional ML techniques if the data size is large. But with small data size, traditional Machine Learning algorithms are preferable. Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition. Few important differences are as below, |Machine Learning|Deep Learning| |:-|:-| | Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on wha",2025-11-18T17:20:54.034302
Explain standard deviation and variance,,hard,mixed,data_science,DS_Interview_Notebook,"![normal-distrubution]( **Variance:** * **In a Nutshell:** Variance measures how spread out a set of data is. A high variance indicates that the data points are widely scattered, while a low variance means they are clustered close together. * **Example:** Imagine two groups of students taking a test. * Group A: Scores are 60, 70, 75, 80, 90 * Group B: Scores are 40, 70, 70, 70, 100 * Group B has a higher variance because its scores are more spread out from the average (70) compared to Group A. *",2025-11-18T17:20:54.034366
Explain confusion matrix,,hard,mixed,data_science,DS_Interview_Notebook,"![]( **What is a Confusion Matrix?** * Imagine a table that helps you see how well your machine learning model is performing, especially when it comes to classification tasks (like deciding if an email is spam or not). * It compares the model's predictions to the actual, true labels of your data. **Why is it useful?** * Beyond Accuracy: A confusion matrix gives you a more detailed look at your model's performance than just overall accuracy. * Spotting Weaknesses: You can see which classes your m",2025-11-18T17:20:54.034499
Why do we need confusion matrix?,,hard,mixed,data_science,DS_Interview_Notebook,"* We can not rely on a single value of accuracy in classification when the classes are imbalanced. * For example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%. * Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. * Confusion matrices are useful becaus",2025-11-18T17:20:54.034521
Explain collinearity and technique to reduce it?,,hard,mixed,data_science,DS_Interview_Notebook,"In statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other. ## Technique to reduce multicollearity * **Remove highly correlated predictors from the model**. If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared",2025-11-18T17:20:54.034545
Difference between statistics and machine learning,,hard,ml,data_science,DS_Interview_Notebook,"* The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables. * Statistics is mathematical study of data. Lots of statistical models that can make predictions, but predictive accuracy is not their strength.",2025-11-18T17:20:54.034558
"In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?",,hard,stats,data_science,DS_Interview_Notebook,"To compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. ```Z score= (X- mean)/Standard Deviation``` Melissa's Z score = (90-75)/10 = 1.5 Ryan's Z score = (90-80)/12 = 0.83 Melissa has performed better.",2025-11-18T17:20:54.034581
What is null hypothesis and alternate hypothesis?,,hard,stats,data_science,DS_Interview_Notebook,"* The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge. * The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true. * So when r",2025-11-18T17:20:54.034604
What is a hypothesis test and p-value?,,hard,stats,data_science,DS_Interview_Notebook,"* A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of ""no effect"" or ""no difference"". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data. * Based on the sample data, the test determines whether to reject the null hypothesis. You use a p-value, to make t",2025-11-18T17:20:54.034660
What is power of hypothesis test? Why is it important?,,hard,stats,data_science,DS_Interview_Notebook,"* Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc. * The probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is.",2025-11-18T17:20:54.034675
What is the difference betweeen K nearest neighbors and K means,,hard,mixed,data_science,DS_Interview_Notebook,"* KNN or K nearest neighbor is a classification algorithm, while K-Means is clustering technique. * KNN is supervised algorithm, K means is unsupervised algorithm. * In KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K referes to the number of neighbors with whom similarity is being compared. * K-means is the process of defining clusters or groups around predefined cent",2025-11-18T17:20:54.034696
Explain Random forest algorithm,,hard,ml,data_science,DS_Interview_Notebook,"* Random forest is supervised learning algorithm and can be used to solve classification and regression problems. * Since decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. * Averaging the results from multiple decision trees help to control the overfitting and result",2025-11-18T17:20:54.034723
Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?,,medium,ml,data_science,DS_Interview_Notebook,"* Yes, Random Forest can be used for both continuous and categorical target (dependent) variables. * In a random forest the classification model refers to the categorical dependent variable, and the regression model refers to the numeric or continuous dependent variable.",2025-11-18T17:20:54.034734
What do you mean by Bagging?,,hard,mixed,data_science,DS_Interview_Notebook,"![EnsembleI_Learning_Bagging]( * In bagging we build independent estimators on different samples of the original data set and average or vote across all the predictions. * Bagging is a short form of **Bootstrap Aggregating**. It is an ensemble learning approach used to improve the stability and accuracy of machine learning algorithms. * Since multiple model predictions are averaged together to form the final predictions, Bagging reduces variance and helps to avoid overfitting. Although it is usu",2025-11-18T17:20:54.034798
What is Out-of-Bag Error in Random Forests?,,hard,mixed,data_science,DS_Interview_Notebook,"* Out-of-Bag is equivalent to validation or test data but it is calculated internally by Random Forest algorithm. In case of Sklearn if we set hyperparameter 'oob_score = True' then Out-of-Bag score will be calculated for every decision tree. * Finally, we aggregate all the errors from all the decision trees and we will determine the overall OOB error rate for the classification. * For more details refer. ",2025-11-18T17:20:54.034816
What is the use of proximity matrix in the random forest algorithm?,,easy,ml,data_science,DS_Interview_Notebook,A proximity matrix is used for the following cases : * Missing value imputation * Detection of outliers,2025-11-18T17:20:54.034823
List down the parameters used to fine-tune the Random Forest.,,medium,mixed,data_science,DS_Interview_Notebook,Two parameters that have to fine-tune to improve the predictions that are important in the random forest algorithm are as follows: * Number of trees used in the forest (n_tree) * Number of random variables used in each of the trees in the forest (mtry),2025-11-18T17:20:54.034836
What is K Fold cross validation? Why do you use it?,,hard,mixed,data_science,DS_Interview_Notebook,"* In case of K Fold cross validation input data is divided into ‘K’ number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method. * This significantly reduce",2025-11-18T17:20:54.034865
How to handle missing data?,,medium,case,data_science,DS_Interview_Notebook,"Data can be missing because of mannual error or can be gennualy missing. * Delete low quality records completely which have too much missing data * Impute the values by educated guess, taking average or regression * Use domain knwledge to impute values",2025-11-18T17:20:54.034877
What is the difference between Bar graph and histogram?,,medium,mixed,data_science,DS_Interview_Notebook,* Bar graph is used for descreate data where as histogram is used for continuous data. * In bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale). * In bar graph the order of the bars can be changed and in histogram order remains same.,2025-11-18T17:20:54.034890
What is the Box and Whisker plot? When should use it?,,hard,mixed,data_science,DS_Interview_Notebook,"* Box and whisker plots are ideal for comparing distributions because the centre, spread and overall range are immediately apparent. * A box and whisker plot is a way of summarizing a set of data measured on an interval scale. * It is often used in explanatory data analysis * Boxplots are a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). * median (Q2/50th Percentile): the middle v",2025-11-18T17:20:54.034926
What is outlier? How to handle them?,,hard,mixed,data_science,DS_Interview_Notebook,"* An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. * Data points above and below 1.5*IQR, are most commonly outliers. Outliers can drastically change the results of the data analysis and statistical modeling. ## Types of the outliers * **Data entry errors** * **Measuremental errors** * **Intentional outliers**. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under ",2025-11-18T17:20:54.035002
"If deleting outliers is not an option, how will you handle them?",,hard,mixed,data_science,DS_Interview_Notebook,"* I will try differen models. Data detected as outliers by linear model, can be fit by non-linear model. * Try normalizing the data, this way the extreame datapoints are pulled to the similar range. * We can use algorithms which are less affected by outliers. * We can also create separate model to handle the outlier data points.",2025-11-18T17:20:54.035017
You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?,,hard,ml,data_science,DS_Interview_Notebook,"* First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictors * Here important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance. * Evaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE. * MSE penalizes large errors, ",2025-11-18T17:20:54.035064
Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.,,hard,coding,data_science,DS_Interview_Notebook,"* This is quadratic equation, f(x) = -4x^2 + 4x + 13 (for a function: ax^2 + bx + c, when a < 0, then function has maximum value) * To find the slope of the function, lets take derivative of it f'(x)= -8x + 4 * At maximum point, slope will be 0 -8x + 4 = 0 x = 0.5 * Now lets put 0.5 in equation to find the maximum values f(0,5) = -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14 * This functiona will have concave shape. So the maximum point is (0.5, 14) * Reference: ",2025-11-18T17:20:54.035086
Below is the output of a correlation matrix from your Exploratory data. Is using all the features in a model appropriate for predicting/inferencing Y?,,hard,ml,data_science,DS_Interview_Notebook,"![]( * We can see from above correlation matrix that there is high correlation(.98) between X1 and X2, also high correlation(.88) between X1 and X3, similarly there is high correlation(.75) between X2 and X3 * All the variables are correlated to each other. In regression this would result in multicollinearity. We can try methods such as dimension reduction, feature selection, stepwise regression to choose the correct input variables for predictiong Y * Second part of question is - should we use ",2025-11-18T17:20:54.035114
What is stepwise regression?,,hard,ml,data_science,DS_Interview_Notebook,"Stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion. Stepwise regression is classified into backward and forward selection. * **Backward selection** starts with a full model, then step by step we reduce the regressor variables and find the model with the least",2025-11-18T17:20:54.035140
You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket.,,hard,mixed,data_science,DS_Interview_Notebook,"* Questions like this will test your out of the box thinking * Step1: Fill 5 lts bucket and empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket. * Step2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it. * Now fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it, so if we pour 1 ltr content from 5 ltr bucket to 3 ltr bucket we are left with 4 ltr content",2025-11-18T17:20:54.035166
Lis the differences between supervised and unsupervised learning,,hard,ml,data_science,DS_Interview_Notebook,"| Supervised learning | Unsupervised leanring |:- |:- Uses labeled data as input | Uses unlabeled data as input Supervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism Common supervised learning algorithms are decision tree, logistic regression, support vector machine etc | K Means clustering, hierarchical clustering etc Reference: ",2025-11-18T17:20:54.035180
Explain the steps in making decision tree?,,hard,mixed,data_science,DS_Interview_Notebook,![]( Below are the common steps in decision tree algorithm * Take the entire data as input * At the root node decision tree selects feature to split the data in two major categories. * Different criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems. * Features are selected for spliting based on highest information gain. * After every split we get decision rules and sub trees. * This process will c,2025-11-18T17:20:54.035207
How do you build random forest model?,,hard,ml,data_science,DS_Interview_Notebook,Ranodm forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. ![]( * Select few random sub sample from given dataset * Construct a decision tree for every sub sample and predict the result. * Perform the voting on prediction from each tree. * At the end select the most voted result as final prediction. * Reference: ,2025-11-18T17:20:54.035226
How do Random Forest handle missing data?,,hard,case,data_science,DS_Interview_Notebook,"Random Forests inherently have two primary ways of handling missing data: 1. **During Training (Building the Trees):** * **For Numerical Features:** Missing values can be imputed using simple strategies like mean or median. * **For Categorical Features:** A new ""missing"" category is often created to handle missing values. This ensures that data points with missing categorical values are still considered during the tree building process. 2. **During Prediction (Making New Predictions):** * **""Sur",2025-11-18T17:20:54.035302
What is model overfitting? How can you avoid it?,,hard,ml,data_science,DS_Interview_Notebook,"Overfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models. ![]( **How To Avoid Overfitting?** * Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually",2025-11-18T17:20:54.035337
There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?,,hard,mixed,data_science,DS_Interview_Notebook,"To find the heavier ball among 9 balls using a balance scale, you can determine the minimum number of weighings required by strategically dividing the balls and comparing their weights. ### Step-by-Step Solution: 1. **First Weighing**: - Divide the 9 balls into three groups of 3 balls each: Group A, Group B, and Group C. - Weigh Group A against Group B. 2. **Analyzing the First Weighing**: - **Case 1**: If the scales balance (i.e., Group A = Group B), it means the heavier ball is in Group C. - *",2025-11-18T17:20:54.035383
"Difference between univariate, bivariate and multivariate analysis?",,easy,mixed,data_science,DS_Interview_Notebook,* Univariate Analysis ![Univariate_Analysis]( * Bivariate Analysis ![Bivariate_Analysis]( * Multivariate Analysis ![Multivariate_Analysis](,2025-11-18T17:20:54.035397
What are feature selection methods to select right variables?,,hard,mixed,data_science,DS_Interview_Notebook,"Feature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out. ## Filter Methods * Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables",2025-11-18T17:20:54.035473
"In you choice of langauge: Write a program that prints the numbers from 1 to 50. But for multiples of three print ""Fizz"" instaed of the number and for the multiples of five print ""Buzz"". For the numbers which are multiples of both three and five print ""FizzBuzz"".",,easy,coding,data_science,DS_Interview_Notebook,,2025-11-18T17:20:54.035486
You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?,,hard,case,data_science,DS_Interview_Notebook,"* There are multiple ways to handle missing values in the data * If dataset is huge we can simply remove the rows containing the missing data * If dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc. * Reference: ",2025-11-18T17:20:54.035505
"For the given point how will you caluclate the Euclidean distance, in Python?",,easy,mixed,data_science,DS_Interview_Notebook,Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors. ![]( Reference: ,2025-11-18T17:20:54.035518
What is the angle between the hour and minute hands of clock when the time is half past six?,,easy,mixed,data_science,DS_Interview_Notebook,![Clock_Puzzle]( Reference: ,2025-11-18T17:20:54.035528
How should you maintain your deployed model?,,hard,ml,data_science,DS_Interview_Notebook,### Monitor Constant monitoring of all the models is needed to determine the performance accuracy of the models ### Evaluate Evaluation metric of the current model is calculated to determine if new algorithm is needed. ### Compare The new models are compared against each other to determine which model performs the best. ### Rebuild The best performing model is re-built on the current set of data. Reference: ,2025-11-18T17:20:54.035543
What are recommender systems?,,hard,mixed,data_science,DS_Interview_Notebook,* The purpose of a recommender system is to suggest relevant items or services to users. * Two major categories of recommender systems are collaboarative filtering and cotent based filtering methods ### Collaborative Filtering * It is based on the past interactions recorded between users and items in order to produce new recommendations. * e.g. Music service recommends track that are often played by other users with similar interests ### Content Based Filtering * Unlike collaborative methods tha,2025-11-18T17:20:54.035574
"'People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?",,medium,ml,data_science,DS_Interview_Notebook,* Its done by recommendation system using collaborative filtering approach. * In case of collaborative filtering past interactions recorded between users and items are used to produce new recommendations.,2025-11-18T17:20:54.035585
"If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?",,hard,stats,data_science,DS_Interview_Notebook,"* Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events. * Trick here is to know the probability of not raining on Saturday and Sunday. * If we subtract the intersection(∩) of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend. ``` = Total probability - (Probability that it will not rain on Saturday) ∩ (Probability that it will not rain ",2025-11-18T17:20:54.035610
How can you select K for K-Means?,,hard,mixed,data_science,DS_Interview_Notebook,"There are two ways to select the number of clusters in case K-Means clustering algorithm ### Visualization * To find the number of clusters manually by data visualization is one of the most common method. * Domain knowledge and proper understanding of given data also help to make more informed decisions. * Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use ‘Elbow Method’ ### Elbow Method * In Elbow method we run the K-Means algorithm multi",2025-11-18T17:20:54.035647
"Explain dimensionality reduction, and its benefits?",,hard,mixed,data_science,DS_Interview_Notebook,* Dimensionality reduction referes to the process of converting a set of data having vast dimensions into data with lesser dimensions(features) to convey similar information concisely. * It helps in data compressing and reducing the storage space * It reduces computation time as less dimensions lead to less computing * It removes redundant features. E.g. There is no point in storing value in two different units * Reference: ,2025-11-18T17:20:54.035665
How can you say that the time series data is stationary?,,hard,case,data_science,DS_Interview_Notebook,"For accurate analysis and forecasting, **trend and seasonality is removed** from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time. ![Stationarity]( Reference: ",2025-11-18T17:20:54.035684
How can you calculate the accuracy using confusion matrix?,,easy,mixed,data_science,DS_Interview_Notebook,Accuracy = (True Positive + true Negative) / Total Obervations,2025-11-18T17:20:54.035691
Write the equations for the precision and recall?,,easy,mixed,data_science,DS_Interview_Notebook,Precision = True Positive / (True Positive + False Positive) Recall = True Positive /(Total Positive + False Negative),2025-11-18T17:20:54.035700
"If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?",,hard,mixed,data_science,DS_Interview_Notebook,"* There are three colors of socks- Red, Blue and White. No of socks is irrelevant here. * Suppose in our first pull we picked Red color sock * In second pull we picked Blue color sock * And in third pull we picked White color sock. * Now in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4! * Reference: ",2025-11-18T17:20:54.035719
Write a SQL query to list all orders with customer information,,easy,mixed,data_science,DS_Interview_Notebook,![SQL_Join](,2025-11-18T17:20:54.035726
Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?,,medium,ml,data_science,DS_Interview_Notebook,``` - K-means clustering - Linear regression - K-NN - Decision tress ``` Using KNN we can compute the missing variable value by using the nearest neighbors.,2025-11-18T17:20:54.035736
"Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes? Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out",,medium,mixed,data_science,DS_Interview_Notebook,"* We have two ropes A and B * Ligt A from both the end and B from one end * When A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining * Now light the other end of B also, it will now burnout in 15 minutes * This we got 30 + 15 = 45 minutes * Reference: ",2025-11-18T17:20:54.035756
"After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?",,hard,ml,data_science,DS_Interview_Notebook,"The most appropriate algorithm for this study is **K-Nearest Neighbors (KNN)**. Here's why KNN is well-suited for this task: 1. **Similarity-Based:** KNN explicitly focuses on finding the most similar data points (users in this case) to a given point (your identified individual types) based on their features or attributes. 2. **No Assumption about Data Distribution:** KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying distribution of your data. This i",2025-11-18T17:20:54.035790
Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?,,hard,mixed,data_science,DS_Interview_Notebook,"In this scenario, the most appropriate analysis method would be **A/B Testing** (or **Split Testing**). **Here's why A/B Testing is the best fit:** * **Controlled Experiment:** A/B Testing allows you to randomly assign visitors to different groups (control group with no coupon, group A with coupon type 1, group B with coupon type 2). This controlled experiment ensures that any differences in purchase behavior can be directly attributed to the presence and type of coupon. * **Measures Impact:** Y",2025-11-18T17:20:54.035870
Explain Principal Componenet Analysis?,,hard,mixed,data_science,DS_Interview_Notebook,"* Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set. * Principal component analysis is a technique for **feature extraction** — so it combines our input variables in a specific way, then we can drop the “**least important**” variables while still retaining the most valuable parts of all of the variables! ",2025-11-18T17:20:54.035912
Explain feature scaling,,hard,mixed,data_science,DS_Interview_Notebook,"* Feature scaling is one of the most important data preprocessing step in machine learning * If we are **changing the range of the features then its called 'scaling'** and if we are **changing the distribution of the features then its called 'normalization/standardization'** ## Scaling * This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1. By scaling your variables, you can help compare different variables on equal footing. * Scaling is requir",2025-11-18T17:20:54.035957
Difference between standardisation and normalization?,,hard,mixed,data_science,DS_Interview_Notebook,"**Standardization** * **What it does:** * Centers the data around zero (mean = 0) * Scales the data to have a standard deviation of one (std = 1) * **Transformation:** * `Z = (X - mean) / std_dev` * **When to use it:** * Algorithms that are sensitive to the scale of features (e.g., linear regression, logistic regression, support vector machines). * When you assume your data follows a normal (Gaussian) distribution (though not strictly required). * When outliers are present, as standardization is",2025-11-18T17:20:54.036008
What is meant by Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"* Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training.This causes the problem of overfitting. * In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets. * Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust",2025-11-18T17:20:54.036101
How to detect Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"* Results are too good too true * In general, if we see that the model which we build is too good to be true (i.,e gives predicted and actual output the same), then we should get suspicious and data leakage cannot be ruled out. * At that time, the model might be somehow memorizing the relations between feature and target instead of learning and generalizing it for the unseen data. * So, it is advised that before the testing, the prior documented results are weighed against the expected results. ",2025-11-18T17:20:54.036141
How to fix the problem of Data Leakage?,,hard,case,data_science,DS_Interview_Notebook,"The main culprit behind this is the way we split our dataset and when. The following steps can prove to be very crucial in preventing data leakage: * Select the features such a way that they do not contain information about the target variable, which is not naturally available at the time of prediction. * Create a Separate Validation Set * To minimize or avoid the problem of data leakage, we should try to set aside a validation set in addition to training and test sets if possible. * The purpose",2025-11-18T17:20:54.036211
What is selection bias?,,hard,mixed,data_science,DS_Interview_Notebook,"* Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the **selection effect**. * **Sampling bias** is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended ",2025-11-18T17:20:54.036236
Difference between supervised and unsupervised learning,,medium,ml,data_science,DS_Interview_Notebook,"|Supervised|Unsupervised| |:-|:-| |Used for prediction|Used for analysis| |Labelled input data|Unlabelled input data| |Data need to be splitted into train/validation/test sets|No split required| |Used in Classification and Regression|Used for clustering, dimension reduction & density estimation|",2025-11-18T17:20:54.036247
Explain normal distribution of data,,hard,stats,data_science,DS_Interview_Notebook,"Data can be distributed (spread out) in different ways, * It can be spread out more on the left (Left skew) ![]( * More on the right (Right Skew) ![]( * It can be all jumbled up ![]( * But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a ""Normal Distribution"" like this: ![]( * The Normal Distribution has: - mean = median = mode - symmetry about the center - 50% of values less than the mean and 50% greater than the mean Refe",2025-11-18T17:20:54.036273
What does it mean when distribution is left skew or right skew?,,hard,stats,data_science,DS_Interview_Notebook,"In a **right-skewed** distribution, the tail on the right side is longer. This means most of the data is clustered on the left, with a few unusually large values pulling the average higher. Think of income distribution - most people earn less, but a few very high earners skew the average upwards. In a **left-skewed** distribution, the tail on the left side is longer. This means most of the data is clustered on the right, with a few unusually small values pulling the average lower. An example cou",2025-11-18T17:20:54.036294
What does the distribution looks like for the average time spend watching youtube per day?,,hard,stats,data_science,DS_Interview_Notebook,"The distribution of average time spent watching YouTube per day is likely to be right-skewed. This means that most people watch YouTube for a relatively short amount of time each day, while a smaller number of users watch for much longer durations. The tail of the distribution extends to the right, indicating the presence of these high-usage viewers",2025-11-18T17:20:54.036308
Expalin covariance and correlation,,hard,mixed,data_science,DS_Interview_Notebook,"* Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables. * “Covariance” indicates the **direction of the linear relationship between variables**. “Correlation” on the other hand measures both the **strength and direction of the linear relationship between two variables**. * In case of High correlation, two sets of data are strongly linked together - Correlation is ",2025-11-18T17:20:54.036332
What is regularization. Why it is usefull?,,hard,mixed,data_science,DS_Interview_Notebook,* Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting. * The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values. * There are two types of regularization as follows: - L1 Regularization or Lasso Regularization. L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weight,2025-11-18T17:20:54.036355
What are confouding varaiables?,,hard,mixed,data_science,DS_Interview_Notebook,"* In statistics, confounder is a variable that influences both the dependent variable and independent avriable. * If you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables.",2025-11-18T17:20:54.036368
Explain ROC curve and AUC,,hard,mixed,data_science,DS_Interview_Notebook,"**ROC Curve (Receiver Operating Characteristic Curve)** and **AUC (Area Under the Curve)** are tools used to evaluate the performance of a classification model, particularly when you want to understand how well the model separates two classes, such as ""spam"" and ""not spam."" ### ROC Curve: 1. **What is the ROC Curve?** - The ROC Curve is a graph that shows the trade-off between the **True Positive Rate (TPR)** and the **False Positive Rate (FPR)** of a model at various thresholds. - **True Positi",2025-11-18T17:20:54.036440
Explain Precision-Recall Curve,,hard,mixed,data_science,DS_Interview_Notebook,"The **Precision-Recall Curve** is a tool used to evaluate the performance of a classification model, especially when dealing with imbalanced datasets where one class is much more common than the other. ### Key Concepts: 1. **Precision**: - Measures how many of the positive predictions made by the model are actually correct. - **Example**: If a model predicts 10 emails as spam and 8 are actually spam, the precision is 8 out of 10, or 80%. 2. **Recall**: - Measures how well the model finds all the",2025-11-18T17:20:54.036502
What is TF-IDF?,,hard,mixed,data_science,DS_Interview_Notebook,**TF-IDF (Term Frequency-Inverse Document Frequency)** is a method used in text analysis to determine how important a word is in a specific document compared to a whole collection of documents (called a corpus). It helps in identifying words that are most relevant to the content of a document. ### Key Concepts: 1. **Term Frequency (TF)**: - This measures how often a word appears in a document. A higher term frequency means that the word is more significant within that document. - **Simple Exampl,2025-11-18T17:20:54.036563
Python or R- which one would you prefer for text analytics?,,medium,mixed,data_science,DS_Interview_Notebook,We will prefer python for following reasons * We can use pandas library which has easy to use data structures and high performance data analysis tools * R is more suitable for ML than text analytics * Python is faster for all types of text analytics.,2025-11-18T17:20:54.036576
What are Eigenvectors and Eigenvalues?,,hard,mixed,data_science,DS_Interview_Notebook,"* In linear algebra, an **eigenvector** is a special vector that, when a linear transformation is applied to it, only changes in scale (gets stretched or shrunk) but not in direction. * The **eigenvalue** associated with that eigenvector is the factor by which it is scaled. **Why are they important?** Eigenvectors and eigenvalues reveal the underlying structure and behavior of linear transformations. They have numerous applications across various fields: * **Image compression:** Eigenvectors can",2025-11-18T17:20:54.036607
Explain the scenario where both false positive and false negative are equally important,,hard,mixed,data_science,DS_Interview_Notebook,"1. **Medical Diagnosis (e.g., Cancer Screening)** * **False Positive:** A patient is told they have cancer when they don't. This leads to unnecessary anxiety, invasive procedures, and potential side effects from treatment. * **False Negative:** A patient with cancer is told they are healthy. This delays crucial treatment, potentially allowing the disease to progress and worsen the prognosis. 2. **Fraud Detection** * **False Positive:** A legitimate transaction is flagged as fraudulent. This inco",2025-11-18T17:20:54.036632
Why feature scalling is required in Gradient Descent Based Algorithms,,hard,ml,data_science,DS_Interview_Notebook,"* Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below: ![Gradient descent formula]( * The presence of feature value X in the formula will affect the step size of the gradient descent. * The difference in ranges of features will cause different step sizes for each feature. * To ensure that the gradient descent moves smoot",2025-11-18T17:20:54.036660
Why feature scalling is required in distance Based Algorithms,,hard,ml,data_science,DS_Interview_Notebook,"* Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity. * For example, let’s say we have data containing high school CGPA scores of students (ranging from 0 to 5) and their future incomes (in thousands Rupees): ![Feature scaling: Unscaled Knn example]( * Since both the features have different scales, there is a chance that higher weightage is given to",2025-11-18T17:20:54.036713
Why feature scaling not required in tree based algorithms,,hard,ml,data_science,DS_Interview_Notebook,"Imagine you're sorting a pile of apples and oranges into two baskets. You could sort them by color (red vs. not red) or by weight (heavy vs. light). * **Tree-based algorithms work like this:** They make decisions based on *thresholds* or *cut-offs* for each feature (like color or weight). They ask questions like: ""Is this fruit red?"" or ""Is this fruit heavier than 1 pound?"". * **Feature scaling doesn't matter here:** * **Color:** It doesn't matter if we represent ""red"" as the number 1 and ""not r",2025-11-18T17:20:54.036762
"Explain the difference between train, validation and test set",,medium,stats,data_science,DS_Interview_Notebook,* Training set is used for model training * Validation set is used for model fine tuning (tune the model's hyperparameters) * Test set is used for model testing. i.e. evaluating the models predictive power and generalization.,2025-11-18T17:20:54.036773
What is Naive Bayes algorithm?,,hard,ml,data_science,DS_Interview_Notebook,"The Naive Bayes algorithm is a simple but surprisingly effective classification algorithm in machine learning. It's based on Bayes' Theorem, which deals with conditional probabilities. **In simpler terms:** Imagine you're trying to decide if an email is spam or not. Naive Bayes looks at the words in the email and asks, ""If an email *is* spam, how likely is it to contain these words?"" It then does the same for non-spam emails. Finally, it compares these probabilities to make its best guess about ",2025-11-18T17:20:54.036819
What is the difference between MLOps and DevOps?,,hard,mixed,data_science,DS_Interview_Notebook,"* MLOps & DevOps have a lot of things in common. However, DevOps include developing and deploying the software application code in production and this code is usually static and does not change rapidly. * MLOps on the other side also includes developing and deploying the ML code in production. However, here the data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code. * Reference: ",2025-11-18T17:20:54.036837
What are the risks associated with Data Science & how MLOps can overcome the same?,,hard,case,data_science,DS_Interview_Notebook,"In Data Science, several risks can impact projects, such as data quality issues, model deployment challenges, model performance degradation, lack of reproducibility, security concerns, and scalability issues. **MLOps (Machine Learning Operations)** helps mitigate these risks by: 1. **Automating Data Quality Checks and Monitoring**: Ensures consistent data quality and detects data drift, which helps maintain model accuracy over time. 2. **Streamlining Model Deployment and Environment Consistency*",2025-11-18T17:20:54.036877
What are the differences between XGBoost and Random Forest Model,,hard,ml,data_science,DS_Interview_Notebook,"| Feature | XGBoost | Random Forest | |------------------------------|----------------------------------------------|---------------------------------------------| | **Technique** | Boosting Technique: Builds trees sequentially, where each tree corrects the errors of the previous one. | Bagging Technique: Builds trees independently in parallel and aggregates their results. | | **Performance** | Generally provides higher accuracy due to its boosting nature. | May have lower accuracy compared to b",2025-11-18T17:20:54.036909
Please explain p-value to someone non-technical,,hard,mixed,data_science,DS_Interview_Notebook,"A p-value is a number that helps us understand if the results we see in an experiment or study are meaningful or if they might have happened just by chance. Imagine you're playing a game of chance, like flipping a coin. You suspect the coin might be rigged to land on heads more often, so you decide to test it. **The ""normal"" assumption (null hypothesis):** The coin is fair, and there's a 50/50 chance of getting heads or tails. **Your experiment:** You flip the coin 100 times and get 60 heads. Hm",2025-11-18T17:20:54.036966
"Average comments per month has dropped over three-month period, despite consistent growth after a new launch. What metric would u investigate?",,hard,mixed,data_science,DS_Interview_Notebook,"**To investigate the drop in average comments per month despite consistent growth after a new launch, I would examine:** 1. **Engagement per User**: Assess if the average number of comments per active user has decreased. Even with user growth, fewer comments per user could explain the overall decline. 2. **Content Type Analysis**: Identify if the types of content driving growth are different from those that typically generate comments. Growth could be from content that attracts more views or rea",2025-11-18T17:20:54.037005
A PM tells you that a weekly active user metric is up by 5% but email notification open rate is down by 2%. WHat would you investigate to dignose this problem?,,hard,mixed,data_science,DS_Interview_Notebook,"Email open rate is calculated by dividing the number of emails opened by the number of emails sent minus any bounces. A good open rate is between 17-28%2. Email notification open rate is a type of email open rate that measures how many users open an email that notifies them about something. Weekly active user metric (WAU) is a measure of how many users are active on a website or app in a given week. It can be influenced by many factors, such as user acquisition, retention, engagement and churn. ",2025-11-18T17:20:54.037046
Explain data drift problem in machine learning,,hard,ml,data_science,DS_Interview_Notebook,**Data drift** is a common problem in machine learning that occurs when the statistical properties of the input data change over time. This change can lead to a decrease in the performance of machine learning models because the model is no longer receiving the same kind of data it was trained on. ### Key Points to Explain Data Drift: 1. **Definition of Data Drift**: - **Data drift** refers to any change in the distribution of data that a machine learning model was trained on compared to the data,2025-11-18T17:20:54.037150
Explain transformer architecture,,hard,mixed,data_science,DS_Interview_Notebook,"The Transformer architecture is a neural network model designed for natural language processing tasks, like translation, summarization, and text generation. It was introduced in the paper ""Attention is All You Need"" by Google in 2017 and has since become the foundation for many advanced NLP models, including BERT and GPT. Key Components of the Transformer: **1. Self-Attention Mechanism:** The core innovation of the Transformer is its self-attention mechanism. This mechanism allows the model to w",2025-11-18T17:20:54.037226
Explain the difference between prediction and forecasting,,hard,mixed,data_science,DS_Interview_Notebook,"* If you think of it like a detective story, prediction is about figuring out who committed the crime based on the evidence. It could be something that happened in the past. * Forecasting is more like trying to prevent a crime before it happens. We look for patterns and clues to anticipate what might occur in the future. * In the context of my work with time series models, prediction might be used to identify any current anomalies or issues in our system based on the data we're receiving. Foreca",2025-11-18T17:20:54.037256
References,,easy,mixed,data_science,DS_Interview_Notebook,*  *  *  *  *  *  * ,2025-11-18T17:20:54.037275
