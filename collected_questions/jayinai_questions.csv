question_text,company,difficulty,question_type,topics,source,answer_text,created_at
Explain Difference between joins,,hard,mixed,,jayinai/data-science-question-answer,"* **(INNER) JOIN**: Returns records that have matching values in both tables
* **LEFT (OUTER) JOIN**: Return all records from the left table, and the matched records from the right table
* **RIGHT (OUTER) JOIN**: Return all records from the right table, and the matched records from the left table
* **FULL (OUTER) JOIN**: Return all records when there is a match in either left or right table



(#data-science-question-answer)


## Tools and Framework

The resources here are only meant to help you",2025-11-17T11:42:20.246097
Explain Spark,,hard,mixed,,jayinai/data-science-question-answer,"Using PySpark API.

* The best resource is of course [Spark's documentation](https://spark.apache.org/docs/latest/). Take a thorough review of the topics
* If you are really time constrained, scan the Spark's documentation and check [PySpark cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf) for the basics


(#data-science-question-answer)


## Statistics and ML In General

* [Project Workflow](#project-workflow)
* [Cross Validation](#cross-vali",2025-11-17T11:42:20.246124
Explain Project Workflow,,hard,mixed,,jayinai/data-science-question-answer,"Given a data science / machine learning project, what steps should we follow? Here's
how I would tackle it:

* **Specify business objective.** Are we trying to win more customers, achieve higher satisfaction, or gain more revenues?
* **Define problem.** What is the specific gap in your ideal world and the real one that requires machine learning to fill? Ask questions that can be addressed using your data and predictive modeling (ML algorithms).
* **Create a common sense baseline.** But before yo",2025-11-17T11:42:20.246149
Explain Cross Validation,,medium,mixed,,jayinai/data-science-question-answer,"Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a validation set to evaluate it. For example, a k-fold cross validation divides the data into k folds (or partitions), trains on each k-1 fold, and evaluate on the remaining 1 fold. This results to k models/evaluations, which can be averaged to get a overall model performance.



(#data-science-question-answer)",2025-11-17T11:42:20.246161
Explain Feature Importance,,medium,mixed,,jayinai/data-science-question-answer,"* In linear models, feature importance can be calculated by the scale of the coefficients
* In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree.  We can get a feature's importance for random forest by computing the averaging depth at which it appears across all trees in the forest.

(#data-science-question-answer)",2025-11-17T11:42:20.246169
Explain Mean Squared Error vs. Mean Absolute Error,,medium,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both measure the average model prediction error; range from 0 to infinity; the lower the better
* Mean Squared Error (MSE) gives higher weights to large error (e.g., being off by 10 just MORE THAN TWICE as bad as being off by 5), whereas Mean Absolute Error (MAE) assign equal weights (being off by 10 is just twice as bad as being off by 5)
* MSE is continuously differentiable, MAE is not (where y_pred == y_true)

(#data-science-question-answer)",2025-11-17T11:42:20.246178
Explain L1 vs L2 regularization,,hard,mixed,,jayinai/data-science-question-answer,"* **Similarity**: both L1 and L2 regularization **prevent overfitting** by shrinking (imposing a penalty) on the coefficients
* **Difference**: L2 (Ridge) shrinks all the coefficient by the same proportions but eliminates none, while L1 (Lasso) can shrink some coefficients to zero, performing variable selection.
* **Which to choose**: If all the features are correlated with the label, ridge outperforms lasso, as the coefficients are never zero in ridge. If only a subset of features are correlate",2025-11-17T11:42:20.246187
Explain Correlation vs Covariance,,hard,mixed,,jayinai/data-science-question-answer,"* Both determine the relationship and measure the dependency between two random variables
* Correlation is when the change in one item may result in the change in the another item, while covariance is when two items vary together (joint variability)
* Covariance is nothing but a measure of correlation. On the contrary, correlation refers to the scaled form of covariance
* Range: correlation is between -1 and +1, while covariance lies between negative infinity and infinity.


(#data-science-quest",2025-11-17T11:42:20.246195
Explain Would adding more data address underfitting,,medium,mixed,,jayinai/data-science-question-answer,"Underfitting happens when a model is not complex enough to learn well from the data. It is the problem of model rather than data size. So a potential way to address underfitting is to increase the model complexity (e.g., to add higher order coefficients for linear model, increase depth for tree-based methods, add more layers / number of neurons for neural networks etc.)

(#data-science-question-answer)",2025-11-17T11:42:20.246203
Explain Activation Function,,medium,mixed,,jayinai/data-science-question-answer,"For neural networks

* Non-linearity: ReLU is often used. Use Leaky ReLU (a small positive gradient for negative input, say, `y = 0.01x` when x < 0) to address dead ReLU issue
* Multi-class: softmax
* Binary: sigmoid
* Regression: linear

(#data-science-question-answer)",2025-11-17T11:42:20.246210
Explain Bagging,,hard,mixed,,jayinai/data-science-question-answer,"To address overfitting, we can use an ensemble method called bagging (bootstrap aggregating),
which reduces the variance of the meta learning algorithm. Bagging can be applied
to decision tree or other algorithms.

Here is a [great illustration](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py) of a single estimator vs. bagging.



* Bagging is when samlping is performed *with* replacement. When sampling is perfor",2025-11-17T11:42:20.246219
Explain Stacking,,hard,mixed,,jayinai/data-science-question-answer,"* Instead of using trivial functions (such as hard voting) to aggregate the predictions from individual learners, train a model to perform this aggregation
* First split the training set into two subsets: the first subset is used to train the learners in the first layer
* Next the first layer learners are used to make predictions (meta features) on the second subset, and those predictions are used to train another models (to obtain the weigts of different learners) in the second layer
* We can t",2025-11-17T11:42:20.246227
Explain Generative vs discriminative,,hard,mixed,,jayinai/data-science-question-answer,"* Discriminative algorithms model *p(y|x; w)*, that is, given the dataset and learned
parameter, what is the probability of y belonging to a specific class. A discriminative algorithm
doesn't care about how the data was generated, it simply categorizes a given example
* Generative algorithms try to model *p(x|y)*, that is, the distribution of features given
that it belongs to a certain class. A generative algorithm models how the data was
generated.

> Given a training set, an algorithm like log",2025-11-17T11:42:20.246240
Explain Parametric vs Nonparametric,,medium,mixed,,jayinai/data-science-question-answer,"* A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model.
* A model where the number of parameters is not determined prior to training. Nonparametric does not mean that they have no parameters. On the contrary, nonparametric models (can) become more and more complex with an increasing amount of data.

(#data-science-question-answer)",2025-11-17T11:42:20.246247
Explain Recommender System,,hard,mixed,,jayinai/data-science-question-answer,"* I put recommend system here since technically it falls neither under supervised nor unsupervised learning
* A recommender system seeks to predict the 'rating' or 'preference' a user would give to items and then recommend items accordingly
* Content based recommender systems recommends items similar to those a given user has liked in the past, based on either explicit (ratings, like/dislike button) or implicit (viewed/finished an article) feedbacks. Content based recommenders work solely with t",2025-11-17T11:42:20.246259
Explain Linear regression,,hard,mixed,,jayinai/data-science-question-answer,"* How to learn the parameter: minimize the cost function
* How to minimize cost function: gradient descent
* Regularization:
    - L1 (Lasso): can shrink certain coef to zero, thus performing feature selection
    - L2 (Ridge): shrink all coef with the same proportion; almost always outperforms L1
    - Elastic Net: combined L1 and L2 priors as regularizer
* Assumes linear relationship between features and the label
* Can add polynomial and interaction features to add non-linearity



(#data-sci",2025-11-17T11:42:20.246268
Explain Logistic regression,,medium,mixed,,jayinai/data-science-question-answer,"* Generalized linear model (GLM) for binary classification problems
* Apply the sigmoid function to the output of linear models, squeezing the target
to range [0, 1]
* Threshold to make prediction: usually if the output > .5, prediction 1; otherwise prediction 0
* A special case of softmax function, which deals with multi-class problems

(#data-science-question-answer)",2025-11-17T11:42:20.246275
Explain Naive Bayes,,hard,mixed,,jayinai/data-science-question-answer,"* Naive Bayes (NB) is a supervised learning algorithm based on applying [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)
* It is called naive because it builds the naive assumption that each feature
are independent of each other
* NB can make different assumptions (i.e., data distributions, such as Gaussian,
Multinomial, Bernoulli)
* Despite the over-simplified assumptions, NB classifier works quite well in real-world
applications, especially for text classification (e.g., spam f",2025-11-17T11:42:20.246284
Explain Decision tree,,hard,mixed,,jayinai/data-science-question-answer,"* Non-parametric, supervised learning algorithms
* Given the training data, a decision tree algorithm divides the feature space into
regions. For inference, we first see which
region does the test data point fall in, and take the mean label values (regression)
or the majority label value (classification).
* **Construction**: top-down, chooses a variable to split the data such that the
target variables within each region are as homogeneous as possible. Two common
metrics: gini impurity or informa",2025-11-17T11:42:20.246301
Explain Random forest,,hard,mixed,,jayinai/data-science-question-answer,"Random forest improves bagging further by adding some randomness. In random forest,
only a subset of features are selected at random to construct a tree (while often not subsample instances).
The benefit is that random forest **decorrelates** the trees.

For example, suppose we have a dataset. There is one very predicative feature, and a couple
of moderately predicative features. In bagging trees, most of the trees
will use this very predicative feature in the top split, and therefore making mos",2025-11-17T11:42:20.246309
Explain Boosting Tree,,hard,mixed,,jayinai/data-science-question-answer,"**How it works**

Boosting builds on weak learners, and in an iterative fashion. In each iteration,
a new learner is added, while all existing learners are kept unchanged. All learners
are weighted based on their performance (e.g., accuracy), and after a weak learner
is added, the data are re-weighted: examples that are misclassified gain more weights,
while examples that are correctly classified lose weights. Thus, future weak learners
focus more on examples that previous weak learners misclass",2025-11-17T11:42:20.246318
Explain RNN and LSTM,,hard,mixed,,jayinai/data-science-question-answer,"RNN is another paradigm of neural network where we have difference layers of cells,
and each cell not only takes as input the cell from the previous layer, but also the previous
cell within the same layer. This gives RNN the power to model sequence.



This seems great, but in practice RNN barely works due to exploding/vanishing gradient, which
is cause by a series of multiplication of the same matrix. To solve this, we can use
a variation of RNN, called long short-term memory (LSTM), which is c",2025-11-17T11:42:20.246333
Explain Clustering,,hard,mixed,,jayinai/data-science-question-answer,"* Clustering is a unsupervised learning algorithm that groups data in such
a way that data points in the same group are more similar to each other than to
those from other groups
* Similarity is usually defined using a distance measure (e.g, Euclidean, Cosine, Jaccard, etc.)
* The goal is usually to discover the underlying structure within the data (usually high dimensional)
* The most common clustering algorithm is K-means, where we define K (the number of clusters)
and the algorithm iterativel",2025-11-17T11:42:20.246342
Explain Principal Component Analysis,,hard,mixed,,jayinai/data-science-question-answer,"* Principal Component Analysis (PCA) is a dimension reduction technique that projects
the data into a lower dimensional space
* PCA uses Singular Value Decomposition (SVD), which is a matrix factorization method
that decomposes a matrix into three smaller matrices (more details of SVD [here](https://en.wikipedia.org/wiki/Singular-value_decomposition))
* PCA finds top N principal components, which are dimensions along which the data vary
(spread out) the most. Intuitively, the more spread out the",2025-11-17T11:42:20.246351
Explain Autoencoder,,medium,mixed,,jayinai/data-science-question-answer,"* The aim of an autoencoder is to learn a representation (encoding) for a set of data
* An autoencoder always consists of two parts, the encoder and the decoder. The encoder would find a lower dimension representation (latent variable) of the original input, while the decoder is used to reconstruct from the lower-dimension vector such that the distance between the original and reconstruction is minimized
* Can be used for data denoising and dimensionality reduction",2025-11-17T11:42:20.246359
Explain Generative Adversarial Network,,hard,mixed,,jayinai/data-science-question-answer,"* Generative Adversarial Network (GAN) is an unsupervised learning algorithm that also has supervised flavor: using supervised loss as part of training
* GAN typically has two major components: the **generator** and the **discriminator**. The generator tries to generate ""fake"" data (e.g, images or sentences) that fool the discriminator into thinking that they're real, while the discriminator tries to distinguish between real and generated data. It's a fight between the two players thus the name ",2025-11-17T11:42:20.246371
Explain Tokenization,,hard,mixed,,jayinai/data-science-question-answer,"* Tokenization is the process of converting a sequence of characters into a sequence of tokens
* Consider this example: `The quick brown fox jumped over the lazy dog`. In this case each word (separated by space) would be a token
* Sometimes tokenization doesn't have a definitive answer. For instance, `O'Neill` can be tokenized to `o` and `neill`, `oneill`, or `o'neill`.
* In some cases tokenization requires language-specific knowledge. For example, it doesn't make sense to tokenize `aren't` into",2025-11-17T11:42:20.246379
Explain Stemming and lemmatization,,hard,mixed,,jayinai/data-science-question-answer,"* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form
* Stemming usually refers to a crude heuristic process that chops off the ends of words
* Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words
* If confronted with the token `saw`, stemming might return just `s`, whereas lemmatization would attempt to return either `see` or `saw` ",2025-11-17T11:42:20.246387
Explain N gram,,hard,mixed,,jayinai/data-science-question-answer,"* n-gram is a contiguous sequence of n items from a given sample of text or speech
* An n-gram of size 1 is referred to as a ""unigram""; size 2 is a ""bigram"" size 3 is a ""trigram"". Larger sizes are sometimes referred to by the value of n in modern language, e.g., ""four-gram"", ""five-gram"", and so on.
* Consider this example: `The quick brown fox jumped over the lazy dog.`
  - bigram would be `the quick`, `quick brown`, `brown fox`, ..., i.e, every two consecutive words (or tokens)
  - trigram woul",2025-11-17T11:42:20.246394
Explain Bag of Words,,hard,mixed,,jayinai/data-science-question-answer,"* Why? Machine learning models cannot work with raw text directly; rather, they take numerical values as input.
* Bag of words (BoW) builds a **vocabulary** of all the unique words in our dataset, and associate a unique index to each word in the vocabulary
* It is called a ""bag"" of words, because it is a representation that completely ignores the order of words
* Consider this example of two sentences: (1) `John likes to watch movies, especially horor movies.`, (2) `Mary likes movies too.` We wo",2025-11-17T11:42:20.246402
Explain word2vec,,hard,mixed,,jayinai/data-science-question-answer,"* Shallow, two-layer neural networks that are trained to construct linguistic context of words
* Takes as input a large corpus, and produce a vector space, typically of several hundred
dimension, and each word in the corpus is assigned a vector in the space
* The key idea is **context**: words that occur often in the same context should have same/opposite
meanings.
* Two flavors
    - continuous bag of words (CBOW): the model predicts the current word given a window of surrounding context words
",2025-11-17T11:42:20.246413
Explain Cron job,,hard,mixed,,jayinai/data-science-question-answer,"The software utility **cron** is a **time-based job scheduler** in Unix-like computer operating systems. People who set up and maintain software environments use cron to schedule jobs (commands or shell scripts) to run periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration -- though its general-purpose nature makes it useful for things like downloading files from the Internet and downloading email at regular intervals.



Tools:
* [Apache Ai",2025-11-17T11:42:20.246423
Explain Linux,,medium,mixed,,jayinai/data-science-question-answer,"Using **Ubuntu** as an example.

* Become root: `sudo su`
* Install package: `sudo apt-get install <package>`

(#data-science-question-answer)


Confession: some images are adopted from the internet without proper credit. If you are the author and this would be an issue for you, please let me know.",2025-11-17T11:42:20.246431
